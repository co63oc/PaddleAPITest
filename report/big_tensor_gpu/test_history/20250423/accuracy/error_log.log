2025-04-21 17:42:30.328113 test begin: paddle.argmax(Tensor([13, 1371215, 4, 16, 2],"float32"), axis=-1, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745228562 (unix time) try "date -d @1745228562" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x18669) received by PID 99945 (TID 0x7fc70b0b7700) from PID 99945 ***]

2025-04-21 17:43:32.243699 test begin: paddle.argmax(Tensor([13, 2, 2742430, 16, 2],"float32"), axis=-1, )

W0421 17:44:48.507494 125220 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 17:44:48.508312 125220 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745228690 (unix time) try "date -d @1745228690" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1e8b5) received by PID 125109 (TID 0x7ff6a9dc2700) from PID 125109 ***]

2025-04-21 17:45:33.421355 test begin: paddle.argmax(Tensor([13, 2, 4, 10969719, 2],"float32"), axis=-1, )

W0421 17:46:49.613667 127150 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 17:46:49.614543 127150 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745228811 (unix time) try "date -d @1745228811" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1f02b) received by PID 127019 (TID 0x7f9e4f34a700) from PID 127019 ***]

2025-04-21 17:47:32.187165 test begin: paddle.argmax(Tensor([2, 536870913, 4],"float16"), axis=-1, keepdim=True, )

W0421 17:49:03.267870 129008 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 17:49:03.268615 129008 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745228944 (unix time) try "date -d @1745228944" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1f794) received by PID 128916 (TID 0x7f2835dc2700) from PID 128916 ***]

2025-04-21 17:51:01.331865 test begin: paddle.argmax(Tensor([3, 17674763, 3, 3, 3, 3],"float16"), axis=0, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745229152 (unix time) try "date -d @1745229152" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1ffbb) received by PID 131003 (TID 0x7f2f9d935700) from PID 131003 ***]

2025-04-21 17:53:11.822616 test begin: paddle.argmax(Tensor([3, 3, 17674763, 3, 3, 3],"float16"), axis=0, )

W0421 17:54:43.544217 134494 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 17:54:43.545055 134494 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745229284 (unix time) try "date -d @1745229284" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x20bef) received by PID 134127 (TID 0x7f9ef5b85700) from PID 134127 ***]

2025-04-21 17:55:24.472439 test begin: paddle.argmax(Tensor([3, 3, 3, 17674763, 3, 3],"float16"), axis=0, )

W0421 17:57:09.760780 136891 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 17:57:09.761775 136891 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745229431 (unix time) try "date -d @1745229431" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2163f) received by PID 136767 (TID 0x7f58c52b7700) from PID 136767 ***]

2025-04-21 17:57:50.125911 test begin: paddle.argmax(Tensor([3, 3, 3, 3, 17674763, 3],"float16"), axis=0, )

W0421 17:59:51.495784 138826 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 17:59:51.496721 138826 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745229592 (unix time) try "date -d @1745229592" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x21dd7) received by PID 138711 (TID 0x7f3c6c7c3700) from PID 138711 ***]

2025-04-21 18:00:31.174549 test begin: paddle.argmax(Tensor([3, 3, 3, 3, 3, 17674763],"float16"), axis=0, )

W0421 18:02:04.379863 140986 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:02:04.380713 140986 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745229725 (unix time) try "date -d @1745229725" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x22639) received by PID 140857 (TID 0x7f6be12b7700) from PID 140857 ***]

2025-04-21 18:03:59.824288 test begin: paddle.argmax(Tensor([357913942, 3, 4],"float16"), axis=-1, keepdim=True, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745229926 (unix time) try "date -d @1745229926" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x22fbe) received by PID 143294 (TID 0x7feef987e700) from PID 143294 ***]

2025-04-21 18:06:11.249684 test begin: paddle.argmax(Tensor([4, 16777217, 4, 4, 4],"float16"), axis=0, )

W0421 18:08:00.813795 147037 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:08:00.814874 147037 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745230082 (unix time) try "date -d @1745230082" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x23dfc) received by PID 146940 (TID 0x7f133c949700) from PID 146940 ***]

2025-04-21 18:08:41.215557 test begin: paddle.argmax(Tensor([4, 4, 16777217, 4, 4],"float16"), axis=0, )

W0421 18:10:15.465294 149700 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:10:15.466225 149700 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745230217 (unix time) try "date -d @1745230217" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2474e) received by PID 149326 (TID 0x7f0b1c949700) from PID 149326 ***]

2025-04-21 18:10:54.500901 test begin: paddle.argmax(Tensor([4, 4, 4, 16777217, 4],"float16"), axis=0, )

W0421 18:12:25.390719 151859 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:12:25.391605 151859 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745230346 (unix time) try "date -d @1745230346" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x250b1) received by PID 151729 (TID 0x7f98a387e700) from PID 151729 ***]

2025-04-21 18:13:06.515617 test begin: paddle.argmax(Tensor([4, 4, 4, 4, 16777217],"float16"), axis=0, )

W0421 18:14:45.769251 154001 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:14:45.770107 154001 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745230487 (unix time) try "date -d @1745230487" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x25929) received by PID 153897 (TID 0x7fc312949700) from PID 153897 ***]

2025-04-21 18:16:44.619280 test begin: paddle.argmax(Tensor([8912897, 2, 4, 16, 2],"float32"), axis=-1, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745230618 (unix time) try "date -d @1745230618" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x26490) received by PID 156816 (TID 0x7f49a0949700) from PID 156816 ***]

2025-04-21 18:20:09.210427 test begin: paddle.argmax(x=Tensor([3, 3, 477218589],"float16"), axis=1, keepdim=False, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745230822 (unix time) try "date -d @1745230822" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x26dfa) received by PID 159226 (TID 0x7f0b07935700) from PID 159226 ***]

2025-04-21 18:22:32.113337 test begin: paddle.argmax(x=Tensor([357913942, 3, 4],"float16"), axis=1, keepdim=False, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745230970 (unix time) try "date -d @1745230970" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x279f0) received by PID 162288 (TID 0x7f0013f48700) from PID 162288 ***]

2025-04-21 18:26:13.478529 test begin: paddle.argmin(Tensor([3, 17674763, 3, 3, 3, 3],"float16"), axis=0, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745231188 (unix time) try "date -d @1745231188" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x64d) received by PID 1613 (TID 0x7f38847c3700) from PID 1613 ***]

2025-04-21 18:27:10.280797 test begin: paddle.argmin(Tensor([3, 3, 17674763, 3, 3, 3],"float16"), axis=0, )

W0421 18:28:44.435709  4976 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:28:44.436875  4976 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745231325 (unix time) try "date -d @1745231325" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x12f1) received by PID 4849 (TID 0x7f3e25935700) from PID 4849 ***]

2025-04-21 18:29:26.854130 test begin: paddle.argmin(Tensor([3, 3, 3, 17674763, 3, 3],"float16"), axis=0, )

W0421 18:30:59.089285  7867 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:30:59.090113  7867 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745231460 (unix time) try "date -d @1745231460" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1e50) received by PID 7760 (TID 0x7fba83f48700) from PID 7760 ***]

2025-04-21 18:31:40.853103 test begin: paddle.argmin(Tensor([3, 3, 3, 3, 17674763, 3],"float16"), axis=0, )

W0421 18:33:12.610355 10075 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:33:12.611100 10075 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745231594 (unix time) try "date -d @1745231594" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x26e4) received by PID 9956 (TID 0x7f88c94f4700) from PID 9956 ***]

2025-04-21 18:33:50.367873 test begin: paddle.argmin(Tensor([3, 3, 3, 3, 3, 17674763],"float16"), axis=0, )

W0421 18:35:23.303540 12567 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:35:23.304538 12567 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745231724 (unix time) try "date -d @1745231724" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x3042) received by PID 12354 (TID 0x7f1351935700) from PID 12354 ***]

2025-04-21 18:37:20.628190 test begin: paddle.argmin(Tensor([4, 16777217, 4, 4, 4],"float16"), axis=0, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745231927 (unix time) try "date -d @1745231927" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x3a36) received by PID 14902 (TID 0x7fb327f48700) from PID 14902 ***]

2025-04-21 18:39:29.108480 test begin: paddle.argmin(Tensor([4, 4, 16777217, 4, 4],"float16"), axis=0, )

W0421 18:41:12.719733 18817 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:41:12.720715 18817 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745232074 (unix time) try "date -d @1745232074" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x4905) received by PID 18693 (TID 0x7fb21334a700) from PID 18693 ***]

2025-04-21 18:41:18.248526 test begin: paddle.argmin(Tensor([4, 4, 4, 16777217, 4],"float16"), axis=0, )

W0421 18:43:04.406847 21014 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:43:04.407729 21014 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745232185 (unix time) try "date -d @1745232185" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x507c) received by PID 20604 (TID 0x7fd357dc2700) from PID 20604 ***]

2025-04-21 18:43:47.878755 test begin: paddle.argmin(Tensor([4, 4, 4, 4, 16777217],"float16"), axis=0, )

W0421 18:45:19.036835 24174 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:45:19.037792 24174 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745232320 (unix time) try "date -d @1745232320" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x5e06) received by PID 24070 (TID 0x7f301f2b7700) from PID 24070 ***]

2025-04-21 18:49:54.643882 test begin: paddle.argmin(x=Tensor([3, 3, 477218589],"float16"), axis=1, keepdim=False, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745232609 (unix time) try "date -d @1745232609" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x694a) received by PID 26954 (TID 0x7fc449dc2700) from PID 26954 ***]

2025-04-21 18:51:45.724821 test begin: paddle.argmin(x=Tensor([357913942, 3, 4],"float16"), axis=1, keepdim=False, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745232721 (unix time) try "date -d @1745232721" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x7a8c) received by PID 31372 (TID 0x7fb857b85700) from PID 31372 ***]

2025-04-21 18:55:06.964563 test begin: paddle.argsort(Tensor([228170138, 10],"int64"), axis=1, stable=True, )

[paddle error] paddle.argsort(Tensor([228170138, 10],"int64"), axis=1, stable=True, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 18:55:37.651156 test begin: paddle.argsort(Tensor([22817014, 100],"int64"), axis=1, stable=True, )

[paddle error] paddle.argsort(Tensor([22817014, 100],"int64"), axis=1, stable=True, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 18:55:58.461205 test begin: paddle.argsort(Tensor([26, 87757746],"int64"), axis=-1, )

[paddle error] paddle.argsort(Tensor([26, 87757746],"int64"), axis=-1, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 18:56:18.550203 test begin: paddle.argsort(Tensor([285212673, 4, 2],"int64"), axis=2, stable=True, )

[paddle error] paddle.argsort(Tensor([285212673, 4, 2],"int64"), axis=2, stable=True, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 18:56:52.821408 test begin: paddle.argsort(Tensor([3, 380283564, 2],"int64"), axis=2, stable=True, )

[paddle error] paddle.argsort(Tensor([3, 380283564, 2],"int64"), axis=2, stable=True, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 18:57:29.523512 test begin: paddle.argsort(Tensor([3, 4, 190141782],"int64"), axis=2, stable=True, )

[paddle error] paddle.argsort(Tensor([3, 4, 190141782],"int64"), axis=2, stable=True, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 18:57:54.006081 test begin: paddle.argsort(Tensor([3, 760567127],"int64"), axis=1, stable=True, )

[paddle error] paddle.argsort(Tensor([3, 760567127],"int64"), axis=1, stable=True, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 18:58:19.612380 test begin: paddle.argsort(Tensor([35651585, 64],"int64"), axis=-1, )

[paddle error] paddle.argsort(Tensor([35651585, 64],"int64"), axis=-1, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 18:58:45.935294 test begin: paddle.argsort(Tensor([5, 456340276],"int64"), axis=1, stable=True, )

[paddle error] paddle.argsort(Tensor([5, 456340276],"int64"), axis=1, stable=True, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 18:59:09.130645 test begin: paddle.argsort(Tensor([570425345, 4],"int64"), axis=1, stable=True, )

[paddle error] paddle.argsort(Tensor([570425345, 4],"int64"), axis=1, stable=True, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 19:17:04.763196 test begin: paddle.bitwise_left_shift(Tensor([1],"int16"), Tensor([4294967297],"int16"), )

W0421 19:17:59.565644 59823 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 19:17:59.566409 59823 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.bitwise_left_shift(Tensor([1],"int16"), Tensor([4294967297],"int16"), ) 
 (InvalidArgument) The 0-th dimension of input tensor is expected to be equal with the 0-th dimension of output tensor 1 or 1, but received 4294967297.
  [Hint: Expected in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1 == true, but received in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1:0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/kernels/funcs/dims_simplifier.h:144)

2025-04-21 19:17:59.991488 test begin: paddle.bitwise_left_shift(Tensor([1],"uint8"), Tensor([4294967297],"uint8"), )

[paddle error] paddle.bitwise_left_shift(Tensor([1],"uint8"), Tensor([4294967297],"uint8"), ) 
 (InvalidArgument) The 0-th dimension of input tensor is expected to be equal with the 0-th dimension of output tensor 1 or 1, but received 4294967297.
  [Hint: Expected in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1 == true, but received in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1:0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/kernels/funcs/dims_simplifier.h:144)

2025-04-21 19:18:46.451188 test begin: paddle.bitwise_left_shift(Tensor([4294967297],"int16"), Tensor([1],"int16"), )

[paddle error] paddle.bitwise_left_shift(Tensor([4294967297],"int16"), Tensor([1],"int16"), ) 
 (InvalidArgument) The 0-th dimension of input tensor is expected to be equal with the 0-th dimension of output tensor 1 or 1, but received 4294967297.
  [Hint: Expected in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1 == true, but received in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1:0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/kernels/funcs/dims_simplifier.h:144)

2025-04-21 19:18:58.489687 test begin: paddle.bitwise_left_shift(Tensor([4294967297],"uint8"), Tensor([1],"uint8"), )

[paddle error] paddle.bitwise_left_shift(Tensor([4294967297],"uint8"), Tensor([1],"uint8"), ) 
 (InvalidArgument) The 0-th dimension of input tensor is expected to be equal with the 0-th dimension of output tensor 1 or 1, but received 4294967297.
  [Hint: Expected in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1 == true, but received in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1:0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/kernels/funcs/dims_simplifier.h:144)

2025-04-21 19:19:05.135108 test begin: paddle.bitwise_right_shift(Tensor([1],"int8"), Tensor([4294967297],"int8"), )

[paddle error] paddle.bitwise_right_shift(Tensor([1],"int8"), Tensor([4294967297],"int8"), ) 
 (InvalidArgument) The 0-th dimension of input tensor is expected to be equal with the 0-th dimension of output tensor 1 or 1, but received 4294967297.
  [Hint: Expected in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1 == true, but received in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1:0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/kernels/funcs/dims_simplifier.h:144)

2025-04-21 19:19:52.167246 test begin: paddle.bitwise_right_shift(Tensor([1],"uint8"), Tensor([4294967297],"uint8"), )

[paddle error] paddle.bitwise_right_shift(Tensor([1],"uint8"), Tensor([4294967297],"uint8"), ) 
 (InvalidArgument) The 0-th dimension of input tensor is expected to be equal with the 0-th dimension of output tensor 1 or 1, but received 4294967297.
  [Hint: Expected in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1 == true, but received in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1:0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/kernels/funcs/dims_simplifier.h:144)

2025-04-21 19:19:59.445565 test begin: paddle.bitwise_right_shift(Tensor([4294967297],"int8"), Tensor([1],"int8"), )

[paddle error] paddle.bitwise_right_shift(Tensor([4294967297],"int8"), Tensor([1],"int8"), ) 
 (InvalidArgument) The 0-th dimension of input tensor is expected to be equal with the 0-th dimension of output tensor 1 or 1, but received 4294967297.
  [Hint: Expected in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1 == true, but received in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1:0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/kernels/funcs/dims_simplifier.h:144)

2025-04-21 19:20:06.230881 test begin: paddle.bitwise_right_shift(Tensor([4294967297],"uint8"), Tensor([1],"uint8"), )

[paddle error] paddle.bitwise_right_shift(Tensor([4294967297],"uint8"), Tensor([1],"uint8"), ) 
 (InvalidArgument) The 0-th dimension of input tensor is expected to be equal with the 0-th dimension of output tensor 1 or 1, but received 4294967297.
  [Hint: Expected in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1 == true, but received in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1:0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/kernels/funcs/dims_simplifier.h:144)

2025-04-21 19:20:12.343535 test begin: paddle.bmm(x=Tensor([2, 380283564, 3],"float32"), y=Tensor([2, 3, 2],"float32"), )

/usr/local/lib/python3.9/dist-packages/torch/autograd/graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:135.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[accuracy error] backward  paddle.bmm(x=Tensor([2, 380283564, 3],"float32"), y=Tensor([2, 3, 2],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 12 / 12 (100%)
Max absolute difference: 1733.5198
Max relative difference: 2.7911196
 x: array([[[ 315.71692 ,  836.3276  ],
        [  94.326935,  -55.610794],
        [ 844.3806  , 1923.4807  ]],...
 y: array([[[ 1573.2762 ,   469.66425],
        [-1254.2701 ,  1677.9089 ],
        [ -842.4478 ,  2270.216  ]],...
2025-04-21 19:24:53.734198 test begin: paddle.bmm(x=Tensor([2, 715827883, 3],"float16"), y=Tensor([2, 3, 2],"float16"), )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745235106 (unix time) try "date -d @1745235106" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xe938) received by PID 59704 (TID 0x7ff5787c3700) from PID 59704 ***]

2025-04-21 20:15:55.913779 test begin: paddle.chunk(Tensor([1, 1, 1, 4294967297],"float16"), 2, axis=-1, )

[paddle error] paddle.chunk(Tensor([1, 1, 1, 4294967297],"float16"), 2, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 2, input(X)'s shape = [1, 1, 1, 4294967297], Attr(dim) = 3.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 20:16:19.013665 test begin: paddle.chunk(Tensor([1, 1, 64, 67108865],"float16"), 2, axis=-1, )

[paddle error] paddle.chunk(Tensor([1, 1, 64, 67108865],"float16"), 2, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 2, input(X)'s shape = [1, 1, 64, 67108865], Attr(dim) = 3.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 20:16:39.114018 test begin: paddle.chunk(Tensor([1, 11, 207427399],"float32"), chunks=2, axis=-1, )

[paddle error] paddle.chunk(Tensor([1, 11, 207427399],"float32"), chunks=2, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 2, input(X)'s shape = [1, 11, 207427399], Attr(dim) = 2.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 20:17:04.847488 test begin: paddle.chunk(Tensor([1, 4, 20, 28521268],"float32"), 3, axis=-1, )

[paddle error] paddle.chunk(Tensor([1, 4, 20, 28521268],"float32"), 3, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 3, input(X)'s shape = [1, 4, 20, 28521268], Attr(dim) = 3.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 20:17:26.791257 test begin: paddle.chunk(Tensor([13, 16, 10969719],"float32"), chunks=2, axis=-1, )

[paddle error] paddle.chunk(Tensor([13, 16, 10969719],"float32"), chunks=2, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 2, input(X)'s shape = [13, 16, 10969719], Attr(dim) = 2.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 20:17:48.587376 test begin: paddle.chunk(Tensor([13, 175515491, 1],"float32"), 4, axis=1, )

[paddle error] paddle.chunk(Tensor([13, 175515491, 1],"float32"), 4, axis=1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 4, input(X)'s shape = [13, 175515491, 1], Attr(dim) = 1.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:3 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 20:18:08.651326 test begin: paddle.chunk(Tensor([13, 4, 7, 6268411],"float32"), 3, axis=-1, )

[paddle error] paddle.chunk(Tensor([13, 4, 7, 6268411],"float32"), 3, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 3, input(X)'s shape = [13, 4, 7, 6268411], Attr(dim) = 3.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 20:18:31.273058 test begin: paddle.chunk(Tensor([13, 5484860, 32],"float32"), 3, axis=1, )

[paddle error] paddle.chunk(Tensor([13, 5484860, 32],"float32"), 3, axis=1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 3, input(X)'s shape = [13, 5484860, 32], Attr(dim) = 1.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:2 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 20:18:52.038540 test begin: paddle.chunk(Tensor([13, 56, 3134206],"float32"), 3, axis=-1, )

[paddle error] paddle.chunk(Tensor([13, 56, 3134206],"float32"), 3, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 3, input(X)'s shape = [13, 56, 3134206], Attr(dim) = 2.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 20:19:11.024960 test begin: paddle.chunk(Tensor([16, 5593, 25500],"float32"), 2, axis=1, )

[paddle error] paddle.chunk(Tensor([16, 5593, 25500],"float32"), 2, axis=1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 2, input(X)'s shape = [16, 5593, 25500], Attr(dim) = 1.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 20:19:30.514115 test begin: paddle.chunk(Tensor([2048, 1114113],"float32"), 2, axis=-1, )

[paddle error] paddle.chunk(Tensor([2048, 1114113],"float32"), 2, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 2, input(X)'s shape = [2048, 1114113], Attr(dim) = 1.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 20:19:48.896264 test begin: paddle.chunk(Tensor([4, 139265, 64, 64],"float32"), 3, axis=1, )

[paddle error] paddle.chunk(Tensor([4, 139265, 64, 64],"float32"), 3, axis=1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 3, input(X)'s shape = [4, 139265, 64, 64], Attr(dim) = 1.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:2 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 20:20:09.314383 test begin: paddle.chunk(Tensor([4, 262145, 64, 64],"float16"), 3, axis=1, )

[paddle error] paddle.chunk(Tensor([4, 262145, 64, 64],"float16"), 3, axis=1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 3, input(X)'s shape = [4, 262145, 64, 64], Attr(dim) = 1.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:2 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 20:20:32.577563 test begin: paddle.chunk(Tensor([52, 4, 7, 1567103],"float32"), 3, axis=-1, )

[paddle error] paddle.chunk(Tensor([52, 4, 7, 1567103],"float32"), 3, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 3, input(X)'s shape = [52, 4, 7, 1567103], Attr(dim) = 3.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:2 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 20:20:54.218625 test begin: paddle.chunk(Tensor([52, 5484860, 8],"float32"), 3, axis=1, )

[paddle error] paddle.chunk(Tensor([52, 5484860, 8],"float32"), 3, axis=1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 3, input(X)'s shape = [52, 5484860, 8], Attr(dim) = 1.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:2 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 20:21:14.676835 test begin: paddle.chunk(Tensor([8192, 278529],"float32"), 2, axis=-1, )

[paddle error] paddle.chunk(Tensor([8192, 278529],"float32"), 2, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 2, input(X)'s shape = [8192, 278529], Attr(dim) = 1.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 20:21:32.608944 test begin: paddle.chunk(x=Tensor([1431655766, 3],"float16"), chunks=3, axis=0, )

[paddle error] paddle.chunk(x=Tensor([1431655766, 3],"float16"), chunks=3, axis=0, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 3, input(X)'s shape = [1431655766, 3], Attr(dim) = 0.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:2 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 20:21:51.530940 test begin: paddle.chunk(x=Tensor([2281701379],"bool"), chunks=3, axis=0, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.chunk(x=Tensor([2281701379],"bool"), chunks=3, axis=0, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 3, input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 20:21:55.616305 test begin: paddle.chunk(x=Tensor([3, 1431655766],"float16"), chunks=3, axis=-1, )

[paddle error] paddle.chunk(x=Tensor([3, 1431655766],"float16"), chunks=3, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 3, input(X)'s shape = [3, 1431655766], Attr(dim) = 1.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:2 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 20:22:15.660668 test begin: paddle.chunk(x=Tensor([3, 760567127],"float32"), chunks=3, axis=-1, )

[paddle error] paddle.chunk(x=Tensor([3, 760567127],"float32"), chunks=3, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 3, input(X)'s shape = [3, 760567127], Attr(dim) = 1.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:2 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 20:22:36.951713 test begin: paddle.chunk(x=Tensor([3, 760567127],"int32"), chunks=3, axis=-1, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.chunk(x=Tensor([3, 760567127],"int32"), chunks=3, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 3, input(X)'s shape = [3, 760567127], Attr(dim) = 1.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:2 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 20:23:38.723695 test begin: paddle.chunk(x=Tensor([3, 760567127],"int64"), chunks=3, axis=-1, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.chunk(x=Tensor([3, 760567127],"int64"), chunks=3, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 3, input(X)'s shape = [3, 760567127], Attr(dim) = 1.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:2 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 20:24:54.295604 test begin: paddle.chunk(x=Tensor([760567127, 3],"float32"), chunks=3, axis=0, )

[paddle error] paddle.chunk(x=Tensor([760567127, 3],"float32"), chunks=3, axis=0, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 3, input(X)'s shape = [760567127, 3], Attr(dim) = 0.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:2 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 20:25:15.472615 test begin: paddle.chunk(x=Tensor([760567127, 3],"int32"), chunks=3, axis=0, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.chunk(x=Tensor([760567127, 3],"int32"), chunks=3, axis=0, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 3, input(X)'s shape = [760567127, 3], Attr(dim) = 0.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:2 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 20:25:30.724071 test begin: paddle.chunk(x=Tensor([760567127, 3],"int64"), chunks=3, axis=0, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.chunk(x=Tensor([760567127, 3],"int64"), chunks=3, axis=0, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 3, input(X)'s shape = [760567127, 3], Attr(dim) = 0.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:2 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-22 03:10:58.705406 test begin: paddle.conj(Tensor([2, 20, 2, 53687092],"float32"), )

W0422 03:14:12.092226 78843 backward.cc:441] While running Node (ConjGradNode) raises a std::exception: paddle::memory::allocation::BadAlloc
[paddle error] paddle.conj(Tensor([2, 20, 2, 53687092],"float32"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   ConjGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::conj(paddle::Tensor const&)
4   void phi::ConjKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*)
5   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
6   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 16.000000GB memory on GPU 0, 71.911072GB memory has been allocated and available memory is only 7.273804GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /host_home/wanghuan29/Paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-04-22 03:31:00.901699 test begin: paddle.conj(Tensor([2, 20, 35791395, 3],"float32"), )

W0422 03:33:50.738952 78880 backward.cc:441] While running Node (ConjGradNode) raises a std::exception: paddle::memory::allocation::BadAlloc
[paddle error] paddle.conj(Tensor([2, 20, 35791395, 3],"float32"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   ConjGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::conj(paddle::Tensor const&)
4   void phi::ConjKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*)
5   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
6   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 16.000000GB memory on GPU 0, 71.911072GB memory has been allocated and available memory is only 7.273804GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /host_home/wanghuan29/Paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-04-22 03:50:00.801326 test begin: paddle.conj(Tensor([2, 357913942, 2, 3],"float32"), )

W0422 03:53:32.562389 78902 backward.cc:441] While running Node (ConjGradNode) raises a std::exception: paddle::memory::allocation::BadAlloc
[paddle error] paddle.conj(Tensor([2, 357913942, 2, 3],"float32"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   ConjGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::conj(paddle::Tensor const&)
4   void phi::ConjKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*)
5   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
6   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 16.000000GB memory on GPU 0, 71.911072GB memory has been allocated and available memory is only 7.273804GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /host_home/wanghuan29/Paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-04-22 04:10:19.606570 test begin: paddle.conj(Tensor([35791395, 20, 2, 3],"float32"), )

W0422 04:13:55.144889 78929 backward.cc:441] While running Node (ConjGradNode) raises a std::exception: paddle::memory::allocation::BadAlloc
[paddle error] paddle.conj(Tensor([35791395, 20, 2, 3],"float32"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   ConjGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::conj(paddle::Tensor const&)
4   void phi::ConjKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*)
5   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
6   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 16.000000GB memory on GPU 0, 71.911072GB memory has been allocated and available memory is only 7.273804GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /host_home/wanghuan29/Paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-04-22 04:13:57.217831 test begin: paddle.copysign(Tensor([10, 228170138],"float32"), Tensor([10, 228170138],"float32"), )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745266560 (unix time) try "date -d @1745266560" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x127c1) received by PID 75713 (TID 0x7fc5615fe700) from PID 75713 ***]

2025-04-22 04:16:48.210748 test begin: paddle.copysign(Tensor([10, 429496730],"float16"), Tensor([10, 429496730],"float16"), )

W0422 04:18:36.736577 79028 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 04:18:36.737818 79028 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745267186 (unix time) try "date -d @1745267186" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1345d) received by PID 78941 (TID 0x7f2d59f48700) from PID 78941 ***]

2025-04-22 04:27:18.220816 test begin: paddle.copysign(Tensor([107374183, 20, 2],"float16"), Tensor([107374183, 20, 2],"float16"), )

W0422 04:29:11.928048 79130 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 04:29:11.929999 79130 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745267813 (unix time) try "date -d @1745267813" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x134c2) received by PID 79042 (TID 0x7fb9edf48700) from PID 79042 ***]

2025-04-22 04:37:38.618470 test begin: paddle.copysign(Tensor([114085069, 20],"float32"), Tensor([114085069, 20],"float32"), )

W0422 04:39:12.586651 79236 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 04:39:12.587882 79236 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745268022 (unix time) try "date -d @1745268022" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1352d) received by PID 79149 (TID 0x7fb49dabb700) from PID 79149 ***]

2025-04-22 04:41:04.843677 test begin: paddle.copysign(Tensor([12, 178956971, 2],"float16"), Tensor([12, 178956971, 2],"float16"), )

W0422 04:42:52.891187 79329 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 04:42:52.892431 79329 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745268636 (unix time) try "date -d @1745268636" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1358a) received by PID 79242 (TID 0x7f1409dc2700) from PID 79242 ***]

2025-04-22 04:51:22.912596 test begin: paddle.copysign(Tensor([12, 20, 17895698],"float16"), Tensor([12, 20, 17895698],"float16"), )

W0422 04:53:13.368793 79428 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 04:53:13.370013 79428 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745269256 (unix time) try "date -d @1745269256" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x135ed) received by PID 79341 (TID 0x7f1502949700) from PID 79341 ***]

2025-04-22 05:01:49.860674 test begin: paddle.copysign(Tensor([12, 20, 9507090],"float32"), Tensor([12, 20, 9507090],"float32"), )

W0422 05:03:28.296324 79529 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 05:03:28.297614 79529 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745269498 (unix time) try "date -d @1745269498" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x13652) received by PID 79442 (TID 0x7f8187dc2700) from PID 79442 ***]

2025-04-22 05:05:45.108133 test begin: paddle.copysign(Tensor([12, 95070891, 2],"float32"), Tensor([12, 95070891, 2],"float32"), )

W0422 05:07:52.793655 79712 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 05:07:52.795058 79712 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745269771 (unix time) try "date -d @1745269771" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x13709) received by PID 79625 (TID 0x7fe820949700) from PID 79625 ***]

2025-04-22 05:10:25.842424 test begin: paddle.copysign(Tensor([1203073, 17, 5, 6, 7],"float16"), Tensor([1203073, 17, 5, 6, 7],"float16"), )

W0422 05:12:36.029244 79811 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 05:12:36.030601 79811 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745270437 (unix time) try "date -d @1745270437" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1376c) received by PID 79724 (TID 0x7f6f73d0b700) from PID 79724 ***]

2025-04-22 05:21:21.172431 test begin: paddle.copysign(Tensor([2, 107374183, 4, 5],"float16"), Tensor([2, 107374183, 4, 5],"float16"), )

W0422 05:23:09.576709 79921 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 05:23:09.577809 79921 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745271053 (unix time) try "date -d @1745271053" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x137da) received by PID 79834 (TID 0x7f7dd1dc2700) from PID 79834 ***]

2025-04-22 05:31:44.081862 test begin: paddle.copysign(Tensor([2, 3, 143165577, 5],"float16"), Tensor([2, 3, 143165577, 5],"float16"), )

W0422 05:33:37.292950 80035 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 05:33:37.294096 80035 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745271731 (unix time) try "date -d @1745271731" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1384b) received by PID 79947 (TID 0x7f0362949700) from PID 79947 ***]

2025-04-22 05:43:04.501924 test begin: paddle.copysign(Tensor([2, 3, 4, 178956971],"float16"), Tensor([2, 3, 4, 178956971],"float16"), )

W0422 05:44:57.902175 80135 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 05:44:57.903431 80135 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745272413 (unix time) try "date -d @1745272413" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x138b0) received by PID 80048 (TID 0x7f9bffdc2700) from PID 80048 ***]

2025-04-22 05:54:26.957855 test begin: paddle.copysign(Tensor([214748365, 20],"float16"), Tensor([214748365, 20],"float16"), )

W0422 05:56:16.606256 80235 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 05:56:16.607766 80235 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745273051 (unix time) try "date -d @1745273051" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x13914) received by PID 80148 (TID 0x7f89087c3700) from PID 80148 ***]

2025-04-22 06:05:04.579041 test begin: paddle.copysign(Tensor([214748365, 4, 5],"float16"), Tensor([4, 5],"float16"), )

W0422 06:06:55.597579 80339 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 06:06:55.598796 80339 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   CopysignGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::copysign_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor*, paddle::Tensor*)
4   void phi::CopySignGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::ReduceWrapper<phi::dtype::float16>(phi::GPUContext const&, int, phi::DenseTensor*, phi::DenseTensor*)
6   void phi::SumKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
7   void phi::SumRawKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
8   void phi::funcs::ReduceKernel<phi::dtype::float16, phi::dtype::float16, phi::kps::AddFunctor, phi::kps::IdentityFunctor<phi::dtype::float16, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<phi::dtype::float16, float> const&, std::vector<int, std::allocator<int> > const&)
9   phi::DenseTensor::~DenseTensor()
10  std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
11  paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745273684 (unix time) try "date -d @1745273684" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1397b) received by PID 80251 (TID 0x7f59f8949700) from PID 80251 ***]

2025-04-22 06:15:29.966442 test begin: paddle.copysign(Tensor([214748365, 4, 5],"float16"), Tensor([5],"float16"), )

W0422 06:17:09.611774 80445 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 06:17:09.613018 80445 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   CopysignGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::copysign_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor*, paddle::Tensor*)
4   void phi::CopySignGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::ReduceWrapper<phi::dtype::float16>(phi::GPUContext const&, int, phi::DenseTensor*, phi::DenseTensor*)
6   void phi::SumKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
7   void phi::SumRawKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
8   void phi::funcs::ReduceKernel<phi::dtype::float16, phi::dtype::float16, phi::kps::AddFunctor, phi::kps::IdentityFunctor<phi::dtype::float16, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<phi::dtype::float16, float> const&, std::vector<int, std::allocator<int> > const&)
9   phi::DenseTensor::~DenseTensor()
10  std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
11  paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745274307 (unix time) try "date -d @1745274307" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x139e6) received by PID 80358 (TID 0x7ff629b85700) from PID 80358 ***]

2025-04-22 06:25:54.227690 test begin: paddle.copysign(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), )

W0422 06:27:31.773478 80544 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 06:27:31.774708 80544 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
/host_home/wanghuan29/Paddle/build/python/paddle/tensor/math.py:8249: UserWarning: The shape of broadcast output [-1] is different from the input tensor x with shape: [2281701379], please make sure you are using copysign api correctly.
  warnings.warn(
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745274538 (unix time) try "date -d @1745274538" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x13a49) received by PID 80457 (TID 0x7fe39a7c3700) from PID 80457 ***]

2025-04-22 06:29:44.239777 test begin: paddle.copysign(Tensor([3, 286331154, 5],"float16"), Tensor([5],"float16"), )

W0422 06:31:23.110637 80638 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 06:31:23.111805 80638 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   CopysignGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::copysign_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor*, paddle::Tensor*)
4   void phi::CopySignGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::ReduceWrapper<phi::dtype::float16>(phi::GPUContext const&, int, phi::DenseTensor*, phi::DenseTensor*)
6   void phi::SumKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
7   void phi::SumRawKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
8   void phi::funcs::ReduceKernel<phi::dtype::float16, phi::dtype::float16, phi::kps::AddFunctor, phi::kps::IdentityFunctor<phi::dtype::float16, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<phi::dtype::float16, float> const&, std::vector<int, std::allocator<int> > const&)
9   phi::DenseTensor::~DenseTensor()
10  std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
11  paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745275183 (unix time) try "date -d @1745275183" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x13aa7) received by PID 80551 (TID 0x7f79bf4f4700) from PID 80551 ***]

2025-04-22 06:40:24.833968 test begin: paddle.copysign(Tensor([4, 5],"float16"), Tensor([214748365, 4, 5],"float16"), )

W0422 06:42:12.829439 80737 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 06:42:12.830595 80737 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
/host_home/wanghuan29/Paddle/build/python/paddle/tensor/math.py:8249: UserWarning: The shape of broadcast output [214748365, 4, 5] is different from the input tensor x with shape: [4, 5], please make sure you are using copysign api correctly.
  warnings.warn(
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   CopysignGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::copysign_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor*, paddle::Tensor*)
4   void phi::CopySignGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::ReduceWrapper<phi::dtype::float16>(phi::GPUContext const&, int, phi::DenseTensor*, phi::DenseTensor*)
6   void phi::SumKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
7   void phi::SumRawKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
8   void phi::funcs::ReduceKernel<phi::dtype::float16, phi::dtype::float16, phi::kps::AddFunctor, phi::kps::IdentityFunctor<phi::dtype::float16, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<phi::dtype::float16, float> const&, std::vector<int, std::allocator<int> > const&)
9   phi::DenseTensor::~DenseTensor()
10  std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
11  paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745275788 (unix time) try "date -d @1745275788" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x13b0a) received by PID 80650 (TID 0x7fe7427c3700) from PID 80650 ***]

2025-04-22 06:50:40.741154 test begin: paddle.copysign(Tensor([57042535, 20, 2],"float32"), Tensor([57042535, 20, 2],"float32"), )

W0422 06:52:37.488003 80839 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 06:52:37.489255 80839 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745276040 (unix time) try "date -d @1745276040" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x13b70) received by PID 80752 (TID 0x7f10cff48700) from PID 80752 ***]

2025-04-22 06:54:45.351320 test begin: paddle.copysign(Tensor([71582789, 3, 4, 5],"float16"), Tensor([71582789, 3, 4, 5],"float16"), )

W0422 06:56:45.998778 80933 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 06:56:46.000149 80933 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745276726 (unix time) try "date -d @1745276726" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x13bce) received by PID 80846 (TID 0x7f765f935700) from PID 80846 ***]

2025-04-22 07:06:14.790816 test begin: paddle.copysign(Tensor([8, 17, 5, 6, 1052689],"float16"), Tensor([8, 17, 5, 6, 1052689],"float16"), )

W0422 07:08:08.560361 81039 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 07:08:08.561601 81039 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745277384 (unix time) try "date -d @1745277384" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x13c38) received by PID 80952 (TID 0x7fdb75744700) from PID 80952 ***]

2025-04-22 07:17:07.084249 test begin: paddle.copysign(Tensor([8, 17, 5, 902305, 7],"float16"), Tensor([8, 17, 5, 902305, 7],"float16"), )

W0422 07:19:03.715167 81144 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 07:19:03.716310 81144 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745278043 (unix time) try "date -d @1745278043" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x13ca0) received by PID 81056 (TID 0x7f1cd9dc2700) from PID 81056 ***]

2025-04-22 07:28:15.472382 test begin: paddle.copysign(Tensor([8, 17, 751921, 6, 7],"float16"), Tensor([8, 17, 751921, 6, 7],"float16"), )

W0422 07:30:13.201913 81251 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 07:30:13.203269 81251 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745278683 (unix time) try "date -d @1745278683" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x13d0c) received by PID 81164 (TID 0x7f42214f4700) from PID 81164 ***]

2025-04-22 07:38:48.314181 test begin: paddle.copysign(Tensor([8, 2556529, 5, 6, 7],"float16"), Tensor([8, 2556529, 5, 6, 7],"float16"), )

W0422 07:40:37.755167 81353 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 07:40:37.756260 81353 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745279297 (unix time) try "date -d @1745279297" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x13d72) received by PID 81266 (TID 0x7f900a7c3700) from PID 81266 ***]

2025-04-22 07:51:09.651757 test begin: paddle.cross(Tensor([760567127, 3],"float32"), Tensor([760567127, 3],"float32"), axis=1, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745279503 (unix time) try "date -d @1745279503" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x13dd8) received by PID 81368 (TID 0x7f9b707c3700) from PID 81368 ***]

2025-04-22 07:52:28.653088 test begin: paddle.cross(x=Tensor([1431655766, 3],"float16"), y=Tensor([1431655766, 3],"float16"), )

/usr/local/lib/python3.9/dist-packages/torch/utils/_device.py:106: UserWarning: Using torch.cross without specifying the dim arg is deprecated.
Please either pass the dim explicitly or simply use torch.linalg.cross.
The default value of dim will change to agree with that of linalg.cross in a future release. (Triggered internally at ../aten/src/ATen/native/Cross.cpp:62.)
  return func(*args, **kwargs)
W0422 07:54:18.547194 81705 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 07:54:18.548291 81705 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745279660 (unix time) try "date -d @1745279660" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x13ed1) received by PID 81617 (TID 0x7fd821dc2700) from PID 81617 ***]

2025-04-22 07:55:02.815494 test begin: paddle.cross(x=Tensor([1431655766, 3],"float16"), y=Tensor([1431655766, 3],"float16"), axis=-1, )

W0422 07:56:55.196871 81889 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 07:56:55.198033 81889 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745279816 (unix time) try "date -d @1745279816" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x13f33) received by PID 81715 (TID 0x7ff2427c3700) from PID 81715 ***]

2025-04-22 07:57:01.128317 test begin: paddle.cross(x=Tensor([1431655766, 3],"float16"), y=Tensor([1431655766, 3],"float16"), axis=1, )

W0422 07:58:49.030117 81983 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 07:58:49.031231 81983 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745279930 (unix time) try "date -d @1745279930" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x13fe8) received by PID 81896 (TID 0x7fa2cc949700) from PID 81896 ***]

2025-04-22 09:14:57.746783 test begin: paddle.cross(x=Tensor([3, 3, 477218589],"float16"), y=Tensor([3, 3, 477218589],"float16"), axis=1, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745284530 (unix time) try "date -d @1745284530" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x140f7) received by PID 82167 (TID 0x7f0d227c3700) from PID 82167 ***]

2025-04-22 09:41:42.446264 test begin: paddle.cross(x=Tensor([3, 477218589, 3],"float16"), y=Tensor([3, 477218589, 3],"float16"), axis=2, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745286134 (unix time) try "date -d @1745286134" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1a28b) received by PID 107147 (TID 0x7fddb67c3700) from PID 107147 ***]

2025-04-22 09:51:08.134684 test begin: paddle.cross(x=Tensor([477218589, 3, 3],"float16"), y=Tensor([477218589, 3, 3],"float16"), axis=1, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745286768 (unix time) try "date -d @1745286768" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x22b9a) received by PID 142234 (TID 0x7faf19dc2700) from PID 142234 ***]

2025-04-22 09:53:34.080290 test begin: paddle.cross(x=Tensor([477218589, 3, 3],"float16"), y=Tensor([477218589, 3, 3],"float16"), axis=2, )

W0422 09:55:37.959239 155550 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 09:55:37.960484 155550 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745286939 (unix time) try "date -d @1745286939" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x25f28) received by PID 155432 (TID 0x7fb733dc2700) from PID 155432 ***]

2025-04-22 09:56:17.709625 test begin: paddle.cross(x=Tensor([760567127, 3],"float32"), y=Tensor([760567127, 3],"float32"), )

/usr/local/lib/python3.9/dist-packages/torch/utils/_device.py:106: UserWarning: Using torch.cross without specifying the dim arg is deprecated.
Please either pass the dim explicitly or simply use torch.linalg.cross.
The default value of dim will change to agree with that of linalg.cross in a future release. (Triggered internally at ../aten/src/ATen/native/Cross.cpp:62.)
  return func(*args, **kwargs)
W0422 09:58:04.589236 157906 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 09:58:04.591531 157906 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745287086 (unix time) try "date -d @1745287086" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x26856) received by PID 157782 (TID 0x7fbfd9dc2700) from PID 157782 ***]

2025-04-22 09:58:51.874190 test begin: paddle.cross(x=Tensor([760567127, 3],"int32"), y=Tensor([760567127, 3],"int32"), )

/usr/local/lib/python3.9/dist-packages/torch/utils/_device.py:106: UserWarning: Using torch.cross without specifying the dim arg is deprecated.
Please either pass the dim explicitly or simply use torch.linalg.cross.
The default value of dim will change to agree with that of linalg.cross in a future release. (Triggered internally at ../aten/src/ATen/native/Cross.cpp:62.)
  return func(*args, **kwargs)
W0422 10:00:02.285897 159686 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 10:00:02.287305 159686 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745287203 (unix time) try "date -d @1745287203" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x26f69) received by PID 159593 (TID 0x7fb42bdc2700) from PID 159593 ***]

2025-04-22 10:04:35.092019 test begin: paddle.cumsum(Tensor([1, 2281701379],"float32"), axis=0, )

[torch error] paddle.cumsum(Tensor([1, 2281701379],"float32"), axis=0, ) 
 CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-04-22 10:04:45.341860 test begin: paddle.cumsum(Tensor([114085069, 20],"int64"), axis=-1, )

W0422 10:06:24.270411   818 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 10:06:24.271955   818 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745287584 (unix time) try "date -d @1745287584" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2c3) received by PID 707 (TID 0x7f557334a700) from PID 707 ***]

2025-04-22 10:07:10.031720 test begin: paddle.cumsum(Tensor([114085069, 20],"int64"), axis=1, )

W0422 10:08:33.609714  2529 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 10:08:33.610613  2529 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745287714 (unix time) try "date -d @1745287714" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x965) received by PID 2405 (TID 0x7f27afdc2700) from PID 2405 ***]

2025-04-22 10:09:21.259210 test begin: paddle.cumsum(Tensor([1140850690, 2],"int64"), axis=-1, )

W0422 10:11:11.488446  4099 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 10:11:11.490381  4099 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745287874 (unix time) try "date -d @1745287874" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xfab) received by PID 4011 (TID 0x7f004fdc2700) from PID 4011 ***]

2025-04-22 10:11:57.447760 test begin: paddle.cumsum(Tensor([120, 19014179],"int32"), axis=0, )

W0422 10:13:06.639437  5920 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 10:13:06.640501  5920 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745287986 (unix time) try "date -d @1745287986" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x16a9) received by PID 5801 (TID 0x7fb785b85700) from PID 5801 ***]

2025-04-22 10:13:52.313124 test begin: paddle.cumsum(Tensor([16, 142606337],"int32"), axis=0, )

W0422 10:15:10.825222  7421 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 10:15:10.826413  7421 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745288111 (unix time) try "date -d @1745288111" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1c87) received by PID 7303 (TID 0x7f1d992b7700) from PID 7303 ***]

2025-04-22 10:15:57.137399 test begin: paddle.cumsum(Tensor([207427399, 11],"int64"), axis=-1, )

W0422 10:17:38.357463  9354 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 10:17:38.358517  9354 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745288259 (unix time) try "date -d @1745288259" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x23fe) received by PID 9214 (TID 0x7fd653b85700) from PID 9214 ***]

2025-04-22 10:18:24.453648 test begin: paddle.cumsum(Tensor([21939437, 104],"int64"), 1, )

W0422 10:20:14.739459 17769 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 10:20:14.740839 17769 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745288415 (unix time) try "date -d @1745288415" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x445c) received by PID 17500 (TID 0x7f7ed9b85700) from PID 17500 ***]

2025-04-22 10:21:04.108949 test begin: paddle.cumsum(Tensor([22152441, 103],"int64"), 1, )

W0422 10:23:23.848110 52442 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 10:23:23.854135 52442 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745288604 (unix time) try "date -d @1745288604" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xcbe4) received by PID 52196 (TID 0x7f8f3c949700) from PID 52196 ***]

2025-04-22 10:26:12.991885 test begin: paddle.cumsum(Tensor([2281701379],"float32"), axis=0, )

[accuracy error] paddle.cumsum(Tensor([2281701379],"float32"), axis=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1060438 / 2281701379 (0.0465%)
Max absolute difference: 0.04541016
Max relative difference: 22655.5
 x: array([-3.370891e-01, -1.299835e-01, -4.737458e-02, ...,  1.610660e+03,
        1.610616e+03,  1.611058e+03], dtype=float32)
 y: array([-3.370891e-01, -1.299835e-01, -4.737458e-02, ...,  1.610638e+03,
        1.610594e+03,  1.611036e+03], dtype=float32)
2025-04-22 10:30:34.644251 test begin: paddle.cumsum(Tensor([22817014, 100],"int64"), 1, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745289124 (unix time) try "date -d @1745289124" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x12d1e) received by PID 77086 (TID 0x7f242427f700) from PID 77086 ***]

2025-04-22 10:32:51.453616 test begin: paddle.cumsum(Tensor([24, 95070891],"int32"), axis=0, )

W0422 10:34:15.487766 102592 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 10:34:15.488811 102592 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745289255 (unix time) try "date -d @1745289255" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x18f34) received by PID 102196 (TID 0x7f1585dc2700) from PID 102196 ***]

2025-04-22 10:34:59.572299 test begin: paddle.cumsum(Tensor([285212673, 2, 4],"float32"), axis=1, )

W0422 10:36:23.421375 105866 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 10:36:23.422549 105866 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745289385 (unix time) try "date -d @1745289385" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x19d0e) received by PID 105742 (TID 0x7f1bebdc2700) from PID 105742 ***]

2025-04-22 10:37:11.353357 test begin: paddle.cumsum(Tensor([3, 190141782, 4],"float32"), axis=1, )

W0422 10:40:04.031450 108760 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 10:40:04.035228 108760 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.cumsum(Tensor([3, 190141782, 4],"float32"), axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 10:40:06.826049 test begin: paddle.cumsum(Tensor([3, 2, 380283564],"float32"), axis=1, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745289672 (unix time) try "date -d @1745289672" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1a869) received by PID 108649 (TID 0x7fb79934a700) from PID 108649 ***]

2025-04-22 10:41:55.576491 test begin: paddle.cumsum(Tensor([3, 760567127],"int64"), axis=-2, )

W0422 10:43:30.796778 152665 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 10:43:30.798611 152665 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745289812 (unix time) try "date -d @1745289812" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x253db) received by PID 152539 (TID 0x7f4de3abb700) from PID 152539 ***]

2025-04-22 10:44:15.924513 test begin: paddle.cumsum(Tensor([3, 760567127],"int64"), axis=0, )

W0422 10:45:36.908669 158167 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 10:45:36.909632 158167 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745289939 (unix time) try "date -d @1745289939" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2695f) received by PID 158047 (TID 0x7f76c2949700) from PID 158047 ***]

2025-04-22 10:46:31.005245 test begin: paddle.cumsum(Tensor([325957340, 7],"int32"), axis=1, )

W0422 10:49:09.327404  8617 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 10:49:09.328423  8617 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745290149 (unix time) try "date -d @1745290149" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x11c3) received by PID 4547 (TID 0x7fe017f48700) from PID 4547 ***]

2025-04-22 10:49:55.877760 test begin: paddle.cumsum(Tensor([38028357, 60],"int32"), axis=0, )

W0422 10:51:16.311295 36269 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 10:51:16.312392 36269 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745290276 (unix time) try "date -d @1745290276" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x8d56) received by PID 36182 (TID 0x7f9fe32b7700) from PID 36182 ***]

2025-04-22 10:52:02.051695 test begin: paddle.cumsum(Tensor([49602204, 46],"int64"), axis=1, )

W0422 10:53:23.955045 38907 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 10:53:23.955962 38907 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745290404 (unix time) try "date -d @1745290404" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x9784) received by PID 38788 (TID 0x7f166ff48700) from PID 38788 ***]

2025-04-22 10:54:06.548771 test begin: paddle.cumsum(Tensor([50704476, 45],"int64"), axis=1, )

W0422 10:55:31.552425 41873 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 10:55:31.553579 41873 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745290531 (unix time) try "date -d @1745290531" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xa1e5) received by PID 41445 (TID 0x7f9953f48700) from PID 41445 ***]

2025-04-22 10:56:18.276094 test begin: paddle.cumsum(Tensor([51856850, 44],"int64"), axis=1, )

W0422 10:57:52.260701 48039 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 10:57:52.261906 48039 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745290672 (unix time) try "date -d @1745290672" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xb94b) received by PID 47435 (TID 0x7f7c71f48700) from PID 47435 ***]

2025-04-22 10:58:36.073066 test begin: paddle.cumsum(Tensor([570425345, 4],"int64"), axis=-2, )

W0422 11:03:15.392808 52057 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 11:03:15.393774 52057 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.cumsum(Tensor([570425345, 4],"int64"), axis=-2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 11:03:17.996237 test begin: paddle.cumsum(Tensor([570425345, 4],"int64"), axis=0, )

[cuda error] paddle.cumsum(Tensor([570425345, 4],"int64"), axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 11:06:24.829456 test begin: paddle.cumsum(Tensor([760567127, 3],"float32"), axis=0, )

[cuda error] paddle.cumsum(Tensor([760567127, 3],"float32"), axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 11:11:49.607037 test begin: paddle.cumsum(Tensor([76057, 30000],"float32"), axis=-1, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745291533 (unix time) try "date -d @1745291533" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xcaeb) received by PID 51947 (TID 0x7f26556f8700) from PID 51947 ***]

2025-04-22 11:13:03.446961 test begin: paddle.cumsum(x=Tensor([1, 1398102, 96, 32],"float16"), axis=2, )

W0422 11:14:42.936017 106615 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 11:14:42.937244 106615 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745291683 (unix time) try "date -d @1745291683" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x19e73) received by PID 106099 (TID 0x7f5bddabb700) from PID 106099 ***]

2025-04-22 11:15:28.585575 test begin: paddle.cumsum(x=Tensor([1, 1431655766, 1, 3],"float16"), axis=3, )

W0422 11:17:53.482746 117110 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 11:17:53.483814 117110 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745291874 (unix time) try "date -d @1745291874" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1c8a6) received by PID 116902 (TID 0x7f33f1dc2700) from PID 116902 ***]

2025-04-22 11:18:38.923986 test begin: paddle.cumsum(x=Tensor([1, 16, 8388609, 32],"float16"), axis=2, )

W0422 11:20:40.482239 124396 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 11:20:40.483549 124396 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745292040 (unix time) try "date -d @1745292040" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1e54c) received by PID 124236 (TID 0x7ffaf7935700) from PID 124236 ***]

2025-04-22 11:21:24.372628 test begin: paddle.cumsum(x=Tensor([1, 16, 96, 2796203],"float16"), axis=2, )

W0422 11:23:12.356791 130755 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 11:23:12.358970 130755 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745292192 (unix time) try "date -d @1745292192" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1fe3b) received by PID 130619 (TID 0x7f2685dc2700) from PID 130619 ***]

2025-04-22 11:23:55.526144 test begin: paddle.cumsum(x=Tensor([1, 2, 715827883, 3],"float16"), axis=3, )

W0422 11:26:18.604185 137834 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 11:26:18.605363 137834 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745292379 (unix time) try "date -d @1745292379" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x219f6) received by PID 137718 (TID 0x7f76936f8700) from PID 137718 ***]

2025-04-22 11:26:59.961496 test begin: paddle.cumsum(x=Tensor([715827883, 2, 1, 3],"float16"), axis=-4, )

W0422 11:32:04.573441 147095 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 11:32:04.574573 147095 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745292724 (unix time) try "date -d @1745292724" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x23e27) received by PID 146983 (TID 0x7fae1134a700) from PID 146983 ***]

2025-04-22 11:32:49.886071 test begin: paddle.cumsum(x=Tensor([715827883, 2, 1, 3],"float16"), axis=3, )

W0422 11:35:01.380338 154216 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 11:35:01.382815 154216 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745292902 (unix time) try "date -d @1745292902" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x259f1) received by PID 154097 (TID 0x7f542a949700) from PID 154097 ***]

2025-04-22 11:35:43.256620 test begin: paddle.cumsum(x=Tensor([87382, 16, 96, 32],"float16"), axis=2, )

W0422 11:37:23.994232 157090 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 11:37:23.995398 157090 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745293044 (unix time) try "date -d @1745293044" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2651b) received by PID 156955 (TID 0x7f406fdc2700) from PID 156955 ***]

2025-04-22 11:40:03.214243 test begin: paddle.diag(x=Tensor([2, 2147483649],"float16"), offset=-1, )

W0422 11:41:57.076385 162094 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 11:41:57.077798 162094 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745293318 (unix time) try "date -d @1745293318" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x27798) received by PID 161688 (TID 0x7f19f187e700) from PID 161688 ***]

2025-04-22 11:42:37.819446 test begin: paddle.diag(x=Tensor([2, 2147483649],"float16"), offset=0, )

W0422 11:44:23.061252  1783 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 11:44:23.062449  1783 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745293463 (unix time) try "date -d @1745293463" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x663) received by PID 1635 (TID 0x7ff547744700) from PID 1635 ***]

2025-04-22 11:45:02.152506 test begin: paddle.diag(x=Tensor([2, 2147483649],"float16"), offset=2, )

W0422 11:46:44.456809  4302 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 11:46:44.458016  4302 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745293605 (unix time) try "date -d @1745293605" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1053) received by PID 4179 (TID 0x7f989c7c3700) from PID 4179 ***]

2025-04-22 11:47:23.121125 test begin: paddle.diagonal(x=Tensor([29826162, 6, 6, 2, 2],"float16"), axis1=3, axis2=4, )

W0422 11:48:58.258229  6817 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 11:48:58.259440  6817 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.diagonal(x=Tensor([29826162, 6, 6, 2, 2],"float16"), axis1=3, axis2=4, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 11:53:30.936169 test begin: paddle.diff(Tensor([2, 1140850690],"float32"), n=1, axis=0, prepend=None, append=Tensor([1, 1140850690],"float32"), )

[accuracy error] paddle.diff(Tensor([2, 1140850690],"float32"), n=1, axis=0, prepend=None, append=Tensor([1, 1140850690],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2235831206 / 2281701380 (98%)
Max absolute difference: 0.9999832
Max relative difference: 1.
 x: array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)
 y: array([[ 0.46886 , -0.265605, -0.848823, ..., -0.431124,  0.548978,
         0.177721],
       [-0.46886 ,  0.265605,  0.848823, ...,  0.431124, -0.548978,
        -0.177721]], dtype=float32)
2025-04-22 11:56:55.339535 test begin: paddle.diff(Tensor([2, 1140850690],"float32"), n=1, axis=0, prepend=Tensor([3, 1140850690],"float32"), append=None, )

[accuracy error] paddle.diff(Tensor([2, 1140850690],"float32"), n=1, axis=0, prepend=Tensor([3, 1140850690],"float32"), append=None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4471677563 / 4563402760 (98%)
Max absolute difference: 0.9999832
Max relative difference: 1.
 x: array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)
 y: array([[ 0.46886 , -0.265605, -0.848823, ..., -0.431124,  0.548978,
         0.177721],
       [-0.565143,  0.10355 ,  0.226909, ...,  0.08984 ,  0.130834,...
2025-04-22 12:01:00.510730 test begin: paddle.diff(Tensor([2, 1140850690],"float32"), n=2, axis=0, prepend=None, append=Tensor([2, 1140850690],"float32"), )

[accuracy error] paddle.diff(Tensor([2, 1140850690],"float32"), n=2, axis=0, prepend=None, append=Tensor([2, 1140850690],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2258703580 / 2281701380 (99%)
Max absolute difference: 1.9999664
Max relative difference: 1.
 x: array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)
 y: array([[-0.93772 ,  0.53121 ,  1.697646, ...,  0.862249, -1.097956,
        -0.355442],
       [ 0.93772 , -0.53121 , -1.697646, ..., -0.862249,  1.097956,
         0.355442]], dtype=float32)
2025-04-22 12:03:10.242234 test begin: paddle.diff(Tensor([2, 1140850690],"float32"), n=2, axis=0, prepend=Tensor([2, 1140850690],"float32"), append=None, )

[accuracy error] paddle.diff(Tensor([2, 1140850690],"float32"), n=2, axis=0, prepend=Tensor([2, 1140850690],"float32"), append=None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2258703580 / 2281701380 (99%)
Max absolute difference: 1.9999664
Max relative difference: 1.
 x: array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)
 y: array([[-0.93772 ,  0.53121 ,  1.697646, ...,  0.862249, -1.097956,
        -0.355442],
       [ 0.93772 , -0.53121 , -1.697646, ..., -0.862249,  1.097956,
         0.355442]], dtype=float32)
2025-04-22 12:05:30.775288 test begin: paddle.diff(Tensor([2, 2147483649],"float16"), axis=1, )

[cuda error] paddle.diff(Tensor([2, 2147483649],"float16"), axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 12:05:42.207495 test begin: paddle.diff(Tensor([2, 4],"float32"), n=1, axis=0, prepend=None, append=Tensor([570425345, 4],"float32"), )

[accuracy error] paddle.diff(Tensor([2, 4],"float32"), n=1, axis=0, prepend=None, append=Tensor([570425345, 4],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2235824537 / 2281701384 (98%)
Max absolute difference: 0.99996746
Max relative difference: 1.
 x: array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.],...
 y: array([[-0.026151,  0.459108, -0.082962,  0.088723],
       [ 0.026151, -0.459108,  0.082962, -0.088723],
       [-0.026151,  0.459108, -0.082962,  0.088723],...
2025-04-22 12:07:34.585920 test begin: paddle.diff(Tensor([2, 4],"float32"), n=1, axis=0, prepend=Tensor([2, 4],"float32"), append=Tensor([570425345, 4],"float32"), )

[accuracy error] paddle.diff(Tensor([2, 4],"float32"), n=1, axis=0, prepend=Tensor([2, 4],"float32"), append=Tensor([570425345, 4],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2235824545 / 2281701392 (98%)
Max absolute difference: 0.99996746
Max relative difference: 1.
 x: array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.],...
 y: array([[-0.026151,  0.459108, -0.082962,  0.088723],
       [ 0.026151, -0.459108,  0.082962, -0.088723],
       [-0.026151,  0.459108, -0.082962,  0.088723],...
2025-04-22 12:09:26.648253 test begin: paddle.diff(Tensor([2, 4],"float32"), n=1, axis=0, prepend=Tensor([570425345, 4],"float32"), append=None, )

[accuracy error] paddle.diff(Tensor([2, 4],"float32"), n=1, axis=0, prepend=Tensor([570425345, 4],"float32"), append=None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2235824537 / 2281701384 (98%)
Max absolute difference: 0.99996746
Max relative difference: 1.
 x: array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.],...
 y: array([[-0.026151,  0.459108, -0.082962,  0.088723],
       [ 0.867729, -0.160703, -0.228945,  0.85831 ],
       [-0.057133, -0.534816,  0.254641, -0.878369],...
2025-04-22 12:11:36.094881 test begin: paddle.diff(Tensor([2, 4],"float32"), n=1, axis=0, prepend=Tensor([570425345, 4],"float32"), append=Tensor([2, 4],"float32"), )

[accuracy error] paddle.diff(Tensor([2, 4],"float32"), n=1, axis=0, prepend=Tensor([570425345, 4],"float32"), append=Tensor([2, 4],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2235824545 / 2281701392 (98%)
Max absolute difference: 0.99996746
Max relative difference: 1.
 x: array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.],...
 y: array([[-0.026151,  0.459108, -0.082962,  0.088723],
       [ 0.867729, -0.160703, -0.228945,  0.85831 ],
       [-0.057133, -0.534816,  0.254641, -0.878369],...
2025-04-22 12:13:32.868173 test begin: paddle.diff(Tensor([2, 4],"float32"), n=2, axis=0, prepend=None, append=Tensor([570425345, 4],"float32"), )

[accuracy error] paddle.diff(Tensor([2, 4],"float32"), n=2, axis=0, prepend=None, append=Tensor([570425345, 4],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2258655195 / 2281701380 (99%)
Max absolute difference: 1.9986682
Max relative difference: 1.
 x: array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.],...
 y: array([[ 0.052302, -0.918215,  0.165924, -0.177446],
       [-0.052302,  0.918215, -0.165924,  0.177446],
       [ 0.89388 , -0.619811, -0.145983,  0.769588],...
2025-04-22 12:15:19.236896 test begin: paddle.diff(Tensor([2, 4],"float32"), n=2, axis=0, prepend=Tensor([2, 4],"float32"), append=Tensor([570425345, 4],"float32"), )

[accuracy error] paddle.diff(Tensor([2, 4],"float32"), n=2, axis=0, prepend=Tensor([2, 4],"float32"), append=Tensor([570425345, 4],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2258655203 / 2281701388 (99%)
Max absolute difference: 1.9986682
Max relative difference: 1.
 x: array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.],...
 y: array([[ 0.052302, -0.918215,  0.165924, -0.177446],
       [-0.052302,  0.918215, -0.165924,  0.177446],
       [ 0.052302, -0.918215,  0.165924, -0.177446],...
2025-04-22 12:17:31.339520 test begin: paddle.diff(Tensor([2, 4],"float32"), n=2, axis=0, prepend=Tensor([570425345, 4],"float32"), append=None, )

[accuracy error] paddle.diff(Tensor([2, 4],"float32"), n=2, axis=0, prepend=Tensor([570425345, 4],"float32"), append=None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2258655195 / 2281701380 (99%)
Max absolute difference: 1.9986682
Max relative difference: 1.
 x: array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.],...
 y: array([[ 0.89388 , -0.619811, -0.145983,  0.769588],
       [-0.924862, -0.374113,  0.483585, -1.736679],
       [-0.733603,  1.013799, -0.753501,  1.073614],...
2025-04-22 12:19:21.030017 test begin: paddle.diff(Tensor([2, 4],"float32"), n=2, axis=0, prepend=Tensor([570425345, 4],"float32"), append=Tensor([2, 4],"float32"), )

[accuracy error] paddle.diff(Tensor([2, 4],"float32"), n=2, axis=0, prepend=Tensor([570425345, 4],"float32"), append=Tensor([2, 4],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2258655203 / 2281701388 (99%)
Max absolute difference: 1.9986682
Max relative difference: 1.
 x: array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.],...
 y: array([[ 0.89388 , -0.619811, -0.145983,  0.769588],
       [-0.924862, -0.374113,  0.483585, -1.736679],
       [-0.733603,  1.013799, -0.753501,  1.073614],...
2025-04-22 12:21:08.080063 test begin: paddle.diff(Tensor([2281701379],"float32"), )

[accuracy error] paddle.diff(Tensor([2281701379],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2235839332 / 2281701378 (98%)
Max absolute difference: 0.99997824
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)
 y: array([ 0.166925,  0.66252 , -0.920856, ..., -0.244812, -0.031433,
        0.471268], dtype=float32)
2025-04-22 12:23:09.869611 test begin: paddle.diff(Tensor([2281701379],"float32"), n=1, axis=-1, prepend=None, append=None, )

[accuracy error] paddle.diff(Tensor([2281701379],"float32"), n=1, axis=-1, prepend=None, append=None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2235839332 / 2281701378 (98%)
Max absolute difference: 0.99997824
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)
 y: array([ 0.166925,  0.66252 , -0.920856, ..., -0.244812, -0.031433,
        0.471268], dtype=float32)
2025-04-22 12:25:14.295318 test begin: paddle.diff(Tensor([2281701379],"float32"), n=2, axis=0, prepend=None, append=None, )

[accuracy error] paddle.diff(Tensor([2281701379],"float32"), n=2, axis=0, prepend=None, append=None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2258649492 / 2281701377 (99%)
Max absolute difference: 1.9986087
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)
 y: array([ 0.495595, -1.583376,  0.986117, ..., -0.745643,  0.213379,
        0.502701], dtype=float32)
2025-04-22 12:27:29.658728 test begin: paddle.diff(Tensor([4, 1073741825],"float16"), axis=1, )

[cuda error] paddle.diff(Tensor([4, 1073741825],"float16"), axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 12:27:43.763504 test begin: paddle.diff(Tensor([570425345, 4],"float32"), n=1, axis=0, prepend=None, append=None, )

[accuracy error] paddle.diff(Tensor([570425345, 4],"float32"), n=1, axis=0, prepend=None, append=None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2235824529 / 2281701376 (98%)
Max absolute difference: 0.99996746
Max relative difference: 1.
 x: array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.],...
 y: array([[-0.026151,  0.459108, -0.082962,  0.088723],
       [ 0.867729, -0.160703, -0.228945,  0.85831 ],
       [-0.057133, -0.534816,  0.254641, -0.878369],...
2025-04-22 12:29:31.930791 test begin: paddle.diff(Tensor([570425345, 4],"float32"), n=1, axis=0, prepend=None, append=Tensor([1, 4],"float32"), )

[accuracy error] paddle.diff(Tensor([570425345, 4],"float32"), n=1, axis=0, prepend=None, append=Tensor([1, 4],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2235824533 / 2281701380 (98%)
Max absolute difference: 0.99996746
Max relative difference: 1.
 x: array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.],...
 y: array([[-0.026151,  0.459108, -0.082962,  0.088723],
       [ 0.867729, -0.160703, -0.228945,  0.85831 ],
       [-0.057133, -0.534816,  0.254641, -0.878369],...
2025-04-22 12:31:24.530041 test begin: paddle.diff(Tensor([570425345, 4],"float32"), n=1, axis=0, prepend=None, append=Tensor([570425345, 4],"float32"), )

[accuracy error] paddle.diff(Tensor([570425345, 4],"float32"), n=1, axis=0, prepend=None, append=Tensor([570425345, 4],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4471649062 / 4563402756 (98%)
Max absolute difference: 0.99996746
Max relative difference: 1.
 x: array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.],...
 y: array([[-0.026151,  0.459108, -0.082962,  0.088723],
       [ 0.867729, -0.160703, -0.228945,  0.85831 ],
       [-0.057133, -0.534816,  0.254641, -0.878369],...
2025-04-22 12:35:30.235351 test begin: paddle.full_like(Tensor([2281701379, 1],"float32"), fill_value=2, )

[accuracy error] paddle.full_like(Tensor([2281701379, 1],"float32"), fill_value=2, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 2.
Max relative difference: 1.
 x: array([[0.],
       [0.],
       [0.],...
 y: array([[2.],
       [2.],
       [2.],...
2025-04-22 12:37:22.611679 test begin: paddle.full_like(Tensor([2281701379, 1],"float32"), fill_value=6, )

[accuracy error] paddle.full_like(Tensor([2281701379, 1],"float32"), fill_value=6, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 6.
Max relative difference: 1.
 x: array([[0.],
       [0.],
       [0.],...
 y: array([[6.],
       [6.],
       [6.],...
2025-04-22 12:39:14.393067 test begin: paddle.full_like(Tensor([2281701379, 1],"int64"), -1, )

[accuracy error] paddle.full_like(Tensor([2281701379, 1],"int64"), -1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 1
Max relative difference: 1.
 x: array([[0],
       [0],
       [0],...
 y: array([[-1],
       [-1],
       [-1],...
2025-04-22 12:44:43.277220 test begin: paddle.full_like(Tensor([2281701379, 1],"int64"), 1, )

[accuracy error] paddle.full_like(Tensor([2281701379, 1],"int64"), 1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 1
Max relative difference: 1.
 x: array([[0],
       [0],
       [0],...
 y: array([[1],
       [1],
       [1],...
2025-04-22 12:48:32.845223 test begin: paddle.full_like(Tensor([2281701379, 1],"int64"), 151643, )

[accuracy error] paddle.full_like(Tensor([2281701379, 1],"int64"), 151643, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 151643
Max relative difference: 1.
 x: array([[0],
       [0],
       [0],...
 y: array([[151643],
       [151643],
       [151643],...
2025-04-22 12:52:32.238552 test begin: paddle.full_like(Tensor([2281701379, 1],"int64"), 2, )

[accuracy error] paddle.full_like(Tensor([2281701379, 1],"int64"), 2, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 2
Max relative difference: 1.
 x: array([[0],
       [0],
       [0],...
 y: array([[2],
       [2],
       [2],...
2025-04-22 12:56:54.325446 test begin: paddle.full_like(Tensor([2281701379, 1],"int64"), 255, )

[accuracy error] paddle.full_like(Tensor([2281701379, 1],"int64"), 255, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 255
Max relative difference: 1.
 x: array([[0],
       [0],
       [0],...
 y: array([[255],
       [255],
       [255],...
2025-04-22 13:00:57.583449 test begin: paddle.full_like(Tensor([2281701379, 1],"int64"), 3, )

[accuracy error] paddle.full_like(Tensor([2281701379, 1],"int64"), 3, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 3
Max relative difference: 1.
 x: array([[0],
       [0],
       [0],...
 y: array([[3],
       [3],
       [3],...
2025-04-22 13:04:53.414864 test begin: paddle.full_like(Tensor([2281701379, 1],"int64"), 7, )

[accuracy error] paddle.full_like(Tensor([2281701379, 1],"int64"), 7, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 7
Max relative difference: 1.
 x: array([[0],
       [0],
       [0],...
 y: array([[7],
       [7],
       [7],...
2025-04-22 13:09:06.649933 test begin: paddle.full_like(Tensor([2281701379, 1],"int64"), 98, )

[accuracy error] paddle.full_like(Tensor([2281701379, 1],"int64"), 98, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 98
Max relative difference: 1.
 x: array([[0],
       [0],
       [0],...
 y: array([[98],
       [98],
       [98],...
2025-04-22 13:12:47.657064 test begin: paddle.full_like(Tensor([2281701379],"float32"), 0.5, )

[accuracy error] paddle.full_like(Tensor([2281701379],"float32"), 0.5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 0.5
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)
 y: array([0.5, 0.5, 0.5, ..., 0.5, 0.5, 0.5], dtype=float32)
2025-04-22 13:14:53.961252 test begin: paddle.full_like(Tensor([2281701379],"float32"), 1.0, )

[accuracy error] paddle.full_like(Tensor([2281701379],"float32"), 1.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)
 y: array([1., 1., 1., ..., 1., 1., 1.], dtype=float32)
2025-04-22 13:16:40.578300 test begin: paddle.full_like(Tensor([2281701379],"float32"), fill_value=1023, )

[accuracy error] paddle.full_like(Tensor([2281701379],"float32"), fill_value=1023, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 1023.
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)
 y: array([1023., 1023., 1023., ..., 1023., 1023., 1023.], dtype=float32)
2025-04-22 13:18:46.690397 test begin: paddle.full_like(Tensor([2281701379],"float32"), fill_value=1183, )

[accuracy error] paddle.full_like(Tensor([2281701379],"float32"), fill_value=1183, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 1183.
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)
 y: array([1183., 1183., 1183., ..., 1183., 1183., 1183.], dtype=float32)
2025-04-22 13:20:33.088357 test begin: paddle.full_like(Tensor([2281701379],"float32"), fill_value=16383, )

[accuracy error] paddle.full_like(Tensor([2281701379],"float32"), fill_value=16383, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 16383.
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)
 y: array([16383., 16383., 16383., ..., 16383., 16383., 16383.], dtype=float32)
2025-04-22 13:22:30.641817 test begin: paddle.full_like(Tensor([2281701379],"float32"), fill_value=19, )

[accuracy error] paddle.full_like(Tensor([2281701379],"float32"), fill_value=19, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 19.
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)
 y: array([19., 19., 19., ..., 19., 19., 19.], dtype=float32)
2025-04-22 13:24:41.409692 test begin: paddle.full_like(Tensor([2281701379],"float32"), fill_value=2, )

[accuracy error] paddle.full_like(Tensor([2281701379],"float32"), fill_value=2, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 2.
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)
 y: array([2., 2., 2., ..., 2., 2., 2.], dtype=float32)
2025-04-22 13:26:39.220324 test begin: paddle.full_like(Tensor([2281701379],"float32"), fill_value=31, )

[accuracy error] paddle.full_like(Tensor([2281701379],"float32"), fill_value=31, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 31.
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)
 y: array([31., 31., 31., ..., 31., 31., 31.], dtype=float32)
2025-04-22 13:28:29.897339 test begin: paddle.full_like(Tensor([2281701379],"float32"), fill_value=35967, )

[accuracy error] paddle.full_like(Tensor([2281701379],"float32"), fill_value=35967, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 35967.
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)
 y: array([35967., 35967., 35967., ..., 35967., 35967., 35967.], dtype=float32)
2025-04-22 13:30:26.366845 test begin: paddle.full_like(Tensor([2281701379],"float32"), fill_value=36, )

[accuracy error] paddle.full_like(Tensor([2281701379],"float32"), fill_value=36, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 36.
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)
 y: array([36., 36., 36., ..., 36., 36., 36.], dtype=float32)
2025-04-22 13:32:30.000561 test begin: paddle.full_like(Tensor([2281701379],"float32"), fill_value=5, )

[accuracy error] paddle.full_like(Tensor([2281701379],"float32"), fill_value=5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 5.
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)
 y: array([5., 5., 5., ..., 5., 5., 5.], dtype=float32)
2025-04-22 13:34:34.176587 test begin: paddle.full_like(Tensor([2281701379],"float32"), fill_value=511, )

[accuracy error] paddle.full_like(Tensor([2281701379],"float32"), fill_value=511, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 511.
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)
 y: array([511., 511., 511., ..., 511., 511., 511.], dtype=float32)
2025-04-22 13:36:43.761247 test begin: paddle.full_like(Tensor([2281701379],"float32"), fill_value=7, )

[accuracy error] paddle.full_like(Tensor([2281701379],"float32"), fill_value=7, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 7.
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)
 y: array([7., 7., 7., ..., 7., 7., 7.], dtype=float32)
2025-04-22 13:38:53.413880 test begin: paddle.full_like(Tensor([2281701379],"int64"), 1, )

[accuracy error] paddle.full_like(Tensor([2281701379],"int64"), 1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 1
Max relative difference: 1.
 x: array([0, 0, 0, ..., 0, 0, 0])
 y: array([1, 1, 1, ..., 1, 1, 1])
2025-04-22 13:43:06.930862 test begin: paddle.full_like(Tensor([2281701379],"int64"), 10, )

[accuracy error] paddle.full_like(Tensor([2281701379],"int64"), 10, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 10
Max relative difference: 1.
 x: array([0, 0, 0, ..., 0, 0, 0])
 y: array([10, 10, 10, ..., 10, 10, 10])
2025-04-22 13:47:05.655413 test begin: paddle.full_like(Tensor([2281701379],"int64"), 11, )

[accuracy error] paddle.full_like(Tensor([2281701379],"int64"), 11, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 11
Max relative difference: 1.
 x: array([0, 0, 0, ..., 0, 0, 0])
 y: array([11, 11, 11, ..., 11, 11, 11])
2025-04-22 13:51:25.492753 test begin: paddle.full_like(Tensor([2281701379],"int64"), 12, )

[accuracy error] paddle.full_like(Tensor([2281701379],"int64"), 12, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 12
Max relative difference: 1.
 x: array([0, 0, 0, ..., 0, 0, 0])
 y: array([12, 12, 12, ..., 12, 12, 12])
2025-04-22 13:55:29.565583 test begin: paddle.full_like(Tensor([2281701379],"int64"), 13, )

[accuracy error] paddle.full_like(Tensor([2281701379],"int64"), 13, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 13
Max relative difference: 1.
 x: array([0, 0, 0, ..., 0, 0, 0])
 y: array([13, 13, 13, ..., 13, 13, 13])
2025-04-22 14:00:03.733508 test begin: paddle.full_like(Tensor([2281701379],"int64"), 14, )

[accuracy error] paddle.full_like(Tensor([2281701379],"int64"), 14, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 14
Max relative difference: 1.
 x: array([0, 0, 0, ..., 0, 0, 0])
 y: array([14, 14, 14, ..., 14, 14, 14])
2025-04-22 14:04:07.250721 test begin: paddle.full_like(Tensor([2281701379],"int64"), 15, )

[accuracy error] paddle.full_like(Tensor([2281701379],"int64"), 15, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 15
Max relative difference: 1.
 x: array([0, 0, 0, ..., 0, 0, 0])
 y: array([15, 15, 15, ..., 15, 15, 15])
2025-04-22 14:08:10.860046 test begin: paddle.full_like(Tensor([2281701379],"int64"), 2, )

[accuracy error] paddle.full_like(Tensor([2281701379],"int64"), 2, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 2
Max relative difference: 1.
 x: array([0, 0, 0, ..., 0, 0, 0])
 y: array([2, 2, 2, ..., 2, 2, 2])
2025-04-22 14:12:15.359193 test begin: paddle.full_like(Tensor([2281701379],"int64"), 3, )

[accuracy error] paddle.full_like(Tensor([2281701379],"int64"), 3, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 3
Max relative difference: 1.
 x: array([0, 0, 0, ..., 0, 0, 0])
 y: array([3, 3, 3, ..., 3, 3, 3])
2025-04-22 14:16:17.619803 test begin: paddle.full_like(Tensor([2281701379],"int64"), 4, )

[accuracy error] paddle.full_like(Tensor([2281701379],"int64"), 4, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 4
Max relative difference: 1.
 x: array([0, 0, 0, ..., 0, 0, 0])
 y: array([4, 4, 4, ..., 4, 4, 4])
2025-04-22 14:20:04.802078 test begin: paddle.full_like(Tensor([2281701379],"int64"), 5, )

[accuracy error] paddle.full_like(Tensor([2281701379],"int64"), 5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 5
Max relative difference: 1.
 x: array([0, 0, 0, ..., 0, 0, 0])
 y: array([5, 5, 5, ..., 5, 5, 5])
2025-04-22 14:24:00.511645 test begin: paddle.full_like(Tensor([2281701379],"int64"), 6, )

[accuracy error] paddle.full_like(Tensor([2281701379],"int64"), 6, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 6
Max relative difference: 1.
 x: array([0, 0, 0, ..., 0, 0, 0])
 y: array([6, 6, 6, ..., 6, 6, 6])
2025-04-22 14:27:58.441009 test begin: paddle.full_like(Tensor([2281701379],"int64"), 7, )

[accuracy error] paddle.full_like(Tensor([2281701379],"int64"), 7, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 7
Max relative difference: 1.
 x: array([0, 0, 0, ..., 0, 0, 0])
 y: array([7, 7, 7, ..., 7, 7, 7])
2025-04-22 14:31:54.342420 test begin: paddle.full_like(Tensor([2281701379],"int64"), 8, )

[accuracy error] paddle.full_like(Tensor([2281701379],"int64"), 8, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 8
Max relative difference: 1.
 x: array([0, 0, 0, ..., 0, 0, 0])
 y: array([8, 8, 8, ..., 8, 8, 8])
2025-04-22 14:35:51.968010 test begin: paddle.full_like(Tensor([2281701379],"int64"), 9, )

[accuracy error] paddle.full_like(Tensor([2281701379],"int64"), 9, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 9
Max relative difference: 1.
 x: array([0, 0, 0, ..., 0, 0, 0])
 y: array([9, 9, 9, ..., 9, 9, 9])
2025-04-22 14:40:13.818542 test begin: paddle.full_like(Tensor([228170138, 10],"int64"), 3, )

[accuracy error] paddle.full_like(Tensor([228170138, 10],"int64"), 3, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 3
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[3, 3, 3, ..., 3, 3, 3],
       [3, 3, 3, ..., 3, 3, 3],
       [3, 3, 3, ..., 3, 3, 3],...
2025-04-22 15:09:05.663541 test begin: paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[1,3,],list[1,0,],], )

[accuracy error] paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[1,3,],list[1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4218488192 / 4294967300 (98.2%)
Max absolute difference: 1.833
Max relative difference: 1.434
 x: array([[[[ 0.3438 ,  0.01501,  0.04297, -0.01982, -0.2026 ]],

        [[ 0.     ,  0.     ,  0.     ,  0.     ,  0.     ]],...
 y: array([[[[ 0.4644  , -0.28    , -0.099   , -0.1879  , -0.5215  ]],

        [[-0.2974  ,  0.782   ,  0.9043  ,  0.16    ,  0.4055  ]],...
2025-04-22 16:14:24.044844 test begin: paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[3,2,1,],list[2,0,1,],], )

[accuracy error] paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[3,2,1,],list[2,0,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 842121446 / 858993460 (98%)
Max absolute difference: 2.75
Max relative difference: 4.926
 x: array([[ 0.12085,  0.05646,  0.08246,  0.0961 , -0.0362 ],
       [ 0.     ,  0.     ,  0.     ,  0.     ,  0.     ],
       [ 0.     ,  0.     ,  0.     ,  0.     ,  0.     ],...
 y: array([[ 0.11945 ,  0.1181  ,  0.013916, -0.2258  , -0.3103  ],
       [ 0.03406 ,  0.1704  ,  0.024   ,  0.6865  ,  0.7637  ],
       [-0.4348  ,  0.1159  ,  0.4912  , -0.268   , -0.373   ],...
2025-04-23 10:43:24.468435 test begin: paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[0,3,2,1,],list[2,1,3,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[0,3,2,1,],list[2,1,3,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 526.5
Max relative difference: 0.3806
 x: array(-856.5, dtype=float16)
 y: array(-1383., dtype=float16)
2025-04-23 11:59:13.430723 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[1,2,3,0,],list[1,3,0,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[1,2,3,0,],list[1,3,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 262.
Max relative difference: 0.1895
 x: array(-1645., dtype=float16)
 y: array(-1383., dtype=float16)
2025-04-23 11:59:26.525871 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[1,2,3,0,],list[3,2,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[1,2,3,0,],list[3,2,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 262.
Max relative difference: 0.1895
 x: array(-1645., dtype=float16)
 y: array(-1383., dtype=float16)
2025-04-23 11:59:37.274945 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[1,3,2,0,],list[1,3,2,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[1,3,2,0,],list[1,3,2,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 262.
Max relative difference: 0.1895
 x: array(-1645., dtype=float16)
 y: array(-1383., dtype=float16)
2025-04-23 12:23:23.943943 test begin: paddle.triu(x=Tensor([3, 715827883, 2],"float16"), diagonal=-1, )

W0423 12:25:21.200744  4550 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0423 12:25:21.202000  4550 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.triu(x=Tensor([3, 715827883, 2],"float16"), diagonal=-1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 4294967298 (1.16e-07%)
Max absolute difference: 0.441
Max relative difference: 0.
 x: array([[[ 0.009476,  0.02628 ],
        [ 0.1406  , -0.0801  ],
        [ 0.      ,  0.06726 ],...
 y: array([[[ 0.009476,  0.02628 ],
        [ 0.1406  , -0.0801  ],
        [ 0.      ,  0.06726 ],...
2025-04-23 12:35:58.739497 test begin: paddle.triu(x=Tensor([3, 715827883, 2],"float16"), diagonal=-5, )

[accuracy error] paddle.triu(x=Tensor([3, 715827883, 2],"float16"), diagonal=-5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 13 / 4294967298 (3.03e-07%)
Max absolute difference: 0.441
Max relative difference: 0.
 x: array([[[ 0.009476,  0.02628 ],
        [ 0.1406  , -0.0801  ],
        [-0.2474  ,  0.06726 ],...
 y: array([[[ 0.009476,  0.02628 ],
        [ 0.1406  , -0.0801  ],
        [-0.2474  ,  0.06726 ],...
2025-04-21 14:35:08.086367 test begin: paddle.diff(Tensor([570425345, 4],"float32"), n=1, axis=0, prepend=Tensor([2, 4],"float32"), append=Tensor([2, 4],"float32"), )

W0421 14:36:21.972749 100205 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 14:36:21.973675 100205 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.diff(Tensor([570425345, 4],"float32"), n=1, axis=0, prepend=Tensor([2, 4],"float32"), append=Tensor([2, 4],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2235821702 / 2281701392 (98%)
Max absolute difference: 0.9999827
Max relative difference: 1.
 x: array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.],...
 y: array([[ 7.203296e-01, -4.043877e-04, -6.826711e-01,  1.023355e-01],
       [-7.203296e-01,  4.043877e-04,  6.826711e-01, -1.023355e-01],
       [ 7.203296e-01, -4.043877e-04, -6.826711e-01,  1.023355e-01],...
2025-04-21 14:38:00.210422 test begin: paddle.diff(Tensor([570425345, 4],"float32"), n=1, axis=0, prepend=Tensor([3, 4],"float32"), append=None, )

[accuracy error] paddle.diff(Tensor([570425345, 4],"float32"), n=1, axis=0, prepend=Tensor([3, 4],"float32"), append=None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2235821700 / 2281701388 (98%)
Max absolute difference: 0.9999827
Max relative difference: 1.
 x: array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.],...
 y: array([[ 7.203296e-01, -4.043877e-04, -6.826711e-01,  1.023355e-01],
       [-6.185815e-01, -2.887007e-01,  8.537814e-01,  6.439756e-01],
       [-1.017482e-01,  2.891051e-01, -1.711103e-01, -7.463111e-01],...
2025-04-21 14:40:09.277124 test begin: paddle.diff(Tensor([570425345, 4],"float32"), n=1, axis=0, prepend=Tensor([570425345, 4],"float32"), append=None, )

[accuracy error] paddle.diff(Tensor([570425345, 4],"float32"), n=1, axis=0, prepend=Tensor([570425345, 4],"float32"), append=None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4471643382 / 4563402756 (98%)
Max absolute difference: 0.9999827
Max relative difference: 1.
 x: array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.],...
 y: array([[ 7.203296e-01, -4.043877e-04, -6.826711e-01,  1.023355e-01],
       [-6.185815e-01, -2.887007e-01,  8.537814e-01,  6.439756e-01],
       [-8.210818e-02,  2.291050e-02, -2.656048e-01, -1.661430e-01],...
2025-04-21 14:45:10.667755 test begin: paddle.diff(Tensor([570425345, 4],"float32"), n=2, axis=0, prepend=None, append=Tensor([2, 4],"float32"), )

[accuracy error] paddle.diff(Tensor([570425345, 4],"float32"), n=2, axis=0, prepend=None, append=Tensor([2, 4],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2258651047 / 2281701380 (99%)
Max absolute difference: 1.9981426
Max relative difference: 1.
 x: array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.],...
 y: array([[-1.338911, -0.288296,  1.536453,  0.54164 ],
       [ 0.536473,  0.311611, -1.119386, -0.810119],
       [ 0.133966,  0.404186,  0.16979 , -0.373856],...
2025-04-21 14:47:02.746170 test begin: paddle.diff(Tensor([570425345, 4],"float32"), n=2, axis=0, prepend=None, append=Tensor([570425345, 4],"float32"), )

[accuracy error] paddle.diff(Tensor([570425345, 4],"float32"), n=2, axis=0, prepend=None, append=Tensor([570425345, 4],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4517302086 / 4563402752 (99%)
Max absolute difference: 1.9981426
Max relative difference: 1.
 x: array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.],...
 y: array([[-1.338911, -0.288296,  1.536453,  0.54164 ],
       [ 0.536473,  0.311611, -1.119386, -0.810119],
       [ 0.133966,  0.404186,  0.16979 , -0.373856],...
2025-04-21 14:52:06.945622 test begin: paddle.diff(Tensor([570425345, 4],"float32"), n=2, axis=0, prepend=Tensor([2, 4],"float32"), append=None, )

[accuracy error] paddle.diff(Tensor([570425345, 4],"float32"), n=2, axis=0, prepend=Tensor([2, 4],"float32"), append=None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2258651045 / 2281701380 (99%)
Max absolute difference: 1.9981426
Max relative difference: 1.
 x: array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.],...
 y: array([[-1.440659e+00,  8.087754e-04,  1.365342e+00, -2.046709e-01],
       [ 1.440659e+00, -8.087754e-04, -1.365342e+00,  2.046709e-01],
       [-1.338911e+00, -2.882963e-01,  1.536453e+00,  5.416401e-01],...
2025-04-21 14:54:57.051999 test begin: paddle.diff(Tensor([570425345, 4],"float32"), n=2, axis=0, prepend=Tensor([2, 4],"float32"), append=Tensor([2, 4],"float32"), )

[accuracy error] paddle.diff(Tensor([570425345, 4],"float32"), n=2, axis=0, prepend=Tensor([2, 4],"float32"), append=Tensor([2, 4],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2258651053 / 2281701388 (99%)
Max absolute difference: 1.9981426
Max relative difference: 1.
 x: array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.],...
 y: array([[-1.440659e+00,  8.087754e-04,  1.365342e+00, -2.046709e-01],
       [ 1.440659e+00, -8.087754e-04, -1.365342e+00,  2.046709e-01],
       [-1.338911e+00, -2.882963e-01,  1.536453e+00,  5.416401e-01],...
2025-04-21 14:57:16.106325 test begin: paddle.diff(Tensor([570425345, 4],"float32"), n=2, axis=0, prepend=Tensor([570425345, 4],"float32"), append=None, )

[accuracy error] paddle.diff(Tensor([570425345, 4],"float32"), n=2, axis=0, prepend=Tensor([570425345, 4],"float32"), append=None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4517302086 / 4563402752 (99%)
Max absolute difference: 1.9981426
Max relative difference: 1.
 x: array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.],...
 y: array([[-1.338911, -0.288296,  1.536453,  0.54164 ],
       [ 0.536473,  0.311611, -1.119386, -0.810119],
       [ 0.133966,  0.404186,  0.16979 , -0.373856],...
2025-04-21 15:03:20.825390 test begin: paddle.diff(x=Tensor([10, 429496730],"float16"), axis=0, prepend=Tensor([4, 429496730],"float16"), append=Tensor([4, 429496730],"float16"), )

[accuracy error] paddle.diff(x=Tensor([10, 429496730],"float16"), axis=0, prepend=Tensor([4, 429496730],"float16"), append=Tensor([4, 429496730],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 7154575644 / 7301444410 (98%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[ 0.2461 , -0.1431 , -0.1133 , ...,  0.07904, -0.05212, -0.06152],
       [-0.2607 ,  0.03052,  0.1776 , ...,  0.1362 , -0.3604 , -0.834  ],
       [-0.1155 ,  0.7153 , -0.2477 , ..., -0.4238 , -0.08215,  0.5493 ],...
2025-04-21 15:27:57.865047 test begin: paddle.diff(x=Tensor([10, 4],"float16"), axis=0, prepend=Tensor([1073741825, 4],"float16"), append=Tensor([4, 4],"float16"), )

[accuracy error] paddle.diff(x=Tensor([10, 4],"float16"), axis=0, prepend=Tensor([1073741825, 4],"float16"), append=Tensor([4, 4],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208574561 / 4294967352 (98%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.],...
 y: array([[-5.2148e-01, -1.1035e-01,  5.6543e-01, -1.2207e-04],
       [ 5.1270e-02,  2.5879e-02, -6.4648e-01,  6.7993e-02],
       [ 4.0088e-01,  6.9629e-01,  5.6250e-01,  1.8677e-01],...
2025-04-21 15:41:09.040930 test begin: paddle.diff(x=Tensor([10, 4],"float16"), axis=0, prepend=Tensor([4, 4],"float16"), append=Tensor([1073741825, 4],"float16"), )

[accuracy error] paddle.diff(x=Tensor([10, 4],"float16"), axis=0, prepend=Tensor([4, 4],"float16"), append=Tensor([1073741825, 4],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208574561 / 4294967352 (98%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.],...
 y: array([[-5.2148e-01, -1.1035e-01,  5.6543e-01, -1.2207e-04],
       [ 5.1270e-02,  2.5879e-02, -6.4648e-01,  6.7993e-02],
       [ 4.0088e-01,  6.9629e-01,  5.6250e-01,  1.8677e-01],...
2025-04-21 15:54:03.728160 test begin: paddle.diff(x=Tensor([1073741825, 4],"float16"), axis=0, prepend=Tensor([4, 4],"float16"), append=Tensor([4, 4],"float16"), )

[accuracy error] paddle.diff(x=Tensor([1073741825, 4],"float16"), axis=0, prepend=Tensor([4, 4],"float16"), append=Tensor([4, 4],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208574538 / 4294967328 (98%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.],...
 y: array([[-5.2148e-01, -1.1035e-01,  5.6543e-01, -1.2207e-04],
       [ 5.1270e-02,  2.5879e-02, -6.4648e-01,  6.7993e-02],
       [ 4.0088e-01,  6.9629e-01,  5.6250e-01,  1.8677e-01],...
2025-04-21 16:07:01.265909 test begin: paddle.diff(x=Tensor([10],"float16"), prepend=Tensor([4294967297],"float16"), )

[accuracy error] paddle.diff(x=Tensor([10],"float16"), prepend=Tensor([4294967297],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208570587 / 4294967306 (98%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float16)
 y: array([-0.4294 ,  0.1311 ,  0.01306, ..., -0.5527 , -0.1846 , -0.04395],
      dtype=float16)
2025-04-21 16:20:45.637859 test begin: paddle.diff(x=Tensor([10],"float16"), prepend=Tensor([4294967297],"float16"), append=Tensor([4],"float16"), )

[accuracy error] paddle.diff(x=Tensor([10],"float16"), prepend=Tensor([4294967297],"float16"), append=Tensor([4],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208570591 / 4294967310 (98%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float16)
 y: array([-0.4294 ,  0.1311 ,  0.01306, ..., -0.4294 ,  0.1311 ,  0.01306],
      dtype=float16)
2025-04-21 16:34:15.625640 test begin: paddle.diff(x=Tensor([10],"float16"), prepend=Tensor([4],"float16"), append=Tensor([4294967297],"float16"), )

[accuracy error] paddle.diff(x=Tensor([10],"float16"), prepend=Tensor([4],"float16"), append=Tensor([4294967297],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208570591 / 4294967310 (98%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float16)
 y: array([-0.4294 ,  0.1311 ,  0.01306, ...,  0.1196 ,  0.6035 , -0.4521 ],
      dtype=float16)
2025-04-21 16:46:54.205699 test begin: paddle.diff(x=Tensor([2281701379],"float32"), )

[accuracy error] paddle.diff(x=Tensor([2281701379],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2235837274 / 2281701378 (98%)
Max absolute difference: 0.9999871
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)
 y: array([ 0.473377,  0.073578, -0.586193, ..., -0.64248 , -0.019695,
        0.433382], dtype=float32)
2025-04-21 16:49:01.417883 test begin: paddle.diff(x=Tensor([2281701379],"int32"), )

[accuracy error] paddle.diff(x=Tensor([2281701379],"int32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281683894 / 2281701378 (100%)
Max absolute difference: 131067
Max relative difference: 1.
 x: array([0, 0, 0, ..., 0, 0, 0], dtype=int32)
 y: array([115039, -55068, -17327, ...,  70232, -45945, -41732], dtype=int32)
2025-04-21 16:53:10.314306 test begin: paddle.diff(x=Tensor([2281701379],"int64"), )

[accuracy error] paddle.diff(x=Tensor([2281701379],"int64"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281684002 / 2281701378 (100%)
Max absolute difference: 131068
Max relative difference: 1.
 x: array([0, 0, 0, ..., 0, 0, 0])
 y: array([-26213, -26298,   5149, ...,  -1117, -23338, -36981])
2025-04-21 16:58:27.926201 test begin: paddle.diff(x=Tensor([4, 1073741825],"float16"), )

[cuda error] paddle.diff(x=Tensor([4, 1073741825],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 16:58:39.576516 test begin: paddle.diff(x=Tensor([4, 4, 268435457],"float16"), )

[cuda error] paddle.diff(x=Tensor([4, 4, 268435457],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 16:58:49.787019 test begin: paddle.diff(x=Tensor([4294967297],"float16"), )

[accuracy error] paddle.diff(x=Tensor([4294967297],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208570577 / 4294967296 (98%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float16)
 y: array([-0.4294 ,  0.1311 ,  0.01306, ...,  0.1196 ,  0.6035 , -0.4521 ],
      dtype=float16)
2025-04-21 17:11:32.460023 test begin: paddle.diff(x=Tensor([4294967297],"float16"), prepend=Tensor([4294967297],"float16"), )

[accuracy error] paddle.diff(x=Tensor([4294967297],"float16"), prepend=Tensor([4294967297],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 8417141155 / 8589934593 (98%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float16)
 y: array([-0.4294 ,  0.1311 ,  0.01306, ...,  0.1196 ,  0.6035 , -0.4521 ],
      dtype=float16)
2025-04-21 17:38:32.121020 test begin: paddle.diff(x=Tensor([4294967297],"float16"), prepend=Tensor([4],"float16"), )

[accuracy error] paddle.diff(x=Tensor([4294967297],"float16"), prepend=Tensor([4],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208570581 / 4294967300 (98%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float16)
 y: array([-0.4294 ,  0.1311 ,  0.01306, ...,  0.1196 ,  0.6035 , -0.4521 ],
      dtype=float16)
2025-04-21 17:52:11.783074 test begin: paddle.diff(x=Tensor([4294967297],"float16"), prepend=Tensor([4],"float16"), append=Tensor([4],"float16"), )

[accuracy error] paddle.diff(x=Tensor([4294967297],"float16"), prepend=Tensor([4],"float16"), append=Tensor([4],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208570585 / 4294967304 (98%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float16)
 y: array([-0.4294 ,  0.1311 ,  0.01306, ..., -0.4294 ,  0.1311 ,  0.01306],
      dtype=float16)
2025-04-21 18:05:11.276047 test begin: paddle.diff(x=Tensor([67108865, 4, 4, 4],"float16"), )

[cuda error] paddle.diff(x=Tensor([67108865, 4, 4, 4],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 18:05:23.685417 test begin: paddle.diff(x=Tensor([67108865, 4, 4, 4],"float16"), axis=-2, )

[cuda error] paddle.diff(x=Tensor([67108865, 4, 4, 4],"float16"), axis=-2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 18:05:33.291875 test begin: paddle.diff(x=Tensor([67108865, 4, 4, 4],"float16"), axis=2, )

[cuda error] paddle.diff(x=Tensor([67108865, 4, 4, 4],"float16"), axis=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 18:05:42.105618 test begin: paddle.digamma(Tensor([10, 10, 10, 4294968],"float16"), )

[accuracy error] paddle.digamma(Tensor([10, 10, 10, 4294968],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[[-8.5469e+00,  2.0195e+00,  4.7773e+00, ..., -2.2656e+00,
           1.1617e+01, -5.9102e+00],
         [-9.1600e+03, -3.6758e+00,  1.2438e+01, ..., -4.1367e+00,...
 y: array([[[[-8.5469e+00,  2.0195e+00,  4.7773e+00, ..., -2.2656e+00,
           1.1617e+01, -5.9102e+00],
         [-9.1600e+03, -3.6758e+00,  1.2438e+01, ..., -4.1367e+00,...
2025-04-21 18:06:30.003423 test begin: paddle.digamma(Tensor([10, 10, 21474837, 2],"float16"), )

[accuracy error] paddle.digamma(Tensor([10, 10, 21474837, 2],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[[-8.5469e+00,  2.0195e+00],
         [ 4.7773e+00,  5.2617e+00],
         [ 9.7266e-01,  7.9199e-01],...
 y: array([[[[-8.5469e+00,  2.0195e+00],
         [ 4.7773e+00,  5.2617e+00],
         [ 9.7266e-01,  7.9199e-01],...
2025-04-21 18:07:35.332696 test begin: paddle.digamma(Tensor([10, 10, 42949673],"float16"), )

[accuracy error] paddle.digamma(Tensor([10, 10, 42949673],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[-8.5469e+00,  2.0195e+00,  4.7773e+00, ..., -3.1484e+00,
         -9.6875e+00, -8.9375e+00],
        [ 1.0928e+00,  1.6357e+00,  5.0117e+00, ..., -3.7227e+00,...
 y: array([[[-8.5469e+00,  2.0195e+00,  4.7773e+00, ..., -3.1484e+00,
         -9.6875e+00, -8.9375e+00],
        [ 1.0928e+00,  1.6357e+00,  5.0117e+00, ..., -3.7227e+00,...
2025-04-21 18:08:39.748442 test begin: paddle.digamma(Tensor([10, 21474837, 10, 2],"float16"), )

[accuracy error] paddle.digamma(Tensor([10, 21474837, 10, 2],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[[-8.5469e+00,  2.0195e+00],
         [ 4.7773e+00,  5.2617e+00],
         [ 9.7266e-01,  7.9199e-01],...
 y: array([[[[-8.5469e+00,  2.0195e+00],
         [ 4.7773e+00,  5.2617e+00],
         [ 9.7266e-01,  7.9199e-01],...
2025-04-21 18:09:31.913402 test begin: paddle.digamma(Tensor([10, 42949673, 10],"float16"), )

[accuracy error] paddle.digamma(Tensor([10, 42949673, 10],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[-8.5469e+00,  2.0195e+00,  4.7773e+00, ...,  5.2578e+00,
          1.5166e+00,  1.0459e+00],
        [ 2.7988e+00,  9.7969e+00, -1.9188e+01, ..., -2.4121e+00,...
 y: array([[[-8.5469e+00,  2.0195e+00,  4.7773e+00, ...,  5.2578e+00,
          1.5166e+00,  1.0459e+00],
        [ 2.7988e+00,  9.7969e+00, -1.9188e+01, ..., -2.4121e+00,...
2025-04-21 18:10:23.837014 test begin: paddle.digamma(Tensor([1398102, 3, 32, 32],"float16"), )

[accuracy error] paddle.digamma(Tensor([1398102, 3, 32, 32],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[[-8.5469e+00,  2.0195e+00,  4.7773e+00, ...,  1.0086e+01,
           5.4844e+00, -2.5371e+00],
         [ 9.3408e-01, -2.7090e+00, -2.7266e+00, ...,  1.8344e+01,...
 y: array([[[[-8.5469e+00,  2.0195e+00,  4.7773e+00, ...,  1.0086e+01,
           5.4844e+00, -2.5371e+00],
         [ 9.3408e-01, -2.7090e+00, -2.7266e+00, ...,  1.8344e+01,...
2025-04-21 18:11:16.175938 test begin: paddle.digamma(Tensor([2, 1073741825, 2],"float16"), )

[accuracy error] paddle.digamma(Tensor([2, 1073741825, 2],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[ -8.55  ,   2.02  ],
        [  4.777 ,   5.26  ],
        [  0.9727,   0.792 ],...
 y: array([[[ -8.55  ,   2.02  ],
        [  4.777 ,   5.26  ],
        [  0.9727,   0.792 ],...
2025-04-21 18:12:09.772463 test begin: paddle.digamma(Tensor([2, 5, 429496730],"float16"), )

[accuracy error] paddle.digamma(Tensor([2, 5, 429496730],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[-8.5469e+00,  2.0195e+00,  4.7773e+00, ...,  1.3488e+02,
         -4.2930e+00, -2.1914e+00],
        [-2.8066e+00,  4.8682e-01,  2.2695e+00, ..., -1.4422e+01,...
 y: array([[[-8.5469e+00,  2.0195e+00,  4.7773e+00, ...,  1.3488e+02,
         -4.2930e+00, -2.1914e+00],
        [-2.8066e+00,  4.8682e-01,  2.2695e+00, ..., -1.4422e+01,...
2025-04-21 18:13:07.634732 test begin: paddle.digamma(Tensor([21474837, 10, 10, 2],"float16"), )

[accuracy error] paddle.digamma(Tensor([21474837, 10, 10, 2],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[[-8.5469e+00,  2.0195e+00],
         [ 4.7773e+00,  5.2617e+00],
         [ 9.7266e-01,  7.9199e-01],...
 y: array([[[[-8.5469e+00,  2.0195e+00],
         [ 4.7773e+00,  5.2617e+00],
         [ 9.7266e-01,  7.9199e-01],...
2025-04-21 18:14:10.030638 test begin: paddle.digamma(Tensor([4294967297],"float16"), )

[accuracy error] paddle.digamma(Tensor([4294967297],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([ -8.55 ,   2.02 ,   4.777, ...,   6.223,  -2.164, -99.9  ],
      dtype=float16)
 y: array([ -8.55 ,   2.02 ,   4.777, ...,   6.223,  -2.164, -99.9  ],
      dtype=float16)
2025-04-21 18:15:15.923971 test begin: paddle.digamma(Tensor([42949673, 10, 10],"float16"), )

[accuracy error] paddle.digamma(Tensor([42949673, 10, 10],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[-8.5469e+00,  2.0195e+00,  4.7773e+00, ...,  5.2578e+00,
          1.5166e+00,  1.0459e+00],
        [ 2.7988e+00,  9.7969e+00, -1.9188e+01, ..., -2.4121e+00,...
 y: array([[[-8.5469e+00,  2.0195e+00,  4.7773e+00, ...,  5.2578e+00,
          1.5166e+00,  1.0459e+00],
        [ 2.7988e+00,  9.7969e+00, -1.9188e+01, ..., -2.4121e+00,...
2025-04-21 18:16:14.873312 test begin: paddle.digamma(Tensor([429496730, 5, 2],"float16"), )

[accuracy error] paddle.digamma(Tensor([429496730, 5, 2],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[ -8.55  ,   2.02  ],
        [  4.777 ,   5.26  ],
        [  0.9727,   0.792 ],...
 y: array([[[ -8.55  ,   2.02  ],
        [  4.777 ,   5.26  ],
        [  0.9727,   0.792 ],...
2025-04-21 18:17:15.996639 test begin: paddle.digamma(Tensor([8, 3, 32, 5592406],"float16"), )

[accuracy error] paddle.digamma(Tensor([8, 3, 32, 5592406],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[[-8.5469e+00,  2.0195e+00,  4.7773e+00, ...,  3.9031e+01,
           7.6855e-01, -1.0141e+01],
         [ 6.0486e-02, -2.1543e+00, -2.3016e+01, ..., -2.0430e+00,...
 y: array([[[[-8.5469e+00,  2.0195e+00,  4.7773e+00, ...,  3.9031e+01,
           7.6855e-01, -1.0141e+01],
         [ 6.0486e-02, -2.1543e+00, -2.3016e+01, ..., -2.0430e+00,...
2025-04-21 18:18:10.634314 test begin: paddle.digamma(Tensor([8, 3, 5592406, 32],"float16"), )

[accuracy error] paddle.digamma(Tensor([8, 3, 5592406, 32],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[[-8.5469e+00,  2.0195e+00,  4.7773e+00, ...,  1.0086e+01,
           5.4844e+00, -2.5371e+00],
         [ 9.3408e-01, -2.7090e+00, -2.7266e+00, ...,  1.8344e+01,...
 y: array([[[[-8.5469e+00,  2.0195e+00,  4.7773e+00, ...,  1.0086e+01,
           5.4844e+00, -2.5371e+00],
         [ 9.3408e-01, -2.7090e+00, -2.7266e+00, ...,  1.8344e+01,...
2025-04-21 18:19:05.068576 test begin: paddle.digamma(Tensor([8, 524289, 32, 32],"float16"), )

[accuracy error] paddle.digamma(Tensor([8, 524289, 32, 32],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[[-8.5469e+00,  2.0195e+00,  4.7773e+00, ...,  1.0086e+01,
           5.4844e+00, -2.5371e+00],
         [ 9.3408e-01, -2.7090e+00, -2.7266e+00, ...,  1.8344e+01,...
 y: array([[[[-8.5469e+00,  2.0195e+00,  4.7773e+00, ...,  1.0086e+01,
           5.4844e+00, -2.5371e+00],
         [ 9.3408e-01, -2.7090e+00, -2.7266e+00, ...,  1.8344e+01,...
2025-04-21 18:20:08.487596 test begin: paddle.digamma(x=Tensor([119304648, 6, 6],"float16"), )

[accuracy error] paddle.digamma(x=Tensor([119304648, 6, 6],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[-8.5469e+00,  2.0195e+00,  4.7773e+00,  5.2617e+00,
          9.7266e-01,  7.9199e-01],
        [-2.6367e+00,  5.2578e+00,  1.5166e+00,  1.0459e+00,...
 y: array([[[-8.5469e+00,  2.0195e+00,  4.7773e+00,  5.2617e+00,
          9.7266e-01,  7.9199e-01],
        [-2.6367e+00,  5.2578e+00,  1.5166e+00,  1.0459e+00,...
2025-04-21 18:21:01.627647 test begin: paddle.digamma(x=Tensor([1431655766, 3],"float16"), )

[accuracy error] paddle.digamma(x=Tensor([1431655766, 3],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[ -8.55  ,   2.02  ,   4.777 ],
       [  5.26  ,   0.9727,   0.792 ],
       [ -2.637 ,   5.258 ,   1.517 ],...
 y: array([[ -8.55  ,   2.02  ,   4.777 ],
       [  5.26  ,   0.9727,   0.792 ],
       [ -2.637 ,   5.258 ,   1.517 ],...
2025-04-21 18:21:57.205424 test begin: paddle.digamma(x=Tensor([19884108, 6, 6, 6],"float16"), )

[accuracy error] paddle.digamma(x=Tensor([19884108, 6, 6, 6],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[[-8.5469e+00,  2.0195e+00,  4.7773e+00,  5.2617e+00,
           9.7266e-01,  7.9199e-01],
         [-2.6367e+00,  5.2578e+00,  1.5166e+00,  1.0459e+00,...
 y: array([[[[-8.5469e+00,  2.0195e+00,  4.7773e+00,  5.2617e+00,
           9.7266e-01,  7.9199e-01],
         [-2.6367e+00,  5.2578e+00,  1.5166e+00,  1.0459e+00,...
2025-04-21 18:23:01.855632 test begin: paddle.digamma(x=Tensor([3, 1431655766],"float16"), )

[accuracy error] paddle.digamma(x=Tensor([3, 1431655766],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[ -8.55 ,   2.02 ,   4.777, ...,  -3.31 , -14.26 ,  -1.989],
       [  1.719,  -2.857,   4.96 , ...,  -3.934,   6.156, -23.19 ],
       [  2.68 , -17.88 ,   5.273, ...,  -2.164, -99.9  ,   9.445]],
      dtype=float16)
 y: array([[ -8.55 ,   2.02 ,   4.777, ...,  -3.31 , -14.26 ,  -1.989],
       [  1.719,  -2.857,   4.96 , ...,  -3.934,   6.156, -23.19 ],
       [  2.68 , -17.88 ,   5.273, ...,  -2.164, -99.9  ,   9.445]],
      dtype=float16)
2025-04-21 18:23:58.963247 test begin: paddle.digamma(x=Tensor([3, 6, 6, 6, 6628036],"float16"), )

[accuracy error] paddle.digamma(x=Tensor([3, 6, 6, 6, 6628036],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[[[-8.5469e+00,  2.0195e+00,  4.7773e+00, ..., -2.6758e+00,
            3.1177e-01,  3.8262e+00],
          [-3.5938e+00, -3.6094e+00, -2.0605e+00, ...,  2.4336e+00,...
 y: array([[[[[-8.5469e+00,  2.0195e+00,  4.7773e+00, ..., -2.6758e+00,
            3.1177e-01,  3.8262e+00],
          [-3.5938e+00, -3.6094e+00, -2.0605e+00, ...,  2.4336e+00,...
2025-04-21 18:24:53.093152 test begin: paddle.digamma(x=Tensor([3, 6, 6, 6628036, 6],"float16"), )

[accuracy error] paddle.digamma(x=Tensor([3, 6, 6, 6628036, 6],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[[[-8.5469e+00,  2.0195e+00,  4.7773e+00,  5.2617e+00,
            9.7266e-01,  7.9199e-01],
          [-2.6367e+00,  5.2578e+00,  1.5166e+00,  1.0459e+00,...
 y: array([[[[[-8.5469e+00,  2.0195e+00,  4.7773e+00,  5.2617e+00,
            9.7266e-01,  7.9199e-01],
          [-2.6367e+00,  5.2578e+00,  1.5166e+00,  1.0459e+00,...
2025-04-21 18:25:42.083482 test begin: paddle.digamma(x=Tensor([3, 6, 6628036, 6, 6],"float16"), )

[accuracy error] paddle.digamma(x=Tensor([3, 6, 6628036, 6, 6],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[[[-8.5469e+00,  2.0195e+00,  4.7773e+00,  5.2617e+00,
            9.7266e-01,  7.9199e-01],
          [-2.6367e+00,  5.2578e+00,  1.5166e+00,  1.0459e+00,...
 y: array([[[[[-8.5469e+00,  2.0195e+00,  4.7773e+00,  5.2617e+00,
            9.7266e-01,  7.9199e-01],
          [-2.6367e+00,  5.2578e+00,  1.5166e+00,  1.0459e+00,...
2025-04-21 18:26:35.929969 test begin: paddle.digamma(x=Tensor([3, 6628036, 6, 6, 6],"float16"), )

[accuracy error] paddle.digamma(x=Tensor([3, 6628036, 6, 6, 6],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[[[-8.5469e+00,  2.0195e+00,  4.7773e+00,  5.2617e+00,
            9.7266e-01,  7.9199e-01],
          [-2.6367e+00,  5.2578e+00,  1.5166e+00,  1.0459e+00,...
 y: array([[[[[-8.5469e+00,  2.0195e+00,  4.7773e+00,  5.2617e+00,
            9.7266e-01,  7.9199e-01],
          [-2.6367e+00,  5.2578e+00,  1.5166e+00,  1.0459e+00,...
2025-04-21 18:27:29.779806 test begin: paddle.digamma(x=Tensor([3314018, 6, 6, 6, 6],"float16"), )

[accuracy error] paddle.digamma(x=Tensor([3314018, 6, 6, 6, 6],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[[[-8.5469e+00,  2.0195e+00,  4.7773e+00,  5.2617e+00,
            9.7266e-01,  7.9199e-01],
          [-2.6367e+00,  5.2578e+00,  1.5166e+00,  1.0459e+00,...
 y: array([[[[[-8.5469e+00,  2.0195e+00,  4.7773e+00,  5.2617e+00,
            9.7266e-01,  7.9199e-01],
          [-2.6367e+00,  5.2578e+00,  1.5166e+00,  1.0459e+00,...
2025-04-21 18:28:33.760480 test begin: paddle.digamma(x=Tensor([6, 119304648, 6],"float16"), )

[accuracy error] paddle.digamma(x=Tensor([6, 119304648, 6],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[-8.5469e+00,  2.0195e+00,  4.7773e+00,  5.2617e+00,
          9.7266e-01,  7.9199e-01],
        [-2.6367e+00,  5.2578e+00,  1.5166e+00,  1.0459e+00,...
 y: array([[[-8.5469e+00,  2.0195e+00,  4.7773e+00,  5.2617e+00,
          9.7266e-01,  7.9199e-01],
        [-2.6367e+00,  5.2578e+00,  1.5166e+00,  1.0459e+00,...
2025-04-21 18:29:36.588524 test begin: paddle.digamma(x=Tensor([6, 19884108, 6, 6],"float16"), )

[accuracy error] paddle.digamma(x=Tensor([6, 19884108, 6, 6],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[[-8.5469e+00,  2.0195e+00,  4.7773e+00,  5.2617e+00,
           9.7266e-01,  7.9199e-01],
         [-2.6367e+00,  5.2578e+00,  1.5166e+00,  1.0459e+00,...
 y: array([[[[-8.5469e+00,  2.0195e+00,  4.7773e+00,  5.2617e+00,
           9.7266e-01,  7.9199e-01],
         [-2.6367e+00,  5.2578e+00,  1.5166e+00,  1.0459e+00,...
2025-04-21 18:30:49.813843 test begin: paddle.digamma(x=Tensor([6, 6, 119304648],"float16"), )

[accuracy error] paddle.digamma(x=Tensor([6, 6, 119304648],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[-8.5469e+00,  2.0195e+00,  4.7773e+00, ...,  2.7949e+00,
         -5.0234e+00,  8.3594e+00],
        [ 2.9844e+00,  2.0488e+00, -3.6289e+00, ..., -6.1797e+00,...
 y: array([[[-8.5469e+00,  2.0195e+00,  4.7773e+00, ...,  2.7949e+00,
         -5.0234e+00,  8.3594e+00],
        [ 2.9844e+00,  2.0488e+00, -3.6289e+00, ..., -6.1797e+00,...
2025-04-21 18:31:43.878493 test begin: paddle.digamma(x=Tensor([6, 6, 19884108, 6],"float16"), )

[accuracy error] paddle.digamma(x=Tensor([6, 6, 19884108, 6],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[[-8.5469e+00,  2.0195e+00,  4.7773e+00,  5.2617e+00,
           9.7266e-01,  7.9199e-01],
         [-2.6367e+00,  5.2578e+00,  1.5166e+00,  1.0459e+00,...
 y: array([[[[-8.5469e+00,  2.0195e+00,  4.7773e+00,  5.2617e+00,
           9.7266e-01,  7.9199e-01],
         [-2.6367e+00,  5.2578e+00,  1.5166e+00,  1.0459e+00,...
2025-04-21 18:32:40.875930 test begin: paddle.digamma(x=Tensor([6, 6, 6, 19884108],"float16"), )

[accuracy error] paddle.digamma(x=Tensor([6, 6, 6, 19884108],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[[-8.5469e+00,  2.0195e+00,  4.7773e+00, ...,  8.3359e+00,
          -2.0547e+00,  2.4336e+00],
         [-2.0656e+01,  3.6719e+00, -4.5938e+01, ...,  1.7334e+00,...
 y: array([[[[-8.5469e+00,  2.0195e+00,  4.7773e+00, ...,  8.3359e+00,
          -2.0547e+00,  2.4336e+00],
         [-2.0656e+01,  3.6719e+00, -4.5938e+01, ...,  1.7334e+00,...
2025-04-21 18:33:42.414653 test begin: paddle.dist(Tensor([1140850690, 2],"float32"), Tensor([1140850690, 2],"float32"), 0, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_dist(_object*, _object*, _object*)
1   dist_ad_func(paddle::Tensor const&, paddle::Tensor const&, float)
2   paddle::experimental::dist(paddle::Tensor const&, paddle::Tensor const&, float)
3   void phi::DistKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, float, phi::DenseTensor*)
4   void phi::funcs::ReduceKernel<float, float, phi::kps::AddFunctor, phi::kps::IdentityFunctor<float, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<float, float> const&, std::vector<int, std::allocator<int> > const&)
5   phi::DenseTensor::~DenseTensor()
6   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
7   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745231634 (unix time) try "date -d @1745231634" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x186f5) received by PID 100085 (TID 0x7f3cbb87e700) from PID 100085 ***]

2025-04-21 18:34:41.083010 test begin: paddle.dist(Tensor([190141782, 2, 3, 2],"float32"), Tensor([1, 1, 3, 1],"float32"), 2, )

W0421 18:35:58.775264 13239 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:35:58.780483 13239 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.dist(Tensor([190141782, 2, 3, 2],"float32"), Tensor([1, 1, 3, 1],"float32"), 2, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 17434.36
Max relative difference: 1.
 x: array(0., dtype=float32)
 y: array(17434.36, dtype=float32)
2025-04-21 18:35:59.322684 test begin: paddle.dist(Tensor([190141782, 2, 3, 2],"float32"), Tensor([190141782, 1, 3, 1],"float32"), 2, )

[accuracy error] paddle.dist(Tensor([190141782, 2, 3, 2],"float32"), Tensor([190141782, 1, 3, 1],"float32"), 2, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 19500.959
Max relative difference: 1.
 x: array(0., dtype=float32)
 y: array(19500.959, dtype=float32)
2025-04-21 18:36:13.583882 test begin: paddle.dist(Tensor([2, 1140850690],"float32"), Tensor([2, 1140850690],"float32"), 0, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_dist(_object*, _object*, _object*)
1   dist_ad_func(paddle::Tensor const&, paddle::Tensor const&, float)
2   paddle::experimental::dist(paddle::Tensor const&, paddle::Tensor const&, float)
3   void phi::DistKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, float, phi::DenseTensor*)
4   void phi::funcs::ReduceKernel<float, float, phi::kps::AddFunctor, phi::kps::IdentityFunctor<float, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<float, float> const&, std::vector<int, std::allocator<int> > const&)
5   phi::DenseTensor::~DenseTensor()
6   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
7   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745231782 (unix time) try "date -d @1745231782" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x3347) received by PID 13127 (TID 0x7f02114f4700) from PID 13127 ***]

2025-04-21 18:37:06.631487 test begin: paddle.dist(Tensor([2, 190141782, 3, 2],"float32"), Tensor([1, 1, 3, 1],"float32"), 2, )

W0421 18:38:29.421247 15862 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:38:29.423128 15862 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.dist(Tensor([2, 190141782, 3, 2],"float32"), Tensor([1, 1, 3, 1],"float32"), 2, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 21034.648
Max relative difference: 1.
 x: array(0., dtype=float32)
 y: array(21034.648, dtype=float32)
2025-04-21 18:38:29.980530 test begin: paddle.dist(Tensor([2, 190141782, 3, 2],"float32"), Tensor([1, 190141782, 3, 1],"float32"), 2, )

[accuracy error] paddle.dist(Tensor([2, 190141782, 3, 2],"float32"), Tensor([1, 190141782, 3, 1],"float32"), 2, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 19501.047
Max relative difference: 1.
 x: array(0., dtype=float32)
 y: array(19501.047, dtype=float32)
2025-04-21 18:38:45.591574 test begin: paddle.dist(Tensor([2, 2, 285212673, 2],"float32"), Tensor([1, 1, 285212673, 1],"float32"), 2, )

[accuracy error] paddle.dist(Tensor([2, 2, 285212673, 2],"float32"), Tensor([1, 1, 285212673, 1],"float32"), 2, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 19500.95
Max relative difference: 1.
 x: array(0., dtype=float32)
 y: array(19500.95, dtype=float32)
2025-04-21 18:38:59.632761 test begin: paddle.dist(Tensor([2, 2, 3, 190141782],"float32"), Tensor([1, 1, 3, 190141782],"float32"), 2, )

[accuracy error] paddle.dist(Tensor([2, 2, 3, 190141782],"float32"), Tensor([1, 1, 3, 190141782],"float32"), 2, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 16888.363
Max relative difference: 1.
 x: array(0., dtype=float32)
 y: array(16888.363, dtype=float32)
2025-04-21 18:39:14.777413 test begin: paddle.dist(Tensor([2, 2, 3, 190141782],"float32"), Tensor([1, 1, 3, 1],"float32"), 2, )

[accuracy error] paddle.dist(Tensor([2, 2, 3, 190141782],"float32"), Tensor([1, 1, 3, 1],"float32"), 2, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 21034.459
Max relative difference: 1.
 x: array(0., dtype=float32)
 y: array(21034.459, dtype=float32)
2025-04-21 18:39:27.697463 test begin: paddle.dist(x=Tensor([1073741825, 4],"float16"), y=Tensor([1073741825, 4],"float16"), )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_dist(_object*, _object*, _object*)
1   dist_ad_func(paddle::Tensor const&, paddle::Tensor const&, float)
2   paddle::experimental::dist(paddle::Tensor const&, paddle::Tensor const&, float)
3   void phi::DistKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, float, phi::DenseTensor*)
4   void phi::funcs::ReduceKernel<phi::dtype::float16, phi::dtype::float16, phi::kps::AddFunctor, phi::kps::IdentityFunctor<float, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<float, float> const&, std::vector<int, std::allocator<int> > const&)
5   phi::DenseTensor::~DenseTensor()
6   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
7   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745232066 (unix time) try "date -d @1745232066" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x3d85) received by PID 15749 (TID 0x7fd9f587e700) from PID 15749 ***]

2025-04-21 18:41:50.211203 test begin: paddle.dist(x=Tensor([1073741825, 4],"float16"), y=Tensor([1073741825, 4],"float16"), p=1, )

W0421 18:43:27.064862 21854 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:43:27.066004 21854 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_dist(_object*, _object*, _object*)
1   dist_ad_func(paddle::Tensor const&, paddle::Tensor const&, float)
2   paddle::experimental::dist(paddle::Tensor const&, paddle::Tensor const&, float)
3   void phi::DistKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, float, phi::DenseTensor*)
4   void phi::funcs::ReduceKernel<phi::dtype::float16, phi::dtype::float16, phi::kps::AddFunctor, phi::kps::IdentityFunctor<float, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<float, float> const&, std::vector<int, std::allocator<int> > const&)
5   phi::DenseTensor::~DenseTensor()
6   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
7   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745232211 (unix time) try "date -d @1745232211" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x53ca) received by PID 21450 (TID 0x7f2c91dc2700) from PID 21450 ***]

2025-04-21 18:43:39.245650 test begin: paddle.dist(x=Tensor([10],"float16"), y=Tensor([429496730, 10],"float16"), )

W0421 18:45:12.822813 24005 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:45:12.823922 24005 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.dist(x=Tensor([10],"float16"), y=Tensor([429496730, 10],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 26512.
Max relative difference: 1.
 x: array(0., dtype=float16)
 y: array(26512., dtype=float16)
2025-04-21 18:45:13.346836 test begin: paddle.dist(x=Tensor([10],"float16"), y=Tensor([429496730, 10],"float16"), p=4, )

[accuracy error] paddle.dist(x=Tensor([10],"float16"), y=Tensor([429496730, 10],"float16"), p=4, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 130.8
Max relative difference: 1.
 x: array(0., dtype=float16)
 y: array(130.8, dtype=float16)
2025-04-21 18:45:25.385364 test begin: paddle.dist(x=Tensor([2, 2147483649],"float16"), y=Tensor([2, 2147483649],"float16"), p=0, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_dist(_object*, _object*, _object*)
1   dist_ad_func(paddle::Tensor const&, paddle::Tensor const&, float)
2   paddle::experimental::dist(paddle::Tensor const&, paddle::Tensor const&, float)
3   void phi::DistKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, float, phi::DenseTensor*)
4   void phi::funcs::ReduceKernel<phi::dtype::float16, phi::dtype::float16, phi::kps::AddFunctor, phi::kps::IdentityFunctor<float, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<float, float> const&, std::vector<int, std::allocator<int> > const&)
5   phi::DenseTensor::~DenseTensor()
6   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
7   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745232336 (unix time) try "date -d @1745232336" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x5d45) received by PID 23877 (TID 0x7f41707c3700) from PID 23877 ***]

2025-04-21 18:45:40.685181 test begin: paddle.dist(x=Tensor([2147483649, 2],"float16"), y=Tensor([2147483649, 2],"float16"), p=0, )

W0421 18:47:09.492883 26543 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:47:09.494025 26543 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_dist(_object*, _object*, _object*)
1   dist_ad_func(paddle::Tensor const&, paddle::Tensor const&, float)
2   paddle::experimental::dist(paddle::Tensor const&, paddle::Tensor const&, float)
3   void phi::DistKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, float, phi::DenseTensor*)
4   void phi::funcs::ReduceKernel<phi::dtype::float16, phi::dtype::float16, phi::kps::AddFunctor, phi::kps::IdentityFunctor<float, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<float, float> const&, std::vector<int, std::allocator<int> > const&)
5   phi::DenseTensor::~DenseTensor()
6   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
7   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745232431 (unix time) try "date -d @1745232431" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x6731) received by PID 26417 (TID 0x7f1c2ddc2700) from PID 26417 ***]

2025-04-21 18:47:51.293145 test begin: paddle.dist(x=Tensor([4, 1073741825],"float16"), y=Tensor([4, 1073741825],"float16"), )

W0421 18:49:31.858974 29033 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:49:31.860173 29033 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_dist(_object*, _object*, _object*)
1   dist_ad_func(paddle::Tensor const&, paddle::Tensor const&, float)
2   paddle::experimental::dist(paddle::Tensor const&, paddle::Tensor const&, float)
3   void phi::DistKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, float, phi::DenseTensor*)
4   void phi::funcs::ReduceKernel<phi::dtype::float16, phi::dtype::float16, phi::kps::AddFunctor, phi::kps::IdentityFunctor<float, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<float, float> const&, std::vector<int, std::allocator<int> > const&)
5   phi::DenseTensor::~DenseTensor()
6   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
7   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745232576 (unix time) try "date -d @1745232576" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x6ffd) received by PID 28669 (TID 0x7ff0bb34a700) from PID 28669 ***]

2025-04-21 18:50:27.971663 test begin: paddle.dist(x=Tensor([4, 1073741825],"float16"), y=Tensor([4, 1073741825],"float16"), p=1, )

W0421 18:52:08.615710 31662 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:52:08.616890 31662 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_dist(_object*, _object*, _object*)
1   dist_ad_func(paddle::Tensor const&, paddle::Tensor const&, float)
2   paddle::experimental::dist(paddle::Tensor const&, paddle::Tensor const&, float)
3   void phi::DistKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, float, phi::DenseTensor*)
4   void phi::funcs::ReduceKernel<phi::dtype::float16, phi::dtype::float16, phi::kps::AddFunctor, phi::kps::IdentityFunctor<float, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<float, float> const&, std::vector<int, std::allocator<int> > const&)
5   phi::DenseTensor::~DenseTensor()
6   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
7   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745232732 (unix time) try "date -d @1745232732" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x7b46) received by PID 31558 (TID 0x7f547b2b7700) from PID 31558 ***]

2025-04-21 18:52:18.272537 test begin: paddle.dist(x=Tensor([4, 570425345],"float32"), y=Tensor([4, 570425345],"float32"), )

W0421 18:53:46.031023 33841 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:53:46.032249 33841 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_dist(_object*, _object*, _object*)
1   dist_ad_func(paddle::Tensor const&, paddle::Tensor const&, float)
2   paddle::experimental::dist(paddle::Tensor const&, paddle::Tensor const&, float)
3   void phi::DistKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, float, phi::DenseTensor*)
4   void phi::funcs::ReduceKernel<float, float, phi::kps::AddFunctor, phi::kps::IdentityFunctor<float, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<float, float> const&, std::vector<int, std::allocator<int> > const&)
5   phi::DenseTensor::~DenseTensor()
6   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
7   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745232826 (unix time) try "date -d @1745232826" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x83b3) received by PID 33715 (TID 0x7f4ff1abb700) from PID 33715 ***]

2025-04-21 18:53:51.887965 test begin: paddle.dist(x=Tensor([4294967297],"float16"), y=Tensor([4294967297],"float16"), )

W0421 18:55:33.346690 35713 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:55:33.347754 35713 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_dist(_object*, _object*, _object*)
1   dist_ad_func(paddle::Tensor const&, paddle::Tensor const&, float)
2   paddle::experimental::dist(paddle::Tensor const&, paddle::Tensor const&, float)
3   void phi::DistKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, float, phi::DenseTensor*)
4   void phi::funcs::ReduceKernel<phi::dtype::float16, phi::dtype::float16, phi::kps::AddFunctor, phi::kps::IdentityFunctor<float, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<float, float> const&, std::vector<int, std::allocator<int> > const&)
5   phi::DenseTensor::~DenseTensor()
6   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
7   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745232937 (unix time) try "date -d @1745232937" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x8b13) received by PID 35603 (TID 0x7f066d34a700) from PID 35603 ***]

2025-04-21 18:56:20.913247 test begin: paddle.dist(x=Tensor([570425345, 4],"float32"), y=Tensor([570425345, 4],"float32"), )

W0421 18:57:48.674818 38454 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:57:48.675928 38454 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_dist(_object*, _object*, _object*)
1   dist_ad_func(paddle::Tensor const&, paddle::Tensor const&, float)
2   paddle::experimental::dist(paddle::Tensor const&, paddle::Tensor const&, float)
3   void phi::DistKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, float, phi::DenseTensor*)
4   void phi::funcs::ReduceKernel<float, float, phi::kps::AddFunctor, phi::kps::IdentityFunctor<float, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<float, float> const&, std::vector<int, std::allocator<int> > const&)
5   phi::DenseTensor::~DenseTensor()
6   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
7   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745233069 (unix time) try "date -d @1745233069" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x95a3) received by PID 38307 (TID 0x7f079a7c3700) from PID 38307 ***]

2025-04-21 19:00:28.714870 test begin: paddle.equal(Tensor([1],"float32"), Tensor([2281701379],"float32"), )

[paddle error] paddle.equal(Tensor([1],"float32"), Tensor([2281701379],"float32"), ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 19:00:37.162471 test begin: paddle.equal(Tensor([1],"int32"), Tensor([2281701379],"int32"), )

[paddle error] paddle.equal(Tensor([1],"int32"), Tensor([2281701379],"int32"), ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 19:01:25.449901 test begin: paddle.equal(Tensor([1],"int64"), Tensor([2281701379],"int64"), )

[paddle error] paddle.equal(Tensor([1],"int64"), Tensor([2281701379],"int64"), ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 19:03:03.741764 test begin: paddle.equal(Tensor([2, 1],"int64"), Tensor([2281701379],"int64"), )

[paddle error] paddle.equal(Tensor([2, 1],"int64"), Tensor([2281701379],"int64"), ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 19:03:20.707087 test begin: paddle.equal(Tensor([2, 2147483649],"float16"), 0, )

[paddle error] paddle.equal(Tensor([2, 2147483649],"float16"), 0, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 19:05:04.406863 test begin: paddle.equal(Tensor([2281701379, 1],"int64"), Tensor([1],"int64"), )

[paddle error] paddle.equal(Tensor([2281701379, 1],"int64"), Tensor([1],"int64"), ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 19:05:20.120473 test begin: paddle.equal(Tensor([2281701379],"float32"), Tensor([1],"float32"), )

[paddle error] paddle.equal(Tensor([2281701379],"float32"), Tensor([1],"float32"), ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 19:05:27.001948 test begin: paddle.equal(Tensor([2281701379],"int32"), 0, )

[paddle error] paddle.equal(Tensor([2281701379],"int32"), 0, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 19:05:35.061771 test begin: paddle.equal(Tensor([2281701379],"int32"), Tensor([1],"int32"), )

[paddle error] paddle.equal(Tensor([2281701379],"int32"), Tensor([1],"int32"), ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 19:05:41.297736 test begin: paddle.equal(Tensor([2281701379],"int64"), 0, )

[paddle error] paddle.equal(Tensor([2281701379],"int64"), 0, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 19:05:51.190173 test begin: paddle.equal(Tensor([2281701379],"int64"), 1, )

[paddle error] paddle.equal(Tensor([2281701379],"int64"), 1, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 19:06:02.928336 test begin: paddle.equal(Tensor([2281701379],"int64"), 1.0, )

[paddle error] paddle.equal(Tensor([2281701379],"int64"), 1.0, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 19:06:13.149490 test begin: paddle.equal(Tensor([2281701379],"int64"), 10, )

[paddle error] paddle.equal(Tensor([2281701379],"int64"), 10, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 19:06:24.863765 test begin: paddle.equal(Tensor([2281701379],"int64"), 2, )

[paddle error] paddle.equal(Tensor([2281701379],"int64"), 2, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 19:06:33.760001 test begin: paddle.equal(Tensor([2281701379],"int64"), Tensor([1],"int64"), )

[paddle error] paddle.equal(Tensor([2281701379],"int64"), Tensor([1],"int64"), ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 19:06:41.240192 test begin: paddle.equal(Tensor([2281701379],"int64"), True, )

[paddle error] paddle.equal(Tensor([2281701379],"int64"), True, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 19:09:28.050076 test begin: paddle.fft.ifftshift(x=Tensor([4, 67108865, 4, 4],"float16"), )

[accuracy error] paddle.fft.ifftshift(x=Tensor([4, 67108865, 4, 4],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 127 / 4294967360 (2.96e-06%)
Max absolute difference: 0.496
Max relative difference: 0.
 x: array([[[[-2.7856e-01, -4.1168e-02,  4.8157e-02,  2.7148e-01],
         [-4.5190e-01, -3.4241e-02,  3.2715e-01, -3.9917e-01],
         [ 6.5979e-02,  4.3335e-02,  2.1643e-01,  3.9990e-01],...
 y: array([[[[-2.7856e-01, -4.1168e-02,  4.8157e-02,  2.7148e-01],
         [-4.5190e-01, -3.4241e-02,  3.2715e-01, -3.9917e-01],
         [ 6.5979e-02,  4.3335e-02,  2.1643e-01,  3.9990e-01],...
2025-04-21 19:22:37.113996 test begin: paddle.fft.ifftshift(x=Tensor([4294967297],"float16"), )

[accuracy error] paddle.fft.ifftshift(x=Tensor([4294967297],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 4294967297 (2.33e-08%)
Max absolute difference: 0.03876
Max relative difference: 0.
 x: array([ 0.4    ,  0.4294 , -0.4915 , ..., -0.1237 , -0.1498 , -0.03876],
      dtype=float16)
 y: array([ 0.4   ,  0.4294, -0.4915, ..., -0.1237, -0.1498,  0.    ],
      dtype=float16)
2025-04-21 19:35:24.249039 test begin: paddle.fft.ifftshift(x=Tensor([536870913, 4, 2],"float16"), )

[accuracy error] paddle.fft.ifftshift(x=Tensor([536870913, 4, 2],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 8 / 4294967304 (1.86e-07%)
Max absolute difference: 0.496
Max relative difference: 0.
 x: array([[[-0.4744  , -0.4294  ],
        [ 0.302   , -0.06793 ],
        [ 0.4294  ,  0.4     ],...
 y: array([[[-0.4744  , -0.4294  ],
        [ 0.302   , -0.06793 ],
        [ 0.4294  ,  0.4     ],...
2025-04-21 19:48:11.032399 test begin: paddle.fft.ifftshift(x=Tensor([53687092, 5, 4, 4],"float16"), )

[accuracy error] paddle.fft.ifftshift(x=Tensor([53687092, 5, 4, 4],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 79 / 4294967360 (1.84e-06%)
Max absolute difference: 0.496
Max relative difference: 0.
 x: array([[[[-0.3015  , -0.3801  ,  0.31    ,  0.07275 ],
         [ 0.3872  ,  0.259   , -0.2603  ,  0.03506 ],
         [ 0.1947  ,  0.1066  , -0.04807 ,  0.1417  ],...
 y: array([[[[-0.3015  , -0.3801  ,  0.31    ,  0.07275 ],
         [ 0.3872  ,  0.259   , -0.2603  ,  0.03506 ],
         [ 0.1947  ,  0.1066  , -0.04807 ,  0.1417  ],...
2025-04-21 20:01:02.732147 test begin: paddle.flatten(Tensor([1, 1024, 1, 4194304],"float32"), 1, )

[paddle error] paddle.flatten(Tensor([1, 1024, 1, 4194304],"float32"), 1, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 1024, 1, 4194304], X's size = 4294967296, 'shape' is [1, 0], the capacity of 'shape' is 1024.
  [Hint: Expected capacity == in_size, but received capacity:1024 != in_size:4294967296.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2257)

2025-04-21 20:01:39.101097 test begin: paddle.flatten(Tensor([1, 1024, 4194304, 1],"float32"), 1, )

[paddle error] paddle.flatten(Tensor([1, 1024, 4194304, 1],"float32"), 1, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 1024, 4194304, 1], X's size = 4294967296, 'shape' is [1, 0], the capacity of 'shape' is 1024.
  [Hint: Expected capacity == in_size, but received capacity:1024 != in_size:4294967296.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2257)

2025-04-21 20:02:16.302163 test begin: paddle.flatten(Tensor([1, 119304648, 6, 6],"float32"), 1, )

[paddle error] paddle.flatten(Tensor([1, 119304648, 6, 6],"float32"), 1, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 119304648, 6, 6], X's size = 4294967328, 'shape' is [1, 32], the capacity of 'shape' is 32.
  [Hint: Expected capacity == in_size, but received capacity:32 != in_size:4294967328.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2257)

2025-04-21 20:02:49.661754 test begin: paddle.flatten(Tensor([1, 16, 44739243, 6],"float32"), 1, )

[paddle error] paddle.flatten(Tensor([1, 16, 44739243, 6],"float32"), 1, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 16, 44739243, 6], X's size = 4294967328, 'shape' is [1, 32], the capacity of 'shape' is 32.
  [Hint: Expected capacity == in_size, but received capacity:32 != in_size:4294967328.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2257)

2025-04-21 20:03:25.340464 test begin: paddle.flatten(Tensor([1, 16, 5, 53687092],"float32"), 1, )

[paddle error] paddle.flatten(Tensor([1, 16, 5, 53687092],"float32"), 1, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 16, 5, 53687092], X's size = 4294967360, 'shape' is [1, 64], the capacity of 'shape' is 64.
  [Hint: Expected capacity == in_size, but received capacity:64 != in_size:4294967360.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2257)

2025-04-21 20:03:59.460386 test begin: paddle.flatten(Tensor([1, 16, 53687092, 5],"float32"), 1, )

[paddle error] paddle.flatten(Tensor([1, 16, 53687092, 5],"float32"), 1, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 16, 53687092, 5], X's size = 4294967360, 'shape' is [1, 64], the capacity of 'shape' is 64.
  [Hint: Expected capacity == in_size, but received capacity:64 != in_size:4294967360.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2257)

2025-04-21 20:04:36.808991 test begin: paddle.flatten(Tensor([1, 16, 6, 44739243],"float32"), 1, )

[paddle error] paddle.flatten(Tensor([1, 16, 6, 44739243],"float32"), 1, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 16, 6, 44739243], X's size = 4294967328, 'shape' is [1, 32], the capacity of 'shape' is 32.
  [Hint: Expected capacity == in_size, but received capacity:32 != in_size:4294967328.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2257)

2025-04-21 20:05:28.016232 test begin: paddle.flatten(Tensor([1, 171798692, 5, 5],"float32"), 1, )

[paddle error] paddle.flatten(Tensor([1, 171798692, 5, 5],"float32"), 1, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 171798692, 5, 5], X's size = 4294967300, 'shape' is [1, 4], the capacity of 'shape' is 4.
  [Hint: Expected capacity == in_size, but received capacity:4 != in_size:4294967300.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2257)

2025-04-21 20:06:00.187440 test begin: paddle.flatten(Tensor([1, 2048, 1, 2097152],"float32"), 1, )

[paddle error] paddle.flatten(Tensor([1, 2048, 1, 2097152],"float32"), 1, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 2048, 1, 2097152], X's size = 4294967296, 'shape' is [1, 0], the capacity of 'shape' is 2048.
  [Hint: Expected capacity == in_size, but received capacity:2048 != in_size:4294967296.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2257)

2025-04-21 20:06:34.982966 test begin: paddle.flatten(Tensor([1, 2048, 2097152, 1],"float32"), 1, )

[paddle error] paddle.flatten(Tensor([1, 2048, 2097152, 1],"float32"), 1, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 2048, 2097152, 1], X's size = 4294967296, 'shape' is [1, 0], the capacity of 'shape' is 2048.
  [Hint: Expected capacity == in_size, but received capacity:2048 != in_size:4294967296.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2257)

2025-04-21 20:07:16.750205 test begin: paddle.flatten(Tensor([1, 512, 1, 8388608],"float32"), 1, )

[paddle error] paddle.flatten(Tensor([1, 512, 1, 8388608],"float32"), 1, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 512, 1, 8388608], X's size = 4294967296, 'shape' is [1, 0], the capacity of 'shape' is 512.
  [Hint: Expected capacity == in_size, but received capacity:512 != in_size:4294967296.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2257)

2025-04-21 20:07:55.689974 test begin: paddle.flatten(Tensor([1, 512, 8388608, 1],"float32"), 1, )

[paddle error] paddle.flatten(Tensor([1, 512, 8388608, 1],"float32"), 1, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 512, 8388608, 1], X's size = 4294967296, 'shape' is [1, 0], the capacity of 'shape' is 512.
  [Hint: Expected capacity == in_size, but received capacity:512 != in_size:4294967296.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2257)

2025-04-21 20:08:29.139324 test begin: paddle.flatten(Tensor([1, 8, 7, 76695845],"float32"), start_axis=1, stop_axis=-1, )

[paddle error] paddle.flatten(Tensor([1, 8, 7, 76695845],"float32"), start_axis=1, stop_axis=-1, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 8, 7, 76695845], X's size = 4294967320, 'shape' is [1, 24], the capacity of 'shape' is 24.
  [Hint: Expected capacity == in_size, but received capacity:24 != in_size:4294967320.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2257)

2025-04-21 20:09:07.556424 test begin: paddle.flatten(Tensor([1, 8, 76695845, 7],"float32"), start_axis=1, stop_axis=-1, )

[paddle error] paddle.flatten(Tensor([1, 8, 76695845, 7],"float32"), start_axis=1, stop_axis=-1, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 8, 76695845, 7], X's size = 4294967320, 'shape' is [1, 24], the capacity of 'shape' is 24.
  [Hint: Expected capacity == in_size, but received capacity:24 != in_size:4294967320.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2257)

2025-04-21 20:09:46.248836 test begin: paddle.flatten(Tensor([1, 87652394, 7, 7],"float32"), 1, )

[paddle error] paddle.flatten(Tensor([1, 87652394, 7, 7],"float32"), 1, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 87652394, 7, 7], X's size = 4294967306, 'shape' is [1, 10], the capacity of 'shape' is 10.
  [Hint: Expected capacity == in_size, but received capacity:10 != in_size:4294967306.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2257)

2025-04-21 20:10:22.235406 test begin: paddle.flatten(Tensor([1, 87652394, 7, 7],"float32"), start_axis=1, stop_axis=-1, )

[paddle error] paddle.flatten(Tensor([1, 87652394, 7, 7],"float32"), start_axis=1, stop_axis=-1, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 87652394, 7, 7], X's size = 4294967306, 'shape' is [1, 10], the capacity of 'shape' is 10.
  [Hint: Expected capacity == in_size, but received capacity:10 != in_size:4294967306.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2257)

2025-04-21 20:11:03.108872 test begin: paddle.flatten(Tensor([1073741824, 4],"float16"), )

[paddle error] paddle.flatten(Tensor([1073741824, 4],"float16"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1073741824, 4], X's size = 4294967296, 'shape' is [0], the capacity of 'shape' is 1073741824.
  [Hint: Expected capacity == in_size, but received capacity:1073741824 != in_size:4294967296.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2257)

2025-04-21 20:11:26.172905 test begin: paddle.flatten(Tensor([1073741824, 4],"float32"), )

[paddle error] paddle.flatten(Tensor([1073741824, 4],"float32"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1073741824, 4], X's size = 4294967296, 'shape' is [0], the capacity of 'shape' is 1073741824.
  [Hint: Expected capacity == in_size, but received capacity:1073741824 != in_size:4294967296.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2257)

2025-04-21 20:12:11.858291 test begin: paddle.flatten(Tensor([107374183, 4, 5],"float64"), )

[paddle error] paddle.flatten(Tensor([107374183, 4, 5],"float64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2147483636], shape[0] = -2147483636.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483636 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:12:57.161250 test begin: paddle.flatten(Tensor([107374183, 4, 5],"int64"), )

[paddle error] paddle.flatten(Tensor([107374183, 4, 5],"int64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2147483636], shape[0] = -2147483636.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483636 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:13:34.623090 test begin: paddle.flatten(Tensor([2, 1024, 1, 2097152],"float32"), 1, )

[paddle error] paddle.flatten(Tensor([2, 1024, 1, 2097152],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483648], shape[1] = -2147483648.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483648 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:14:13.335988 test begin: paddle.flatten(Tensor([2, 1024, 1, 2097152],"float32"), start_axis=1, stop_axis=-1, )

[paddle error] paddle.flatten(Tensor([2, 1024, 1, 2097152],"float32"), start_axis=1, stop_axis=-1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483648], shape[1] = -2147483648.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483648 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:14:44.292802 test begin: paddle.flatten(Tensor([2, 1024, 2097152, 1],"float32"), 1, )

[paddle error] paddle.flatten(Tensor([2, 1024, 2097152, 1],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483648], shape[1] = -2147483648.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483648 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:15:15.325002 test begin: paddle.flatten(Tensor([2, 1024, 2097152, 1],"float32"), start_axis=1, stop_axis=-1, )

[paddle error] paddle.flatten(Tensor([2, 1024, 2097152, 1],"float32"), start_axis=1, stop_axis=-1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483648], shape[1] = -2147483648.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483648 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:15:46.445154 test begin: paddle.flatten(Tensor([2, 16, 26843546, 5],"float32"), 1, )

[paddle error] paddle.flatten(Tensor([2, 16, 26843546, 5],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483616], shape[1] = -2147483616.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483616 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:16:21.803349 test begin: paddle.flatten(Tensor([2, 16, 5, 26843546],"float32"), 1, )

[paddle error] paddle.flatten(Tensor([2, 16, 5, 26843546],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483616], shape[1] = -2147483616.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483616 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:16:55.383574 test begin: paddle.flatten(Tensor([2, 2048, 1, 1048576],"float32"), 1, )

[paddle error] paddle.flatten(Tensor([2, 2048, 1, 1048576],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483648], shape[1] = -2147483648.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483648 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:17:26.680240 test begin: paddle.flatten(Tensor([2, 2048, 1, 1048576],"float32"), start_axis=1, stop_axis=-1, )

[paddle error] paddle.flatten(Tensor([2, 2048, 1, 1048576],"float32"), start_axis=1, stop_axis=-1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483648], shape[1] = -2147483648.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483648 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:18:01.701771 test begin: paddle.flatten(Tensor([2, 2048, 1048576, 1],"float32"), 1, )

[paddle error] paddle.flatten(Tensor([2, 2048, 1048576, 1],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483648], shape[1] = -2147483648.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483648 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:18:35.878179 test begin: paddle.flatten(Tensor([2, 2048, 1048576, 1],"float32"), start_axis=1, stop_axis=-1, )

[paddle error] paddle.flatten(Tensor([2, 2048, 1048576, 1],"float32"), start_axis=1, stop_axis=-1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483648], shape[1] = -2147483648.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483648 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:19:09.201627 test begin: paddle.flatten(Tensor([2, 2147483648, 1, 1],"float32"), 1, )

[paddle error] paddle.flatten(Tensor([2, 2147483648, 1, 1],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483648], shape[1] = -2147483648.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483648 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:19:49.630030 test begin: paddle.flatten(Tensor([2, 2147483648, 1, 1],"float32"), start_axis=1, stop_axis=-1, )

[paddle error] paddle.flatten(Tensor([2, 2147483648, 1, 1],"float32"), start_axis=1, stop_axis=-1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483648], shape[1] = -2147483648.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483648 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:20:28.422076 test begin: paddle.flatten(Tensor([2, 2147483648],"bfloat16"), )

[paddle error] paddle.flatten(Tensor([2, 2147483648],"bfloat16"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [2, 2147483648], X's size = 4294967296, 'shape' is [0], the capacity of 'shape' is 2.
  [Hint: Expected capacity == in_size, but received capacity:2 != in_size:4294967296.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2257)

2025-04-21 20:21:08.746597 test begin: paddle.flatten(Tensor([2, 2147483648],"float32"), )

[paddle error] paddle.flatten(Tensor([2, 2147483648],"float32"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [2, 2147483648], X's size = 4294967296, 'shape' is [0], the capacity of 'shape' is 2.
  [Hint: Expected capacity == in_size, but received capacity:2 != in_size:4294967296.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2257)

2025-04-21 20:21:48.347522 test begin: paddle.flatten(Tensor([2, 2147483648],"int32"), )

[paddle error] paddle.flatten(Tensor([2, 2147483648],"int32"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [2, 2147483648], X's size = 4294967296, 'shape' is [0], the capacity of 'shape' is 2.
  [Hint: Expected capacity == in_size, but received capacity:2 != in_size:4294967296.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2257)

2025-04-21 20:22:12.986471 test begin: paddle.flatten(Tensor([2, 214748365, 5],"float64"), )

[paddle error] paddle.flatten(Tensor([2, 214748365, 5],"float64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2147483646], shape[0] = -2147483646.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483646 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:22:59.603352 test begin: paddle.flatten(Tensor([2, 214748365, 5],"int64"), )

[paddle error] paddle.flatten(Tensor([2, 214748365, 5],"int64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2147483646], shape[0] = -2147483646.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483646 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:23:23.875217 test begin: paddle.flatten(Tensor([2, 21474837, 10, 10],"float32"), 1, )

[paddle error] paddle.flatten(Tensor([2, 21474837, 10, 10],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483596], shape[1] = -2147483596.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483596 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:24:11.535426 test begin: paddle.flatten(Tensor([2, 238609295, 3, 3],"float32"), start_axis=1, stop_axis=-1, )

[paddle error] paddle.flatten(Tensor([2, 238609295, 3, 3],"float32"), start_axis=1, stop_axis=-1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483641], shape[1] = -2147483641.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483641 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:24:54.196502 test begin: paddle.flatten(Tensor([2, 4, 268435457],"float64"), )

[paddle error] paddle.flatten(Tensor([2, 4, 268435457],"float64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2147483640], shape[0] = -2147483640.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483640 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:25:41.008989 test begin: paddle.flatten(Tensor([2, 4, 268435457],"int64"), )

[paddle error] paddle.flatten(Tensor([2, 4, 268435457],"int64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2147483640], shape[0] = -2147483640.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483640 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:26:04.037689 test begin: paddle.flatten(Tensor([2, 4, 536870912],"float32"), )

[paddle error] paddle.flatten(Tensor([2, 4, 536870912],"float32"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [2, 4, 536870912], X's size = 4294967296, 'shape' is [0], the capacity of 'shape' is 2.
  [Hint: Expected capacity == in_size, but received capacity:2 != in_size:4294967296.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2257)

2025-04-21 20:26:42.790976 test begin: paddle.flatten(Tensor([2, 4, 536870912],"int32"), )

[paddle error] paddle.flatten(Tensor([2, 4, 536870912],"int32"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [2, 4, 536870912], X's size = 4294967296, 'shape' is [0], the capacity of 'shape' is 2.
  [Hint: Expected capacity == in_size, but received capacity:2 != in_size:4294967296.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2257)

2025-04-21 20:27:07.756554 test begin: paddle.flatten(Tensor([2, 429496730, 5],"float32"), )

[paddle error] paddle.flatten(Tensor([2, 429496730, 5],"float32"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [2, 429496730, 5], X's size = 4294967300, 'shape' is [4], the capacity of 'shape' is 4.
  [Hint: Expected capacity == in_size, but received capacity:4 != in_size:4294967300.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2257)

2025-04-21 20:27:42.236810 test begin: paddle.flatten(Tensor([2, 429496730, 5],"int32"), )

[paddle error] paddle.flatten(Tensor([2, 429496730, 5],"int32"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [2, 429496730, 5], X's size = 4294967300, 'shape' is [4], the capacity of 'shape' is 4.
  [Hint: Expected capacity == in_size, but received capacity:4 != in_size:4294967300.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2257)

2025-04-21 20:28:07.493288 test begin: paddle.flatten(Tensor([2, 43826197, 7, 7],"float32"), 1, )

[paddle error] paddle.flatten(Tensor([2, 43826197, 7, 7],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483643], shape[1] = -2147483643.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483643 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:28:49.917788 test begin: paddle.flatten(Tensor([2, 512, 1, 4194304],"float32"), 1, )

[paddle error] paddle.flatten(Tensor([2, 512, 1, 4194304],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483648], shape[1] = -2147483648.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483648 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:29:24.774084 test begin: paddle.flatten(Tensor([2, 512, 1, 4194304],"float32"), start_axis=1, stop_axis=-1, )

[paddle error] paddle.flatten(Tensor([2, 512, 1, 4194304],"float32"), start_axis=1, stop_axis=-1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483648], shape[1] = -2147483648.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483648 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:30:02.486327 test begin: paddle.flatten(Tensor([2, 512, 4194304, 1],"float32"), 1, )

[paddle error] paddle.flatten(Tensor([2, 512, 4194304, 1],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483648], shape[1] = -2147483648.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483648 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:30:39.973736 test begin: paddle.flatten(Tensor([2, 512, 4194304, 1],"float32"), start_axis=1, stop_axis=-1, )

[paddle error] paddle.flatten(Tensor([2, 512, 4194304, 1],"float32"), start_axis=1, stop_axis=-1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483648], shape[1] = -2147483648.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483648 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:31:18.986703 test begin: paddle.flatten(Tensor([2, 59652324, 6, 6],"float32"), start_axis=1, stop_axis=-1, )

[paddle error] paddle.flatten(Tensor([2, 59652324, 6, 6],"float32"), start_axis=1, stop_axis=-1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483632], shape[1] = -2147483632.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483632 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:31:53.924245 test begin: paddle.flatten(Tensor([2, 6, 10, 35791395],"float32"), 1, )

[paddle error] paddle.flatten(Tensor([2, 6, 10, 35791395],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483596], shape[1] = -2147483596.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483596 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:32:32.896488 test begin: paddle.flatten(Tensor([2, 6, 35791395, 10],"float32"), 1, )

[paddle error] paddle.flatten(Tensor([2, 6, 35791395, 10],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483596], shape[1] = -2147483596.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483596 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:33:12.773409 test begin: paddle.flatten(Tensor([2, 85899346, 5, 5],"float32"), 1, )

[paddle error] paddle.flatten(Tensor([2, 85899346, 5, 5],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483646], shape[1] = -2147483646.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483646 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:33:47.258217 test begin: paddle.flatten(Tensor([214748365, 4, 5],"float32"), )

[paddle error] paddle.flatten(Tensor([214748365, 4, 5],"float32"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [214748365, 4, 5], X's size = 4294967300, 'shape' is [4], the capacity of 'shape' is 4.
  [Hint: Expected capacity == in_size, but received capacity:4 != in_size:4294967300.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2257)

2025-04-21 20:34:26.736341 test begin: paddle.flatten(Tensor([214748365, 4, 5],"int32"), )

[paddle error] paddle.flatten(Tensor([214748365, 4, 5],"int32"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [214748365, 4, 5], X's size = 4294967300, 'shape' is [4], the capacity of 'shape' is 4.
  [Hint: Expected capacity == in_size, but received capacity:4 != in_size:4294967300.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2257)

2025-04-21 20:34:51.907436 test begin: paddle.flatten(Tensor([268435457, 4, 2],"float64"), )

[paddle error] paddle.flatten(Tensor([268435457, 4, 2],"float64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2147483640], shape[0] = -2147483640.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483640 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:35:37.972410 test begin: paddle.flatten(Tensor([268435457, 4, 2],"int64"), )

[paddle error] paddle.flatten(Tensor([268435457, 4, 2],"int64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2147483640], shape[0] = -2147483640.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483640 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:36:02.028362 test begin: paddle.flatten(Tensor([3, 357913942, 2],"float64"), )

[paddle error] paddle.flatten(Tensor([3, 357913942, 2],"float64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2147483644], shape[0] = -2147483644.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483644 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:36:49.173969 test begin: paddle.flatten(Tensor([3, 357913942, 2],"int64"), )

[paddle error] paddle.flatten(Tensor([3, 357913942, 2],"int64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2147483644], shape[0] = -2147483644.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483644 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:37:12.478172 test begin: paddle.flatten(Tensor([3, 4, 178956971],"float64"), )

[paddle error] paddle.flatten(Tensor([3, 4, 178956971],"float64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2147483644], shape[0] = -2147483644.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483644 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:37:51.609986 test begin: paddle.flatten(Tensor([3, 4, 178956971],"int64"), )

[paddle error] paddle.flatten(Tensor([3, 4, 178956971],"int64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2147483644], shape[0] = -2147483644.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483644 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:38:21.288791 test begin: paddle.flatten(Tensor([3, 4, 357913942],"float32"), )

[paddle error] paddle.flatten(Tensor([3, 4, 357913942],"float32"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [3, 4, 357913942], X's size = 4294967304, 'shape' is [8], the capacity of 'shape' is 8.
  [Hint: Expected capacity == in_size, but received capacity:8 != in_size:4294967304.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2257)

2025-04-21 20:39:08.872743 test begin: paddle.flatten(Tensor([3, 4, 357913942],"int32"), )

[paddle error] paddle.flatten(Tensor([3, 4, 357913942],"int32"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [3, 4, 357913942], X's size = 4294967304, 'shape' is [8], the capacity of 'shape' is 8.
  [Hint: Expected capacity == in_size, but received capacity:8 != in_size:4294967304.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2257)

2025-04-21 20:39:33.932714 test begin: paddle.flatten(Tensor([3, 715827883, 2],"float32"), )

[paddle error] paddle.flatten(Tensor([3, 715827883, 2],"float32"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [3, 715827883, 2], X's size = 4294967298, 'shape' is [2], the capacity of 'shape' is 2.
  [Hint: Expected capacity == in_size, but received capacity:2 != in_size:4294967298.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2257)

2025-04-21 20:40:16.868702 test begin: paddle.flatten(Tensor([3, 715827883, 2],"int32"), )

[paddle error] paddle.flatten(Tensor([3, 715827883, 2],"int32"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [3, 715827883, 2], X's size = 4294967298, 'shape' is [2], the capacity of 'shape' is 2.
  [Hint: Expected capacity == in_size, but received capacity:2 != in_size:4294967298.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2257)

2025-04-21 20:40:45.681412 test begin: paddle.flatten(Tensor([3, 715827883],"float64"), )

[paddle error] paddle.flatten(Tensor([3, 715827883],"float64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2147483647], shape[0] = -2147483647.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483647 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:41:41.162439 test begin: paddle.flatten(Tensor([4, 7, 76695845],"float64"), )

[paddle error] paddle.flatten(Tensor([4, 7, 76695845],"float64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2147483636], shape[0] = -2147483636.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483636 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:42:22.596822 test begin: paddle.flatten(Tensor([4, 89478486, 6],"float64"), )

[paddle error] paddle.flatten(Tensor([4, 89478486, 6],"float64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2147483632], shape[0] = -2147483632.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483632 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:43:04.259666 test begin: paddle.flatten(Tensor([5, 429496730],"float64"), )

[paddle error] paddle.flatten(Tensor([5, 429496730],"float64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2147483646], shape[0] = -2147483646.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483646 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:43:45.643571 test begin: paddle.flatten(Tensor([51130564, 7, 6],"float64"), )

[paddle error] paddle.flatten(Tensor([51130564, 7, 6],"float64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2147483608], shape[0] = -2147483608.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483608 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 20:44:19.265009 test begin: paddle.flatten(Tensor([536870912, 4, 2],"float32"), )

[paddle error] paddle.flatten(Tensor([536870912, 4, 2],"float32"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [536870912, 4, 2], X's size = 4294967296, 'shape' is [0], the capacity of 'shape' is 536870912.
  [Hint: Expected capacity == in_size, but received capacity:536870912 != in_size:4294967296.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2257)

2025-04-21 20:44:54.757456 test begin: paddle.flatten(Tensor([536870912, 4, 2],"int32"), )

[paddle error] paddle.flatten(Tensor([536870912, 4, 2],"int32"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [536870912, 4, 2], X's size = 4294967296, 'shape' is [0], the capacity of 'shape' is 536870912.
  [Hint: Expected capacity == in_size, but received capacity:536870912 != in_size:4294967296.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2257)

2025-04-21 20:45:19.424760 test begin: paddle.flatten(Tensor([536870913, 4],"float64"), )

[paddle error] paddle.flatten(Tensor([536870913, 4],"float64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2147483644], shape[0] = -2147483644.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483644 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-21 21:58:14.002054 test begin: paddle.fmin(Tensor([1],"int64"), Tensor([2281701379],"int64"), )

[paddle error] paddle.fmin(Tensor([1],"int64"), Tensor([2281701379],"int64"), ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 21:58:40.548172 test begin: paddle.fmin(Tensor([2147483649],"int64"), Tensor([1],"int64"), )

[paddle error] paddle.fmin(Tensor([2147483649],"int64"), Tensor([1],"int64"), ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 21:58:58.826488 test begin: paddle.fmin(Tensor([2281701379],"int64"), Tensor([1],"int64"), )

[paddle error] paddle.fmin(Tensor([2281701379],"int64"), Tensor([1],"int64"), ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 23:12:46.587315 test begin: paddle.frac(Tensor([10, 20, 11408507],"float32"), )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745248388 (unix time) try "date -d @1745248388" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x9d73) received by PID 40307 (TID 0x7f4ee5193700) from PID 40307 ***]

2025-04-21 23:13:19.522116 test begin: paddle.frac(Tensor([10, 228170138, 1],"float32"), )

W0421 23:14:41.349215 134973 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 23:14:41.350360 134973 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745248481 (unix time) try "date -d @1745248481" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x20db7) received by PID 134583 (TID 0x7f3e8c949700) from PID 134583 ***]

2025-04-21 23:15:26.104550 test begin: paddle.frac(Tensor([114085069, 20, 1],"float32"), )

W0421 23:16:57.451036 137163 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 23:16:57.452080 137163 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745248617 (unix time) try "date -d @1745248617" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2174f) received by PID 137039 (TID 0x7fc081f48700) from PID 137039 ***]

2025-04-21 23:17:40.983688 test begin: paddle.frac(Tensor([1431655766, 3],"float16"), )

W0421 23:19:23.187494 139532 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 23:19:23.188788 139532 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.frac(Tensor([1431655766, 3],"float16"), ) 
 The data type of input must be one of ['int32', 'int64', 'float32', 'float64'], but got paddle.float16
2025-04-21 23:19:24.197335 test begin: paddle.frac(Tensor([2, 1140850690],"float32"), )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>::~small_vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745248928 (unix time) try "date -d @1745248928" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2208e) received by PID 139406 (TID 0x7f4d827c3700) from PID 139406 ***]

2025-04-21 23:22:56.474756 test begin: paddle.frac(Tensor([2, 2147483649],"float16"), )

W0421 23:24:31.884886 144946 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 23:24:31.886059 144946 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.frac(Tensor([2, 2147483649],"float16"), ) 
 The data type of input must be one of ['int32', 'int64', 'float32', 'float64'], but got paddle.float16
2025-04-21 23:24:32.629121 test begin: paddle.frac(Tensor([760567127, 3],"float32"), )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>::~small_vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745249268 (unix time) try "date -d @1745249268" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x235b0) received by PID 144816 (TID 0x7f5fee7c3700) from PID 144816 ***]

2025-04-21 23:28:35.333440 test begin: paddle.full_like(Tensor([1, 1, 1, 2281701379],"float32"), -3.4028234663852886e+38, )

W0421 23:29:53.393608 150721 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 23:29:53.394542 150721 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.full_like(Tensor([1, 1, 1, 2281701379],"float32"), -3.4028234663852886e+38, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 3.4028235e+38
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32)
 y: array([[[[-3.402823e+38, -3.402823e+38, -3.402823e+38, ...,
          -3.402823e+38, -3.402823e+38, -3.402823e+38]]]], dtype=float32)
2025-04-21 23:31:27.878321 test begin: paddle.full_like(Tensor([1, 1, 2281701379, 1],"float32"), -3.4028234663852886e+38, )

[accuracy error] paddle.full_like(Tensor([1, 1, 2281701379, 1],"float32"), -3.4028234663852886e+38, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 3.4028235e+38
Max relative difference: 1.
 x: array([[[[0.],
         [0.],
         [0.],...
 y: array([[[[-3.402823e+38],
         [-3.402823e+38],
         [-3.402823e+38],...
2025-04-21 23:33:43.867961 test begin: paddle.full_like(Tensor([1, 1048576, 4096],"float32"), 1, )

[accuracy error] paddle.full_like(Tensor([1, 1048576, 4096],"float32"), 1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967296 / 4294967296 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],...
2025-04-21 23:37:52.969948 test begin: paddle.full_like(Tensor([1, 2281701379, 1, 1],"float32"), -3.4028234663852886e+38, )

[accuracy error] paddle.full_like(Tensor([1, 2281701379, 1, 1],"float32"), -3.4028234663852886e+38, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 3.4028235e+38
Max relative difference: 1.
 x: array([[[[0.]],

        [[0.]],...
 y: array([[[[-3.402823e+38]],

        [[-3.402823e+38]],...
2025-04-21 23:39:44.932017 test begin: paddle.full_like(Tensor([1, 2281701379],"float32"), -math.inf, )

[accuracy error] paddle.full_like(Tensor([1, 2281701379],"float32"), -math.inf, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y -inf location mismatch:
 x: array([[0., 0., 0., ..., 0., 0., 0.]], dtype=float32)
 y: array([[-inf, -inf, -inf, ..., -inf, -inf, -inf]], dtype=float32)
2025-04-21 23:40:28.007083 test begin: paddle.full_like(Tensor([1, 2281701379],"int32"), 10, )

[accuracy error] paddle.full_like(Tensor([1, 2281701379],"int32"), 10, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 10
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0]], dtype=int32)
 y: array([[10, 10, 10, ..., 10, 10, 10]], dtype=int32)
2025-04-21 23:45:14.264386 test begin: paddle.full_like(Tensor([1, 2281701379],"int32"), 11, )

[accuracy error] paddle.full_like(Tensor([1, 2281701379],"int32"), 11, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 11
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0]], dtype=int32)
 y: array([[11, 11, 11, ..., 11, 11, 11]], dtype=int32)
2025-04-21 23:48:15.888468 test begin: paddle.full_like(Tensor([1, 2281701379],"int32"), 15, )

[accuracy error] paddle.full_like(Tensor([1, 2281701379],"int32"), 15, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 15
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0]], dtype=int32)
 y: array([[15, 15, 15, ..., 15, 15, 15]], dtype=int32)
2025-04-21 23:51:23.793465 test begin: paddle.full_like(Tensor([1, 2281701379],"int32"), 2, )

[accuracy error] paddle.full_like(Tensor([1, 2281701379],"int32"), 2, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 2
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0]], dtype=int32)
 y: array([[2, 2, 2, ..., 2, 2, 2]], dtype=int32)
2025-04-21 23:54:20.278789 test begin: paddle.full_like(Tensor([1, 2281701379],"int32"), 4, )

[accuracy error] paddle.full_like(Tensor([1, 2281701379],"int32"), 4, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 4
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0]], dtype=int32)
 y: array([[4, 4, 4, ..., 4, 4, 4]], dtype=int32)
2025-04-21 23:57:11.093168 test begin: paddle.full_like(Tensor([1, 2281701379],"int64"), -1, )

[accuracy error] paddle.full_like(Tensor([1, 2281701379],"int64"), -1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 1
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0]])
 y: array([[-1, -1, -1, ..., -1, -1, -1]])
2025-04-22 00:02:58.778609 test begin: paddle.full_like(Tensor([1, 2281701379],"int64"), 1, )

[accuracy error] paddle.full_like(Tensor([1, 2281701379],"int64"), 1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 1
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0]])
 y: array([[1, 1, 1, ..., 1, 1, 1]])
2025-04-22 00:07:16.753735 test begin: paddle.full_like(Tensor([1, 2281701379],"int64"), 2, )

[accuracy error] paddle.full_like(Tensor([1, 2281701379],"int64"), 2, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 2
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0]])
 y: array([[2, 2, 2, ..., 2, 2, 2]])
2025-04-22 00:11:20.455611 test begin: paddle.full_like(Tensor([1, 2281701379],"int64"), 255, )

[accuracy error] paddle.full_like(Tensor([1, 2281701379],"int64"), 255, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 255
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0]])
 y: array([[255, 255, 255, ..., 255, 255, 255]])
2025-04-22 00:15:09.152764 test begin: paddle.full_like(Tensor([1, 2281701379],"int64"), 3, )

[accuracy error] paddle.full_like(Tensor([1, 2281701379],"int64"), 3, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 3
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0]])
 y: array([[3, 3, 3, ..., 3, 3, 3]])
2025-04-22 00:19:14.998085 test begin: paddle.full_like(Tensor([1, 2281701379],"int64"), 7, )

[accuracy error] paddle.full_like(Tensor([1, 2281701379],"int64"), 7, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 7
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0]])
 y: array([[7, 7, 7, ..., 7, 7, 7]])
2025-04-22 00:23:25.253310 test begin: paddle.full_like(Tensor([1, 2281701379],"int64"), 98, )

[accuracy error] paddle.full_like(Tensor([1, 2281701379],"int64"), 98, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 98
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0]])
 y: array([[98, 98, 98, ..., 98, 98, 98]])
2025-04-22 00:27:41.071652 test begin: paddle.full_like(Tensor([1, 3, 760567127],"float32"), fill_value=2, )

[accuracy error] paddle.full_like(Tensor([1, 3, 760567127],"float32"), fill_value=2, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701381 / 2281701381 (100%)
Max absolute difference: 2.
Max relative difference: 1.
 x: array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)
 y: array([[[2., 2., 2., ..., 2., 2., 2.],
        [2., 2., 2., ..., 2., 2., 2.],
        [2., 2., 2., ..., 2., 2., 2.]]], dtype=float32)
2025-04-22 00:29:29.657688 test begin: paddle.full_like(Tensor([1, 300, 7605672],"float32"), 1, )

[accuracy error] paddle.full_like(Tensor([1, 300, 7605672],"float32"), 1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701600 / 2281701600 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 00:32:15.779743 test begin: paddle.full_like(Tensor([1, 557057, 4096],"float32"), 1, )

[accuracy error] paddle.full_like(Tensor([1, 557057, 4096],"float32"), 1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281705472 / 2281705472 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 00:35:16.861309 test begin: paddle.full_like(Tensor([1, 760567127, 3],"float32"), fill_value=2, )

[accuracy error] paddle.full_like(Tensor([1, 760567127, 3],"float32"), fill_value=2, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701381 / 2281701381 (100%)
Max absolute difference: 2.
Max relative difference: 1.
 x: array([[[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],...
 y: array([[[2., 2., 2.],
        [2., 2., 2.],
        [2., 2., 2.],...
2025-04-22 00:37:07.923571 test begin: paddle.full_like(Tensor([10, 228170138],"float32"), -math.inf, )

[accuracy error] paddle.full_like(Tensor([10, 228170138],"float32"), -math.inf, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y -inf location mismatch:
 x: array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[-inf, -inf, -inf, ..., -inf, -inf, -inf],
       [-inf, -inf, -inf, ..., -inf, -inf, -inf],
       [-inf, -inf, -inf, ..., -inf, -inf, -inf],...
2025-04-22 00:37:45.327058 test begin: paddle.full_like(Tensor([10, 228170138],"int64"), 3, )

[accuracy error] paddle.full_like(Tensor([10, 228170138],"int64"), 3, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 3
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[3, 3, 3, ..., 3, 3, 3],
       [3, 3, 3, ..., 3, 3, 3],
       [3, 3, 3, ..., 3, 3, 3],...
2025-04-22 00:42:33.029275 test begin: paddle.full_like(Tensor([10, 228170138],"int64"), 7, )

[accuracy error] paddle.full_like(Tensor([10, 228170138],"int64"), 7, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 7
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[7, 7, 7, ..., 7, 7, 7],
       [7, 7, 7, ..., 7, 7, 7],
       [7, 7, 7, ..., 7, 7, 7],...
2025-04-22 00:46:33.214836 test begin: paddle.full_like(Tensor([1024, 2228225],"float32"), 0.334840825619673, )

[accuracy error] paddle.full_like(Tensor([1024, 2228225],"float32"), 0.334840825619673, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281702400 / 2281702400 (100%)
Max absolute difference: 0.33484083
Max relative difference: 1.
 x: array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[0.334841, 0.334841, 0.334841, ..., 0.334841, 0.334841, 0.334841],
       [0.334841, 0.334841, 0.334841, ..., 0.334841, 0.334841, 0.334841],
       [0.334841, 0.334841, 0.334841, ..., 0.334841, 0.334841, 0.334841],...
2025-04-22 00:49:09.868204 test begin: paddle.full_like(Tensor([106106, 21504],"int32"), 10, )

[accuracy error] paddle.full_like(Tensor([106106, 21504],"int32"), 10, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281703424 / 2281703424 (100%)
Max absolute difference: 10
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[10, 10, 10, ..., 10, 10, 10],
       [10, 10, 10, ..., 10, 10, 10],
       [10, 10, 10, ..., 10, 10, 10],...
2025-04-22 00:52:12.826368 test begin: paddle.full_like(Tensor([106106, 21504],"int32"), 15, )

[accuracy error] paddle.full_like(Tensor([106106, 21504],"int32"), 15, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281703424 / 2281703424 (100%)
Max absolute difference: 15
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[15, 15, 15, ..., 15, 15, 15],
       [15, 15, 15, ..., 15, 15, 15],
       [15, 15, 15, ..., 15, 15, 15],...
2025-04-22 00:55:42.137089 test begin: paddle.full_like(Tensor([1073742, 2125],"int32"), 4, )

[accuracy error] paddle.full_like(Tensor([1073742, 2125],"int32"), 4, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701750 / 2281701750 (100%)
Max absolute difference: 4
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[4, 4, 4, ..., 4, 4, 4],
       [4, 4, 4, ..., 4, 4, 4],
       [4, 4, 4, ..., 4, 4, 4],...
2025-04-22 00:58:41.098990 test begin: paddle.full_like(Tensor([1086525, 2100],"int32"), 80, )

[accuracy error] paddle.full_like(Tensor([1086525, 2100],"int32"), 80, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281702500 / 2281702500 (100%)
Max absolute difference: 80
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[80, 80, 80, ..., 80, 80, 80],
       [80, 80, 80, ..., 80, 80, 80],
       [80, 80, 80, ..., 80, 80, 80],...
2025-04-22 01:02:53.250798 test begin: paddle.full_like(Tensor([114085069, 20],"int64"), 3, )

[accuracy error] paddle.full_like(Tensor([114085069, 20],"int64"), 3, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 3
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[3, 3, 3, ..., 3, 3, 3],
       [3, 3, 3, ..., 3, 3, 3],
       [3, 3, 3, ..., 3, 3, 3],...
2025-04-22 01:07:24.024526 test begin: paddle.full_like(Tensor([114085069, 20],"int64"), 7, )

[accuracy error] paddle.full_like(Tensor([114085069, 20],"int64"), 7, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 7
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[7, 7, 7, ..., 7, 7, 7],
       [7, 7, 7, ..., 7, 7, 7],
       [7, 7, 7, ..., 7, 7, 7],...
2025-04-22 01:11:27.161988 test begin: paddle.full_like(Tensor([1140850690, 2],"float32"), -math.inf, )

[accuracy error] paddle.full_like(Tensor([1140850690, 2],"float32"), -math.inf, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y -inf location mismatch:
 x: array([[0., 0.],
       [0., 0.],
       [0., 0.],...
 y: array([[-inf, -inf],
       [-inf, -inf],
       [-inf, -inf],...
2025-04-22 01:12:25.776787 test begin: paddle.full_like(Tensor([1140850690, 2],"float32"), 0.5, )

[accuracy error] paddle.full_like(Tensor([1140850690, 2],"float32"), 0.5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 0.5
Max relative difference: 1.
 x: array([[0., 0.],
       [0., 0.],
       [0., 0.],...
 y: array([[0.5, 0.5],
       [0.5, 0.5],
       [0.5, 0.5],...
2025-04-22 01:15:00.637429 test begin: paddle.full_like(Tensor([1140850690, 2],"float32"), 3.4028234663852886e+38, )

[accuracy error] paddle.full_like(Tensor([1140850690, 2],"float32"), 3.4028234663852886e+38, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 3.4028235e+38
Max relative difference: 1.
 x: array([[0., 0.],
       [0., 0.],
       [0., 0.],...
 y: array([[3.402823e+38, 3.402823e+38],
       [3.402823e+38, 3.402823e+38],
       [3.402823e+38, 3.402823e+38],...
2025-04-22 01:16:48.917298 test begin: paddle.full_like(Tensor([1140850690, 2],"float32"), math.inf, )

[accuracy error] paddle.full_like(Tensor([1140850690, 2],"float32"), math.inf, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[0., 0.],
       [0., 0.],
       [0., 0.],...
 y: array([[inf, inf],
       [inf, inf],
       [inf, inf],...
2025-04-22 01:17:20.034682 test begin: paddle.full_like(Tensor([1140850690, 2],"int64"), 7, )

[accuracy error] paddle.full_like(Tensor([1140850690, 2],"int64"), 7, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 7
Max relative difference: 1.
 x: array([[0, 0],
       [0, 0],
       [0, 0],...
 y: array([[7, 7],
       [7, 7],
       [7, 7],...
2025-04-22 01:21:51.460650 test begin: paddle.full_like(Tensor([11641334, 4, 7, 7],"float32"), -3.4028234663852886e+38, )

[accuracy error] paddle.full_like(Tensor([11641334, 4, 7, 7],"float32"), -3.4028234663852886e+38, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701464 / 2281701464 (100%)
Max absolute difference: 3.4028235e+38
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[-3.402823e+38, -3.402823e+38, -3.402823e+38, ...,
          -3.402823e+38, -3.402823e+38, -3.402823e+38],
         [-3.402823e+38, -3.402823e+38, -3.402823e+38, ...,...
2025-04-22 01:24:04.456822 test begin: paddle.full_like(Tensor([13, 175515491],"int64"), 1, )

[accuracy error] paddle.full_like(Tensor([13, 175515491],"int64"), 1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701383 / 2281701383 (100%)
Max absolute difference: 1
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[1, 1, 1, ..., 1, 1, 1],
       [1, 1, 1, ..., 1, 1, 1],
       [1, 1, 1, ..., 1, 1, 1],...
2025-04-22 01:28:34.894928 test begin: paddle.full_like(Tensor([13, 3134206, 56],"int64"), 15, )

[accuracy error] paddle.full_like(Tensor([13, 3134206, 56],"int64"), 15, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701968 / 2281701968 (100%)
Max absolute difference: 15
Max relative difference: 1.
 x: array([[[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],...
 y: array([[[15, 15, 15, ..., 15, 15, 15],
        [15, 15, 15, ..., 15, 15, 15],
        [15, 15, 15, ..., 15, 15, 15],...
2025-04-22 01:33:04.696818 test begin: paddle.full_like(Tensor([13, 3134206, 56],"int64"), 31, )

[accuracy error] paddle.full_like(Tensor([13, 3134206, 56],"int64"), 31, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701968 / 2281701968 (100%)
Max absolute difference: 31
Max relative difference: 1.
 x: array([[[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],...
 y: array([[[31, 31, 31, ..., 31, 31, 31],
        [31, 31, 31, ..., 31, 31, 31],
        [31, 31, 31, ..., 31, 31, 31],...
2025-04-22 01:36:54.960971 test begin: paddle.full_like(Tensor([13, 3581949, 7, 7],"float32"), -3.4028234663852886e+38, )

[accuracy error] paddle.full_like(Tensor([13, 3581949, 7, 7],"float32"), -3.4028234663852886e+38, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701513 / 2281701513 (100%)
Max absolute difference: 3.4028235e+38
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[-3.402823e+38, -3.402823e+38, -3.402823e+38, ...,
          -3.402823e+38, -3.402823e+38, -3.402823e+38],
         [-3.402823e+38, -3.402823e+38, -3.402823e+38, ...,...
2025-04-22 01:38:58.471057 test begin: paddle.full_like(Tensor([13, 4, 6268411, 7],"float32"), -3.4028234663852886e+38, )

[accuracy error] paddle.full_like(Tensor([13, 4, 6268411, 7],"float32"), -3.4028234663852886e+38, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701604 / 2281701604 (100%)
Max absolute difference: 3.4028235e+38
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[-3.402823e+38, -3.402823e+38, -3.402823e+38, ...,
          -3.402823e+38, -3.402823e+38, -3.402823e+38],
         [-3.402823e+38, -3.402823e+38, -3.402823e+38, ...,...
2025-04-22 01:41:21.773983 test begin: paddle.full_like(Tensor([13, 4, 7, 6268411],"float32"), -3.4028234663852886e+38, )

[accuracy error] paddle.full_like(Tensor([13, 4, 7, 6268411],"float32"), -3.4028234663852886e+38, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701604 / 2281701604 (100%)
Max absolute difference: 3.4028235e+38
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[-3.402823e+38, -3.402823e+38, -3.402823e+38, ...,
          -3.402823e+38, -3.402823e+38, -3.402823e+38],
         [-3.402823e+38, -3.402823e+38, -3.402823e+38, ...,...
2025-04-22 01:43:52.718034 test begin: paddle.full_like(Tensor([13, 56, 3134206],"int64"), 15, )

[accuracy error] paddle.full_like(Tensor([13, 56, 3134206],"int64"), 15, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701968 / 2281701968 (100%)
Max absolute difference: 15
Max relative difference: 1.
 x: array([[[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],...
 y: array([[[15, 15, 15, ..., 15, 15, 15],
        [15, 15, 15, ..., 15, 15, 15],
        [15, 15, 15, ..., 15, 15, 15],...
2025-04-22 01:48:28.735313 test begin: paddle.full_like(Tensor([13, 56, 3134206],"int64"), 31, )

[accuracy error] paddle.full_like(Tensor([13, 56, 3134206],"int64"), 31, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701968 / 2281701968 (100%)
Max absolute difference: 31
Max relative difference: 1.
 x: array([[[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],...
 y: array([[[31, 31, 31, ..., 31, 31, 31],
        [31, 31, 31, ..., 31, 31, 31],
        [31, 31, 31, ..., 31, 31, 31],...
2025-04-22 01:52:34.954469 test begin: paddle.full_like(Tensor([134217729, 17],"int64"), 3, )

[accuracy error] paddle.full_like(Tensor([134217729, 17],"int64"), 3, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701393 / 2281701393 (100%)
Max absolute difference: 3
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[3, 3, 3, ..., 3, 3, 3],
       [3, 3, 3, ..., 3, 3, 3],
       [3, 3, 3, ..., 3, 3, 3],...
2025-04-22 01:56:27.791537 test begin: paddle.full_like(Tensor([148, 5, 3083381],"float32"), -math.inf, )

[accuracy error] paddle.full_like(Tensor([148, 5, 3083381],"float32"), -math.inf, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y -inf location mismatch:
 x: array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[-inf, -inf, -inf, ..., -inf, -inf, -inf],
        [-inf, -inf, -inf, ..., -inf, -inf, -inf],
        [-inf, -inf, -inf, ..., -inf, -inf, -inf],...
2025-04-22 01:57:13.260802 test begin: paddle.full_like(Tensor([148, 5, 3083381],"float32"), 3.4028234663852886e+38, )

[accuracy error] paddle.full_like(Tensor([148, 5, 3083381],"float32"), 3.4028234663852886e+38, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701940 / 2281701940 (100%)
Max absolute difference: 3.4028235e+38
Max relative difference: 1.
 x: array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[3.402823e+38, 3.402823e+38, 3.402823e+38, ..., 3.402823e+38,
         3.402823e+38, 3.402823e+38],
        [3.402823e+38, 3.402823e+38, 3.402823e+38, ..., 3.402823e+38,...
2025-04-22 01:59:26.620288 test begin: paddle.full_like(Tensor([148, 5, 3083381],"float32"), math.inf, )

[accuracy error] paddle.full_like(Tensor([148, 5, 3083381],"float32"), math.inf, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[inf, inf, inf, ..., inf, inf, inf],
        [inf, inf, inf, ..., inf, inf, inf],
        [inf, inf, inf, ..., inf, inf, inf],...
2025-04-22 02:00:04.478829 test begin: paddle.full_like(Tensor([148, 5138968, 3],"float32"), -math.inf, )

[accuracy error] paddle.full_like(Tensor([148, 5138968, 3],"float32"), -math.inf, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y -inf location mismatch:
 x: array([[[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],...
 y: array([[[-inf, -inf, -inf],
        [-inf, -inf, -inf],
        [-inf, -inf, -inf],...
2025-04-22 02:00:45.028870 test begin: paddle.full_like(Tensor([148, 5138968, 3],"float32"), 3.4028234663852886e+38, )

[accuracy error] paddle.full_like(Tensor([148, 5138968, 3],"float32"), 3.4028234663852886e+38, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701792 / 2281701792 (100%)
Max absolute difference: 3.4028235e+38
Max relative difference: 1.
 x: array([[[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],...
 y: array([[[3.402823e+38, 3.402823e+38, 3.402823e+38],
        [3.402823e+38, 3.402823e+38, 3.402823e+38],
        [3.402823e+38, 3.402823e+38, 3.402823e+38],...
2025-04-22 02:02:48.780971 test begin: paddle.full_like(Tensor([148, 5138968, 3],"float32"), math.inf, )

[accuracy error] paddle.full_like(Tensor([148, 5138968, 3],"float32"), math.inf, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],...
 y: array([[[inf, inf, inf],
        [inf, inf, inf],
        [inf, inf, inf],...
2025-04-22 02:03:17.532414 test begin: paddle.full_like(Tensor([152113426, 5, 3],"float32"), -math.inf, )

[accuracy error] paddle.full_like(Tensor([152113426, 5, 3],"float32"), -math.inf, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y -inf location mismatch:
 x: array([[[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],...
 y: array([[[-inf, -inf, -inf],
        [-inf, -inf, -inf],
        [-inf, -inf, -inf],...
2025-04-22 02:04:02.234252 test begin: paddle.full_like(Tensor([152113426, 5, 3],"float32"), 3.4028234663852886e+38, )

[accuracy error] paddle.full_like(Tensor([152113426, 5, 3],"float32"), 3.4028234663852886e+38, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701390 / 2281701390 (100%)
Max absolute difference: 3.4028235e+38
Max relative difference: 1.
 x: array([[[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],...
 y: array([[[3.402823e+38, 3.402823e+38, 3.402823e+38],
        [3.402823e+38, 3.402823e+38, 3.402823e+38],
        [3.402823e+38, 3.402823e+38, 3.402823e+38],...
2025-04-22 02:06:40.446307 test begin: paddle.full_like(Tensor([152113426, 5, 3],"float32"), math.inf, )

[accuracy error] paddle.full_like(Tensor([152113426, 5, 3],"float32"), math.inf, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],...
 y: array([[[inf, inf, inf],
        [inf, inf, inf],
        [inf, inf, inf],...
2025-04-22 02:07:14.210471 test begin: paddle.full_like(Tensor([16, 142606337],"float32"), 1.0, )

[accuracy error] paddle.full_like(Tensor([16, 142606337],"float32"), 1.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701392 / 2281701392 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[1., 1., 1., ..., 1., 1., 1.],
       [1., 1., 1., ..., 1., 1., 1.],
       [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 02:09:15.401733 test begin: paddle.full_like(Tensor([16, 142606337],"int32"), 4, )

[accuracy error] paddle.full_like(Tensor([16, 142606337],"int32"), 4, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701392 / 2281701392 (100%)
Max absolute difference: 4
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[4, 4, 4, ..., 4, 4, 4],
       [4, 4, 4, ..., 4, 4, 4],
       [4, 4, 4, ..., 4, 4, 4],...
2025-04-22 02:12:18.676408 test begin: paddle.full_like(Tensor([16, 142606337],"int32"), 80, )

[accuracy error] paddle.full_like(Tensor([16, 142606337],"int32"), 80, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701392 / 2281701392 (100%)
Max absolute difference: 80
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[80, 80, 80, ..., 80, 80, 80],
       [80, 80, 80, ..., 80, 80, 80],
       [80, 80, 80, ..., 80, 80, 80],...
2025-04-22 02:15:55.775367 test begin: paddle.full_like(Tensor([17, 134217729],"int64"), 3, )

[accuracy error] paddle.full_like(Tensor([17, 134217729],"int64"), 3, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701393 / 2281701393 (100%)
Max absolute difference: 3
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[3, 3, 3, ..., 3, 3, 3],
       [3, 3, 3, ..., 3, 3, 3],
       [3, 3, 3, ..., 3, 3, 3],...
2025-04-22 02:19:51.340501 test begin: paddle.full_like(Tensor([178956971, 3, 4],"float64"), fill_value=1, )

[accuracy error] paddle.full_like(Tensor([178956971, 3, 4],"float64"), fill_value=1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2147483652 / 2147483652 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.]],...
 y: array([[[1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.]],...
2025-04-22 02:24:49.814472 test begin: paddle.full_like(Tensor([1857, 300, 4096],"float32"), 1, )

[accuracy error] paddle.full_like(Tensor([1857, 300, 4096],"float32"), 1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281881600 / 2281881600 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 02:26:50.923610 test begin: paddle.full_like(Tensor([188633, 12096],"int32"), 1, )

[accuracy error] paddle.full_like(Tensor([188633, 12096],"int32"), 1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281704768 / 2281704768 (100%)
Max absolute difference: 1
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[1, 1, 1, ..., 1, 1, 1],
       [1, 1, 1, ..., 1, 1, 1],
       [1, 1, 1, ..., 1, 1, 1],...
2025-04-22 02:29:35.649952 test begin: paddle.full_like(Tensor([188633, 12096],"int32"), 80, )

[accuracy error] paddle.full_like(Tensor([188633, 12096],"int32"), 80, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281704768 / 2281704768 (100%)
Max absolute difference: 80
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[80, 80, 80, ..., 80, 80, 80],
       [80, 80, 80, ..., 80, 80, 80],
       [80, 80, 80, ..., 80, 80, 80],...
2025-04-22 02:33:01.025916 test begin: paddle.full_like(Tensor([1948, 1171305],"float32"), -math.inf, )

[accuracy error] paddle.full_like(Tensor([1948, 1171305],"float32"), -math.inf, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y -inf location mismatch:
 x: array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[-inf, -inf, -inf, ..., -inf, -inf, -inf],
       [-inf, -inf, -inf, ..., -inf, -inf, -inf],
       [-inf, -inf, -inf, ..., -inf, -inf, -inf],...
2025-04-22 02:33:42.382224 test begin: paddle.full_like(Tensor([1948, 1171305],"float32"), 3.4028234663852886e+38, )

[accuracy error] paddle.full_like(Tensor([1948, 1171305],"float32"), 3.4028234663852886e+38, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281702140 / 2281702140 (100%)
Max absolute difference: 3.4028235e+38
Max relative difference: 1.
 x: array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[3.402823e+38, 3.402823e+38, 3.402823e+38, ..., 3.402823e+38,
        3.402823e+38, 3.402823e+38],
       [3.402823e+38, 3.402823e+38, 3.402823e+38, ..., 3.402823e+38,...
2025-04-22 02:35:45.398478 test begin: paddle.full_like(Tensor([1948, 1171305],"float32"), math.inf, )

[accuracy error] paddle.full_like(Tensor([1948, 1171305],"float32"), math.inf, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[inf, inf, inf, ..., inf, inf, inf],
       [inf, inf, inf, ..., inf, inf, inf],
       [inf, inf, inf, ..., inf, inf, inf],...
2025-04-22 02:36:16.501571 test begin: paddle.full_like(Tensor([2, 1, 1, 1140850690],"float32"), -3.4028234663852886e+38, )

[accuracy error] paddle.full_like(Tensor([2, 1, 1, 1140850690],"float32"), -3.4028234663852886e+38, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 3.4028235e+38
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.]]],


       [[[0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32)
 y: array([[[[-3.402823e+38, -3.402823e+38, -3.402823e+38, ...,
          -3.402823e+38, -3.402823e+38, -3.402823e+38]]],
...
2025-04-22 02:38:03.946892 test begin: paddle.full_like(Tensor([2, 1, 10, 114085069],"float32"), -3.4028234663852886e+38, )

[accuracy error] paddle.full_like(Tensor([2, 1, 10, 114085069],"float32"), -3.4028234663852886e+38, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 3.4028235e+38
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[-3.402823e+38, -3.402823e+38, -3.402823e+38, ...,
          -3.402823e+38, -3.402823e+38, -3.402823e+38],
         [-3.402823e+38, -3.402823e+38, -3.402823e+38, ...,...
2025-04-22 02:40:10.892180 test begin: paddle.full_like(Tensor([2, 1, 114085069, 10],"float32"), -3.4028234663852886e+38, )

[accuracy error] paddle.full_like(Tensor([2, 1, 114085069, 10],"float32"), -3.4028234663852886e+38, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 3.4028235e+38
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[-3.402823e+38, -3.402823e+38, -3.402823e+38, ...,
          -3.402823e+38, -3.402823e+38, -3.402823e+38],
         [-3.402823e+38, -3.402823e+38, -3.402823e+38, ...,...
2025-04-22 02:43:16.652957 test begin: paddle.full_like(Tensor([2, 1, 162978670, 7],"float32"), -3.4028234663852886e+38, )

[accuracy error] paddle.full_like(Tensor([2, 1, 162978670, 7],"float32"), -3.4028234663852886e+38, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 3.4028235e+38
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[-3.402823e+38, -3.402823e+38, -3.402823e+38, ...,
          -3.402823e+38, -3.402823e+38, -3.402823e+38],
         [-3.402823e+38, -3.402823e+38, -3.402823e+38, ...,...
2025-04-22 02:45:32.809586 test begin: paddle.full_like(Tensor([2, 1, 190141782, 6],"float32"), -3.4028234663852886e+38, )

[accuracy error] paddle.full_like(Tensor([2, 1, 190141782, 6],"float32"), -3.4028234663852886e+38, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701384 / 2281701384 (100%)
Max absolute difference: 3.4028235e+38
Max relative difference: 1.
 x: array([[[[0., 0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0., 0.],...
 y: array([[[[-3.402823e+38, -3.402823e+38, -3.402823e+38, -3.402823e+38,
          -3.402823e+38, -3.402823e+38],
         [-3.402823e+38, -3.402823e+38, -3.402823e+38, -3.402823e+38,...
2025-04-22 02:47:37.930944 test begin: paddle.full_like(Tensor([2, 1073741825],"float64"), 0.5, )

[accuracy error] paddle.full_like(Tensor([2, 1073741825],"float64"), 0.5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2147483650 / 2147483650 (100%)
Max absolute difference: 0.5
Max relative difference: 1.
 x: array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]])
 y: array([[0.5, 0.5, 0.5, ..., 0.5, 0.5, 0.5],
       [0.5, 0.5, 0.5, ..., 0.5, 0.5, 0.5]])
2025-04-22 02:51:49.440212 test begin: paddle.full_like(Tensor([2, 1140850690],"float32"), -10.0, )

[accuracy error] paddle.full_like(Tensor([2, 1140850690],"float32"), -10.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 10.
Max relative difference: 1.
 x: array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)
 y: array([[-10., -10., -10., ..., -10., -10., -10.],
       [-10., -10., -10., ..., -10., -10., -10.]], dtype=float32)
2025-04-22 02:53:42.872756 test begin: paddle.full_like(Tensor([2, 1140850690],"float32"), -3.4028234663852886e+38, )

[accuracy error] paddle.full_like(Tensor([2, 1140850690],"float32"), -3.4028234663852886e+38, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 3.4028235e+38
Max relative difference: 1.
 x: array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)
 y: array([[-3.402823e+38, -3.402823e+38, -3.402823e+38, ..., -3.402823e+38,
        -3.402823e+38, -3.402823e+38],
       [-3.402823e+38, -3.402823e+38, -3.402823e+38, ..., -3.402823e+38,
        -3.402823e+38, -3.402823e+38]], dtype=float32)
2025-04-22 02:55:57.234652 test begin: paddle.full_like(Tensor([2, 1140850690],"float32"), -math.inf, )

[accuracy error] paddle.full_like(Tensor([2, 1140850690],"float32"), -math.inf, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y -inf location mismatch:
 x: array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)
 y: array([[-inf, -inf, -inf, ..., -inf, -inf, -inf],
       [-inf, -inf, -inf, ..., -inf, -inf, -inf]], dtype=float32)
2025-04-22 02:56:46.029311 test begin: paddle.full_like(Tensor([2, 1140850690],"float32"), 0.5, )

[accuracy error] paddle.full_like(Tensor([2, 1140850690],"float32"), 0.5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 0.5
Max relative difference: 1.
 x: array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)
 y: array([[0.5, 0.5, 0.5, ..., 0.5, 0.5, 0.5],
       [0.5, 0.5, 0.5, ..., 0.5, 0.5, 0.5]], dtype=float32)
2025-04-22 02:59:09.202547 test begin: paddle.full_like(Tensor([2, 1140850690],"float32"), 1.0, )

[accuracy error] paddle.full_like(Tensor([2, 1140850690],"float32"), 1.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)
 y: array([[1., 1., 1., ..., 1., 1., 1.],
       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32)
2025-04-22 03:01:57.803333 test begin: paddle.full_like(Tensor([2, 1140850690],"float32"), 100.0, )

[accuracy error] paddle.full_like(Tensor([2, 1140850690],"float32"), 100.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 100.
Max relative difference: 1.
 x: array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)
 y: array([[100., 100., 100., ..., 100., 100., 100.],
       [100., 100., 100., ..., 100., 100., 100.]], dtype=float32)
2025-04-22 03:04:40.215641 test begin: paddle.full_like(Tensor([2, 1140850690],"float32"), 2.0, )

[accuracy error] paddle.full_like(Tensor([2, 1140850690],"float32"), 2.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 2.
Max relative difference: 1.
 x: array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)
 y: array([[2., 2., 2., ..., 2., 2., 2.],
       [2., 2., 2., ..., 2., 2., 2.]], dtype=float32)
2025-04-22 03:07:17.092511 test begin: paddle.full_like(Tensor([2, 1140850690],"float32"), 3.4028234663852886e+38, )

[accuracy error] paddle.full_like(Tensor([2, 1140850690],"float32"), 3.4028234663852886e+38, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 3.4028235e+38
Max relative difference: 1.
 x: array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)
 y: array([[3.402823e+38, 3.402823e+38, 3.402823e+38, ..., 3.402823e+38,
        3.402823e+38, 3.402823e+38],
       [3.402823e+38, 3.402823e+38, 3.402823e+38, ..., 3.402823e+38,
        3.402823e+38, 3.402823e+38]], dtype=float32)
2025-04-22 03:09:21.335704 test begin: paddle.full_like(Tensor([2, 1140850690],"float32"), fill_value=2, )

[accuracy error] paddle.full_like(Tensor([2, 1140850690],"float32"), fill_value=2, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 2.
Max relative difference: 1.
 x: array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)
 y: array([[2., 2., 2., ..., 2., 2., 2.],
       [2., 2., 2., ..., 2., 2., 2.]], dtype=float32)
2025-04-22 03:11:34.347818 test begin: paddle.full_like(Tensor([2, 1140850690],"float32"), fill_value=math.inf, )

[accuracy error] paddle.full_like(Tensor([2, 1140850690],"float32"), fill_value=math.inf, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)
 y: array([[inf, inf, inf, ..., inf, inf, inf],
       [inf, inf, inf, ..., inf, inf, inf]], dtype=float32)
2025-04-22 03:12:07.135209 test begin: paddle.full_like(Tensor([2, 1140850690],"float32"), fill_value=nan, )

[accuracy error] paddle.full_like(Tensor([2, 1140850690],"float32"), fill_value=nan, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)
 y: array([[nan, nan, nan, ..., nan, nan, nan],
       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)
2025-04-22 03:12:36.129234 test begin: paddle.full_like(Tensor([2, 1140850690],"float32"), math.inf, )

[accuracy error] paddle.full_like(Tensor([2, 1140850690],"float32"), math.inf, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)
 y: array([[inf, inf, inf, ..., inf, inf, inf],
       [inf, inf, inf, ..., inf, inf, inf]], dtype=float32)
2025-04-22 03:13:04.945479 test begin: paddle.full_like(Tensor([2, 1140850690],"int32"), 1, )

[accuracy error] paddle.full_like(Tensor([2, 1140850690],"int32"), 1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 1
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)
 y: array([[1, 1, 1, ..., 1, 1, 1],
       [1, 1, 1, ..., 1, 1, 1]], dtype=int32)
2025-04-22 03:15:56.316109 test begin: paddle.full_like(Tensor([2, 1140850690],"int64"), -1, )

[accuracy error] paddle.full_like(Tensor([2, 1140850690],"int64"), -1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 1
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0]])
 y: array([[-1, -1, -1, ..., -1, -1, -1],
       [-1, -1, -1, ..., -1, -1, -1]])
2025-04-22 03:20:27.427873 test begin: paddle.full_like(Tensor([2, 1140850690],"int64"), 151643, )

[accuracy error] paddle.full_like(Tensor([2, 1140850690],"int64"), 151643, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 151643
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0]])
 y: array([[151643, 151643, 151643, ..., 151643, 151643, 151643],
       [151643, 151643, 151643, ..., 151643, 151643, 151643]])
2025-04-22 03:24:33.610510 test begin: paddle.full_like(Tensor([2, 1140850690],"int64"), 2, )

[accuracy error] paddle.full_like(Tensor([2, 1140850690],"int64"), 2, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 2
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0]])
 y: array([[2, 2, 2, ..., 2, 2, 2],
       [2, 2, 2, ..., 2, 2, 2]])
2025-04-22 03:29:16.538128 test begin: paddle.full_like(Tensor([2, 1140850690],"int64"), 255, )

[accuracy error] paddle.full_like(Tensor([2, 1140850690],"int64"), 255, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 255
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0]])
 y: array([[255, 255, 255, ..., 255, 255, 255],
       [255, 255, 255, ..., 255, 255, 255]])
2025-04-22 03:33:54.877075 test begin: paddle.full_like(Tensor([2, 1140850690],"int64"), 3, )

[accuracy error] paddle.full_like(Tensor([2, 1140850690],"int64"), 3, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 3
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0]])
 y: array([[3, 3, 3, ..., 3, 3, 3],
       [3, 3, 3, ..., 3, 3, 3]])
2025-04-22 03:38:29.789154 test begin: paddle.full_like(Tensor([2, 1140850690],"int64"), 7, )

[accuracy error] paddle.full_like(Tensor([2, 1140850690],"int64"), 7, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 7
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0]])
 y: array([[7, 7, 7, ..., 7, 7, 7],
       [7, 7, 7, ..., 7, 7, 7]])
2025-04-22 03:44:16.872866 test begin: paddle.full_like(Tensor([2, 1140850690],"int64"), 98, )

[accuracy error] paddle.full_like(Tensor([2, 1140850690],"int64"), 98, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 98
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0]])
 y: array([[98, 98, 98, ..., 98, 98, 98],
       [98, 98, 98, ..., 98, 98, 98]])
2025-04-22 03:49:14.194047 test begin: paddle.full_like(Tensor([2, 11408507, 10, 10],"float32"), -3.4028234663852886e+38, )

[accuracy error] paddle.full_like(Tensor([2, 11408507, 10, 10],"float32"), -3.4028234663852886e+38, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701400 / 2281701400 (100%)
Max absolute difference: 3.4028235e+38
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[-3.402823e+38, -3.402823e+38, -3.402823e+38, ...,
          -3.402823e+38, -3.402823e+38, -3.402823e+38],
         [-3.402823e+38, -3.402823e+38, -3.402823e+38, ...,...
2025-04-22 03:51:06.013309 test begin: paddle.full_like(Tensor([2, 162978670, 1, 7],"float32"), -3.4028234663852886e+38, )

[accuracy error] paddle.full_like(Tensor([2, 162978670, 1, 7],"float32"), -3.4028234663852886e+38, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 3.4028235e+38
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.]],...
 y: array([[[[-3.402823e+38, -3.402823e+38, -3.402823e+38, ...,
          -3.402823e+38, -3.402823e+38, -3.402823e+38]],
...
2025-04-22 03:52:57.151699 test begin: paddle.full_like(Tensor([2, 190141782, 1, 6],"float32"), -3.4028234663852886e+38, )

[accuracy error] paddle.full_like(Tensor([2, 190141782, 1, 6],"float32"), -3.4028234663852886e+38, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701384 / 2281701384 (100%)
Max absolute difference: 3.4028235e+38
Max relative difference: 1.
 x: array([[[[0., 0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0., 0.]],...
 y: array([[[[-3.402823e+38, -3.402823e+38, -3.402823e+38, -3.402823e+38,
          -3.402823e+38, -3.402823e+38]],
...
2025-04-22 03:54:57.181334 test begin: paddle.full_like(Tensor([20, 114085069],"int64"), 3, )

[accuracy error] paddle.full_like(Tensor([20, 114085069],"int64"), 3, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 3
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[3, 3, 3, ..., 3, 3, 3],
       [3, 3, 3, ..., 3, 3, 3],
       [3, 3, 3, ..., 3, 3, 3],...
2025-04-22 03:59:44.147939 test begin: paddle.full_like(Tensor([20, 114085069],"int64"), 7, )

[accuracy error] paddle.full_like(Tensor([20, 114085069],"int64"), 7, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 7
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[7, 7, 7, ..., 7, 7, 7],
       [7, 7, 7, ..., 7, 7, 7],
       [7, 7, 7, ..., 7, 7, 7],...
2025-04-22 04:03:36.700946 test begin: paddle.full_like(Tensor([205393, 11109],"int32"), 1, )

[accuracy error] paddle.full_like(Tensor([205393, 11109],"int32"), 1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281710837 / 2281710837 (100%)
Max absolute difference: 1
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[1, 1, 1, ..., 1, 1, 1],
       [1, 1, 1, ..., 1, 1, 1],
       [1, 1, 1, ..., 1, 1, 1],...
2025-04-22 04:06:27.432161 test begin: paddle.full_like(Tensor([205393, 11109],"int32"), 80, )

[accuracy error] paddle.full_like(Tensor([205393, 11109],"int32"), 80, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281710837 / 2281710837 (100%)
Max absolute difference: 80
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[80, 80, 80, ..., 80, 80, 80],
       [80, 80, 80, ..., 80, 80, 80],
       [80, 80, 80, ..., 80, 80, 80],...
2025-04-22 04:09:30.570481 test begin: paddle.full_like(Tensor([2147483649, 1],"float64"), 0.5, )

[accuracy error] paddle.full_like(Tensor([2147483649, 1],"float64"), 0.5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2147483649 / 2147483649 (100%)
Max absolute difference: 0.5
Max relative difference: 1.
 x: array([[0.],
       [0.],
       [0.],...
 y: array([[0.5],
       [0.5],
       [0.5],...
2025-04-22 04:12:59.812838 test begin: paddle.full_like(Tensor([2147483649, 1],"float64"), 1.7976931348623157e+308, )

[accuracy error] paddle.full_like(Tensor([2147483649, 1],"float64"), 1.7976931348623157e+308, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2147483649 / 2147483649 (100%)
Max absolute difference: 1.79769313e+308
Max relative difference: 1.
 x: array([[0.],
       [0.],
       [0.],...
 y: array([[1.797693e+308],
       [1.797693e+308],
       [1.797693e+308],...
2025-04-22 04:16:34.693997 test begin: paddle.full_like(Tensor([219, 10418728],"int32"), 1.0, None, None, )

[accuracy error] paddle.full_like(Tensor([219, 10418728],"int32"), 1.0, None, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701432 / 2281701432 (100%)
Max absolute difference: 1
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[1, 1, 1, ..., 1, 1, 1],
       [1, 1, 1, ..., 1, 1, 1],
       [1, 1, 1, ..., 1, 1, 1],...
2025-04-22 04:19:05.983715 test begin: paddle.full_like(Tensor([221848, 10285],"int32"), 11, )

[accuracy error] paddle.full_like(Tensor([221848, 10285],"int32"), 11, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281706680 / 2281706680 (100%)
Max absolute difference: 11
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[11, 11, 11, ..., 11, 11, 11],
       [11, 11, 11, ..., 11, 11, 11],
       [11, 11, 11, ..., 11, 11, 11],...
2025-04-22 04:22:22.878097 test begin: paddle.full_like(Tensor([2228225, 1024],"float32"), 0.334840825619673, )

[accuracy error] paddle.full_like(Tensor([2228225, 1024],"float32"), 0.334840825619673, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281702400 / 2281702400 (100%)
Max absolute difference: 0.33484083
Max relative difference: 1.
 x: array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[0.334841, 0.334841, 0.334841, ..., 0.334841, 0.334841, 0.334841],
       [0.334841, 0.334841, 0.334841, ..., 0.334841, 0.334841, 0.334841],
       [0.334841, 0.334841, 0.334841, ..., 0.334841, 0.334841, 0.334841],...
2025-04-22 04:24:12.306568 test begin: paddle.full_like(Tensor([224489, 10164],"int32"), 1, )

[accuracy error] paddle.full_like(Tensor([224489, 10164],"int32"), 1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281706196 / 2281706196 (100%)
Max absolute difference: 1
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[1, 1, 1, ..., 1, 1, 1],
       [1, 1, 1, ..., 1, 1, 1],
       [1, 1, 1, ..., 1, 1, 1],...
2025-04-22 04:27:15.523393 test begin: paddle.full_like(Tensor([224489, 10164],"int32"), 80, )

[accuracy error] paddle.full_like(Tensor([224489, 10164],"int32"), 80, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281706196 / 2281706196 (100%)
Max absolute difference: 80
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[80, 80, 80, ..., 80, 80, 80],
       [80, 80, 80, ..., 80, 80, 80],
       [80, 80, 80, ..., 80, 80, 80],...
2025-04-22 04:30:42.331611 test begin: paddle.full_like(Tensor([2281701379, 1, 1, 1],"float32"), -3.4028234663852886e+38, )

[accuracy error] paddle.full_like(Tensor([2281701379, 1, 1, 1],"float32"), -3.4028234663852886e+38, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 3.4028235e+38
Max relative difference: 1.
 x: array([[[[0.]]],

...
 y: array([[[[-3.402823e+38]]],

...
2025-04-22 04:33:26.838775 test begin: paddle.full_like(Tensor([2281701379, 1],"float32"), 0.5, )

[accuracy error] paddle.full_like(Tensor([2281701379, 1],"float32"), 0.5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 0.5
Max relative difference: 1.
 x: array([[0.],
       [0.],
       [0.],...
 y: array([[0.5],
       [0.5],
       [0.5],...
2025-04-22 04:36:18.883134 test begin: paddle.hypot(Tensor([1],"float32"), Tensor([2281701379],"float32"), )

[paddle error] paddle.hypot(Tensor([1],"float32"), Tensor([2281701379],"float32"), ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-22 04:36:45.347404 test begin: paddle.hypot(Tensor([2281701379],"float32"), Tensor([1],"float32"), )

[paddle error] paddle.hypot(Tensor([2281701379],"float32"), Tensor([1],"float32"), ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-22 04:37:15.751449 test begin: paddle.inner(Tensor([42949673, 10, 10],"float16"), Tensor([2, 10],"float16"), )

[accuracy error] backward  paddle.inner(Tensor([42949673, 10, 10],"float16"), Tensor([2, 10],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2 / 20 (10%)
Max absolute difference: 16.
Max relative difference: 0.02501
 x: array([[ 3182. ,  2242. ,  2358. ,  -155.2,  2744. ,  -727. , -2072. ,
          980.5,  -938. ,   935. ],
       [ 2936. , -1547. ,  -634. ,  1595. , -2120. ,  1009. ,  1610. ,
        -2902. ,  -125.5,  1556. ]], dtype=float16)
 y: array([[ 3182.  ,  2240.  ,  2368.  ,  -157.2 ,  2746.  ,  -728.5 ,
        -2070.  ,   976.5 ,  -937.  ,   930.  ],
       [ 2926.  , -1546.  ,  -633.5 ,  1588.  , -2120.  ,  1002.5 ,
         1602.  , -2918.  ,  -122.44,  1559.  ]], dtype=float16)
2025-04-22 04:48:25.232378 test begin: paddle.inner(Tensor([5, 85899346, 10],"float16"), Tensor([2, 10],"float16"), )

[accuracy error] backward  paddle.inner(Tensor([5, 85899346, 10],"float16"), Tensor([2, 10],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2 / 20 (10%)
Max absolute difference: 16.
Max relative difference: 0.02501
 x: array([[ 3182. ,  2242. ,  2358. ,  -155.2,  2744. ,  -727. , -2072. ,
          980.5,  -938. ,   935. ],
       [ 2936. , -1547. ,  -634. ,  1595. , -2120. ,  1009. ,  1610. ,
        -2902. ,  -125.5,  1556. ]], dtype=float16)
 y: array([[ 3182.  ,  2240.  ,  2368.  ,  -157.2 ,  2746.  ,  -728.5 ,
        -2070.  ,   976.5 ,  -937.  ,   930.  ],
       [ 2926.  , -1546.  ,  -633.5 ,  1588.  , -2120.  ,  1002.5 ,
         1602.  , -2918.  ,  -122.44,  1559.  ]], dtype=float16)
2025-04-22 04:58:34.939625 test begin: paddle.inner(x=Tensor([2, 5, 3, 143165577],"float16"), y=Tensor([3, 2, 5, 143165577],"float16"), )

[accuracy error] paddle.inner(x=Tensor([2, 5, 3, 143165577],"float16"), y=Tensor([3, 2, 5, 143165577],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 68 / 900 (7.56%)
Max absolute difference: 12.
Max relative difference: 3.664
 x: array([[[[[[        inf, -3.0325e+02,  9.0250e+02, -1.5910e+03,
            -8.3500e+02],
           [ 6.6800e+02,  8.0300e+02, -2.1000e+02, -3.7900e+02,...
 y: array([[[[[[        inf, -3.0325e+02,  9.0150e+02, -1.5920e+03,
            -8.3400e+02],
           [ 6.6750e+02,  8.0400e+02, -2.0988e+02, -3.7825e+02,...
2025-04-22 04:59:01.424790 test begin: paddle.isclose(Tensor([10, 429496730],"float16"), Tensor([10, 429496730],"float16"), rtol=1e-05, atol=1e-08, )

[accuracy error] paddle.isclose(Tensor([10, 429496730],"float16"), Tensor([10, 429496730],"float16"), rtol=1e-05, atol=1e-08, ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294967300 (100%)
 x: array([[ True,  True,  True, ..., False, False, False],
       [False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False],...
 y: array([[ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],...
2025-04-22 05:00:15.540683 test begin: paddle.isclose(Tensor([429496730, 10],"float16"), Tensor([429496730, 10],"float16"), rtol=1e-05, atol=1e-08, )

[accuracy error] paddle.isclose(Tensor([429496730, 10],"float16"), Tensor([429496730, 10],"float16"), rtol=1e-05, atol=1e-08, ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294967300 (100%)
 x: array([[ True,  True,  True, ..., False, False, False],
       [False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False],...
 y: array([[ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],...
2025-04-22 05:01:26.651126 test begin: paddle.isclose(x=Tensor([214748365, 4, 5],"float16"), y=Tensor([214748365, 4, 5],"float16"), )

[accuracy error] paddle.isclose(x=Tensor([214748365, 4, 5],"float16"), y=Tensor([214748365, 4, 5],"float16"), ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294967300 (100%)
 x: array([[[ True,  True,  True,  True, False],
        [False, False, False, False, False],
        [False, False, False, False, False],...
 y: array([[[ True,  True,  True,  True,  True],
        [ True,  True,  True,  True,  True],
        [ True,  True,  True,  True,  True],...
2025-04-22 05:02:39.873301 test begin: paddle.isclose(x=Tensor([2281701379],"float32"), y=Tensor([2281701379],"float32"), )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745269374 (unix time) try "date -d @1745269374" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x24c45) received by PID 150597 (TID 0x7f2a93744700) from PID 150597 ***]

2025-04-22 05:03:43.946270 test begin: paddle.isclose(x=Tensor([3, 286331154, 5],"float16"), y=Tensor([3, 286331154, 5],"float16"), )

W0422 05:05:17.961342 79622 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 05:05:17.962711 79622 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.isclose(x=Tensor([3, 286331154, 5],"float16"), y=Tensor([3, 286331154, 5],"float16"), ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294967310 (100%)
 x: array([[[ True,  True,  True,  True,  True],
        [ True,  True,  True,  True,  True],
        [ True,  True,  True,  True, False],...
 y: array([[[ True,  True,  True,  True,  True],
        [ True,  True,  True,  True,  True],
        [ True,  True,  True,  True,  True],...
2025-04-22 05:06:33.649387 test begin: paddle.isclose(x=Tensor([3, 4, 357913942],"float16"), y=Tensor([3, 4, 357913942],"float16"), )

[accuracy error] paddle.isclose(x=Tensor([3, 4, 357913942],"float16"), y=Tensor([3, 4, 357913942],"float16"), ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294967304 (100%)
 x: array([[[ True,  True,  True, ..., False, False, False],
        [False, False, False, ..., False, False, False],
        [False, False, False, ..., False, False, False],...
 y: array([[[ True,  True,  True, ...,  True,  True,  True],
        [ True,  True,  True, ...,  True,  True,  True],
        [ True,  True,  True, ...,  True,  True,  True],...
2025-04-22 05:08:00.951181 test begin: paddle.isclose(x=Tensor([4294967297],"float16"), y=Tensor([4294967297],"float16"), )

[accuracy error] paddle.isclose(x=Tensor([4294967297],"float16"), y=Tensor([4294967297],"float16"), ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294967297 (100%)
 x: array([ True, False, False, ..., False, False, False])
 y: array([ True,  True,  True, ...,  True,  True,  True])
2025-04-22 05:09:30.681537 test begin: paddle.isfinite(Tensor([11, 17, 12201612],"int32"), )

[cuda error] paddle.isfinite(Tensor([11, 17, 12201612],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 05:10:17.201641 test begin: paddle.isfinite(Tensor([11, 17, 22967740],"int32"), )

[accuracy error] paddle.isfinite(Tensor([11, 17, 22967740],"int32"), ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294967380 (100%)
 x: array([[[ True,  True,  True, ..., False, False, False],
        [False, False, False, ..., False, False, False],
        [False, False, False, ..., False, False, False],...
 y: array([[[ True,  True,  True, ...,  True,  True,  True],
        [ True,  True,  True, ...,  True,  True,  True],
        [ True,  True,  True, ...,  True,  True,  True],...
2025-04-22 05:11:42.145583 test begin: paddle.isfinite(Tensor([11, 207427399],"float32"), )

[cuda error] paddle.isfinite(Tensor([11, 207427399],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 05:13:31.315126 test begin: paddle.isfinite(Tensor([11, 20742740, 10],"int32"), )

[cuda error] paddle.isfinite(Tensor([11, 20742740, 10],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 05:13:39.201299 test begin: paddle.isfinite(Tensor([11, 390451573],"float32"), )

[accuracy error] paddle.isfinite(Tensor([11, 390451573],"float32"), ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294967303 (100%)
 x: array([[ True,  True,  True, ..., False, False, False],
       [False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False],...
 y: array([[ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],...
2025-04-22 05:14:55.340028 test begin: paddle.isfinite(Tensor([11, 39045158, 10],"int32"), )

[accuracy error] paddle.isfinite(Tensor([11, 39045158, 10],"int32"), ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294967380 (100%)
 x: array([[[ True,  True,  True, ...,  True,  True,  True],
        [ True,  True,  True, ...,  True,  True,  True],
        [ True,  True,  True, ...,  True,  True,  True],...
 y: array([[[ True,  True,  True, ...,  True,  True,  True],
        [ True,  True,  True, ...,  True,  True,  True],
        [ True,  True,  True, ...,  True,  True,  True],...
2025-04-22 05:16:06.782003 test begin: paddle.isfinite(Tensor([1203073, 17, 5, 6, 7],"float16"), )

[accuracy error] paddle.isfinite(Tensor([1203073, 17, 5, 6, 7],"float16"), ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294970610 (100%)
 x: array([[[[[ True,  True,  True, ...,  True,  True,  True],
          [ True,  True,  True, ...,  True,  True,  True],
          [ True,  True,  True, ...,  True,  True,  True],...
 y: array([[[[[ True,  True,  True, ...,  True,  True,  True],
          [ True,  True,  True, ...,  True,  True,  True],
          [ True,  True,  True, ...,  True,  True,  True],...
2025-04-22 05:17:32.600446 test begin: paddle.isfinite(Tensor([134217729, 17],"float32"), )

[cuda error] paddle.isfinite(Tensor([134217729, 17],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 05:17:40.595455 test begin: paddle.isfinite(Tensor([13421773, 17, 10],"int32"), )

[cuda error] paddle.isfinite(Tensor([13421773, 17, 10],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 05:17:48.105030 test begin: paddle.isfinite(Tensor([1431655765, 3],"float32"), )

[cuda error] paddle.isfinite(Tensor([1431655765, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 05:18:03.604525 test begin: paddle.isfinite(Tensor([2, 107374183, 4, 5],"float16"), )

[accuracy error] paddle.isfinite(Tensor([2, 107374183, 4, 5],"float16"), ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294967320 (100%)
 x: array([[[[ True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True],...
 y: array([[[[ True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True],...
2025-04-22 05:19:12.603485 test begin: paddle.isfinite(Tensor([2, 2147483648],"float32"), )

[cuda error] paddle.isfinite(Tensor([2, 2147483648],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 05:19:22.949110 test begin: paddle.isfinite(Tensor([2, 3, 143165577, 5],"float16"), )

[accuracy error] paddle.isfinite(Tensor([2, 3, 143165577, 5],"float16"), ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294967310 (100%)
 x: array([[[[ True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True],
         [ True,  True,  True,  True, False],...
 y: array([[[[ True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True],...
2025-04-22 05:20:39.439183 test begin: paddle.isfinite(Tensor([2, 3, 4, 178956971],"float16"), )

[accuracy error] paddle.isfinite(Tensor([2, 3, 4, 178956971],"float16"), ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294967304 (100%)
 x: array([[[[ True,  True,  True, ..., False, False, False],
         [False, False, False, ..., False, False, False],
         [False, False, False, ..., False, False, False],...
 y: array([[[[ True,  True,  True, ...,  True,  True,  True],
         [ True,  True,  True, ...,  True,  True,  True],
         [ True,  True,  True, ...,  True,  True,  True],...
2025-04-22 05:21:49.926293 test begin: paddle.isfinite(Tensor([2, 3, 4, 89478486],"float64"), )

[cuda error] paddle.isfinite(Tensor([2, 3, 4, 89478486],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 05:23:49.248101 test begin: paddle.isfinite(Tensor([2, 3, 71582789, 5],"float64"), )

[cuda error] paddle.isfinite(Tensor([2, 3, 71582789, 5],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 05:23:59.274885 test begin: paddle.isfinite(Tensor([2, 53687092, 4, 5],"float64"), )

[cuda error] paddle.isfinite(Tensor([2, 53687092, 4, 5],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 05:24:09.455909 test begin: paddle.isfinite(Tensor([2147483649],"int64"), )

[cuda error] paddle.isfinite(Tensor([2147483649],"int64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 05:25:06.324953 test begin: paddle.isfinite(Tensor([2281701379],"int64"), )

[cuda error] paddle.isfinite(Tensor([2281701379],"int64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 05:25:17.165803 test begin: paddle.isfinite(Tensor([252645135, 17],"float32"), )

[cuda error] paddle.isfinite(Tensor([252645135, 17],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 05:25:33.000473 test begin: paddle.isfinite(Tensor([25264514, 17, 10],"int32"), )

[accuracy error] paddle.isfinite(Tensor([25264514, 17, 10],"int32"), ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294967380 (100%)
 x: array([[[ True,  True,  True, ...,  True,  True,  True],
        [ True,  True,  True, ...,  True,  True,  True],
        [ True,  True,  True, ...,  True,  True,  True],...
 y: array([[[ True,  True,  True, ...,  True,  True,  True],
        [ True,  True,  True, ...,  True,  True,  True],
        [ True,  True,  True, ...,  True,  True,  True],...
2025-04-22 05:26:45.108403 test begin: paddle.std(Tensor([1, 107374183, 4, 10],"float16"), list[1,2,], True, False, )

W0422 05:26:57.823208 79936 dygraph_functions.cc:84735] got different data type, run type promotion automatically, this may cause data type been changed.
[accuracy error] paddle.std(Tensor([1, 107374183, 4, 10],"float16"), list[1,2,], True, False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]], dtype=float16)
 y: array([[0.2886, 0.2886, 0.2886, 0.2886, 0.2886, 0.2886, 0.2886, 0.2886,
        0.2886, 0.2886]], dtype=float16)
2025-04-22 05:26:58.382057 test begin: paddle.std(Tensor([1, 107374183, 4, 10],"float16"), list[1,3,], False, False, )

[accuracy error] paddle.std(Tensor([1, 107374183, 4, 10],"float16"), list[1,3,], False, False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[nan, nan, nan, nan]], dtype=float16)
 y: array([[0.2886, 0.2886, 0.2886, 0.2886]], dtype=float16)
2025-04-22 05:27:09.363892 test begin: paddle.std(Tensor([1, 107374183, 4, 10],"float16"), list[1,3,], True, False, )

[accuracy error] paddle.std(Tensor([1, 107374183, 4, 10],"float16"), list[1,3,], True, False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[nan, nan, nan, nan]], dtype=float16)
 y: array([[0.2886, 0.2886, 0.2886, 0.2886]], dtype=float16)
2025-04-22 05:27:20.315917 test begin: paddle.std(Tensor([1, 107374183, 4, 10],"float16"), tuple(1,3,), True, False, )

[accuracy error] paddle.std(Tensor([1, 107374183, 4, 10],"float16"), tuple(1,3,), True, False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[nan, nan, nan, nan]], dtype=float16)
 y: array([[0.2886, 0.2886, 0.2886, 0.2886]], dtype=float16)
2025-04-22 05:30:47.440562 test begin: paddle.std(Tensor([1, 3, 143165577, 10],"float16"), 2, True, False, )

[accuracy error] paddle.std(Tensor([1, 3, 143165577, 10],"float16"), 2, True, False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]]],
      dtype=float16)
 y: array([[[0.2886, 0.2886, 0.2886, 0.2886, 0.2886, 0.2886, 0.2886, 0.2886,
         0.2886, 0.2886],
        [0.2886, 0.2886, 0.2886, 0.2886, 0.2886, 0.2886, 0.2886, 0.2886,...
2025-04-22 05:31:00.504196 test begin: paddle.std(Tensor([1, 3, 143165577, 10],"float16"), list[1,2,], True, False, )

[accuracy error] paddle.std(Tensor([1, 3, 143165577, 10],"float16"), list[1,2,], True, False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]], dtype=float16)
 y: array([[0.2886, 0.2886, 0.2886, 0.2886, 0.2886, 0.2886, 0.2886, 0.2886,
        0.2886, 0.2886]], dtype=float16)
2025-04-22 05:32:53.458093 test begin: paddle.std(Tensor([1, 3, 4, 357913942],"float16"), 2, True, False, )

[accuracy error] backward  paddle.std(Tensor([1, 3, 4, 357913942],"float16"), 2, True, False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 34 / 4294967304 (7.92e-07%)
Max absolute difference: 0.02625
Max relative difference: 256.
 x: array([[[[-0.1404  , -0.1554  , -0.0689  , ...,  0.1454  ,  0.05566 ,
          -0.02817 ],
         [-0.1287  ,  0.002531, -0.1005  , ..., -0.09314 ,  0.007057,...
 y: array([[[[-0.1404  , -0.1554  , -0.069   , ...,  0.1453  ,  0.0557  ,
          -0.02817 ],
         [-0.1287  ,  0.00252 , -0.1006  , ..., -0.0931  ,  0.007057,...
2025-04-22 06:00:29.082946 test begin: paddle.std(Tensor([1, 3, 4, 357913942],"float16"), list[1,3,], False, False, )

[accuracy error] paddle.std(Tensor([1, 3, 4, 357913942],"float16"), list[1,3,], False, False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[nan, nan, nan, nan]], dtype=float16)
 y: array([[0.2886, 0.2886, 0.2886, 0.2886]], dtype=float16)
2025-04-22 06:00:41.530230 test begin: paddle.std(Tensor([1, 3, 4, 357913942],"float16"), list[1,3,], True, False, )

[accuracy error] paddle.std(Tensor([1, 3, 4, 357913942],"float16"), list[1,3,], True, False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[nan, nan, nan, nan]], dtype=float16)
 y: array([[0.2886, 0.2886, 0.2886, 0.2886]], dtype=float16)
2025-04-22 06:00:51.767797 test begin: paddle.std(Tensor([1, 3, 4, 357913942],"float16"), tuple(1,3,), True, False, )

[accuracy error] paddle.std(Tensor([1, 3, 4, 357913942],"float16"), tuple(1,3,), True, False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[nan, nan, nan, nan]], dtype=float16)
 y: array([[0.2886, 0.2886, 0.2886, 0.2886]], dtype=float16)
2025-04-22 06:07:28.212038 test begin: paddle.std(Tensor([2281701379],"float32"), )

[paddle error] paddle.std(Tensor([2281701379],"float32"), ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-22 06:11:06.144270 test begin: paddle.std(Tensor([35791395, 3, 4, 10],"float16"), 2, True, False, )

[accuracy error] backward  paddle.std(Tensor([35791395, 3, 4, 10],"float16"), 2, True, False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 35 / 4294967400 (8.15e-07%)
Max absolute difference: 0.01965
Max relative difference: 320.
 x: array([[[[ 7.8979e-02,  2.4628e-02, -1.4307e-01, ..., -2.9022e-02,
           2.1436e-01,  4.9225e-02],
         [-1.8994e-01,  2.6566e-02,  1.4111e-01, ...,  5.9723e-02,...
 y: array([[[[ 7.8857e-02,  2.4628e-02, -1.4319e-01, ..., -2.9007e-02,
           2.1472e-01,  4.9225e-02],
         [-1.9006e-01,  2.6566e-02,  1.4111e-01, ...,  5.9723e-02,...
2025-04-22 07:15:01.893180 test begin: paddle.std(Tensor([4294967295],"float32"), )

[paddle error] paddle.std(Tensor([4294967295],"float32"), ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-22 07:16:52.200943 test begin: paddle.std(x=Tensor([1431655766, 3],"float16"), )

[accuracy error] paddle.std(x=Tensor([1431655766, 3],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array(nan, dtype=float16)
 y: array(0.2886, dtype=float16)
2025-04-22 07:18:28.568411 test begin: paddle.std(x=Tensor([2, 2147483649],"float16"), )

[paddle error] paddle.std(x=Tensor([2, 2147483649],"float16"), ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-22 07:18:42.332025 test begin: paddle.std(x=Tensor([3, 3, 477218589],"float16"), )

[accuracy error] paddle.std(x=Tensor([3, 3, 477218589],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array(nan, dtype=float16)
 y: array(0.2886, dtype=float16)
2025-04-22 07:18:53.108133 test begin: paddle.std(x=Tensor([3, 3, 477218589],"float16"), axis=0, )

[accuracy error] backward  paddle.std(x=Tensor([3, 3, 477218589],"float16"), axis=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[ 0.08844 ,  0.01479 ,  0.0636  , ..., -0.1549  , -0.01994 ,
         -0.00873 ],
        [-0.00898 ,  0.11285 ,  0.01622 , ..., -0.009636,  0.01749 ,...
 y: array([[[ 0.08844 ,  0.01478 ,  0.0636  , ..., -0.1548  , -0.01994 ,
         -0.0087  ],
        [-0.00898 ,  0.1129  ,  0.01622 , ..., -0.009636,  0.01749 ,...
2025-04-22 07:22:17.871905 test begin: paddle.std(x=Tensor([3, 3, 477218589],"float16"), axis=0, unbiased=False, )

[accuracy error] backward  paddle.std(x=Tensor([3, 3, 477218589],"float16"), axis=0, unbiased=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[ 0.0722  ,  0.01208 ,  0.05197 , ..., -0.1265  , -0.01628 ,
         -0.00711 ],
        [-0.007328,  0.0921  ,  0.01322 , ..., -0.00787 ,  0.014275,...
 y: array([[[ 0.07227 ,  0.01207 ,  0.05197 , ..., -0.1265  , -0.01628 ,
         -0.007095],
        [-0.007324,  0.09216 ,  0.01324 , ..., -0.00787 ,  0.014275,...
2025-04-22 07:57:29.043787 test begin: paddle.std(x=Tensor([3, 477218589, 3],"float16"), )

[accuracy error] paddle.std(x=Tensor([3, 477218589, 3],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array(nan, dtype=float16)
 y: array(0.2886, dtype=float16)
2025-04-22 07:57:41.983898 test begin: paddle.std(x=Tensor([3, 477218589, 3],"float16"), axis=0, )

[accuracy error] backward  paddle.std(x=Tensor([3, 477218589, 3],"float16"), axis=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[ 0.08844 ,  0.01479 ,  0.0636  ],
        [-0.02599 ,  0.1274  , -0.1328  ],
        [-0.0391  ,  0.02646 ,  0.04538 ],...
 y: array([[[ 0.08844 ,  0.01478 ,  0.0636  ],
        [-0.026   ,  0.1274  , -0.1328  ],
        [-0.0391  ,  0.02644 ,  0.04538 ],...
2025-04-22 08:01:04.323873 test begin: paddle.std(x=Tensor([3, 477218589, 3],"float16"), axis=0, unbiased=False, )

[accuracy error] backward  paddle.std(x=Tensor([3, 477218589, 3],"float16"), axis=0, unbiased=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[ 0.0722  ,  0.01208 ,  0.05197 ],
        [-0.02122 ,  0.10406 , -0.1084  ],
        [-0.03192 ,  0.02162 ,  0.03705 ],...
 y: array([[[ 0.07227 ,  0.01207 ,  0.05197 ],
        [-0.02122 ,  0.1041  , -0.1085  ],
        [-0.03192 ,  0.02159 ,  0.03705 ],...
2025-04-22 08:04:42.222337 test begin: paddle.std(x=Tensor([3, 477218589, 3],"float16"), axis=list[0,1,], )

[accuracy error] paddle.std(x=Tensor([3, 477218589, 3],"float16"), axis=list[0,1,], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([nan, nan, nan], dtype=float16)
 y: array([0.2886, 0.2886, 0.2886], dtype=float16)
2025-04-22 08:04:57.446012 test begin: paddle.std(x=Tensor([3, 477218589, 3],"float16"), axis=tuple(0,1,), )

[accuracy error] paddle.std(x=Tensor([3, 477218589, 3],"float16"), axis=tuple(0,1,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([nan, nan, nan], dtype=float16)
 y: array([0.2886, 0.2886, 0.2886], dtype=float16)
2025-04-22 08:05:08.612503 test begin: paddle.std(x=Tensor([3, 477218589, 3],"float16"), axis=tuple(0,1,), keepdim=True, )

[accuracy error] paddle.std(x=Tensor([3, 477218589, 3],"float16"), axis=tuple(0,1,), keepdim=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[nan, nan, nan]]], dtype=float16)
 y: array([[[0.2886, 0.2886, 0.2886]]], dtype=float16)
2025-04-22 08:05:19.510166 test begin: paddle.std(x=Tensor([477218589, 3, 3],"float16"), )

[accuracy error] paddle.std(x=Tensor([477218589, 3, 3],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array(nan, dtype=float16)
 y: array(0.2886, dtype=float16)
2025-04-22 08:05:29.906279 test begin: paddle.std(x=Tensor([477218589, 3, 3],"float16"), axis=0, )

[accuracy error] paddle.std(x=Tensor([477218589, 3, 3],"float16"), axis=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[nan, nan, nan],
       [nan, nan, nan],
       [nan, nan, nan]], dtype=float16)
 y: array([[0.2886, 0.2886, 0.2886],
       [0.2886, 0.2886, 0.2886],
       [0.2886, 0.2886, 0.2886]], dtype=float16)
2025-04-22 08:05:40.645872 test begin: paddle.std(x=Tensor([477218589, 3, 3],"float16"), axis=0, unbiased=False, )

[accuracy error] paddle.std(x=Tensor([477218589, 3, 3],"float16"), axis=0, unbiased=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[nan, nan, nan],
       [nan, nan, nan],
       [nan, nan, nan]], dtype=float16)
 y: array([[0.2886, 0.2886, 0.2886],
       [0.2886, 0.2886, 0.2886],
       [0.2886, 0.2886, 0.2886]], dtype=float16)
2025-04-22 08:05:51.739433 test begin: paddle.std(x=Tensor([477218589, 3, 3],"float16"), axis=list[0,1,], )

[accuracy error] paddle.std(x=Tensor([477218589, 3, 3],"float16"), axis=list[0,1,], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([nan, nan, nan], dtype=float16)
 y: array([0.2886, 0.2886, 0.2886], dtype=float16)
2025-04-22 08:06:02.302714 test begin: paddle.std(x=Tensor([477218589, 3, 3],"float16"), axis=tuple(0,1,), )

[accuracy error] paddle.std(x=Tensor([477218589, 3, 3],"float16"), axis=tuple(0,1,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([nan, nan, nan], dtype=float16)
 y: array([0.2886, 0.2886, 0.2886], dtype=float16)
2025-04-22 08:06:12.856609 test begin: paddle.std(x=Tensor([477218589, 3, 3],"float16"), axis=tuple(0,1,), keepdim=True, )

[accuracy error] paddle.std(x=Tensor([477218589, 3, 3],"float16"), axis=tuple(0,1,), keepdim=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[nan, nan, nan]]], dtype=float16)
 y: array([[[0.2886, 0.2886, 0.2886]]], dtype=float16)
2025-04-22 08:32:42.642120 test begin: paddle.sum(Tensor([1, 2281701379, 1],"float32"), 1, )

[accuracy error] paddle.sum(Tensor([1, 2281701379, 1],"float32"), 1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 555.71875
Max relative difference: 0.03597325
 x: array([[-16003.83]], dtype=float32)
 y: array([[-15448.111]], dtype=float32)
2025-04-22 09:50:54.675306 test begin: paddle.full_like(Tensor([760567127, 3],"float32"), -3.4028234663852886e+38, )

W0422 09:52:18.171627 153530 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 09:52:18.172573 153530 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.full_like(Tensor([760567127, 3],"float32"), -3.4028234663852886e+38, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701381 / 2281701381 (100%)
Max absolute difference: 3.4028235e+38
Max relative difference: 1.
 x: array([[0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],...
 y: array([[-3.402823e+38, -3.402823e+38, -3.402823e+38],
       [-3.402823e+38, -3.402823e+38, -3.402823e+38],
       [-3.402823e+38, -3.402823e+38, -3.402823e+38],...
2025-04-22 09:53:57.426687 test begin: paddle.full_like(Tensor([760567127, 3],"float32"), -math.inf, )

[accuracy error] paddle.full_like(Tensor([760567127, 3],"float32"), -math.inf, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y -inf location mismatch:
 x: array([[0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],...
 y: array([[-inf, -inf, -inf],
       [-inf, -inf, -inf],
       [-inf, -inf, -inf],...
2025-04-22 09:54:37.690228 test begin: paddle.full_like(Tensor([760567127, 3],"float32"), 3.4028234663852886e+38, )

[accuracy error] paddle.full_like(Tensor([760567127, 3],"float32"), 3.4028234663852886e+38, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701381 / 2281701381 (100%)
Max absolute difference: 3.4028235e+38
Max relative difference: 1.
 x: array([[0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],...
 y: array([[3.402823e+38, 3.402823e+38, 3.402823e+38],
       [3.402823e+38, 3.402823e+38, 3.402823e+38],
       [3.402823e+38, 3.402823e+38, 3.402823e+38],...
2025-04-22 09:56:55.038226 test begin: paddle.full_like(Tensor([760567127, 3],"float32"), fill_value=math.inf, )

[accuracy error] paddle.full_like(Tensor([760567127, 3],"float32"), fill_value=math.inf, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],...
 y: array([[inf, inf, inf],
       [inf, inf, inf],
       [inf, inf, inf],...
2025-04-22 09:57:31.260304 test begin: paddle.full_like(Tensor([760567127, 3],"float32"), fill_value=nan, )

[accuracy error] paddle.full_like(Tensor([760567127, 3],"float32"), fill_value=nan, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],...
 y: array([[nan, nan, nan],
       [nan, nan, nan],
       [nan, nan, nan],...
2025-04-22 09:58:01.594146 test begin: paddle.full_like(Tensor([760567127, 3],"float32"), math.inf, )

[accuracy error] paddle.full_like(Tensor([760567127, 3],"float32"), math.inf, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],...
 y: array([[inf, inf, inf],
       [inf, inf, inf],
       [inf, inf, inf],...
2025-04-22 09:58:38.864099 test begin: paddle.full_like(Tensor([760567127, 3],"int64"), 7, )

[accuracy error] paddle.full_like(Tensor([760567127, 3],"int64"), 7, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701381 / 2281701381 (100%)
Max absolute difference: 7
Max relative difference: 1.
 x: array([[0, 0, 0],
       [0, 0, 0],
       [0, 0, 0],...
 y: array([[7, 7, 7],
       [7, 7, 7],
       [7, 7, 7],...
2025-04-22 10:04:00.458338 test begin: paddle.full_like(Tensor([83837, 27216],"int32"), 10, )

[accuracy error] paddle.full_like(Tensor([83837, 27216],"int32"), 10, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281707792 / 2281707792 (100%)
Max absolute difference: 10
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[10, 10, 10, ..., 10, 10, 10],
       [10, 10, 10, ..., 10, 10, 10],
       [10, 10, 10, ..., 10, 10, 10],...
2025-04-22 10:08:20.708354 test begin: paddle.full_like(Tensor([897955, 2541],"int32"), 80, )

[accuracy error] paddle.full_like(Tensor([897955, 2541],"int32"), 80, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281703655 / 2281703655 (100%)
Max absolute difference: 80
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[80, 80, 80, ..., 80, 80, 80],
       [80, 80, 80, ..., 80, 80, 80],
       [80, 80, 80, ..., 80, 80, 80],...
2025-04-22 10:11:24.365451 test begin: paddle.full_like(Tensor([93991, 24276],"int32"), 10, )

[accuracy error] paddle.full_like(Tensor([93991, 24276],"int32"), 10, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281725516 / 2281725516 (100%)
Max absolute difference: 10
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[10, 10, 10, ..., 10, 10, 10],
       [10, 10, 10, ..., 10, 10, 10],
       [10, 10, 10, ..., 10, 10, 10],...
2025-04-22 10:14:15.431639 test begin: paddle.full_like(Tensor([9834920, 232],"int32"), 1.0, None, None, )

[accuracy error] paddle.full_like(Tensor([9834920, 232],"int32"), 1.0, None, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701440 / 2281701440 (100%)
Max absolute difference: 1
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[1, 1, 1, ..., 1, 1, 1],
       [1, 1, 1, ..., 1, 1, 1],
       [1, 1, 1, ..., 1, 1, 1],...
2025-04-22 10:17:35.542583 test begin: paddle.full_like(x=Tensor([2281701379],"bool"), fill_value=1, )

[accuracy error] paddle.full_like(x=Tensor([2281701379],"bool"), fill_value=1, ) 
 
Arrays are not equal

Mismatched elements: 2281701379 / 2281701379 (100%)
 x: array([False, False, False, ..., False, False, False])
 y: array([ True,  True,  True, ...,  True,  True,  True])
2025-04-22 10:19:19.962428 test begin: paddle.full_like(x=Tensor([2281701379],"float32"), fill_value=1, )

[accuracy error] paddle.full_like(x=Tensor([2281701379],"float32"), fill_value=1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)
 y: array([1., 1., 1., ..., 1., 1., 1.], dtype=float32)
2025-04-22 10:22:14.473545 test begin: paddle.full_like(x=Tensor([2281701379],"int32"), fill_value=1, )

[accuracy error] paddle.full_like(x=Tensor([2281701379],"int32"), fill_value=1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 1
Max relative difference: 1.
 x: array([0, 0, 0, ..., 0, 0, 0], dtype=int32)
 y: array([1, 1, 1, ..., 1, 1, 1], dtype=int32)
2025-04-22 10:27:21.877018 test begin: paddle.gcd(Tensor([2281701379],"int32"), Tensor([1],"int32"), )

[paddle error] paddle.gcd(Tensor([2281701379],"int32"), Tensor([1],"int32"), ) 
 (InvalidArgument) The [0] th of Inputs(X) and Inputs(Y) should be same. But received X's shape is [1], Y's shape is [2281701379]
  [Hint: Expected x_dims[i] == y_dims[i], but received x_dims[i]:1 != y_dims[i]:2281701379.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/multiary.cc:5638)

2025-04-22 10:27:38.813941 test begin: paddle.gcd(Tensor([2281701379],"int64"), Tensor([1],"int64"), )

[paddle error] paddle.gcd(Tensor([2281701379],"int64"), Tensor([1],"int64"), ) 
 (InvalidArgument) The [0] th of Inputs(X) and Inputs(Y) should be same. But received X's shape is [1], Y's shape is [2281701379]
  [Hint: Expected x_dims[i] == y_dims[i], but received x_dims[i]:1 != y_dims[i]:2281701379.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/multiary.cc:5638)

2025-04-22 10:28:17.369407 test begin: paddle.gcd(x=Tensor([2281701379],"int32"), y=Tensor([1],"int32"), )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.gcd(x=Tensor([2281701379],"int32"), y=Tensor([1],"int32"), ) 
 (InvalidArgument) The [0] th of Inputs(X) and Inputs(Y) should be same. But received X's shape is [1], Y's shape is [2281701379]
  [Hint: Expected x_dims[i] == y_dims[i], but received x_dims[i]:1 != y_dims[i]:2281701379.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/multiary.cc:5638)

2025-04-22 10:28:36.482533 test begin: paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[2,3,1,],list[1,0,2,],], )

/usr/local/lib/python3.9/dist-packages/torch/autograd/graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:135.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[accuracy error] paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[2,3,1,],list[1,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 3188.
Max relative difference: 7.5
 x: array([[-2023. ],
       [ -288.8],
       [ -647.5],...
 y: array([[-1853.  ],
       [  579.  ],
       [-2692.  ],...
2025-04-22 10:30:06.769146 test begin: paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,0,2,1,],list[2,1,0,3,],], )

[accuracy error] paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,0,2,1,],list[2,1,0,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 3578.
Max relative difference: 0.798
 x: array(-906.5, dtype=float16)
 y: array(-4484., dtype=float16)
2025-04-22 11:54:45.022126 test begin: paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,2,1,0,],list[0,1,3,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,2,1,0,],list[0,1,3,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 3578.
Max relative difference: 0.798
 x: array(-905.5, dtype=float16)
 y: array(-4484., dtype=float16)
2025-04-22 11:54:57.106686 test begin: paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,2,1,],list[2,0,1,],], )

[accuracy error] paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,2,1,],list[2,0,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 3188.
Max relative difference: 7.5
 x: array([[-2023. ],
       [ -288.8],
       [ -647.5],...
 y: array([[-1853.  ],
       [  579.  ],
       [-2692.  ],...
2025-04-22 11:55:08.401800 test begin: paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 171798692, 1, 1],"float16"), list[list[0,1,3,2,],list[0,2,1,3,],], )

[accuracy error] paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 171798692, 1, 1],"float16"), list[list[0,1,3,2,],list[0,2,1,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array(-inf, dtype=float16)
 y: array(inf, dtype=float16)
2025-04-22 11:55:20.102572 test begin: paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 171798692, 1, 1],"float16"), list[list[0,1,3,],list[0,3,1,],], )

[accuracy error] paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 171798692, 1, 1],"float16"), list[list[0,1,3,],list[0,3,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[-inf],
       [ inf],
       [-inf],...
 y: array([[ inf],
       [-inf],
       [ inf],...
2025-04-22 11:55:32.138248 test begin: paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 171798692, 1, 1],"float16"), list[list[0,2,3,],list[0,2,1,],], )

[accuracy error] paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 171798692, 1, 1],"float16"), list[list[0,2,3,],list[0,2,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 171793211 / 171798692 (100%)
Max absolute difference: 25648.
Max relative difference: 1.616
 x: array([[ 1692.],
       [ 1605.],
       [-1371.],...
 y: array([[-2750.],
       [-2610.],
       [ 2228.],...
2025-04-22 11:56:11.507874 test begin: paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 171798692, 1, 1],"float16"), list[list[0,3,2,1,],list[2,1,3,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 171798692, 1, 1],"float16"), list[list[0,3,2,1,],list[2,1,3,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array(-inf, dtype=float16)
 y: array(inf, dtype=float16)
2025-04-22 11:56:26.058325 test begin: paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 171798692, 1, 1],"float16"), list[list[1,0,3,2,],list[3,0,1,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 171798692, 1, 1],"float16"), list[list[1,0,3,2,],list[3,0,1,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array(-inf, dtype=float16)
 y: array(inf, dtype=float16)
2025-04-22 12:06:19.993246 test begin: paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 171798692, 1, 1],"float16"), list[list[1,2,3,0,],list[3,2,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 171798692, 1, 1],"float16"), list[list[1,2,3,0,],list[3,2,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array(-inf, dtype=float16)
 y: array(inf, dtype=float16)
2025-04-22 12:16:36.914089 test begin: paddle.full_like(Tensor([228170138, 10],"int64"), 7, )

W0422 12:18:11.950505 39536 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 12:18:11.952682 39536 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.full_like(Tensor([228170138, 10],"int64"), 7, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 7
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[7, 7, 7, ..., 7, 7, 7],
       [7, 7, 7, ..., 7, 7, 7],
       [7, 7, 7, ..., 7, 7, 7],...
2025-04-22 12:21:52.017745 test begin: paddle.full_like(Tensor([22817014, 1, 10, 10],"float32"), -3.4028234663852886e+38, )

[accuracy error] paddle.full_like(Tensor([22817014, 1, 10, 10],"float32"), -3.4028234663852886e+38, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701400 / 2281701400 (100%)
Max absolute difference: 3.4028235e+38
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[-3.402823e+38, -3.402823e+38, -3.402823e+38, ...,
          -3.402823e+38, -3.402823e+38, -3.402823e+38],
         [-3.402823e+38, -3.402823e+38, -3.402823e+38, ...,...
2025-04-22 12:25:11.427800 test begin: paddle.full_like(Tensor([253522376, 3, 3],"float32"), fill_value=2, )

[accuracy error] paddle.full_like(Tensor([253522376, 3, 3],"float32"), fill_value=2, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701384 / 2281701384 (100%)
Max absolute difference: 2.
Max relative difference: 1.
 x: array([[[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]],...
 y: array([[[2., 2., 2.],
        [2., 2., 2.],
        [2., 2., 2.]],...
2025-04-22 12:27:19.261972 test begin: paddle.full_like(Tensor([265933, 8580],"int32"), 4, )

[accuracy error] paddle.full_like(Tensor([265933, 8580],"int32"), 4, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281705140 / 2281705140 (100%)
Max absolute difference: 4
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[4, 4, 4, ..., 4, 4, 4],
       [4, 4, 4, ..., 4, 4, 4],
       [4, 4, 4, ..., 4, 4, 4],...
2025-04-22 12:30:56.754359 test begin: paddle.full_like(Tensor([271632, 8400],"int32"), 2, )

[accuracy error] paddle.full_like(Tensor([271632, 8400],"int32"), 2, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281708800 / 2281708800 (100%)
Max absolute difference: 2
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[2, 2, 2, ..., 2, 2, 2],
       [2, 2, 2, ..., 2, 2, 2],
       [2, 2, 2, ..., 2, 2, 2],...
2025-04-22 12:34:03.187618 test begin: paddle.full_like(Tensor([285213, 8000],"float32"), -math.inf, )

[accuracy error] paddle.full_like(Tensor([285213, 8000],"float32"), -math.inf, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y -inf location mismatch:
 x: array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[-inf, -inf, -inf, ..., -inf, -inf, -inf],
       [-inf, -inf, -inf, ..., -inf, -inf, -inf],
       [-inf, -inf, -inf, ..., -inf, -inf, -inf],...
2025-04-22 12:34:50.713792 test begin: paddle.full_like(Tensor([3, 760567127],"float32"), -1.0, )

[accuracy error] paddle.full_like(Tensor([3, 760567127],"float32"), -1.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701381 / 2281701381 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)
 y: array([[-1., -1., -1., ..., -1., -1., -1.],
       [-1., -1., -1., ..., -1., -1., -1.],
       [-1., -1., -1., ..., -1., -1., -1.]], dtype=float32)
2025-04-22 12:37:11.134679 test begin: paddle.full_like(Tensor([3, 760567127],"float32"), 1.0, )

[accuracy error] paddle.full_like(Tensor([3, 760567127],"float32"), 1.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701381 / 2281701381 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)
 y: array([[1., 1., 1., ..., 1., 1., 1.],
       [1., 1., 1., ..., 1., 1., 1.],
       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32)
2025-04-22 12:39:17.194837 test begin: paddle.full_like(Tensor([3, 760567127],"int64"), 7, )

[accuracy error] paddle.full_like(Tensor([3, 760567127],"int64"), 7, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701381 / 2281701381 (100%)
Max absolute difference: 7
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0]])
 y: array([[7, 7, 7, ..., 7, 7, 7],
       [7, 7, 7, ..., 7, 7, 7],
       [7, 7, 7, ..., 7, 7, 7]])
2025-04-22 12:45:27.347029 test begin: paddle.full_like(Tensor([325957340, 1, 1, 7],"float32"), -3.4028234663852886e+38, )

[accuracy error] paddle.full_like(Tensor([325957340, 1, 1, 7],"float32"), -3.4028234663852886e+38, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 3.4028235e+38
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.]]],

...
 y: array([[[[-3.402823e+38, -3.402823e+38, -3.402823e+38, ...,
          -3.402823e+38, -3.402823e+38, -3.402823e+38]]],
...
2025-04-22 12:47:22.100712 test begin: paddle.full_like(Tensor([325957340, 7],"int64"), 15, )

[accuracy error] paddle.full_like(Tensor([325957340, 7],"int64"), 15, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 15
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[15, 15, 15, ..., 15, 15, 15],
       [15, 15, 15, ..., 15, 15, 15],
       [15, 15, 15, ..., 15, 15, 15],...
2025-04-22 12:52:19.435179 test begin: paddle.full_like(Tensor([35651585, 64],"float32"), 1.0, )

[accuracy error] paddle.full_like(Tensor([35651585, 64],"float32"), 1.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701440 / 2281701440 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[1., 1., 1., ..., 1., 1., 1.],
       [1., 1., 1., ..., 1., 1., 1.],
       [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 12:55:20.013581 test begin: paddle.full_like(Tensor([356516, 6400],"int32"), 4, )

[accuracy error] paddle.full_like(Tensor([356516, 6400],"int32"), 4, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281702400 / 2281702400 (100%)
Max absolute difference: 4
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[4, 4, 4, ..., 4, 4, 4],
       [4, 4, 4, ..., 4, 4, 4],
       [4, 4, 4, ..., 4, 4, 4],...
2025-04-22 12:58:26.164349 test begin: paddle.full_like(Tensor([380283564, 1, 1, 6],"float32"), -3.4028234663852886e+38, )

[accuracy error] paddle.full_like(Tensor([380283564, 1, 1, 6],"float32"), -3.4028234663852886e+38, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701384 / 2281701384 (100%)
Max absolute difference: 3.4028235e+38
Max relative difference: 1.
 x: array([[[[0., 0., 0., 0., 0., 0.]]],

...
 y: array([[[[-3.402823e+38, -3.402823e+38, -3.402823e+38, -3.402823e+38,
          -3.402823e+38, -3.402823e+38]]],
...
2025-04-22 13:00:46.200884 test begin: paddle.full_like(Tensor([4, 536870913],"float64"), fill_value=6, )

[accuracy error] paddle.full_like(Tensor([4, 536870913],"float64"), fill_value=6, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2147483652 / 2147483652 (100%)
Max absolute difference: 6.
Max relative difference: 1.
 x: array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]])
 y: array([[6., 6., 6., ..., 6., 6., 6.],
       [6., 6., 6., ..., 6., 6., 6.],
       [6., 6., 6., ..., 6., 6., 6.],
       [6., 6., 6., ..., 6., 6., 6.]])
2025-04-22 13:05:45.121859 test begin: paddle.full_like(Tensor([4, 570425345],"float32"), fill_value=6, )

[accuracy error] paddle.full_like(Tensor([4, 570425345],"float32"), fill_value=6, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 6.
Max relative difference: 1.
 x: array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)
 y: array([[6., 6., 6., ..., 6., 6., 6.],
       [6., 6., 6., ..., 6., 6., 6.],
       [6., 6., 6., ..., 6., 6., 6.],
       [6., 6., 6., ..., 6., 6., 6.]], dtype=float32)
2025-04-22 13:07:34.256810 test begin: paddle.full_like(Tensor([4, 7, 76695845],"float64"), fill_value=5, )

[accuracy error] paddle.full_like(Tensor([4, 7, 76695845],"float64"), fill_value=5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2147483660 / 2147483660 (100%)
Max absolute difference: 5.
Max relative difference: 1.
 x: array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[5., 5., 5., ..., 5., 5., 5.],
        [5., 5., 5., ..., 5., 5., 5.],
        [5., 5., 5., ..., 5., 5., 5.],...
2025-04-22 13:11:02.639070 test begin: paddle.full_like(Tensor([419431, 5440],"int32"), 11, )

[accuracy error] paddle.full_like(Tensor([419431, 5440],"int32"), 11, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281704640 / 2281704640 (100%)
Max absolute difference: 11
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[11, 11, 11, ..., 11, 11, 11],
       [11, 11, 11, ..., 11, 11, 11],
       [11, 11, 11, ..., 11, 11, 11],...
2025-04-22 13:14:20.045626 test begin: paddle.full_like(Tensor([456340276, 5],"float32"), -3.4028234663852886e+38, )

[accuracy error] paddle.full_like(Tensor([456340276, 5],"float32"), -3.4028234663852886e+38, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 3.4028235e+38
Max relative difference: 1.
 x: array([[0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.],...
 y: array([[-3.402823e+38, -3.402823e+38, -3.402823e+38, -3.402823e+38,
        -3.402823e+38],
       [-3.402823e+38, -3.402823e+38, -3.402823e+38, -3.402823e+38,...
2025-04-22 13:17:27.072279 test begin: paddle.full_like(Tensor([476447, 4789],"int32"), 11, )

[accuracy error] paddle.full_like(Tensor([476447, 4789],"int32"), 11, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281704683 / 2281704683 (100%)
Max absolute difference: 11
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[11, 11, 11, ..., 11, 11, 11],
       [11, 11, 11, ..., 11, 11, 11],
       [11, 11, 11, ..., 11, 11, 11],...
2025-04-22 13:20:15.024917 test begin: paddle.full_like(Tensor([547828, 4165],"int32"), 11, )

[accuracy error] paddle.full_like(Tensor([547828, 4165],"int32"), 11, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281703620 / 2281703620 (100%)
Max absolute difference: 11
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[11, 11, 11, ..., 11, 11, 11],
       [11, 11, 11, ..., 11, 11, 11],
       [11, 11, 11, ..., 11, 11, 11],...
2025-04-22 13:23:12.747275 test begin: paddle.full_like(Tensor([570425345, 4],"float32"), -3.4028234663852886e+38, )

[accuracy error] paddle.full_like(Tensor([570425345, 4],"float32"), -3.4028234663852886e+38, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 3.4028235e+38
Max relative difference: 1.
 x: array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.],...
 y: array([[-3.402823e+38, -3.402823e+38, -3.402823e+38, -3.402823e+38],
       [-3.402823e+38, -3.402823e+38, -3.402823e+38, -3.402823e+38],
       [-3.402823e+38, -3.402823e+38, -3.402823e+38, -3.402823e+38],...
2025-04-22 13:25:53.754985 test begin: paddle.full_like(Tensor([570425345, 4],"float32"), 3.4028234663852886e+38, )

[accuracy error] paddle.full_like(Tensor([570425345, 4],"float32"), 3.4028234663852886e+38, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 3.4028235e+38
Max relative difference: 1.
 x: array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.],...
 y: array([[3.402823e+38, 3.402823e+38, 3.402823e+38, 3.402823e+38],
       [3.402823e+38, 3.402823e+38, 3.402823e+38, 3.402823e+38],
       [3.402823e+38, 3.402823e+38, 3.402823e+38, 3.402823e+38],...
2025-04-22 13:27:42.977735 test begin: paddle.full_like(Tensor([570425345, 4],"float32"), math.inf, )

[accuracy error] paddle.full_like(Tensor([570425345, 4],"float32"), math.inf, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.],...
 y: array([[inf, inf, inf, inf],
       [inf, inf, inf, inf],
       [inf, inf, inf, inf],...
2025-04-22 13:28:14.116145 test begin: paddle.full_like(Tensor([57042535, 40],"float32"), -1.0, )

[accuracy error] paddle.full_like(Tensor([57042535, 40],"float32"), -1.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701400 / 2281701400 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[-1., -1., -1., ..., -1., -1., -1.],
       [-1., -1., -1., ..., -1., -1., -1.],
       [-1., -1., -1., ..., -1., -1., -1.],...
2025-04-22 13:31:02.813477 test begin: paddle.full_like(Tensor([57042535, 40],"float32"), 1.0, )

[accuracy error] paddle.full_like(Tensor([57042535, 40],"float32"), 1.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701400 / 2281701400 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[1., 1., 1., ..., 1., 1., 1.],
       [1., 1., 1., ..., 1., 1., 1.],
       [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 13:33:23.684865 test begin: paddle.full_like(Tensor([59417, 38402],"int32"), 4, )

[accuracy error] paddle.full_like(Tensor([59417, 38402],"int32"), 4, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281731634 / 2281731634 (100%)
Max absolute difference: 4
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[4, 4, 4, ..., 4, 4, 4],
       [4, 4, 4, ..., 4, 4, 4],
       [4, 4, 4, ..., 4, 4, 4],...
2025-04-22 13:36:19.868418 test begin: paddle.full_like(Tensor([6, 380283564],"int64"), 2, )

[accuracy error] paddle.full_like(Tensor([6, 380283564],"int64"), 2, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701384 / 2281701384 (100%)
Max absolute difference: 2
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[2, 2, 2, ..., 2, 2, 2],
       [2, 2, 2, ..., 2, 2, 2],
       [2, 2, 2, ..., 2, 2, 2],...
2025-04-22 13:41:36.954793 test begin: paddle.full_like(Tensor([6, 380283564],"int64"), 255, )

[accuracy error] paddle.full_like(Tensor([6, 380283564],"int64"), 255, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701384 / 2281701384 (100%)
Max absolute difference: 255
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[255, 255, 255, ..., 255, 255, 255],
       [255, 255, 255, ..., 255, 255, 255],
       [255, 255, 255, ..., 255, 255, 255],...
2025-04-22 13:47:43.126945 test begin: paddle.full_like(Tensor([6, 380283564],"int64"), 98, )

[accuracy error] paddle.full_like(Tensor([6, 380283564],"int64"), 98, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701384 / 2281701384 (100%)
Max absolute difference: 98
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[98, 98, 98, ..., 98, 98, 98],
       [98, 98, 98, ..., 98, 98, 98],
       [98, 98, 98, ..., 98, 98, 98],...
2025-04-22 13:51:43.157839 test begin: paddle.full_like(Tensor([61906, 36858],"int32"), 4, )

[accuracy error] paddle.full_like(Tensor([61906, 36858],"int32"), 4, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281731348 / 2281731348 (100%)
Max absolute difference: 4
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[4, 4, 4, ..., 4, 4, 4],
       [4, 4, 4, ..., 4, 4, 4],
       [4, 4, 4, ..., 4, 4, 4],...
2025-04-22 13:55:05.982825 test begin: paddle.full_like(Tensor([634159, 3598],"int32"), 11, )

[accuracy error] paddle.full_like(Tensor([634159, 3598],"int32"), 11, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281704082 / 2281704082 (100%)
Max absolute difference: 11
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[11, 11, 11, ..., 11, 11, 11],
       [11, 11, 11, ..., 11, 11, 11],
       [11, 11, 11, ..., 11, 11, 11],...
2025-04-22 13:58:07.446732 test begin: paddle.full_like(Tensor([67908, 33600],"int32"), 10, )

[accuracy error] paddle.full_like(Tensor([67908, 33600],"int32"), 10, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281708800 / 2281708800 (100%)
Max absolute difference: 10
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[10, 10, 10, ..., 10, 10, 10],
       [10, 10, 10, ..., 10, 10, 10],
       [10, 10, 10, ..., 10, 10, 10],...
2025-04-22 14:01:06.700011 test begin: paddle.full_like(Tensor([7, 325957340],"int64"), 15, )

[accuracy error] paddle.full_like(Tensor([7, 325957340],"int64"), 15, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 15
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[15, 15, 15, ..., 15, 15, 15],
       [15, 15, 15, ..., 15, 15, 15],
       [15, 15, 15, ..., 15, 15, 15],...
2025-04-22 14:05:02.473835 test begin: paddle.full_like(Tensor([727584, 56, 56],"int64"), 15, )

[accuracy error] paddle.full_like(Tensor([727584, 56, 56],"int64"), 15, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281703424 / 2281703424 (100%)
Max absolute difference: 15
Max relative difference: 1.
 x: array([[[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],...
 y: array([[[15, 15, 15, ..., 15, 15, 15],
        [15, 15, 15, ..., 15, 15, 15],
        [15, 15, 15, ..., 15, 15, 15],...
2025-04-22 14:08:46.494467 test begin: paddle.full_like(Tensor([727584, 56, 56],"int64"), 31, )

[accuracy error] paddle.full_like(Tensor([727584, 56, 56],"int64"), 31, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281703424 / 2281703424 (100%)
Max absolute difference: 31
Max relative difference: 1.
 x: array([[[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],...
 y: array([[[31, 31, 31, ..., 31, 31, 31],
        [31, 31, 31, ..., 31, 31, 31],
        [31, 31, 31, ..., 31, 31, 31],...
2025-04-22 14:12:57.196163 test begin: paddle.full_like(Tensor([75245, 30324],"int32"), 10, )

[accuracy error] paddle.full_like(Tensor([75245, 30324],"int32"), 10, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281729380 / 2281729380 (100%)
Max absolute difference: 10
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[10, 10, 10, ..., 10, 10, 10],
       [10, 10, 10, ..., 10, 10, 10],
       [10, 10, 10, ..., 10, 10, 10],...
2025-04-22 14:16:25.442178 test begin: paddle.full_like(Tensor([760567127, 3],"float32"), -10.0, )

[accuracy error] paddle.full_like(Tensor([760567127, 3],"float32"), -10.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701381 / 2281701381 (100%)
Max absolute difference: 10.
Max relative difference: 1.
 x: array([[0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],...
 y: array([[-10., -10., -10.],
       [-10., -10., -10.],
       [-10., -10., -10.],...
2025-04-22 14:18:27.565201 test begin: paddle.full_like(Tensor([760567127, 3],"float32"), 0.5, )

[accuracy error] paddle.full_like(Tensor([760567127, 3],"float32"), 0.5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701381 / 2281701381 (100%)
Max absolute difference: 0.5
Max relative difference: 1.
 x: array([[0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],...
 y: array([[0.5, 0.5, 0.5],
       [0.5, 0.5, 0.5],
       [0.5, 0.5, 0.5],...
2025-04-22 14:20:16.753747 test begin: paddle.full_like(Tensor([760567127, 3],"float32"), 1.0, )

[accuracy error] paddle.full_like(Tensor([760567127, 3],"float32"), 1.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701381 / 2281701381 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],...
 y: array([[1., 1., 1.],
       [1., 1., 1.],
       [1., 1., 1.],...
2025-04-22 14:22:53.105660 test begin: paddle.full_like(Tensor([760567127, 3],"float32"), 100.0, )

[accuracy error] paddle.full_like(Tensor([760567127, 3],"float32"), 100.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701381 / 2281701381 (100%)
Max absolute difference: 100.
Max relative difference: 1.
 x: array([[0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],...
 y: array([[100., 100., 100.],
       [100., 100., 100.],
       [100., 100., 100.],...
2025-04-22 14:25:09.637587 test begin: paddle.full_like(Tensor([760567127, 3],"float32"), 2.0, )

[accuracy error] paddle.full_like(Tensor([760567127, 3],"float32"), 2.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701381 / 2281701381 (100%)
Max absolute difference: 2.
Max relative difference: 1.
 x: array([[0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],...
 y: array([[2., 2., 2.],
       [2., 2., 2.],
       [2., 2., 2.],...
2025-04-22 14:27:22.402366 test begin: paddle.trunc(Tensor([10, 20, 11408507],"float32"), )

W0422 14:28:45.391570 30242 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 14:28:45.392736 30242 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745303326 (unix time) try "date -d @1745303326" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x75a4) received by PID 30116 (TID 0x7f480b744700) from PID 30116 ***]

2025-04-22 14:28:51.226524 test begin: paddle.trunc(Tensor([10, 228170138, 1],"float32"), )

W0422 14:30:15.632628 33535 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 14:30:15.633767 33535 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745303416 (unix time) try "date -d @1745303416" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x8286) received by PID 33414 (TID 0x7f3db9dc2700) from PID 33414 ***]

2025-04-22 14:30:55.876899 test begin: paddle.trunc(Tensor([114085069, 20, 1],"float32"), )

W0422 14:32:19.197189 37711 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 14:32:19.198259 37711 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745303540 (unix time) try "date -d @1745303540" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x9220) received by PID 37408 (TID 0x7f02c74f4700) from PID 37408 ***]

2025-04-22 14:33:04.552821 test begin: paddle.trunc(Tensor([114085069, 20],"float32"), )

W0422 14:34:30.031183 41922 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 14:34:30.032267 41922 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745303671 (unix time) try "date -d @1745303671" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xa322) received by PID 41762 (TID 0x7f66ae949700) from PID 41762 ***]

2025-04-22 14:35:17.315483 test begin: paddle.trunc(Tensor([20, 114085069],"float32"), )

W0422 14:36:38.949783 46569 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 14:36:38.950994 46569 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745303799 (unix time) try "date -d @1745303799" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xb56e) received by PID 46446 (TID 0x7fcdfbb85700) from PID 46446 ***]

2025-04-22 14:37:21.754877 test begin: paddle.trunc(input=Tensor([6, 380283564],"int32"), )

W0422 14:38:29.631616 50307 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 14:38:29.633601 50307 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745303910 (unix time) try "date -d @1745303910" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xc422) received by PID 50210 (TID 0x7f85d0949700) from PID 50210 ***]

2025-04-22 14:39:11.209592 test begin: paddle.trunc(input=Tensor([6, 6, 119304648],"float16"), )

W0422 14:40:51.509922 54031 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 14:40:51.511137 54031 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745304052 (unix time) try "date -d @1745304052" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xd28f) received by PID 53903 (TID 0x7f6f7f34a700) from PID 53903 ***]

2025-04-22 14:41:34.980411 test begin: paddle.trunc(input=Tensor([6, 6, 19884108, 6],"float16"), )

W0422 14:43:22.916754 58503 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 14:43:22.918258 58503 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745304203 (unix time) try "date -d @1745304203" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xe3e9) received by PID 58345 (TID 0x7f9b5bf48700) from PID 58345 ***]

2025-04-22 14:44:02.032559 test begin: paddle.trunc(input=Tensor([6, 6, 6, 19884108],"float16"), )

W0422 14:45:40.097868 62728 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 14:45:40.098973 62728 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745304341 (unix time) try "date -d @1745304341" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xf483) received by PID 62595 (TID 0x7fe47f2b7700) from PID 62595 ***]

2025-04-22 14:46:18.223833 test begin: paddle.unique_consecutive(x=Tensor([570425345, 4],"float32"), return_inverse=True, return_counts=True, axis=0, )

W0422 14:47:39.614604 67646 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 14:47:39.615398 67646 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_unique_consecutive(_object*, _object*, _object*)
1   unique_consecutive_ad_func(paddle::Tensor const&, bool, bool, std::vector<int, std::allocator<int> >, phi::DataType)
2   paddle::experimental::unique_consecutive(paddle::Tensor const&, bool, bool, std::vector<int, std::allocator<int> > const&, phi::DataType)
3   void phi::UniqueConsecutiveKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, bool, bool, std::vector<int, std::allocator<int> > const&, phi::DataType, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*)
4   void phi::IndexSelect<phi::GPUContext, float, long>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*, int)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745304477 (unix time) try "date -d @1745304477" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x7fa939ffd010) received by PID 67526 (TID 0x7fc733744700) from PID 973066256 ***]

2025-04-22 15:01:16.726134 test begin: paddle.var(Tensor([1, 107374183, 4, 10],"float16"), list[1,2,], True, False, )

[accuracy error] paddle.var(Tensor([1, 107374183, 4, 10],"float16"), list[1,2,], True, False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]], dtype=float16)
 y: array([[0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833,
        0.0833, 0.0833]], dtype=float16)
2025-04-22 15:01:32.671903 test begin: paddle.var(Tensor([1, 107374183, 4, 10],"float16"), list[1,3,], False, False, )

[accuracy error] paddle.var(Tensor([1, 107374183, 4, 10],"float16"), list[1,3,], False, False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[nan, nan, nan, nan]], dtype=float16)
 y: array([[0.0833, 0.0833, 0.0833, 0.0833]], dtype=float16)
2025-04-22 15:01:43.485557 test begin: paddle.var(Tensor([1, 107374183, 4, 10],"float16"), list[1,3,], True, False, )

[accuracy error] paddle.var(Tensor([1, 107374183, 4, 10],"float16"), list[1,3,], True, False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[nan, nan, nan, nan]], dtype=float16)
 y: array([[0.0833, 0.0833, 0.0833, 0.0833]], dtype=float16)
2025-04-22 15:01:54.491070 test begin: paddle.var(Tensor([1, 107374183, 4, 10],"float16"), tuple(1,3,), True, False, )

[accuracy error] paddle.var(Tensor([1, 107374183, 4, 10],"float16"), tuple(1,3,), True, False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[nan, nan, nan, nan]], dtype=float16)
 y: array([[0.0833, 0.0833, 0.0833, 0.0833]], dtype=float16)
2025-04-22 15:02:06.019199 test begin: paddle.var(Tensor([1, 3, 143165577, 10],"float16"), 2, True, False, )

[accuracy error] paddle.var(Tensor([1, 3, 143165577, 10],"float16"), 2, True, False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]]],
      dtype=float16)
 y: array([[[0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833,
         0.0833, 0.0833],
        [0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833,...
2025-04-22 15:02:18.891055 test begin: paddle.var(Tensor([1, 3, 143165577, 10],"float16"), list[1,2,], True, False, )

[accuracy error] paddle.var(Tensor([1, 3, 143165577, 10],"float16"), list[1,2,], True, False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]], dtype=float16)
 y: array([[0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833,
        0.0833, 0.0833]], dtype=float16)
2025-04-22 15:28:11.006198 test begin: paddle.var(Tensor([1, 3, 4, 357913942],"float16"), list[1,3,], False, False, )

[accuracy error] paddle.var(Tensor([1, 3, 4, 357913942],"float16"), list[1,3,], False, False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[nan, nan, nan, nan]], dtype=float16)
 y: array([[0.0833, 0.0833, 0.0833, 0.0833]], dtype=float16)
2025-04-22 15:28:23.393991 test begin: paddle.var(Tensor([1, 3, 4, 357913942],"float16"), list[1,3,], True, False, )

[accuracy error] paddle.var(Tensor([1, 3, 4, 357913942],"float16"), list[1,3,], True, False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[nan, nan, nan, nan]], dtype=float16)
 y: array([[0.0833, 0.0833, 0.0833, 0.0833]], dtype=float16)
2025-04-22 15:28:34.892725 test begin: paddle.var(Tensor([1, 3, 4, 357913942],"float16"), tuple(1,3,), True, False, )

[accuracy error] paddle.var(Tensor([1, 3, 4, 357913942],"float16"), tuple(1,3,), True, False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[nan, nan, nan, nan]], dtype=float16)
 y: array([[0.0833, 0.0833, 0.0833, 0.0833]], dtype=float16)
2025-04-22 15:33:17.658064 test begin: paddle.var(Tensor([16, 268435457],"float16"), axis=-1, keepdim=True, )

[accuracy error] paddle.var(Tensor([16, 268435457],"float16"), axis=-1, keepdim=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[nan],
       [nan],
       [nan],...
 y: array([[0.0833],
       [0.0833],
       [0.0833],...
2025-04-22 15:59:58.494525 test begin: paddle.var(Tensor([2281701379],"float32"), )

[paddle error] paddle.var(Tensor([2281701379],"float32"), ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-22 17:09:08.398342 test begin: paddle.triu(Tensor([1, 1, 2048, 2097153],"float16"), )

W0422 17:10:50.866935 11353 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 17:10:50.868233 11353 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.triu(Tensor([1, 1, 2048, 2097153],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 8041 / 4294969344 (0.000187%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[-0.2544 ,  0.498  ,  0.2411 , ..., -0.373  , -0.47   ,
          -0.2224 ],
         [ 0.     , -0.473  , -0.282  , ..., -0.4753 ,  0.2334 ,...
 y: array([[[[-0.2544 ,  0.498  ,  0.2411 , ..., -0.373  , -0.47   ,
          -0.2224 ],
         [ 0.     , -0.473  , -0.282  , ..., -0.4753 ,  0.2334 ,...
2025-04-22 17:23:05.087665 test begin: paddle.triu(Tensor([1, 390451573, 1, 11],"float16"), diagonal=1, )

[accuracy error] paddle.triu(Tensor([1, 390451573, 1, 11],"float16"), diagonal=1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 318867112 / 4294967303 (7.42%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[ 0.     ,  0.498  ,  0.2411 , ...,  0.325  , -0.144  ,
           0.203  ]],
...
 y: array([[[[ 0.     ,  0.498  ,  0.2411 , ...,  0.325  , -0.144  ,
           0.203  ]],
...
2025-04-22 17:35:41.780768 test begin: paddle.triu(Tensor([1, 429496730, 1, 10],"float16"), diagonal=1, )

[accuracy error] paddle.triu(Tensor([1, 429496730, 1, 10],"float16"), diagonal=1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 631354068 / 4294967300 (14.7%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[ 0.      ,  0.498   ,  0.2411  , ..., -0.263   ,  0.325   ,
          -0.144   ]],
...
 y: array([[[[ 0.     ,  0.498  ,  0.2411 , ..., -0.263  ,  0.325  ,
          -0.144  ]],
...
2025-04-22 17:48:03.171725 test begin: paddle.triu(Tensor([114085069, 20, 1],"float32"), 0, )

[accuracy error] paddle.triu(Tensor([114085069, 20, 1],"float32"), 0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 6576841 / 2281701380 (0.288%)
Max absolute difference: 0.49999973
Max relative difference: 0.
 x: array([[[-0.378602],
        [ 0.      ],
        [ 0.      ],...
 y: array([[[-0.378602],
        [ 0.      ],
        [ 0.      ],...
2025-04-22 17:51:28.133772 test begin: paddle.triu(Tensor([2, 1, 1, 2147483649],"float16"), )

[accuracy error] paddle.triu(Tensor([2, 1, 1, 2147483649],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 4294967298 (1.16e-07%)
Max absolute difference: 0.2854
Max relative difference: 0.
 x: array([[[[-0.2544,  0.498 ,  0.2411, ...,  0.3694,  0.3503,  0.393 ]]],

...
 y: array([[[[-0.2544,  0.498 ,  0.2411, ...,  0.3694,  0.3503,  0.393 ]]],

...
2025-04-22 18:04:15.653891 test begin: paddle.triu(Tensor([2, 21262215, 1, 101],"float16"), )

[accuracy error] paddle.triu(Tensor([2, 21262215, 1, 101],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 121413466 / 4294967430 (2.83%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[-0.2544  ,  0.498   ,  0.2411  , ...,  0.2184  ,  0.3943  ,
           0.1837  ]],
...
 y: array([[[[-0.2544  ,  0.498   ,  0.2411  , ...,  0.2184  ,  0.3943  ,
           0.1837  ]],
...
2025-04-23 09:58:17.802186 test begin: paddle.triu(Tensor([390451573, 1, 1, 11],"float16"), diagonal=1, )

W0423 10:00:13.638967 40362 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0423 10:00:13.641036 40362 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.triu(Tensor([390451573, 1, 1, 11],"float16"), diagonal=1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 318864449 / 4294967303 (7.42%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[ 0.      ,  0.4648  ,  0.3325  , ..., -0.14    ,  0.3562  ,
          -0.4731  ]]],
...
 y: array([[[[ 0.    ,  0.4648,  0.3325, ..., -0.14  ,  0.3562, -0.4731]]],

...
2025-04-23 10:13:04.573023 test begin: paddle.triu(Tensor([42524429, 1, 1, 101],"float16"), )

[accuracy error] paddle.triu(Tensor([42524429, 1, 1, 101],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 121413271 / 4294967329 (2.83%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[ 0.2045  ,  0.4648  ,  0.3325  , ..., -0.2025  ,  0.0848  ,
          -0.3408  ]]],
...
 y: array([[[[ 0.2045,  0.4648,  0.3325, ..., -0.2025,  0.0848, -0.3408]]],

...
2025-04-23 10:26:18.851058 test begin: paddle.triu(Tensor([429496730, 1, 1, 10],"float16"), diagonal=1, )

[accuracy error] paddle.triu(Tensor([429496730, 1, 1, 10],"float16"), diagonal=1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 631354476 / 4294967300 (14.7%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[ 0.     ,  0.4648 ,  0.3325 , ...,  0.1637 , -0.14   ,
           0.3562 ]]],
...
 y: array([[[[ 0.     ,  0.4648 ,  0.3325 , ...,  0.1637 , -0.14   ,
           0.3562 ]]],
...
2025-04-23 10:39:27.814240 test begin: paddle.triu(x=Tensor([1073741825, 2, 2],"float16"), diagonal=-1, )

[accuracy error] paddle.triu(x=Tensor([1073741825, 2, 2],"float16"), diagonal=-1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2104513883 / 4294967300 (49%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[ 0.2045  ,  0.4648  ],
        [ 0.3325  ,  0.2727  ]],
...
 y: array([[[ 0.2045 ,  0.4648 ],
        [ 0.3325 ,  0.2727 ]],
...
2025-04-23 10:51:45.542928 test begin: paddle.triu(x=Tensor([1073741825, 2, 2],"float16"), diagonal=-5, )

[accuracy error] paddle.triu(x=Tensor([1073741825, 2, 2],"float16"), diagonal=-5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2104513883 / 4294967300 (49%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[ 0.2045  ,  0.4648  ],
        [ 0.3325  ,  0.2727  ]],
...
 y: array([[[ 0.2045 ,  0.4648 ],
        [ 0.3325 ,  0.2727 ]],
...
2025-04-23 11:03:54.388748 test begin: paddle.triu(x=Tensor([1073741825, 2, 2],"float16"), diagonal=0, )

[accuracy error] paddle.triu(x=Tensor([1073741825, 2, 2],"float16"), diagonal=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1578383713 / 4294967300 (36.7%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[ 0.2045  ,  0.4648  ],
        [ 0.      ,  0.2727  ]],
...
 y: array([[[ 0.2045 ,  0.4648 ],
        [ 0.     ,  0.2727 ]],
...
2025-04-23 11:16:02.155270 test begin: paddle.triu(x=Tensor([1073741825, 2, 2],"float16"), diagonal=1, )

[accuracy error] paddle.triu(x=Tensor([1073741825, 2, 2],"float16"), diagonal=1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 526124453 / 4294967300 (12.2%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[ 0.      ,  0.4648  ],
        [ 0.      ,  0.      ]],
...
 y: array([[[0.    , 0.4648],
        [0.    , 0.    ]],
...
2025-04-23 11:27:45.389356 test begin: paddle.triu(x=Tensor([3, 2, 715827883],"float16"), diagonal=-1, )

[accuracy error] paddle.triu(x=Tensor([3, 2, 715827883],"float16"), diagonal=-1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 4294967298 (1.16e-07%)
Max absolute difference: 0.4675
Max relative difference: 0.
 x: array([[[ 0.2045  ,  0.4648  ,  0.3325  , ...,  0.001687,  0.1108  ,
         -0.3223  ],
        [ 0.1403  ,  0.2761  , -0.4573  , ..., -0.4348  ,  0.0653  ,...
 y: array([[[ 0.2045  ,  0.4648  ,  0.3325  , ...,  0.001687,  0.1108  ,
         -0.3223  ],
        [ 0.1403  ,  0.2761  , -0.4573  , ..., -0.4348  ,  0.0653  ,...
2025-04-23 11:40:55.802207 test begin: paddle.triu(x=Tensor([3, 2, 715827883],"float16"), diagonal=-5, )

[accuracy error] paddle.triu(x=Tensor([3, 2, 715827883],"float16"), diagonal=-5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 4294967298 (1.16e-07%)
Max absolute difference: 0.4675
Max relative difference: 0.
 x: array([[[ 0.2045  ,  0.4648  ,  0.3325  , ...,  0.001687,  0.1108  ,
         -0.3223  ],
        [ 0.1403  ,  0.2761  , -0.4573  , ..., -0.4348  ,  0.0653  ,...
 y: array([[[ 0.2045  ,  0.4648  ,  0.3325  , ...,  0.001687,  0.1108  ,
         -0.3223  ],
        [ 0.1403  ,  0.2761  , -0.4573  , ..., -0.4348  ,  0.0653  ,...
2025-04-23 11:54:06.796181 test begin: paddle.triu(x=Tensor([3, 2, 715827883],"float16"), diagonal=0, )

[accuracy error] paddle.triu(x=Tensor([3, 2, 715827883],"float16"), diagonal=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 4294967298 (1.16e-07%)
Max absolute difference: 0.4675
Max relative difference: 0.
 x: array([[[ 0.2045  ,  0.4648  ,  0.3325  , ...,  0.001687,  0.1108  ,
         -0.3223  ],
        [ 0.      ,  0.2761  , -0.4573  , ..., -0.4348  ,  0.0653  ,...
 y: array([[[ 0.2045  ,  0.4648  ,  0.3325  , ...,  0.001687,  0.1108  ,
         -0.3223  ],
        [ 0.      ,  0.2761  , -0.4573  , ..., -0.4348  ,  0.0653  ,...
2025-04-23 12:07:33.401189 test begin: paddle.triu(x=Tensor([3, 2, 715827883],"float16"), diagonal=1, )

[accuracy error] paddle.triu(x=Tensor([3, 2, 715827883],"float16"), diagonal=1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 4294967298 (1.16e-07%)
Max absolute difference: 0.4675
Max relative difference: 0.
 x: array([[[ 0.      ,  0.4648  ,  0.3325  , ...,  0.001687,  0.1108  ,
         -0.3223  ],
        [ 0.      ,  0.      , -0.4573  , ..., -0.4348  ,  0.0653  ,...
 y: array([[[ 0.      ,  0.4648  ,  0.3325  , ...,  0.001687,  0.1108  ,
         -0.3223  ],
        [ 0.      ,  0.      , -0.4573  , ..., -0.4348  ,  0.0653  ,...
2025-04-23 12:20:45.946105 test begin: paddle.triu(x=Tensor([3, 2, 715827883],"float16"), diagonal=5, )

[accuracy error] paddle.triu(x=Tensor([3, 2, 715827883],"float16"), diagonal=5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 4294967298 (1.16e-07%)
Max absolute difference: 0.4675
Max relative difference: 0.
 x: array([[[ 0.      ,  0.      ,  0.      , ...,  0.001687,  0.1108  ,
         -0.3223  ],
        [ 0.      ,  0.      ,  0.      , ..., -0.4348  ,  0.0653  ,...
 y: array([[[ 0.      ,  0.      ,  0.      , ...,  0.001687,  0.1108  ,
         -0.3223  ],
        [ 0.      ,  0.      ,  0.      , ..., -0.4348  ,  0.0653  ,...
2025-04-23 12:33:52.398138 test begin: paddle.triu(x=Tensor([3, 715827883, 2],"float16"), diagonal=-1, )

[accuracy error] paddle.triu(x=Tensor([3, 715827883, 2],"float16"), diagonal=-1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 4294967298 (1.16e-07%)
Max absolute difference: 0.3726
Max relative difference: 0.
 x: array([[[ 0.2045,  0.4648],
        [ 0.3325,  0.2727],
        [ 0.    ,  0.3645],...
 y: array([[[ 0.2045,  0.4648],
        [ 0.3325,  0.2727],
        [ 0.    ,  0.3645],...
2025-04-23 12:45:06.869435 test begin: paddle.triu(x=Tensor([3, 715827883, 2],"float16"), diagonal=-5, )

[accuracy error] paddle.triu(x=Tensor([3, 715827883, 2],"float16"), diagonal=-5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 13 / 4294967298 (3.03e-07%)
Max absolute difference: 0.4348
Max relative difference: 0.
 x: array([[[ 0.2045,  0.4648],
        [ 0.3325,  0.2727],
        [ 0.362 ,  0.3645],...
 y: array([[[ 0.2045,  0.4648],
        [ 0.3325,  0.2727],
        [ 0.362 ,  0.3645],...
2025-04-23 12:56:26.714134 test begin: paddle.triu(x=Tensor([3, 715827883, 2],"float16"), diagonal=0, )

[accuracy error] paddle.triu(x=Tensor([3, 715827883, 2],"float16"), diagonal=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 3 / 4294967298 (6.98e-08%)
Max absolute difference: 0.3472
Max relative difference: 0.
 x: array([[[ 0.2045,  0.4648],
        [ 0.    ,  0.2727],
        [ 0.    ,  0.    ],...
 y: array([[[ 0.2045,  0.4648],
        [ 0.    ,  0.2727],
        [ 0.    ,  0.    ],...
2025-04-21 14:35:10.515268 test begin: paddle.isfinite(Tensor([289, 280, 376, 25, 3],"float32"), )

W0421 14:36:21.242520 100351 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 14:36:21.243398 100351 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.isfinite(Tensor([289, 280, 376, 25, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 14:36:21.353018 test begin: paddle.isfinite(Tensor([35791395, 3, 4, 5],"float64"), )

[cuda error] paddle.isfinite(Tensor([35791395, 3, 4, 5],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 14:37:41.504498 test begin: paddle.isfinite(Tensor([4, 1834166, 311],"float32"), )

[cuda error] paddle.isfinite(Tensor([4, 1834166, 311],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 14:37:49.434314 test begin: paddle.isfinite(Tensor([4, 20228, 376, 25, 3],"float32"), )

[cuda error] paddle.isfinite(Tensor([4, 20228, 376, 25, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 14:37:56.282437 test begin: paddle.isfinite(Tensor([4, 280, 27164, 25, 3],"float32"), )

[cuda error] paddle.isfinite(Tensor([4, 280, 27164, 25, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 14:38:02.480550 test begin: paddle.isfinite(Tensor([4, 280, 376, 1807, 3],"float32"), )

[cuda error] paddle.isfinite(Tensor([4, 280, 376, 1807, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 14:38:09.059153 test begin: paddle.isfinite(Tensor([4, 280, 376, 25, 217],"float32"), )

[cuda error] paddle.isfinite(Tensor([4, 280, 376, 25, 217],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 14:38:15.759364 test begin: paddle.isfinite(Tensor([4, 94, 6068355],"float32"), )

[cuda error] paddle.isfinite(Tensor([4, 94, 6068355],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 14:38:22.408818 test begin: paddle.isfinite(Tensor([71582789, 3, 4, 5],"float16"), )

[accuracy error] paddle.isfinite(Tensor([71582789, 3, 4, 5],"float16"), ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294967340 (100%)
 x: array([[[[ True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True],...
 y: array([[[[ True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True],...
2025-04-21 14:40:45.872853 test begin: paddle.isfinite(Tensor([78050, 94, 311],"float32"), )

[cuda error] paddle.isfinite(Tensor([78050, 94, 311],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 14:40:52.755290 test begin: paddle.isfinite(Tensor([8, 17, 5, 6, 1052689],"float16"), )

[accuracy error] paddle.isfinite(Tensor([8, 17, 5, 6, 1052689],"float16"), ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294971120 (100%)
 x: array([[[[[ True,  True,  True, ..., False, False, False],
          [False, False, False, ..., False, False, False],
          [False, False, False, ..., False, False, False],...
 y: array([[[[[ True,  True,  True, ...,  True,  True,  True],
          [ True,  True,  True, ...,  True,  True,  True],
          [ True,  True,  True, ...,  True,  True,  True],...
2025-04-21 14:41:57.438575 test begin: paddle.isfinite(Tensor([8, 17, 5, 902305, 7],"float16"), )

[accuracy error] paddle.isfinite(Tensor([8, 17, 5, 902305, 7],"float16"), ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294971800 (100%)
 x: array([[[[[ True,  True,  True, ...,  True,  True,  True],
          [ True,  True,  True, ...,  True,  True,  True],
          [ True,  True,  True, ...,  True,  True,  True],...
 y: array([[[[[ True,  True,  True, ...,  True,  True,  True],
          [ True,  True,  True, ...,  True,  True,  True],
          [ True,  True,  True, ...,  True,  True,  True],...
2025-04-21 14:43:22.537917 test begin: paddle.isfinite(Tensor([8, 17, 751921, 6, 7],"float16"), )

[accuracy error] paddle.isfinite(Tensor([8, 17, 751921, 6, 7],"float16"), ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294972752 (100%)
 x: array([[[[[ True,  True,  True, ...,  True,  True,  True],
          [ True,  True,  True, ...,  True,  True,  True],
          [ True,  True,  True, ...,  True,  True,  True],...
 y: array([[[[[ True,  True,  True, ...,  True,  True,  True],
          [ True,  True,  True, ...,  True,  True,  True],
          [ True,  True,  True, ...,  True,  True,  True],...
2025-04-21 14:44:40.890143 test begin: paddle.isfinite(Tensor([8, 2556529, 5, 6, 7],"float16"), )

[accuracy error] paddle.isfinite(Tensor([8, 2556529, 5, 6, 7],"float16"), ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294968720 (100%)
 x: array([[[[[ True,  True,  True, ...,  True,  True,  True],
          [ True,  True,  True, ...,  True,  True,  True],
          [ True,  True,  True, ...,  True,  True,  True],...
 y: array([[[[[ True,  True,  True, ...,  True,  True,  True],
          [ True,  True,  True, ...,  True,  True,  True],
          [ True,  True,  True, ...,  True,  True,  True],...
2025-04-21 14:46:03.103489 test begin: paddle.isfinite(x=Tensor([4294967297],"float16"), )

[accuracy error] paddle.isfinite(x=Tensor([4294967297],"float16"), ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294967297 (100%)
 x: array([ True, False, False, ..., False, False, False])
 y: array([ True,  True,  True, ...,  True,  True,  True])
2025-04-21 15:03:48.654297 test begin: paddle.isinf(Tensor([10, 214748365],"float64"), )

[cuda error] paddle.isinf(Tensor([10, 214748365],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:04:04.832109 test begin: paddle.isinf(Tensor([10, 228170138],"float32"), )

[cuda error] paddle.isinf(Tensor([10, 228170138],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:04:13.797423 test begin: paddle.isinf(Tensor([10186167, 7, 32],"float32"), )

[cuda error] paddle.isinf(Tensor([10186167, 7, 32],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:04:22.022436 test begin: paddle.isinf(Tensor([1073741825, 2],"float64"), )

[cuda error] paddle.isinf(Tensor([1073741825, 2],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:04:35.694359 test begin: paddle.isinf(Tensor([11, 17, 12201612],"int32"), )

[cuda error] paddle.isinf(Tensor([11, 17, 12201612],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:04:44.553291 test begin: paddle.isinf(Tensor([11, 207427399],"float32"), )

[cuda error] paddle.isinf(Tensor([11, 207427399],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:04:52.233880 test begin: paddle.isinf(Tensor([11, 20742740, 10],"int32"), )

[cuda error] paddle.isinf(Tensor([11, 20742740, 10],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:05:00.634578 test begin: paddle.isinf(Tensor([1140850690, 2],"float32"), )

[cuda error] paddle.isinf(Tensor([1140850690, 2],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:05:09.137703 test begin: paddle.isinf(Tensor([134217729, 17],"float32"), )

[cuda error] paddle.isinf(Tensor([134217729, 17],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:05:15.199064 test begin: paddle.isinf(Tensor([13421773, 17, 10],"int32"), )

[cuda error] paddle.isinf(Tensor([13421773, 17, 10],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:05:22.137277 test begin: paddle.isinf(Tensor([14, 10186167, 16],"float32"), )

[cuda error] paddle.isinf(Tensor([14, 10186167, 16],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:05:28.558170 test begin: paddle.isinf(Tensor([14, 40744668, 4],"float32"), )

[cuda error] paddle.isinf(Tensor([14, 40744668, 4],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:05:34.883132 test begin: paddle.isinf(Tensor([14, 5093084, 32],"float32"), )

[cuda error] paddle.isinf(Tensor([14, 5093084, 32],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:05:41.565508 test begin: paddle.isinf(Tensor([14, 64, 2546542],"float32"), )

[cuda error] paddle.isinf(Tensor([14, 64, 2546542],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:05:48.463049 test begin: paddle.isinf(Tensor([14, 7, 23282668],"float32"), )

[cuda error] paddle.isinf(Tensor([14, 7, 23282668],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:05:55.393400 test begin: paddle.isinf(Tensor([178956971, 12],"float64"), )

[cuda error] paddle.isinf(Tensor([178956971, 12],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:06:11.212753 test begin: paddle.isinf(Tensor([190141782, 12],"float32"), )

[cuda error] paddle.isinf(Tensor([190141782, 12],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:06:19.526324 test begin: paddle.isinf(Tensor([2, 1073741825],"float64"), )

[cuda error] paddle.isinf(Tensor([2, 1073741825],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:06:33.302139 test begin: paddle.isinf(Tensor([2, 1140850690],"float32"), )

[cuda error] paddle.isinf(Tensor([2, 1140850690],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:06:42.035156 test begin: paddle.isinf(Tensor([2, 3, 4, 89478486],"float64"), )

[cuda error] paddle.isinf(Tensor([2, 3, 4, 89478486],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:06:57.127574 test begin: paddle.isinf(Tensor([2, 3, 71582789, 5],"float64"), )

[cuda error] paddle.isinf(Tensor([2, 3, 71582789, 5],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:07:07.993520 test begin: paddle.isinf(Tensor([2, 53687092, 4, 5],"float64"), )

[cuda error] paddle.isinf(Tensor([2, 53687092, 4, 5],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:07:18.733436 test begin: paddle.isinf(Tensor([2147483649],"float64"), )

[cuda error] paddle.isinf(Tensor([2147483649],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:07:29.115539 test begin: paddle.isinf(Tensor([2147483649],"int64"), )

[cuda error] paddle.isinf(Tensor([2147483649],"int64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:07:44.226352 test begin: paddle.isinf(Tensor([214748365, 5, 2],"float64"), )

[cuda error] paddle.isinf(Tensor([214748365, 5, 2],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:07:55.168561 test begin: paddle.isinf(Tensor([2228225, 64, 16],"float32"), )

[cuda error] paddle.isinf(Tensor([2228225, 64, 16],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:08:03.962254 test begin: paddle.isinf(Tensor([2281701379],"float32"), )

[cuda error] paddle.isinf(Tensor([2281701379],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:08:10.098706 test begin: paddle.isinf(Tensor([2281701379],"int64"), )

[cuda error] paddle.isinf(Tensor([2281701379],"int64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:08:20.622894 test begin: paddle.isinf(Tensor([252645135, 17],"float32"), )

[cuda error] paddle.isinf(Tensor([252645135, 17],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:08:39.183247 test begin: paddle.isinf(Tensor([35791395, 3, 4, 5],"float64"), )

[cuda error] paddle.isinf(Tensor([35791395, 3, 4, 5],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:08:54.062827 test begin: paddle.isinf(Tensor([4, 268435457, 2],"float64"), )

[cuda error] paddle.isinf(Tensor([4, 268435457, 2],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:09:03.869723 test begin: paddle.isinf(Tensor([4, 5, 107374183],"float64"), )

[cuda error] paddle.isinf(Tensor([4, 5, 107374183],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:09:14.428907 test begin: paddle.isinf(Tensor([4, 536870912, 2],"float32"), )

[cuda error] paddle.isinf(Tensor([4, 536870912, 2],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:09:25.729969 test begin: paddle.isinf(Tensor([4294967295],"float32"), )

[cuda error] paddle.isinf(Tensor([4294967295],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:09:37.813639 test begin: paddle.isinf(Tensor([4294967295],"uint8"), )

[cuda error] paddle.isinf(Tensor([4294967295],"uint8"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:10:23.700303 test begin: paddle.isinf(Tensor([8912897, 64, 4],"float32"), )

[cuda error] paddle.isinf(Tensor([8912897, 64, 4],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:10:32.946771 test begin: paddle.isnan(Tensor([10186167, 7, 32],"float32"), )

[cuda error] paddle.isnan(Tensor([10186167, 7, 32],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:10:39.713287 test begin: paddle.isnan(Tensor([1073741824, 4],"float16"), )

[cuda error] paddle.isnan(Tensor([1073741824, 4],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:10:54.621765 test begin: paddle.isnan(Tensor([1073741824, 4],"float32"), )

[cuda error] paddle.isnan(Tensor([1073741824, 4],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:11:11.910504 test begin: paddle.isnan(Tensor([11, 17, 12201612],"int32"), )

[cuda error] paddle.isnan(Tensor([11, 17, 12201612],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:11:20.637929 test begin: paddle.isnan(Tensor([11, 207427399],"float32"), )

[cuda error] paddle.isnan(Tensor([11, 207427399],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:11:27.607228 test begin: paddle.isnan(Tensor([11, 20742740, 10],"int32"), )

[cuda error] paddle.isnan(Tensor([11, 20742740, 10],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:11:36.091497 test begin: paddle.isnan(Tensor([114, 18837576],"float64"), )

[cuda error] paddle.isnan(Tensor([114, 18837576],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:11:50.312889 test begin: paddle.isnan(Tensor([134217729, 17],"float32"), )

[cuda error] paddle.isnan(Tensor([134217729, 17],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:11:57.003539 test begin: paddle.isnan(Tensor([13421773, 17, 10],"int32"), )

[cuda error] paddle.isnan(Tensor([13421773, 17, 10],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:12:05.429285 test begin: paddle.isnan(Tensor([1393, 64, 160, 160],"float32"), )

[cuda error] paddle.isnan(Tensor([1393, 64, 160, 160],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:12:13.961133 test begin: paddle.isnan(Tensor([14, 10186167, 16],"float32"), )

[cuda error] paddle.isnan(Tensor([14, 10186167, 16],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:12:19.753314 test begin: paddle.isnan(Tensor([14, 1646250, 99],"float32"), )

[cuda error] paddle.isnan(Tensor([14, 1646250, 99],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:12:26.483160 test begin: paddle.isnan(Tensor([14, 40744668, 4],"float32"), )

[cuda error] paddle.isnan(Tensor([14, 40744668, 4],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:12:33.166730 test begin: paddle.isnan(Tensor([14, 5093084, 32],"float32"), )

[cuda error] paddle.isnan(Tensor([14, 5093084, 32],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:12:39.870330 test begin: paddle.isnan(Tensor([14, 64, 2546542],"float32"), )

[cuda error] paddle.isnan(Tensor([14, 64, 2546542],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:12:46.365591 test begin: paddle.isnan(Tensor([14, 7, 23282668],"float32"), )

[cuda error] paddle.isnan(Tensor([14, 7, 23282668],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:12:52.702162 test begin: paddle.isnan(Tensor([1431655765, 3],"float32"), )

[cuda error] paddle.isnan(Tensor([1431655765, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:13:06.629175 test begin: paddle.isnan(Tensor([16, 134217729],"float64"), )

[cuda error] paddle.isnan(Tensor([16, 134217729],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:13:20.473478 test begin: paddle.isnan(Tensor([16, 142606337],"float32"), )

[cuda error] paddle.isnan(Tensor([16, 142606337],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:13:28.524522 test begin: paddle.isnan(Tensor([16, 268435456],"float32"), )

[cuda error] paddle.isnan(Tensor([16, 268435456],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:13:41.400425 test begin: paddle.isnan(Tensor([1633, 3, 375, 1242],"float32"), )

[cuda error] paddle.isnan(Tensor([1633, 3, 375, 1242],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:13:48.340796 test begin: paddle.isnan(Tensor([2, 1073741824, 2],"float32"), )

[cuda error] paddle.isnan(Tensor([2, 1073741824, 2],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:14:00.511696 test begin: paddle.isnan(Tensor([2, 2147483648],"float32"), )

[cuda error] paddle.isnan(Tensor([2, 2147483648],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:14:12.697774 test begin: paddle.isnan(Tensor([2, 3, 4, 89478486],"float64"), )

[cuda error] paddle.isnan(Tensor([2, 3, 4, 89478486],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:14:22.691132 test begin: paddle.isnan(Tensor([2, 3, 4, 95070891],"float32"), )

[cuda error] paddle.isnan(Tensor([2, 3, 4, 95070891],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:14:29.272491 test begin: paddle.isnan(Tensor([2, 3, 71582789, 5],"float64"), )

[cuda error] paddle.isnan(Tensor([2, 3, 71582789, 5],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:14:43.124983 test begin: paddle.isnan(Tensor([2, 3, 76056713, 5],"float32"), )

[cuda error] paddle.isnan(Tensor([2, 3, 76056713, 5],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:14:51.504342 test begin: paddle.isnan(Tensor([2, 4, 268435457],"float64"), )

[cuda error] paddle.isnan(Tensor([2, 4, 268435457],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:15:01.608509 test begin: paddle.isnan(Tensor([2, 4, 536870912],"float32"), )

[cuda error] paddle.isnan(Tensor([2, 4, 536870912],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:15:15.439349 test begin: paddle.isnan(Tensor([2, 536870913, 2],"float64"), )

[cuda error] paddle.isnan(Tensor([2, 536870913, 2],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:15:26.474697 test begin: paddle.isnan(Tensor([2, 53687092, 4, 5],"float64"), )

[cuda error] paddle.isnan(Tensor([2, 53687092, 4, 5],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:15:41.907629 test begin: paddle.isnan(Tensor([2, 57042535, 4, 5],"float32"), )

[cuda error] paddle.isnan(Tensor([2, 57042535, 4, 5],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:15:49.456235 test begin: paddle.isnan(Tensor([2147483648, 2],"float32"), )

[cuda error] paddle.isnan(Tensor([2147483648, 2],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:16:02.078708 test begin: paddle.isnan(Tensor([2147483649, 1],"float64"), )

[cuda error] paddle.isnan(Tensor([2147483649, 1],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:16:13.063879 test begin: paddle.isnan(Tensor([2147483649],"float64"), )

[cuda error] paddle.isnan(Tensor([2147483649],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:16:22.259358 test begin: paddle.isnan(Tensor([2147483649],"int64"), )

[cuda error] paddle.isnan(Tensor([2147483649],"int64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:16:39.934946 test begin: paddle.isnan(Tensor([2228225, 64, 16],"float32"), )

[cuda error] paddle.isnan(Tensor([2228225, 64, 16],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:16:47.682547 test begin: paddle.isnan(Tensor([2281701379],"float32"), )

[cuda error] paddle.isnan(Tensor([2281701379],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:16:54.548225 test begin: paddle.isnan(Tensor([2281701379],"int64"), )

[cuda error] paddle.isnan(Tensor([2281701379],"int64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:17:05.710872 test begin: paddle.isnan(Tensor([252645135, 17],"float32"), )

[cuda error] paddle.isnan(Tensor([252645135, 17],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:17:19.506379 test begin: paddle.isnan(Tensor([268435457, 4, 2],"float64"), )

[cuda error] paddle.isnan(Tensor([268435457, 4, 2],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:17:33.556412 test begin: paddle.isnan(Tensor([268435457, 4, 2],"int64"), )

[cuda error] paddle.isnan(Tensor([268435457, 4, 2],"int64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:17:43.133869 test begin: paddle.isnan(Tensor([286331153, 5, 3],"float32"), )

[cuda error] paddle.isnan(Tensor([286331153, 5, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:17:56.959335 test begin: paddle.isnan(Tensor([3, 1431655765],"float16"), )

[cuda error] paddle.isnan(Tensor([3, 1431655765],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:18:06.790627 test begin: paddle.isnan(Tensor([3, 1431655765],"float32"), )

[cuda error] paddle.isnan(Tensor([3, 1431655765],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:18:23.368295 test begin: paddle.isnan(Tensor([3, 357913942, 2],"float64"), )

[cuda error] paddle.isnan(Tensor([3, 357913942, 2],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:18:33.913131 test begin: paddle.isnan(Tensor([3, 357913942, 2],"int64"), )

[cuda error] paddle.isnan(Tensor([3, 357913942, 2],"int64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:18:49.452597 test begin: paddle.isnan(Tensor([3, 4, 178956971],"float64"), )

[cuda error] paddle.isnan(Tensor([3, 4, 178956971],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:19:00.313528 test begin: paddle.isnan(Tensor([3, 4, 178956971],"int64"), )

[cuda error] paddle.isnan(Tensor([3, 4, 178956971],"int64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:19:12.381786 test begin: paddle.isnan(Tensor([3, 715827883],"float64"), )

[cuda error] paddle.isnan(Tensor([3, 715827883],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:19:23.620998 test begin: paddle.isnan(Tensor([3292499, 7, 99],"float32"), )

[cuda error] paddle.isnan(Tensor([3292499, 7, 99],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:19:30.939775 test begin: paddle.isnan(Tensor([33554433, 64],"float64"), )

[cuda error] paddle.isnan(Tensor([33554433, 64],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:19:43.990764 test begin: paddle.isnan(Tensor([35651585, 64],"float32"), )

[cuda error] paddle.isnan(Tensor([35651585, 64],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:19:52.608983 test begin: paddle.isnan(Tensor([35791395, 3, 4, 5],"float64"), )

[cuda error] paddle.isnan(Tensor([35791395, 3, 4, 5],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:20:02.614934 test begin: paddle.isnan(Tensor([38028357, 3, 4, 5],"float32"), )

[cuda error] paddle.isnan(Tensor([38028357, 3, 4, 5],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:20:09.110808 test begin: paddle.isnan(Tensor([4, 1225, 375, 1242],"float32"), )

[cuda error] paddle.isnan(Tensor([4, 1225, 375, 1242],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:20:15.081853 test begin: paddle.isnan(Tensor([4, 22283, 160, 160],"float32"), )

[cuda error] paddle.isnan(Tensor([4, 22283, 160, 160],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:20:21.402096 test begin: paddle.isnan(Tensor([4, 3, 153094, 1242],"float32"), )

[cuda error] paddle.isnan(Tensor([4, 3, 153094, 1242],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:20:27.706338 test begin: paddle.isnan(Tensor([4, 3, 375, 507045],"float32"), )

[cuda error] paddle.isnan(Tensor([4, 3, 375, 507045],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:20:33.901258 test begin: paddle.isnan(Tensor([4, 64, 160, 55706],"float32"), )

[cuda error] paddle.isnan(Tensor([4, 64, 160, 55706],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:20:40.151865 test begin: paddle.isnan(Tensor([4, 64, 55706, 160],"float32"), )

[cuda error] paddle.isnan(Tensor([4, 64, 55706, 160],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:20:46.525832 test begin: paddle.isnan(Tensor([400, 5368710],"float64"), )

[cuda error] paddle.isnan(Tensor([400, 5368710],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:20:55.859652 test begin: paddle.isnan(Tensor([4294967295],"float16"), )

[cuda error] paddle.isnan(Tensor([4294967295],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:21:05.612210 test begin: paddle.isnan(Tensor([4294967295],"float32"), )

[cuda error] paddle.isnan(Tensor([4294967295],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:21:20.083708 test begin: paddle.isnan(Tensor([4294967295],"int32"), )

[cuda error] paddle.isnan(Tensor([4294967295],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:21:37.264479 test begin: paddle.isnan(Tensor([5, 429496730],"float64"), )

[cuda error] paddle.isnan(Tensor([5, 429496730],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:21:52.675579 test begin: paddle.isnan(Tensor([5, 858993459],"float32"), )

[cuda error] paddle.isnan(Tensor([5, 858993459],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:22:08.893880 test begin: paddle.isnan(Tensor([536870912, 4, 2],"float32"), )

[cuda error] paddle.isnan(Tensor([536870912, 4, 2],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:22:19.871593 test begin: paddle.isnan(Tensor([536870912, 4, 2],"int32"), )

[cuda error] paddle.isnan(Tensor([536870912, 4, 2],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:22:33.929834 test begin: paddle.isnan(Tensor([536870913, 4],"float64"), )

[cuda error] paddle.isnan(Tensor([536870913, 4],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:22:48.738746 test begin: paddle.isnan(Tensor([67108864, 64],"float32"), )

[cuda error] paddle.isnan(Tensor([67108864, 64],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:23:02.975659 test begin: paddle.isnan(Tensor([858993459, 5],"float32"), )

[cuda error] paddle.isnan(Tensor([858993459, 5],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:23:16.078957 test begin: paddle.isnan(Tensor([8912897, 64, 4],"float32"), )

[cuda error] paddle.isnan(Tensor([8912897, 64, 4],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:23:22.619536 test begin: paddle.isneginf(Tensor([11, 17, 12201612],"int32"), )

[cuda error] paddle.isneginf(Tensor([11, 17, 12201612],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:25:34.398009 test begin: paddle.isneginf(Tensor([11, 207427399],"float32"), )

[cuda error] paddle.isneginf(Tensor([11, 207427399],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:27:54.884719 test begin: paddle.isneginf(Tensor([11, 20742740, 10],"int32"), )

[cuda error] paddle.isneginf(Tensor([11, 20742740, 10],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:30:12.184673 test begin: paddle.isneginf(Tensor([134217729, 17],"float32"), )

[cuda error] paddle.isneginf(Tensor([134217729, 17],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:32:20.932520 test begin: paddle.isneginf(Tensor([13421773, 17, 10],"int32"), )

[cuda error] paddle.isneginf(Tensor([13421773, 17, 10],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:34:39.423081 test begin: paddle.isneginf(Tensor([4294967295],"uint8"), )

/host_home/wanghuan29/Paddle/build/python/paddle/tensor/math.py:8249: UserWarning: The shape of broadcast output [-1] is different from the input tensor x with shape: [4294967295], please make sure you are using copysign api correctly.
  warnings.warn(
[cuda error] paddle.isneginf(Tensor([4294967295],"uint8"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:38:50.314083 test begin: paddle.isposinf(Tensor([11, 17, 12201612],"int32"), )

[cuda error] paddle.isposinf(Tensor([11, 17, 12201612],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:41:07.617065 test begin: paddle.isposinf(Tensor([11, 207427399],"float32"), )

[cuda error] paddle.isposinf(Tensor([11, 207427399],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:43:13.930657 test begin: paddle.isposinf(Tensor([11, 20742740, 10],"int32"), )

[cuda error] paddle.isposinf(Tensor([11, 20742740, 10],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:45:23.595641 test begin: paddle.isposinf(Tensor([134217729, 17],"float32"), )

[cuda error] paddle.isposinf(Tensor([134217729, 17],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:47:38.389522 test begin: paddle.isposinf(Tensor([13421773, 17, 10],"int32"), )

[cuda error] paddle.isposinf(Tensor([13421773, 17, 10],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:49:57.081770 test begin: paddle.isposinf(Tensor([4294967295],"uint8"), )

[cuda error] paddle.isposinf(Tensor([4294967295],"uint8"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 15:53:48.658832 test begin: paddle.isreal(Tensor([134217728, 32],"bfloat16"), )

[accuracy error] paddle.isreal(Tensor([134217728, 32],"bfloat16"), ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294967296 (100%)
 x: array([[False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False],...
 y: array([[ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],...
2025-04-21 15:55:11.898594 test begin: paddle.isreal(Tensor([134217728, 32],"bool"), )

[accuracy error] paddle.isreal(Tensor([134217728, 32],"bool"), ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294967296 (100%)
 x: array([[False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False],...
 y: array([[ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],...
2025-04-21 15:57:42.605396 test begin: paddle.isreal(Tensor([134217728, 32],"float16"), )

[accuracy error] paddle.isreal(Tensor([134217728, 32],"float16"), ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294967296 (100%)
 x: array([[False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False],...
 y: array([[ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],...
2025-04-21 15:59:19.310139 test begin: paddle.isreal(Tensor([134217728, 32],"float32"), )

[accuracy error] paddle.isreal(Tensor([134217728, 32],"float32"), ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294967296 (100%)
 x: array([[False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False],...
 y: array([[ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],...
2025-04-21 16:00:50.485047 test begin: paddle.isreal(Tensor([134217728, 32],"int16"), )

[accuracy error] paddle.isreal(Tensor([134217728, 32],"int16"), ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294967296 (100%)
 x: array([[False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False],...
 y: array([[ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],...
2025-04-21 16:02:42.128518 test begin: paddle.isreal(Tensor([134217728, 32],"int32"), )

[accuracy error] paddle.isreal(Tensor([134217728, 32],"int32"), ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294967296 (100%)
 x: array([[False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False],...
 y: array([[ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],...
2025-04-21 16:04:15.564024 test begin: paddle.isreal(Tensor([1431655765, 3],"bfloat16"), )

[accuracy error] paddle.isreal(Tensor([1431655765, 3],"bfloat16"), ) 
 
Arrays are not equal

Mismatched elements: 4294967295 / 4294967295 (100%)
 x: array([[False, False, False],
       [False, False, False],
       [False, False, False],...
 y: array([[ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],...
2025-04-21 16:05:49.161796 test begin: paddle.isreal(Tensor([1431655765, 3],"bool"), )

[accuracy error] paddle.isreal(Tensor([1431655765, 3],"bool"), ) 
 
Arrays are not equal

Mismatched elements: 4294967295 / 4294967295 (100%)
 x: array([[False, False, False],
       [False, False, False],
       [False, False, False],...
 y: array([[ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],...
2025-04-21 16:06:58.891518 test begin: paddle.isreal(Tensor([1431655765, 3],"float16"), )

[accuracy error] paddle.isreal(Tensor([1431655765, 3],"float16"), ) 
 
Arrays are not equal

Mismatched elements: 4294967295 / 4294967295 (100%)
 x: array([[False, False, False],
       [False, False, False],
       [False, False, False],...
 y: array([[ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],...
2025-04-21 16:08:08.132677 test begin: paddle.isreal(Tensor([1431655765, 3],"float32"), )

[accuracy error] paddle.isreal(Tensor([1431655765, 3],"float32"), ) 
 
Arrays are not equal

Mismatched elements: 4294967295 / 4294967295 (100%)
 x: array([[False, False, False],
       [False, False, False],
       [False, False, False],...
 y: array([[ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],...
2025-04-21 16:09:40.345200 test begin: paddle.isreal(Tensor([1431655765, 3],"int16"), )

[accuracy error] paddle.isreal(Tensor([1431655765, 3],"int16"), ) 
 
Arrays are not equal

Mismatched elements: 4294967295 / 4294967295 (100%)
 x: array([[False, False, False],
       [False, False, False],
       [False, False, False],...
 y: array([[ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],...
2025-04-21 16:10:50.752582 test begin: paddle.isreal(Tensor([1431655765, 3],"int32"), )

[accuracy error] paddle.isreal(Tensor([1431655765, 3],"int32"), ) 
 
Arrays are not equal

Mismatched elements: 4294967295 / 4294967295 (100%)
 x: array([[False, False, False],
       [False, False, False],
       [False, False, False],...
 y: array([[ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],...
2025-04-21 16:12:21.686796 test begin: paddle.isreal(Tensor([2, 1073741825],"float64"), )

[accuracy error] paddle.isreal(Tensor([2, 1073741825],"float64"), ) 
 
Arrays are not equal

Mismatched elements: 2147483650 / 2147483650 (100%)
 x: array([[False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False]])
 y: array([[ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True]])
2025-04-21 16:13:08.506441 test begin: paddle.isreal(Tensor([2, 1073741825],"int64"), )

[accuracy error] paddle.isreal(Tensor([2, 1073741825],"int64"), ) 
 
Arrays are not equal

Mismatched elements: 2147483650 / 2147483650 (100%)
 x: array([[False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False]])
 y: array([[ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True]])
2025-04-21 16:13:57.002645 test begin: paddle.isreal(Tensor([2, 1140850690],"bool"), )

[accuracy error] paddle.isreal(Tensor([2, 1140850690],"bool"), ) 
 
Arrays are not equal

Mismatched elements: 2281701380 / 2281701380 (100%)
 x: array([[False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False]])
 y: array([[ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True]])
2025-04-21 16:14:44.041433 test begin: paddle.isreal(Tensor([2, 2147483648],"bfloat16"), )

[accuracy error] paddle.isreal(Tensor([2, 2147483648],"bfloat16"), ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294967296 (100%)
 x: array([[False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False]])
 y: array([[ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True]])
2025-04-21 16:16:35.620507 test begin: paddle.isreal(Tensor([2, 2147483648],"bool"), )

[accuracy error] paddle.isreal(Tensor([2, 2147483648],"bool"), ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294967296 (100%)
 x: array([[False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False]])
 y: array([[ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True]])
2025-04-21 16:17:49.389165 test begin: paddle.isreal(Tensor([2, 2147483648],"float16"), )

[accuracy error] paddle.isreal(Tensor([2, 2147483648],"float16"), ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294967296 (100%)
 x: array([[False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False]])
 y: array([[ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True]])
2025-04-21 16:19:10.467953 test begin: paddle.isreal(Tensor([2, 2147483648],"float32"), )

[accuracy error] paddle.isreal(Tensor([2, 2147483648],"float32"), ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294967296 (100%)
 x: array([[False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False]])
 y: array([[ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True]])
2025-04-21 16:20:26.895123 test begin: paddle.isreal(Tensor([2, 2147483648],"int16"), )

[accuracy error] paddle.isreal(Tensor([2, 2147483648],"int16"), ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294967296 (100%)
 x: array([[False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False]])
 y: array([[ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True]])
2025-04-21 16:21:46.548205 test begin: paddle.isreal(Tensor([2, 2147483648],"int32"), )

[accuracy error] paddle.isreal(Tensor([2, 2147483648],"int32"), ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294967296 (100%)
 x: array([[False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False]])
 y: array([[ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True]])
2025-04-21 16:23:21.606228 test begin: paddle.isreal(Tensor([64, 33554433],"float64"), )

[accuracy error] paddle.isreal(Tensor([64, 33554433],"float64"), ) 
 
Arrays are not equal

Mismatched elements: 2147483712 / 2147483712 (100%)
 x: array([[False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False],...
 y: array([[ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],...
2025-04-21 16:24:15.170878 test begin: paddle.isreal(Tensor([64, 33554433],"int64"), )

[accuracy error] paddle.isreal(Tensor([64, 33554433],"int64"), ) 
 
Arrays are not equal

Mismatched elements: 2147483712 / 2147483712 (100%)
 x: array([[False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False],...
 y: array([[ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],...
2025-04-21 16:24:58.932323 test begin: paddle.isreal(Tensor([64, 67108864],"bfloat16"), )

[accuracy error] paddle.isreal(Tensor([64, 67108864],"bfloat16"), ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294967296 (100%)
 x: array([[False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False],...
 y: array([[ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],...
2025-04-21 16:26:33.133349 test begin: paddle.isreal(Tensor([64, 67108864],"bool"), )

[accuracy error] paddle.isreal(Tensor([64, 67108864],"bool"), ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294967296 (100%)
 x: array([[False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False],...
 y: array([[ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],...
2025-04-21 16:28:05.225865 test begin: paddle.isreal(Tensor([64, 67108864],"float16"), )

[accuracy error] paddle.isreal(Tensor([64, 67108864],"float16"), ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294967296 (100%)
 x: array([[False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False],...
 y: array([[ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],...
2025-04-21 16:29:21.209117 test begin: paddle.isreal(Tensor([64, 67108864],"float32"), )

[accuracy error] paddle.isreal(Tensor([64, 67108864],"float32"), ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294967296 (100%)
 x: array([[False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False],...
 y: array([[ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],...
2025-04-21 16:30:34.149977 test begin: paddle.isreal(Tensor([64, 67108864],"int16"), )

[accuracy error] paddle.isreal(Tensor([64, 67108864],"int16"), ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294967296 (100%)
 x: array([[False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False],...
 y: array([[ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],...
2025-04-21 16:31:42.519887 test begin: paddle.isreal(Tensor([64, 67108864],"int32"), )

[accuracy error] paddle.isreal(Tensor([64, 67108864],"int32"), ) 
 
Arrays are not equal

Mismatched elements: 4294967296 / 4294967296 (100%)
 x: array([[False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False],...
 y: array([[ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],...
2025-04-21 16:33:06.216978 test begin: paddle.isreal(Tensor([67108865, 32],"float64"), )

[accuracy error] paddle.isreal(Tensor([67108865, 32],"float64"), ) 
 
Arrays are not equal

Mismatched elements: 2147483680 / 2147483680 (100%)
 x: array([[False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False],...
 y: array([[ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],...
2025-04-21 16:33:53.602423 test begin: paddle.isreal(Tensor([67108865, 32],"int64"), )

[accuracy error] paddle.isreal(Tensor([67108865, 32],"int64"), ) 
 
Arrays are not equal

Mismatched elements: 2147483680 / 2147483680 (100%)
 x: array([[False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False],
       [False, False, False, ..., False, False, False],...
 y: array([[ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],...
2025-04-21 16:34:46.386469 test begin: paddle.isreal(Tensor([715827883, 3],"float64"), )

[accuracy error] paddle.isreal(Tensor([715827883, 3],"float64"), ) 
 
Arrays are not equal

Mismatched elements: 2147483649 / 2147483649 (100%)
 x: array([[False, False, False],
       [False, False, False],
       [False, False, False],...
 y: array([[ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],...
2025-04-21 16:35:28.802154 test begin: paddle.isreal(Tensor([715827883, 3],"int64"), )

[accuracy error] paddle.isreal(Tensor([715827883, 3],"int64"), ) 
 
Arrays are not equal

Mismatched elements: 2147483649 / 2147483649 (100%)
 x: array([[False, False, False],
       [False, False, False],
       [False, False, False],...
 y: array([[ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],...
2025-04-21 16:36:12.189808 test begin: paddle.isreal(Tensor([760567127, 3],"bool"), )

[accuracy error] paddle.isreal(Tensor([760567127, 3],"bool"), ) 
 
Arrays are not equal

Mismatched elements: 2281701381 / 2281701381 (100%)
 x: array([[False, False, False],
       [False, False, False],
       [False, False, False],...
 y: array([[ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],...
2025-04-21 17:10:05.386632 test begin: paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([3, 27163112, 28],"float32"), 0.36, )

[accuracy error] backward  paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([3, 27163112, 28],"float32"), 0.36, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 738.5581
Max relative difference: 1.
 x: array([[[0.]]], dtype=float32)
 y: array([[[738.5581]]], dtype=float32)
2025-04-21 17:11:38.277016 test begin: paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([3, 28, 27163112],"float32"), 0.36, )

[accuracy error] backward  paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([3, 28, 27163112],"float32"), 0.36, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 738.5581
Max relative difference: 1.
 x: array([[[0.]]], dtype=float32)
 y: array([[[738.5581]]], dtype=float32)
2025-04-21 17:13:38.746616 test begin: paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([3, 8, 95070891],"float32"), 0.3, )

[accuracy error] backward  paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([3, 8, 95070891],"float32"), 0.3, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 806.9336
Max relative difference: 1.
 x: array([[[0.]]], dtype=float32)
 y: array([[[806.9336]]], dtype=float32)
2025-04-21 17:15:23.199284 test begin: paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([3, 95070891, 8],"float32"), 0.3, )

[accuracy error] backward  paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([3, 95070891, 8],"float32"), 0.3, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 806.9336
Max relative difference: 1.
 x: array([[[0.]]], dtype=float32)
 y: array([[[806.9336]]], dtype=float32)
2025-04-21 17:17:17.550131 test begin: paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([35651585, 8, 8],"float32"), 0.3, )

[accuracy error] backward  paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([35651585, 8, 8],"float32"), 0.3, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 806.6631
Max relative difference: 1.
 x: array([[[0.]]], dtype=float32)
 y: array([[[806.6631]]], dtype=float32)
2025-04-21 17:19:06.251968 test begin: paddle.lerp(Tensor([1, 28, 28],"float32"), Tensor([2910334, 28, 28],"float32"), 1.0, )

[accuracy error] backward  paddle.lerp(Tensor([1, 28, 28],"float32"), Tensor([2910334, 28, 28],"float32"), 1.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2235612262 / 2281701856 (98%)
Max absolute difference: 0.5
Max relative difference: 1.
 x: array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[-0.14111 ,  0.44771 , -0.48392 , ..., -0.462687, -0.394162,
          0.187472],
        [-0.050389,  0.398551, -0.167473, ..., -0.350034, -0.140473,...
2025-04-21 17:23:42.346050 test begin: paddle.lerp(Tensor([1, 3, 3],"float16"), Tensor([477218589, 3, 3],"float16"), Tensor([1, 3, 3],"float16"), )

[accuracy error] backward  paddle.lerp(Tensor([1, 3, 3],"float16"), Tensor([477218589, 3, 3],"float16"), Tensor([1, 3, 3],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 9 / 9 (100%)
Max absolute difference: 6116.
Max relative difference: 1.
 x: array([[[ 0.6323 , -0.11237,  0.1227 ],
        [-0.3704 ,  0.473  ,  0.     ],
        [ 0.     ,  0.     ,  0.     ]]], dtype=float16)
 y: array([[[ 1236. , -3820. , -1850. ],
        [-2316. , -4816. ,  -353.2],
        [ 1378. ,   918. , -6116. ]]], dtype=float16)
2025-04-21 17:32:15.433745 test begin: paddle.lerp(Tensor([1, 3, 3],"float32"), Tensor([1, 3, 3],"float32"), Tensor([253522376, 3, 3],"float32"), )

W0421 17:34:27.809835 114324 backward.cc:437] While running Node (LerpGradNode) raises an EnforceNotMet exception
[paddle error] paddle.lerp(Tensor([1, 3, 3],"float32"), Tensor([1, 3, 3],"float32"), Tensor([253522376, 3, 3],"float32"), ) 
 (InvalidArgument) numel is expected to be greater than or equal 0, but received -2013265912.
  [Hint: Expected numel >= 0, but received numel:-2013265912 < 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/backends/gpu/gpu_launch_config.h:115)

2025-04-21 17:34:47.710030 test begin: paddle.lerp(Tensor([1, 3, 4],"float16"), Tensor([1, 3, 4],"float16"), Tensor([357913942, 3, 4],"float16"), )

W0421 17:36:41.594462 116857 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 17:36:41.595587 116857 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] backward  paddle.lerp(Tensor([1, 3, 4],"float16"), Tensor([1, 3, 4],"float16"), Tensor([357913942, 3, 4],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 12 / 12 (100%)
Max absolute difference: 8100.
Max relative difference: 1.001
 x: array([[[ 0.0075 , -0.0836 ,  0.283  ,  0.4658 ],
        [-0.4932 , -0.06274, -0.1925 ,  0.55   ],
        [ 0.     ,  0.     ,  0.     ,  0.     ]]], dtype=float16)
 y: array([[[ -973.5,  1470. , -1015. , -4860. ],
        [ 8100. ,   668.5,  5824. ,  3846. ],
        [ 6272. , -1947. ,  6680. ,  1516. ]]], dtype=float16)
2025-04-21 17:45:40.691240 test begin: paddle.lerp(Tensor([1, 3, 4],"float16"), Tensor([357913942, 3, 4],"float16"), Tensor([1, 3, 4],"float16"), )

[accuracy error] backward  paddle.lerp(Tensor([1, 3, 4],"float16"), Tensor([357913942, 3, 4],"float16"), Tensor([1, 3, 4],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 12 / 12 (100%)
Max absolute difference: 8792.
Max relative difference: 1.
 x: array([[[ 0.0075 , -0.0836 ,  0.283  ,  0.4658 ],
        [-0.4932 , -0.06274, -0.1925 ,  0.55   ],
        [ 0.     ,  0.     ,  0.     ,  0.     ]]], dtype=float16)
 y: array([[[  543. ,  -316.8,  2444. , -3258. ],
        [ 7140. ,  -766.5,  2330. ,  6672. ],
        [ 7676. , -2990. ,  8792. ,    71.4]]], dtype=float16)
2025-04-21 17:54:46.486856 test begin: paddle.lerp(Tensor([1, 3],"float16"), Tensor([1431655766, 3],"float16"), Tensor([1, 3],"float16"), )

[accuracy error] backward  paddle.lerp(Tensor([1, 3],"float16"), Tensor([1431655766, 3],"float16"), Tensor([1, 3],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 3 / 3 (100%)
Max absolute difference: 10632.
Max relative difference: 1.
 x: array([[-0.1848,  0.2583,  0.    ]], dtype=float16)
 y: array([[-2000., 10632.,  6936.]], dtype=float16)
2025-04-21 18:03:29.174440 test begin: paddle.lerp(Tensor([1, 8, 8],"float32"), Tensor([35651585, 8, 8],"float32"), 1.1, )

[accuracy error] backward  paddle.lerp(Tensor([1, 8, 8],"float32"), Tensor([35651585, 8, 8],"float32"), 1.1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 64 / 64 (100%)
Max absolute difference: 397.13022
Max relative difference: 1.
 x: array([[[0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0.],...
 y: array([[[  71.15384 ,  316.60324 ,  -21.901627,  -75.11168 ,
         -163.70117 ,   -1.104687, -225.33508 ,  -13.078407],
        [-207.177   ,  154.66199 ,  331.2115  ,  -25.52723 ,...
2025-04-21 18:06:27.083435 test begin: paddle.lerp(Tensor([1],"float16"), Tensor([4294967297],"float16"), Tensor([1],"float16"), )

[accuracy error] backward  paddle.lerp(Tensor([1],"float16"), Tensor([4294967297],"float16"), Tensor([1],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 19408.
Max relative difference: 1.
 x: array([0.3381], dtype=float16)
 y: array([19408.], dtype=float16)
2025-04-21 18:14:55.171667 test begin: paddle.lerp(Tensor([1],"float32"), Tensor([1],"float32"), Tensor([2281701379],"float32"), )

W0421 18:17:08.177336 155965 backward.cc:437] While running Node (LerpGradNode) raises an EnforceNotMet exception
[paddle error] paddle.lerp(Tensor([1],"float32"), Tensor([1],"float32"), Tensor([2281701379],"float32"), ) 
 (InvalidArgument) numel is expected to be greater than or equal 0, but received -2013265917.
  [Hint: Expected numel >= 0, but received numel:-2013265917 < 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/backends/gpu/gpu_launch_config.h:115)

2025-04-21 18:17:23.366192 test begin: paddle.lerp(Tensor([2, 1, 1, 1],"float32"), Tensor([2, 17825793, 8, 8],"float32"), 0.3, )

W0421 18:18:47.793471 159064 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:18:47.794624 159064 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] backward  paddle.lerp(Tensor([2, 1, 1, 1],"float32"), Tensor([2, 17825793, 8, 8],"float32"), 0.3, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2 / 2 (100%)
Max absolute difference: 3481.7246
Max relative difference: 1.
 x: array([[[[0.]]],


       [[[0.]]]], dtype=float32)
 y: array([[[[1309.4213]]],


       [[[3481.7246]]]], dtype=float32)
2025-04-21 18:20:24.205784 test begin: paddle.lerp(Tensor([2, 1, 1, 1],"float32"), Tensor([2, 3, 47535446, 8],"float32"), 0.3, )

[accuracy error] backward  paddle.lerp(Tensor([2, 1, 1, 1],"float32"), Tensor([2, 3, 47535446, 8],"float32"), 0.3, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2 / 2 (100%)
Max absolute difference: 3483.1055
Max relative difference: 1.
 x: array([[[[0.]]],


       [[[0.]]]], dtype=float32)
 y: array([[[[1308.4675]]],


       [[[3483.1055]]]], dtype=float32)
2025-04-21 18:21:54.063315 test begin: paddle.lerp(Tensor([2, 1, 1, 1],"float32"), Tensor([2, 3, 8, 47535446],"float32"), 0.3, )

[accuracy error] backward  paddle.lerp(Tensor([2, 1, 1, 1],"float32"), Tensor([2, 3, 8, 47535446],"float32"), 0.3, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2 / 2 (100%)
Max absolute difference: 3483.1055
Max relative difference: 1.
 x: array([[[[0.]]],


       [[[0.]]]], dtype=float32)
 y: array([[[[1308.4675]]],


       [[[3483.1055]]]], dtype=float32)
2025-04-21 18:24:13.018885 test begin: paddle.lerp(Tensor([2, 1, 8, 8],"float32"), Tensor([2, 17825793, 8, 8],"float32"), 1.1, )

[accuracy error] backward  paddle.lerp(Tensor([2, 1, 8, 8],"float32"), Tensor([2, 17825793, 8, 8],"float32"), 1.1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 128 / 128 (100%)
Max absolute difference: 290.31372
Max relative difference: 1.
 x: array([[[[0., 0., 0., 0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0., 0., 0., 0.],...
 y: array([[[[-117.7237  , -169.25809 ,    6.275991,   38.977364,
           244.96703 , -289.02048 ,  -32.62978 , -162.85011 ],
         [  39.431385,  -99.0575  ,   82.03845 ,  125.52358 ,...
2025-04-21 18:26:04.437458 test begin: paddle.lerp(Tensor([2, 17825793, 8, 8],"float32"), Tensor([2, 17825793, 8, 8],"float32"), 1.1, )

[accuracy error] backward  paddle.lerp(Tensor([2, 17825793, 8, 8],"float32"), Tensor([2, 17825793, 8, 8],"float32"), 1.1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1820786341 / 2281701504 (79.8%)
Max absolute difference: 0.05
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[-3.005971e-02,  4.134880e-03,  4.739728e-03, ...,
          -3.570896e-02, -1.680940e-02, -2.040452e-02],
         [-2.557013e-02,  1.370719e-02, -4.099004e-02, ...,...
2025-04-21 18:31:14.552531 test begin: paddle.lerp(Tensor([2, 17825793, 8, 8],"float32"), Tensor([2, 17825793, 8, 8],"float32"), 2.1, )

[accuracy error] backward  paddle.lerp(Tensor([2, 17825793, 8, 8],"float32"), Tensor([2, 17825793, 8, 8],"float32"), 2.1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2239798317 / 2281701504 (98.2%)
Max absolute difference: 0.55
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[-3.306568e-01,  4.548368e-02,  5.213701e-02, ...,
          -3.927986e-01, -1.849034e-01, -2.244497e-01],
         [-2.812715e-01,  1.507791e-01, -4.508904e-01, ...,...
2025-04-21 18:35:48.642174 test begin: paddle.lerp(Tensor([2, 1],"float32"), Tensor([2, 1140850690],"float32"), Tensor([2, 1],"float32"), )

W0421 18:37:32.472440 14522 backward.cc:437] While running Node (LerpGradNode) raises an EnforceNotMet exception
[paddle error] paddle.lerp(Tensor([2, 1],"float32"), Tensor([2, 1140850690],"float32"), Tensor([2, 1],"float32"), ) 
 (InvalidArgument) numel is expected to be greater than or equal 0, but received -2013265916.
  [Hint: Expected numel >= 0, but received numel:-2013265916 < 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/backends/gpu/gpu_launch_config.h:115)

2025-04-21 18:37:33.514090 test begin: paddle.lerp(Tensor([2, 1],"float32"), Tensor([2, 1],"float32"), Tensor([2, 1140850690],"float32"), )

W0421 18:39:11.037529 16716 backward.cc:437] While running Node (LerpGradNode) raises an EnforceNotMet exception
[paddle error] paddle.lerp(Tensor([2, 1],"float32"), Tensor([2, 1],"float32"), Tensor([2, 1140850690],"float32"), ) 
 (InvalidArgument) numel is expected to be greater than or equal 0, but received -2013265916.
  [Hint: Expected numel >= 0, but received numel:-2013265916 < 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/backends/gpu/gpu_launch_config.h:115)

2025-04-21 18:39:12.028923 test begin: paddle.linalg.cholesky_solve(x=Tensor([4, 570425345],"float32"), y=Tensor([4, 4],"float32"), )

W0421 18:41:33.489898 18265 backward.cc:441] While running Node (CholeskySolveGradNode) raises a std::exception: paddle::memory::allocation::BadAlloc
[paddle error] paddle.linalg.cholesky_solve(x=Tensor([4, 570425345],"float32"), y=Tensor([4, 4],"float32"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   CholeskySolveGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::cholesky_solve_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, bool, paddle::Tensor*, paddle::Tensor*)
4   void phi::CholeskySolveGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, bool, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::CholeskySolveKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, bool, phi::DenseTensor*)
6   void phi::TransposeKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, phi::DenseTensor*)
7   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
8   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
9   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
12  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
13  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 8.500000GB memory on GPU 0, 73.748962GB memory has been allocated and available memory is only 5.435913GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /host_home/wanghuan29/Paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-04-21 18:41:34.858578 test begin: paddle.linalg.cond(Tensor([142606337, 4, 4],"float32"), p="fro", )

one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [142606337, 4, 4]], which is output 0 of LinalgInvExBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
[accuracy error] paddle.linalg.cond(Tensor([142606337, 4, 4],"float32"), p="fro", ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 531 / 142606337 (0.000372%)
Max absolute difference: 1.0178437e+09
Max relative difference: 2.3095615
 x: array([  8.907641,  25.63176 ,  11.599359, ...,  16.66101 , 184.4398  ,
         6.810084], dtype=float32)
 y: array([  8.907642,  25.63176 ,  11.599356, ...,  16.661013, 184.43945 ,
         6.810084], dtype=float32)
2025-04-21 18:41:57.694092 test begin: paddle.linalg.cond(Tensor([2, 126761188, 3, 3],"float32"), "fro", )

one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [2, 126761188, 3, 3]], which is output 0 of LinalgInvExBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
[paddle error] paddle.linalg.cond(Tensor([2, 126761188, 3, 3],"float32"), "fro", ) 
 (PreconditionNotMet) For batch [48797733]: U(3, 3) is zero, singular U. Please check the matrix value and change it to a non-singular matrix
  [Hint: Expected info[i] == 0, but received info[i]:3 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/kernels/funcs/matrix_inverse.cu:125)

2025-04-21 18:44:01.895447 test begin: paddle.linalg.cond(Tensor([2, 126761188, 3, 3],"float32"), -1, )

W0421 18:45:14.291716 24819 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:45:14.293161 24819 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [2, 126761188, 3, 3]], which is output 0 of LinalgInvExBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
[paddle error] paddle.linalg.cond(Tensor([2, 126761188, 3, 3],"float32"), -1, ) 
 (PreconditionNotMet) For batch [5950197]: U(3, 3) is zero, singular U. Please check the matrix value and change it to a non-singular matrix
  [Hint: Expected info[i] == 0, but received info[i]:3 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/kernels/funcs/matrix_inverse.cu:125)

2025-04-21 18:45:29.838053 test begin: paddle.linalg.cond(Tensor([2, 126761188, 3, 3],"float32"), -math.inf, )

one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [2, 126761188, 3, 3]], which is output 0 of LinalgInvExBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
[paddle error] paddle.linalg.cond(Tensor([2, 126761188, 3, 3],"float32"), -math.inf, ) 
 (PreconditionNotMet) For batch [5950197]: U(3, 3) is zero, singular U. Please check the matrix value and change it to a non-singular matrix
  [Hint: Expected info[i] == 0, but received info[i]:3 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/kernels/funcs/matrix_inverse.cu:125)

2025-04-21 18:45:56.799174 test begin: paddle.linalg.cond(Tensor([2, 126761188, 3, 3],"float32"), 1, )

one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [2, 126761188, 3, 3]], which is output 0 of LinalgInvExBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
[paddle error] paddle.linalg.cond(Tensor([2, 126761188, 3, 3],"float32"), 1, ) 
 (PreconditionNotMet) For batch [5950197]: U(3, 3) is zero, singular U. Please check the matrix value and change it to a non-singular matrix
  [Hint: Expected info[i] == 0, but received info[i]:3 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/kernels/funcs/matrix_inverse.cu:125)

2025-04-21 18:46:24.399749 test begin: paddle.linalg.cond(Tensor([2, 126761188, 3, 3],"float32"), 2, )

[torch error] paddle.linalg.cond(Tensor([2, 126761188, 3, 3],"float32"), 2, ) 
 CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-04-21 18:47:59.679954 test begin: paddle.linalg.cond(Tensor([2, 126761188, 3, 3],"float32"), None, )

[torch error] paddle.linalg.cond(Tensor([2, 126761188, 3, 3],"float32"), None, ) 
 CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-04-21 18:50:37.387659 test begin: paddle.linalg.cond(Tensor([2, 126761188, 3, 3],"float32"), math.inf, )

W0421 18:51:59.990823 31832 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:51:59.991905 31832 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [2, 126761188, 3, 3]], which is output 0 of LinalgInvExBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
[paddle error] paddle.linalg.cond(Tensor([2, 126761188, 3, 3],"float32"), math.inf, ) 
 (PreconditionNotMet) For batch [89080313]: U(3, 3) is zero, singular U. Please check the matrix value and change it to a non-singular matrix
  [Hint: Expected info[i] == 0, but received info[i]:3 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/kernels/funcs/matrix_inverse.cu:125)

2025-04-21 18:52:17.273783 test begin: paddle.linalg.cond(Tensor([63380594, 4, 3, 3],"float32"), "fro", )

one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [63380594, 4, 3, 3]], which is output 0 of LinalgInvExBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
[paddle error] paddle.linalg.cond(Tensor([63380594, 4, 3, 3],"float32"), "fro", ) 
 (PreconditionNotMet) For batch [89080313]: U(3, 3) is zero, singular U. Please check the matrix value and change it to a non-singular matrix
  [Hint: Expected info[i] == 0, but received info[i]:3 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/kernels/funcs/matrix_inverse.cu:125)

2025-04-21 18:52:45.741270 test begin: paddle.linalg.cond(Tensor([63380594, 4, 3, 3],"float32"), "nuc", )

[torch error] paddle.linalg.cond(Tensor([63380594, 4, 3, 3],"float32"), "nuc", ) 
 CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-04-21 18:54:23.325358 test begin: paddle.linalg.cond(Tensor([63380594, 4, 3, 3],"float32"), -1, )

W0421 18:55:47.344619 36228 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:55:47.345755 36228 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [63380594, 4, 3, 3]], which is output 0 of LinalgInvExBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
[paddle error] paddle.linalg.cond(Tensor([63380594, 4, 3, 3],"float32"), -1, ) 
 (PreconditionNotMet) For batch [167874916]: U(3, 3) is zero, singular U. Please check the matrix value and change it to a non-singular matrix
  [Hint: Expected info[i] == 0, but received info[i]:3 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/kernels/funcs/matrix_inverse.cu:125)

2025-04-21 18:56:03.544444 test begin: paddle.linalg.cond(Tensor([63380594, 4, 3, 3],"float32"), -2, )

[torch error] paddle.linalg.cond(Tensor([63380594, 4, 3, 3],"float32"), -2, ) 
 CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-04-21 18:57:39.635553 test begin: paddle.linalg.cond(Tensor([63380594, 4, 3, 3],"float32"), -math.inf, )

W0421 18:58:53.782204 39905 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:58:53.783337 39905 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [63380594, 4, 3, 3]], which is output 0 of LinalgInvExBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
[paddle error] paddle.linalg.cond(Tensor([63380594, 4, 3, 3],"float32"), -math.inf, ) 
 (PreconditionNotMet) For batch [204613948]: U(3, 3) is zero, singular U. Please check the matrix value and change it to a non-singular matrix
  [Hint: Expected info[i] == 0, but received info[i]:3 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/kernels/funcs/matrix_inverse.cu:125)

2025-04-21 18:59:08.786142 test begin: paddle.linalg.cond(Tensor([63380594, 4, 3, 3],"float32"), 1, )

one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [63380594, 4, 3, 3]], which is output 0 of LinalgInvExBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
[paddle error] paddle.linalg.cond(Tensor([63380594, 4, 3, 3],"float32"), 1, ) 
 (PreconditionNotMet) For batch [204613948]: U(3, 3) is zero, singular U. Please check the matrix value and change it to a non-singular matrix
  [Hint: Expected info[i] == 0, but received info[i]:3 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/kernels/funcs/matrix_inverse.cu:125)

2025-04-21 18:59:31.527746 test begin: paddle.linalg.cond(Tensor([63380594, 4, 3, 3],"float32"), 2, )

[torch error] paddle.linalg.cond(Tensor([63380594, 4, 3, 3],"float32"), 2, ) 
 CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-04-21 19:01:05.472149 test begin: paddle.linalg.cond(Tensor([63380594, 4, 3, 3],"float32"), None, )

[torch error] paddle.linalg.cond(Tensor([63380594, 4, 3, 3],"float32"), None, ) 
 CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-04-21 19:03:43.652663 test begin: paddle.linalg.cond(Tensor([63380594, 4, 3, 3],"float32"), math.inf, )

W0421 19:04:55.070638 46570 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 19:04:55.071741 46570 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [63380594, 4, 3, 3]], which is output 0 of LinalgInvExBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
[paddle error] paddle.linalg.cond(Tensor([63380594, 4, 3, 3],"float32"), math.inf, ) 
 (PreconditionNotMet) For batch [159699604]: U(3, 3) is zero, singular U. Please check the matrix value and change it to a non-singular matrix
  [Hint: Expected info[i] == 0, but received info[i]:3 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/kernels/funcs/matrix_inverse.cu:125)

2025-04-21 19:05:09.918711 test begin: paddle.linalg.cov(x=Tensor([4, 1073741825],"float16"), )

/host_home/wanghuan29/Paddle/build/python/paddle/tensor/creation.py:711: RuntimeWarning: overflow encountered in cast
  return ndarray.astype(convert_dtype(dtype))
[accuracy error] paddle.linalg.cov(x=Tensor([4, 1073741825],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[     inf, -0.00568, -0.1061 ,  0.02515],
       [-0.00568,      inf, -0.01653, -0.0371 ],
       [-0.1061 , -0.01653,      inf, -0.03152],
       [ 0.02515, -0.0371 , -0.03152,      inf]], dtype=float16)
 y: array([[nan, -0., -0.,  0.],
       [-0., nan, -0., -0.],
       [-0., -0., nan, -0.],
       [ 0., -0., -0., nan]], dtype=float16)
2025-04-21 19:06:36.729693 test begin: paddle.linalg.det(Tensor([253522376, 3, 3],"float32"), )

W0421 19:08:49.778695 49360 backward.cc:437] While running Node (DetGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.det(Tensor([253522376, 3, 3],"float32"), ) 
 (PreconditionNotMet) For batch [159699604]: U(3, 3) is zero, singular U. Please check the matrix value and change it to a non-singular matrix
  [Hint: Expected info[i] == 0, but received info[i]:3 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/kernels/funcs/matrix_inverse.cu:125)

2025-04-21 19:08:50.375840 test begin: paddle.linalg.det(Tensor([3, 30422686, 5, 5],"float32"), )

[accuracy error] backward  paddle.linalg.det(Tensor([3, 30422686, 5, 5],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 78 / 2281701450 (3.42e-06%)
Max absolute difference: 0.06145274
Max relative difference: 309.31476
 x: array([[[[ 1.084840e-02, -1.330202e-03,  5.058276e-03,  2.674758e-02,
           3.457531e-02],
         [ 9.426802e-03, -2.993305e-03, -1.689709e-02,  1.096906e-02,...
 y: array([[[[ 1.084840e-02, -1.330202e-03,  5.058276e-03,  2.674758e-02,
           3.457531e-02],
         [ 9.426801e-03, -2.993305e-03, -1.689709e-02,  1.096906e-02,...
2025-04-21 19:12:15.832820 test begin: paddle.linalg.det(Tensor([30422686, 3, 5, 5],"float32"), )

[accuracy error] backward  paddle.linalg.det(Tensor([30422686, 3, 5, 5],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 78 / 2281701450 (3.42e-06%)
Max absolute difference: 0.06145274
Max relative difference: 309.31476
 x: array([[[[ 1.084840e-02, -1.330202e-03,  5.058276e-03,  2.674758e-02,
           3.457531e-02],
         [ 9.426802e-03, -2.993305e-03, -1.689709e-02,  1.096906e-02,...
 y: array([[[[ 1.084840e-02, -1.330202e-03,  5.058276e-03,  2.674758e-02,
           3.457531e-02],
         [ 9.426801e-03, -2.993305e-03, -1.689709e-02,  1.096906e-02,...
2025-04-21 19:15:38.755586 test begin: paddle.linalg.lstsq(Tensor([10, 228170138],"float32"), Tensor([10, 8],"float32"), rcond=None, driver="gels", )

[paddle error] paddle.linalg.lstsq(Tensor([10, 228170138],"float32"), Tensor([10, 8],"float32"), rcond=None, driver="gels", ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 19:15:53.419381 test begin: paddle.linalg.lstsq(Tensor([253522376, 9],"float32"), Tensor([253522376, 5],"float32"), rcond=1e-15, driver="gels", )

[torch error] paddle.linalg.lstsq(Tensor([253522376, 9],"float32"), Tensor([253522376, 5],"float32"), rcond=1e-15, driver="gels", ) 
 CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-04-21 19:16:02.973712 test begin: paddle.linalg.lstsq(Tensor([9, 253522376],"float32"), Tensor([9, 5],"float32"), rcond=1e-15, driver="gels", )

[torch error] paddle.linalg.lstsq(Tensor([9, 253522376],"float32"), Tensor([9, 5],"float32"), rcond=1e-15, driver="gels", ) 
 CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-04-21 19:17:16.203777 test begin: paddle.linalg.matrix_norm(x=Tensor([2, 536870913, 4],"float16"), p=-math.inf, axis=list[0,1,], keepdim=False, )

W0421 19:18:46.570578 60269 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 19:18:46.571715 60269 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.linalg.matrix_norm(x=Tensor([2, 536870913, 4],"float16"), p=-math.inf, axis=list[0,1,], keepdim=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([1024., 1024., 1024., 1024.], dtype=float16)
 y: array([inf, inf, inf, inf], dtype=float16)
2025-04-21 19:23:01.021352 test begin: paddle.linalg.matrix_norm(x=Tensor([2, 536870913, 4],"float16"), p=-math.inf, axis=list[0,1,], keepdim=True, )

[accuracy error] paddle.linalg.matrix_norm(x=Tensor([2, 536870913, 4],"float16"), p=-math.inf, axis=list[0,1,], keepdim=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[[1024., 1024., 1024., 1024.]]], dtype=float16)
 y: array([[[inf, inf, inf, inf]]], dtype=float16)
2025-04-21 19:27:28.555240 test begin: paddle.linalg.matrix_rank(Tensor([114085069, 4, 5],"float32"), )

[torch error] paddle.linalg.matrix_rank(Tensor([114085069, 4, 5],"float32"), ) 
 CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-04-21 19:28:48.824858 test begin: paddle.linalg.matrix_rank(Tensor([114085069, 4, 5],"float32"), tol=0.1, )

[torch error] paddle.linalg.matrix_rank(Tensor([114085069, 4, 5],"float32"), tol=0.1, ) 
 CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-04-21 19:30:02.205205 test begin: paddle.linalg.matrix_rank(Tensor([19014179, 4, 5, 6],"float32"), Tensor([3, 4],"float32"), False, )

[torch error] paddle.linalg.matrix_rank(Tensor([19014179, 4, 5, 6],"float32"), Tensor([3, 4],"float32"), False, ) 
 CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-04-21 19:31:13.995168 test begin: paddle.linalg.matrix_rank(Tensor([3, 25352238, 5, 6],"float32"), Tensor([3, 4],"float32"), False, )

[torch error] paddle.linalg.matrix_rank(Tensor([3, 25352238, 5, 6],"float32"), Tensor([3, 4],"float32"), False, ) 
 CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-04-21 19:32:26.330872 test begin: paddle.linalg.norm(Tensor([10, 10, 22817014],"float32"), p=0, axis=-1, )

W0421 19:33:35.354266 76411 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 19:33:35.355453 76411 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[accuracy error] paddle.linalg.norm(Tensor([10, 10, 22817014],"float32"), p=0, axis=-1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 100 / 100 (100%)
Max absolute difference: 6039798.
Max relative difference: 0.2647059
 x: array([[16777216., 16777216., 16777216., 16777216., 16777216., 16777216.,
        16777216., 16777216., 16777216., 16777216.],
       [16777216., 16777216., 16777216., 16777216., 16777216., 16777216.,...
 y: array([[22817014., 22817014., 22817014., 22817014., 22817014., 22817014.,
        22817014., 22817014., 22817014., 22817014.],
       [22817014., 22817014., 22817014., 22817014., 22817014., 22817014.,...
2025-04-21 19:33:37.125239 test begin: paddle.linalg.norm(Tensor([10, 10, 22817014],"float32"), p=1.5, axis=-1, )

[accuracy error] paddle.linalg.norm(Tensor([10, 10, 22817014],"float32"), p=1.5, axis=-1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 100 / 100 (100%)
Max absolute difference: 706.86914
Max relative difference: 0.03236705
 x: array([[21131.924, 21127.488, 21129.7  , 21131.81 , 21133.322, 21129.78 ,
        21131.652, 21130.814, 21131.188, 21128.268],
       [21130.895, 21135.695, 21132.324, 21131.977, 21130.92 , 21136.453,...
 y: array([[21835.639, 21833.725, 21834.709, 21836.713, 21836.959, 21835.406,
        21835.908, 21835.068, 21836.252, 21832.707],
       [21835.139, 21839.176, 21837.523, 21837.453, 21834.53 , 21841.047,...
2025-04-21 19:34:01.650024 test begin: paddle.linalg.norm(Tensor([10, 10, 22817014],"float32"), p=2.0, axis=-1, )

[accuracy error] paddle.linalg.norm(Tensor([10, 10, 22817014],"float32"), p=2.0, axis=-1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 100 / 100 (100%)
Max absolute difference: 19.657593
Max relative difference: 0.01425735
 x: array([[1359.3145, 1359.1493, 1359.2261, 1359.2922, 1359.3787, 1359.2754,
        1359.299 , 1359.2423, 1359.3694, 1359.0562],
       [1359.259 , 1359.5099, 1359.3564, 1359.349 , 1359.2042, 1359.557 ,...
 y: array([[1378.8689, 1378.7644, 1378.8169, 1378.9231, 1378.9127, 1378.8398,
        1378.8646, 1378.8201, 1378.9044, 1378.7076],
       [1378.8336, 1379.0696, 1378.9962, 1378.9653, 1378.7815, 1379.1733,...
2025-04-21 19:34:27.987958 test begin: paddle.linalg.norm(Tensor([10, 10, 22817014],"float32"), p=2.5, axis=-1, )

[accuracy error] paddle.linalg.norm(Tensor([10, 10, 22817014],"float32"), p=2.5, axis=-1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 100 / 100 (100%)
Max absolute difference: 3.9550476
Max relative difference: 0.01487814
 x: array([[261.92593, 261.87448, 261.89304, 261.91434, 261.90793, 261.89948,
        261.9089 , 261.89972, 261.91995, 261.87283],
       [261.89655, 261.93463, 261.92514, 261.91452, 261.886  , 261.94623,...
 y: array([[265.84647, 265.82953, 265.83685, 265.85455, 265.85007, 265.83914,
        265.8435 , 265.83618, 265.85336, 265.8187 ],
       [265.83978, 265.88187, 265.8723 , 265.8622 , 265.8282 , 265.90015,...
2025-04-21 19:34:52.045079 test begin: paddle.linalg.norm(Tensor([10, 10, 22817014],"float32"), p=3.0, axis=-1, )

[accuracy error] paddle.linalg.norm(Tensor([10, 10, 22817014],"float32"), p=3.0, axis=-1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 100 / 100 (100%)
Max absolute difference: 1.049324
Max relative difference: 0.01174605
 x: array([[88.2895  , 88.28463 , 88.28591 , 88.29001 , 88.28835 , 88.28466 ,
        88.28508 , 88.28471 , 88.2942  , 88.27715 ],
       [88.28455 , 88.29828 , 88.29577 , 88.296364, 88.28441 , 88.30598 ,...
 y: array([[89.33493 , 89.33018 , 89.33164 , 89.33684 , 89.33503 , 89.332054,
        89.33352 , 89.33136 , 89.33732 , 89.326385],
       [89.33279 , 89.34588 , 89.3439  , 89.33941 , 89.32858 , 89.35166 ,...
2025-04-21 19:35:16.084642 test begin: paddle.linalg.norm(Tensor([10, 10, 22817014],"float32"), p=math.inf, axis=-1, )

[accuracy error] backward  paddle.linalg.norm(Tensor([10, 10, 22817014],"float32"), p=math.inf, axis=-1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 115 / 2281701400 (5.04e-06%)
Max absolute difference: 0.37294978
Max relative difference: 3.0000002
 x: array([[[ 0., -0., -0., ..., -0., -0.,  0.],
        [ 0.,  0.,  0., ..., -0.,  0., -0.],
        [ 0.,  0.,  0., ..., -0.,  0., -0.],...
 y: array([[[ 0., -0., -0., ..., -0., -0.,  0.],
        [ 0.,  0.,  0., ..., -0.,  0., -0.],
        [ 0.,  0.,  0., ..., -0.,  0., -0.],...
2025-04-21 19:37:10.556181 test begin: paddle.linalg.norm(Tensor([10, 10, 42949673],"float32"), p=0, axis=-1, )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[accuracy error] paddle.linalg.norm(Tensor([10, 10, 42949673],"float32"), p=0, axis=-1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 100 / 100 (100%)
Max absolute difference: 26172456.
Max relative difference: 0.609375
 x: array([[16777216., 16777216., 16777216., 16777216., 16777216., 16777216.,
        16777216., 16777216., 16777216., 16777216.],
       [16777216., 16777216., 16777216., 16777216., 16777216., 16777216.,...
 y: array([[42949672., 42949672., 42949672., 42949672., 42949672., 42949672.,
        42949672., 42949672., 42949672., 42949672.],
       [42949672., 42949672., 42949672., 42949672., 42949672., 42949672.,...
2025-04-21 19:37:26.813707 test begin: paddle.linalg.norm(Tensor([10, 10, 42949673],"float32"), p=1.0, axis=-1, )

[accuracy error] paddle.linalg.norm(Tensor([10, 10, 42949673],"float32"), p=1.0, axis=-1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 100 / 100 (100%)
Max absolute difference: 2352208.5
Max relative difference: 0.2189972
 x: array([[8388607.5, 8388607.5, 8388607.5, 8388607.5, 8388607.5, 8388607.5,
        8388607.5, 8388607.5, 8388607.5, 8388607.5],
       [8388607.5, 8388607.5, 8388607.5, 8388607.5, 8388607.5, 8388607.5,...
 y: array([[10736346., 10737209., 10736684., 10738219., 10737030., 10736104.,
        10738187., 10737623., 10738758., 10737752.],
       [10737874., 10737256., 10737121., 10737314., 10736312., 10738282.,...
2025-04-21 19:38:21.868657 test begin: paddle.linalg.norm(Tensor([10, 10, 42949673],"float32"), p=1.5, axis=-1, )

[accuracy error] paddle.linalg.norm(Tensor([10, 10, 42949673],"float32"), p=1.5, axis=-1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 100 / 100 (100%)
Max absolute difference: 2724.3398
Max relative difference: 0.08183414
 x: array([[30566.273, 30568.412, 30565.578, 30568.352, 30571.057, 30567.352,
        30571.615, 30566.63 , 30576.445, 30571.06 ],
       [30571.992, 30568.955, 30569.436, 30569.322, 30568.611, 30572.547,...
 y: array([[33287.316, 33289.625, 33287.43 , 33291.664, 33288.977, 33286.39 ,
        33292.75 , 33290.152, 33293.96 , 33291.19 ],
       [33291.617, 33290.43 , 33289.113, 33290.113, 33287.14 , 33292.09 ,...
2025-04-21 19:39:30.372998 test begin: paddle.linalg.norm(Tensor([10, 10, 42949673],"float32"), p=2.0, axis=-1, )

[accuracy error] paddle.linalg.norm(Tensor([10, 10, 42949673],"float32"), p=2.0, axis=-1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 100 / 100 (100%)
Max absolute difference: 64.20068
Max relative difference: 0.03393515
 x: array([[1827.6001, 1827.8435, 1827.6221, 1827.7397, 1827.7308, 1827.5878,
        1827.9655, 1827.6985, 1827.9968, 1827.9054],
       [1827.9285, 1827.8043, 1827.7208, 1827.9008, 1827.7053, 1827.7329,...
 y: array([[1891.7107, 1891.8231, 1891.6853, 1891.9047, 1891.7919, 1891.6565,
        1892.0055, 1891.8319, 1892.052 , 1891.9098],
       [1891.9354, 1891.8965, 1891.7926, 1891.8586, 1891.7103, 1891.9336,...
2025-04-21 19:40:33.884829 test begin: paddle.linalg.norm(Tensor([10, 10, 42949673],"float32"), p=2.5, axis=-1, )

[accuracy error] paddle.linalg.norm(Tensor([10, 10, 42949673],"float32"), p=2.5, axis=-1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 100 / 100 (100%)
Max absolute difference: 13.205383
Max relative difference: 0.03856967
 x: array([[329.19708, 329.21518, 329.16266, 329.21164, 329.1892 , 329.1615 ,
        329.22446, 329.2044 , 329.24197, 329.2352 ],
       [329.2131 , 329.20944, 329.19775, 329.2172 , 329.1745 , 329.211  ,...
 y: array([[342.36935, 342.3862 , 342.3609 , 342.39777, 342.38242, 342.35953,
        342.4201 , 342.38614, 342.4257 , 342.40195],
       [342.40686, 342.4034 , 342.38156, 342.3942 , 342.37146, 342.40298,...
2025-04-21 19:41:29.162049 test begin: paddle.linalg.norm(Tensor([10, 10, 42949673],"float32"), p=3.0, axis=-1, )

[accuracy error] paddle.linalg.norm(Tensor([10, 10, 42949673],"float32"), p=3.0, axis=-1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 100 / 100 (100%)
Max absolute difference: 2.7180557
Max relative difference: 0.02464021
 x: array([[107.58544 , 107.595955, 107.58667 , 107.59322 , 107.59203 ,
        107.58723 , 107.6038  , 107.59154 , 107.606804, 107.59894 ],
       [107.60351 , 107.59441 , 107.58786 , 107.59817 , 107.59227 ,...
 y: array([[110.2996  , 110.3039  , 110.295975, 110.307144, 110.30338 ,
        110.296394, 110.315094, 110.303764, 110.31622 , 110.30906 ],
       [110.31075 , 110.31045 , 110.30281 , 110.30698 , 110.3009  ,...
2025-04-21 19:42:24.926517 test begin: paddle.linalg.norm(Tensor([10, 11408507, 20],"float32"), p=math.inf, axis=-1, )

[accuracy error] backward  paddle.linalg.norm(Tensor([10, 11408507, 20],"float32"), p=math.inf, axis=-1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 122 / 2281701400 (5.35e-06%)
Max absolute difference: 0.24897547
Max relative difference: 1.
 x: array([[[ 0.      , -0.      , -0.      , ..., -0.      ,  0.      ,
         -0.      ],
        [-0.      , -0.      ,  0.      , ..., -0.      ,  0.      ,...
 y: array([[[ 0.      , -0.      , -0.      , ..., -0.      ,  0.      ,
         -0.      ],
        [-0.      , -0.      ,  0.      , ..., -0.      ,  0.      ,...
2025-04-21 19:45:06.299202 test begin: paddle.linalg.norm(Tensor([11408507, 10, 20],"float32"), p=math.inf, axis=-1, )

[accuracy error] backward  paddle.linalg.norm(Tensor([11408507, 10, 20],"float32"), p=math.inf, axis=-1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 122 / 2281701400 (5.35e-06%)
Max absolute difference: 0.24897547
Max relative difference: 1.
 x: array([[[ 0.      , -0.      , -0.      , ..., -0.      ,  0.      ,
         -0.      ],
        [-0.      , -0.      ,  0.      , ..., -0.      ,  0.      ,...
 y: array([[[ 0.      , -0.      , -0.      , ..., -0.      ,  0.      ,
         -0.      ],
        [-0.      , -0.      ,  0.      , ..., -0.      ,  0.      ,...
2025-04-21 19:47:10.278802 test begin: paddle.linalg.norm(Tensor([2, 107374183, 4, 5],"float16"), 2.0, 1, False, )

[accuracy error] paddle.linalg.norm(Tensor([2, 107374183, 4, 5],"float16"), 2.0, 1, False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 40 / 40 (100%)
Max absolute difference: 2970.
Max relative difference: 0.9927
 x: array([[[22.62, 22.62, 22.62, 22.62, 22.62],
        [22.62, 22.62, 22.62, 22.62, 22.62],
        [22.62, 22.62, 22.62, 22.62, 22.62],...
 y: array([[[2992., 2992., 2992., 2992., 2992.],
        [2992., 2992., 2992., 2992., 2992.],
        [2992., 2992., 2992., 2992., 2992.],...
2025-04-21 19:49:47.026415 test begin: paddle.linalg.norm(Tensor([2281701379],"float32"), )

[accuracy error] paddle.linalg.norm(Tensor([2281701379],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 13789.02
Max relative difference: 1.
 x: array(0., dtype=float32)
 y: array(13789.02, dtype=float32)
2025-04-21 19:49:59.910462 test begin: paddle.linalg.norm(Tensor([2281701379],"float32"), 2.0, )

[accuracy error] paddle.linalg.norm(Tensor([2281701379],"float32"), 2.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 13789.02
Max relative difference: 1.
 x: array(0., dtype=float32)
 y: array(13789.02, dtype=float32)
2025-04-21 19:50:11.652925 test begin: paddle.linalg.norm(Tensor([2281701379],"float32"), p=1, )

[accuracy error] paddle.linalg.norm(Tensor([2281701379],"float32"), p=1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 5.704175e+08
Max relative difference: 1.
 x: array(0., dtype=float32)
 y: array(5.704175e+08, dtype=float32)
2025-04-21 19:50:23.276802 test begin: paddle.linalg.norm(Tensor([2281701379],"float32"), p=1, axis=0, )

[accuracy error] paddle.linalg.norm(Tensor([2281701379],"float32"), p=1, axis=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 5.704175e+08
Max relative difference: 1.
 x: array(0., dtype=float32)
 y: array(5.704175e+08, dtype=float32)
2025-04-21 19:50:34.515943 test begin: paddle.linalg.norm(Tensor([3, 20, 38028357],"float32"), 0.0, 2, True, )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[accuracy error] paddle.linalg.norm(Tensor([3, 20, 38028357],"float32"), 0.0, 2, True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 60 / 60 (100%)
Max absolute difference: 21251140.
Max relative difference: 0.5588235
 x: array([[[16777216.],
        [16777216.],
        [16777216.],...
 y: array([[[38028356.],
        [38028356.],
        [38028356.],...
2025-04-21 19:50:41.995445 test begin: paddle.linalg.norm(Tensor([3, 20, 38028357],"float32"), 2.0, -1, False, )

[accuracy error] paddle.linalg.norm(Tensor([3, 20, 38028357],"float32"), 2.0, -1, False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 60 / 60 (100%)
Max absolute difference: 53.951782
Max relative difference: 0.03031008
 x: array([[1726.2482, 1726.42  , 1726.3269, 1726.2399, 1726.2408, 1726.4213,
        1726.4645, 1726.5237, 1726.288 , 1726.6862, 1726.5533, 1726.4536,
        1726.7684, 1726.3458, 1726.304 , 1726.5193, 1726.3817, 1726.3425,...
 y: array([[1780.0693, 1780.0961, 1780.1256, 1780.0558, 1780.0568, 1780.0627,
        1780.1597, 1780.3545, 1780.0558, 1780.4286, 1780.2277, 1780.1852,
        1780.3997, 1780.0398, 1780.0693, 1780.1804, 1780.0558, 1780.2334,...
2025-04-21 19:51:22.133180 test begin: paddle.linalg.norm(Tensor([3, 20, 38028357],"float32"), 2.0, 2, True, )

[accuracy error] paddle.linalg.norm(Tensor([3, 20, 38028357],"float32"), 2.0, 2, True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 60 / 60 (100%)
Max absolute difference: 53.951782
Max relative difference: 0.03031008
 x: array([[[1726.2482],
        [1726.42  ],
        [1726.3269],...
 y: array([[[1780.0693],
        [1780.0961],
        [1780.1256],...
2025-04-21 19:52:05.368326 test begin: paddle.linalg.norm(Tensor([3, 20, 38028357],"float32"), math.inf, 2, True, )

[accuracy error] backward  paddle.linalg.norm(Tensor([3, 20, 38028357],"float32"), math.inf, 2, True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 97 / 2281701420 (4.25e-06%)
Max absolute difference: 0.33724284
Max relative difference: 4.
 x: array([[[-0.,  0.,  0., ..., -0., -0., -0.],
        [-0.,  0., -0., ...,  0.,  0.,  0.],
        [ 0., -0.,  0., ..., -0., -0., -0.],...
 y: array([[[-0.,  0.,  0., ..., -0., -0., -0.],
        [-0.,  0., -0., ...,  0.,  0.,  0.],
        [ 0., -0.,  0., ..., -0., -0., -0.],...
2025-04-21 19:53:51.806140 test begin: paddle.linalg.norm(Tensor([3, 20, 71582789],"float32"), 0.0, 2, True, )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[accuracy error] paddle.linalg.norm(Tensor([3, 20, 71582789],"float32"), 0.0, 2, True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 60 / 60 (100%)
Max absolute difference: 54805568.
Max relative difference: 0.765625
 x: array([[[16777216.],
        [16777216.],
        [16777216.],...
 y: array([[[71582784.],
        [71582784.],
        [71582784.],...
2025-04-21 19:54:10.350203 test begin: paddle.linalg.norm(Tensor([3, 20, 71582789],"float32"), 2.0, -1, False, )

[accuracy error] paddle.linalg.norm(Tensor([3, 20, 71582789],"float32"), 2.0, -1, False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 60 / 60 (100%)
Max absolute difference: 394.73145
Max relative difference: 0.16159429
 x: array([[2048., 2048., 2048., 2048., 2048., 2048., 2048., 2048., 2048.,
        2048., 2048., 2048., 2048., 2048., 2048., 2048., 2048., 2048.,
        2048., 2048.],...
 y: array([[2442.23  , 2442.3372, 2442.2776, 2442.3115, 2442.4773, 2442.474 ,
        2442.4656, 2442.2659, 2442.3284, 2442.3928, 2442.3472, 2442.3687,
        2442.2444, 2442.2073, 2442.4346, 2442.4521, 2442.261 , 2442.451 ,...
2025-04-21 19:55:31.496646 test begin: paddle.linalg.norm(Tensor([3, 20, 71582789],"float32"), 2.0, 2, True, )

[accuracy error] paddle.linalg.norm(Tensor([3, 20, 71582789],"float32"), 2.0, 2, True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 60 / 60 (100%)
Max absolute difference: 394.73145
Max relative difference: 0.16159429
 x: array([[[2048.],
        [2048.],
        [2048.],...
 y: array([[[2442.23  ],
        [2442.3372],
        [2442.2776],...
2025-04-21 19:56:48.897172 test begin: paddle.linalg.norm(Tensor([3, 253522376, 3],"float32"), -math.inf, 2, True, )

[accuracy error] backward  paddle.linalg.norm(Tensor([3, 253522376, 3],"float32"), -math.inf, 2, True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 74 / 2281701384 (3.24e-06%)
Max absolute difference: 0.2468179
Max relative difference: 1.
 x: array([[[-0.      ,  0.      ,  0.26595 ],
        [-0.      ,  0.428637,  0.      ],
        [-0.      , -0.484744, -0.      ],...
 y: array([[[-0.      ,  0.      ,  0.26595 ],
        [-0.      ,  0.428637,  0.      ],
        [-0.      , -0.484744, -0.      ],...
2025-04-21 19:59:31.489748 test begin: paddle.linalg.norm(Tensor([3, 253522376, 3],"float32"), math.inf, 2, True, )

[accuracy error] backward  paddle.linalg.norm(Tensor([3, 253522376, 3],"float32"), math.inf, 2, True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 80 / 2281701384 (3.51e-06%)
Max absolute difference: 0.24315059
Max relative difference: 1.
 x: array([[[-0.      ,  0.26595 ,  0.      ],
        [-0.428637,  0.      ,  0.      ],
        [-0.484744, -0.      , -0.      ],...
 y: array([[[-0.      ,  0.26595 ,  0.      ],
        [-0.428637,  0.      ,  0.      ],
        [-0.484744, -0.      , -0.      ],...
2025-04-21 20:02:13.438922 test begin: paddle.linalg.norm(Tensor([380283564, 6],"float32"), )

[accuracy error] paddle.linalg.norm(Tensor([380283564, 6],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 13789.02
Max relative difference: 1.
 x: array(0., dtype=float32)
 y: array(13789.02, dtype=float32)
2025-04-21 20:02:26.898994 test begin: paddle.linalg.norm(Tensor([380283564, 6],"float32"), p=1, axis=list[0,1,], )

[accuracy error] paddle.linalg.norm(Tensor([380283564, 6],"float32"), p=1, axis=list[0,1,], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 86684056.
Max relative difference: 0.91176635
 x: array(8388607.5, dtype=float32)
 y: array(95072664., dtype=float32)
2025-04-21 20:05:51.483124 test begin: paddle.linalg.norm(Tensor([38028357, 20, 3],"float32"), -math.inf, 2, True, )

[accuracy error] backward  paddle.linalg.norm(Tensor([38028357, 20, 3],"float32"), -math.inf, 2, True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 74 / 2281701420 (3.24e-06%)
Max absolute difference: 0.24616742
Max relative difference: 1.
 x: array([[[-0.      ,  0.      ,  0.091619],
        [-0.      ,  0.17039 ,  0.      ],
        [-0.      , -0.255555, -0.      ],...
 y: array([[[-0.      ,  0.      ,  0.091619],
        [-0.      ,  0.17039 ,  0.      ],
        [-0.      , -0.255555, -0.      ],...
2025-04-21 20:08:31.817883 test begin: paddle.linalg.norm(Tensor([38028357, 20, 3],"float32"), math.inf, 2, True, )

[accuracy error] backward  paddle.linalg.norm(Tensor([38028357, 20, 3],"float32"), math.inf, 2, True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 86 / 2281701420 (3.77e-06%)
Max absolute difference: 0.24684945
Max relative difference: 1.
 x: array([[[-0.      ,  0.091619,  0.      ],
        [-0.17039 ,  0.      ,  0.      ],
        [-0.255555, -0.      , -0.      ],...
 y: array([[[-0.      ,  0.091619,  0.      ],
        [-0.17039 ,  0.      ,  0.      ],
        [-0.255555, -0.      , -0.      ],...
2025-04-21 20:11:02.939464 test begin: paddle.linalg.norm(Tensor([4, 1073741824],"float32"), )

[accuracy error] paddle.linalg.norm(Tensor([4, 1073741824],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 18918.486
Max relative difference: 1.
 x: array(0., dtype=float32)
 y: array(18918.486, dtype=float32)
2025-04-21 20:11:27.725627 test begin: paddle.linalg.norm(Tensor([4, 1073741824],"float32"), p=-math.inf, axis=list[0,1,], )

[accuracy error] paddle.linalg.norm(Tensor([4, 1073741824],"float32"), p=-math.inf, axis=list[0,1,], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 2.6004246e+08
Max relative difference: 0.96874946
 x: array(8388607.5, dtype=float32)
 y: array(2.684311e+08, dtype=float32)
2025-04-21 20:18:16.654183 test begin: paddle.linalg.norm(Tensor([4, 570425345],"float32"), )

[accuracy error] paddle.linalg.norm(Tensor([4, 570425345],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 13789.02
Max relative difference: 1.
 x: array(0., dtype=float32)
 y: array(13789.02, dtype=float32)
2025-04-21 20:18:32.456830 test begin: paddle.linalg.norm(Tensor([4, 570425345],"float32"), p=-math.inf, axis=list[0,1,], )

[accuracy error] paddle.linalg.norm(Tensor([4, 570425345],"float32"), p=-math.inf, axis=list[0,1,], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 1.3421342e+08
Max relative difference: 0.9411747
 x: array(8388607.5, dtype=float32)
 y: array(1.42602e+08, dtype=float32)
2025-04-21 20:22:55.590519 test begin: paddle.linalg.norm(Tensor([4294967295],"float32"), )

[accuracy error] paddle.linalg.norm(Tensor([4294967295],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 18918.486
Max relative difference: 1.
 x: array(0., dtype=float32)
 y: array(18918.486, dtype=float32)
2025-04-21 20:23:20.595366 test begin: paddle.linalg.norm(Tensor([4294967295],"float32"), 2.0, )

[accuracy error] paddle.linalg.norm(Tensor([4294967295],"float32"), 2.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 18918.486
Max relative difference: 1.
 x: array(0., dtype=float32)
 y: array(18918.486, dtype=float32)
2025-04-21 20:23:43.059207 test begin: paddle.linalg.norm(Tensor([4294967295],"float32"), p=1, )

[accuracy error] paddle.linalg.norm(Tensor([4294967295],"float32"), p=1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 1.0737356e+09
Max relative difference: 1.
 x: array(0., dtype=float32)
 y: array(1.073736e+09, dtype=float32)
2025-04-21 20:24:05.398540 test begin: paddle.linalg.norm(Tensor([4294967295],"float32"), p=1, axis=0, )

[accuracy error] paddle.linalg.norm(Tensor([4294967295],"float32"), p=1, axis=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 1.0737356e+09
Max relative difference: 1.
 x: array(0., dtype=float32)
 y: array(1.073736e+09, dtype=float32)
2025-04-21 20:27:29.672624 test begin: paddle.linalg.norm(Tensor([50, 50, 1717987],"float16"), p=2.0, axis=-1, )

[accuracy error] paddle.linalg.norm(Tensor([50, 50, 1717987],"float16"), p=2.0, axis=-1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2500 / 2500 (100%)
Max absolute difference: 356.
Max relative difference: 0.9404
 x: array([[22.62, 22.62, 22.62, ..., 22.62, 22.62, 22.62],
       [22.62, 22.62, 22.62, ..., 22.62, 22.62, 22.62],
       [22.62, 22.62, 22.62, ..., 22.62, 22.62, 22.62],...
 y: array([[378.2, 378.2, 378.2, ..., 378.2, 378.8, 378.5],
       [378.2, 378.5, 378.5, ..., 378.5, 378.2, 378.5],
       [378.5, 378.5, 378.8, ..., 378.2, 378.5, 378.2],...
2025-04-21 20:27:45.450861 test begin: paddle.linalg.norm(Tensor([715827883, 6],"float32"), )

[accuracy error] paddle.linalg.norm(Tensor([715827883, 6],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 18918.139
Max relative difference: 0.99998164
 x: array(0.348333, dtype=float32)
 y: array(18918.486, dtype=float32)
2025-04-21 20:28:11.876003 test begin: paddle.linalg.norm(Tensor([715827883, 6],"float32"), p=1, axis=list[0,1,], )

[accuracy error] paddle.linalg.norm(Tensor([715827883, 6],"float32"), p=1, axis=list[0,1,], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 1.705716e+08
Max relative difference: 0.95312583
 x: array(8388607.5, dtype=float32)
 y: array(1.789602e+08, dtype=float32)
2025-04-21 20:35:55.146293 test begin: paddle.linalg.norm(x=Tensor([1431655766, 3],"float16"), axis=None, p="fro", )

[accuracy error] paddle.linalg.norm(x=Tensor([1431655766, 3],"float16"), axis=None, p="fro", ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 18912.
Max relative difference: 1.
 x: array(0.342, dtype=float16)
 y: array(18912., dtype=float16)
2025-04-21 20:36:09.304419 test begin: paddle.linalg.norm(x=Tensor([190141782, 3, 4],"float32"), )

[accuracy error] paddle.linalg.norm(x=Tensor([190141782, 3, 4],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 13789.02
Max relative difference: 1.
 x: array(0., dtype=float32)
 y: array(13789.02, dtype=float32)
2025-04-21 20:36:23.052222 test begin: paddle.linalg.norm(x=Tensor([190141782, 3, 4],"float32"), p=math.inf, axis=0, keepdim=False, )

[accuracy error] backward  paddle.linalg.norm(x=Tensor([190141782, 3, 4],"float32"), p=math.inf, axis=0, keepdim=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 65 / 2281701384 (2.85e-06%)
Max absolute difference: 0.40905586
Max relative difference: 8.
 x: array([[[ 0., -0., -0., -0.],
        [ 0.,  0.,  0., -0.],
        [-0.,  0., -0., -0.]],...
 y: array([[[ 0., -0., -0., -0.],
        [ 0.,  0.,  0., -0.],
        [-0.,  0., -0., -0.]],...
2025-04-21 20:38:04.567523 test begin: paddle.linalg.norm(x=Tensor([190141782, 3, 4],"float32"), p=math.inf, axis=0, keepdim=True, )

[accuracy error] backward  paddle.linalg.norm(x=Tensor([190141782, 3, 4],"float32"), p=math.inf, axis=0, keepdim=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 65 / 2281701384 (2.85e-06%)
Max absolute difference: 0.40905586
Max relative difference: 8.
 x: array([[[ 0., -0., -0., -0.],
        [ 0.,  0.,  0., -0.],
        [-0.,  0., -0., -0.]],...
 y: array([[[ 0., -0., -0., -0.],
        [ 0.,  0.,  0., -0.],
        [-0.,  0., -0., -0.]],...
2025-04-21 20:39:59.311425 test begin: paddle.linalg.norm(x=Tensor([2, 285212673, 4],"float32"), )

[accuracy error] paddle.linalg.norm(x=Tensor([2, 285212673, 4],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 13789.02
Max relative difference: 1.
 x: array(0., dtype=float32)
 y: array(13789.02, dtype=float32)
2025-04-21 20:40:13.177589 test begin: paddle.linalg.norm(x=Tensor([2, 285212673, 4],"float32"), p=math.inf, axis=0, keepdim=False, )

[accuracy error] backward  paddle.linalg.norm(x=Tensor([2, 285212673, 4],"float32"), p=math.inf, axis=0, keepdim=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 94 / 2281701384 (4.12e-06%)
Max absolute difference: 0.24429742
Max relative difference: 1.
 x: array([[[ 0.      , -0.431667, -0.      , -0.205909],
        [-0.      , -0.109781,  0.      , -0.      ],
        [-0.      , -0.      ,  0.      ,  0.357183],...
 y: array([[[ 0.      , -0.431667, -0.      , -0.205909],
        [-0.      , -0.109781,  0.      , -0.      ],
        [-0.      , -0.      ,  0.      ,  0.357183],...
2025-04-21 20:43:02.122935 test begin: paddle.linalg.norm(x=Tensor([2, 285212673, 4],"float32"), p=math.inf, axis=0, keepdim=True, )

[accuracy error] backward  paddle.linalg.norm(x=Tensor([2, 285212673, 4],"float32"), p=math.inf, axis=0, keepdim=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 94 / 2281701384 (4.12e-06%)
Max absolute difference: 0.24429742
Max relative difference: 1.
 x: array([[[ 0.      , -0.431667, -0.      , -0.205909],
        [-0.      , -0.109781,  0.      , -0.      ],
        [-0.      , -0.      ,  0.      ,  0.357183],...
 y: array([[[ 0.      , -0.431667, -0.      , -0.205909],
        [-0.      , -0.109781,  0.      , -0.      ],
        [-0.      , -0.      ,  0.      ,  0.357183],...
2025-04-21 20:46:04.422779 test begin: paddle.linalg.norm(x=Tensor([2, 3, 380283564],"float32"), )

[accuracy error] paddle.linalg.norm(x=Tensor([2, 3, 380283564],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 13789.02
Max relative difference: 1.
 x: array(0., dtype=float32)
 y: array(13789.02, dtype=float32)
2025-04-21 20:46:17.638855 test begin: paddle.linalg.norm(x=Tensor([2, 3, 380283564],"float32"), p=math.inf, axis=0, keepdim=False, )

[accuracy error] backward  paddle.linalg.norm(x=Tensor([2, 3, 380283564],"float32"), p=math.inf, axis=0, keepdim=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 94 / 2281701384 (4.12e-06%)
Max absolute difference: 0.24429742
Max relative difference: 1.
 x: array([[[ 0.      , -0.431667, -0.      , ..., -0.      ,  0.053694,
          0.      ],
        [ 0.26595 ,  0.      ,  0.      , ...,  0.      ,  0.      ,...
 y: array([[[ 0.      , -0.431667, -0.      , ..., -0.      ,  0.053694,
          0.      ],
        [ 0.26595 ,  0.      ,  0.      , ...,  0.      ,  0.      ,...
2025-04-21 20:49:32.310743 test begin: paddle.linalg.norm(x=Tensor([2, 3, 380283564],"float32"), p=math.inf, axis=0, keepdim=True, )

[accuracy error] backward  paddle.linalg.norm(x=Tensor([2, 3, 380283564],"float32"), p=math.inf, axis=0, keepdim=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 94 / 2281701384 (4.12e-06%)
Max absolute difference: 0.24429742
Max relative difference: 1.
 x: array([[[ 0.      , -0.431667, -0.      , ..., -0.      ,  0.053694,
          0.      ],
        [ 0.26595 ,  0.      ,  0.      , ...,  0.      ,  0.      ,...
 y: array([[[ 0.      , -0.431667, -0.      , ..., -0.      ,  0.053694,
          0.      ],
        [ 0.26595 ,  0.      ,  0.      , ...,  0.      ,  0.      ,...
2025-04-21 20:53:17.160233 test begin: paddle.linalg.norm(x=Tensor([2, 3, 715827883],"float16"), )

[accuracy error] paddle.linalg.norm(x=Tensor([2, 3, 715827883],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 18912.
Max relative difference: 1.
 x: array(0.342, dtype=float16)
 y: array(18912., dtype=float16)
2025-04-21 20:53:31.046973 test begin: paddle.linalg.norm(x=Tensor([2, 3, 715827883],"float16"), p=1, axis=list[0,1,], keepdim=False, )

[accuracy error] paddle.linalg.norm(x=Tensor([2, 3, 715827883],"float16"), p=1, axis=list[0,1,], keepdim=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 715827883 / 715827883 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float16)
 y: array([0.6426, 0.584 , 0.6553, ..., 0.7534, 0.7725, 0.902 ], dtype=float16)
2025-04-21 20:55:41.567642 test begin: paddle.linalg.norm(x=Tensor([2, 3, 715827883],"float16"), p=1, axis=list[0,1,], keepdim=True, )

[accuracy error] paddle.linalg.norm(x=Tensor([2, 3, 715827883],"float16"), p=1, axis=list[0,1,], keepdim=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 715827883 / 715827883 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0., 0., 0., ..., 0., 0., 0.]]], dtype=float16)
 y: array([[[0.6426, 0.584 , 0.6553, ..., 0.7534, 0.7725, 0.902 ]]],
      dtype=float16)
2025-04-21 20:57:56.043825 test begin: paddle.linalg.norm(x=Tensor([2, 3, 715827883],"float16"), p=2, axis=-1, keepdim=False, )

[accuracy error] paddle.linalg.norm(x=Tensor([2, 3, 715827883],"float16"), p=2, axis=-1, keepdim=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 6 / 6 (100%)
Max absolute difference: 7700.
Max relative difference: 0.997
 x: array([[22.62, 22.62, 22.62],
       [22.62, 22.62, 22.62]], dtype=float16)
 y: array([[7724., 7724., 7724.],
       [7724., 7724., 7724.]], dtype=float16)
2025-04-21 21:03:18.141244 test begin: paddle.linalg.norm(x=Tensor([2, 3, 715827883],"float16"), p=2, axis=-1, keepdim=True, )

[accuracy error] paddle.linalg.norm(x=Tensor([2, 3, 715827883],"float16"), p=2, axis=-1, keepdim=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 6 / 6 (100%)
Max absolute difference: 7700.
Max relative difference: 0.997
 x: array([[[22.62],
        [22.62],
        [22.62]],...
 y: array([[[7724.],
        [7724.],
        [7724.]],...
2025-04-21 21:13:10.613819 test begin: paddle.linalg.norm(x=Tensor([2, 536870913, 4],"float16"), )

W0421 21:14:39.678812 13471 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 21:14:39.680009 13471 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.linalg.norm(x=Tensor([2, 536870913, 4],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 18912.
Max relative difference: 1.
 x: array(0.7183, dtype=float16)
 y: array(18912., dtype=float16)
2025-04-21 21:14:40.169310 test begin: paddle.linalg.norm(x=Tensor([2, 536870913, 4],"float16"), p=-math.inf, axis=list[0,1,], keepdim=False, )

[accuracy error] paddle.linalg.norm(x=Tensor([2, 536870913, 4],"float16"), p=-math.inf, axis=list[0,1,], keepdim=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([1024., 1024., 1024., 1024.], dtype=float16)
 y: array([inf, inf, inf, inf], dtype=float16)
2025-04-21 21:19:06.598254 test begin: paddle.linalg.norm(x=Tensor([2, 536870913, 4],"float16"), p=-math.inf, axis=list[0,1,], keepdim=True, )

[accuracy error] paddle.linalg.norm(x=Tensor([2, 536870913, 4],"float16"), p=-math.inf, axis=list[0,1,], keepdim=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[[1024., 1024., 1024., 1024.]]], dtype=float16)
 y: array([[[inf, inf, inf, inf]]], dtype=float16)
2025-04-21 21:23:35.360199 test begin: paddle.linalg.norm(x=Tensor([2, 536870913, 4],"float16"), p=1, axis=list[0,1,], keepdim=False, )

[accuracy error] paddle.linalg.norm(x=Tensor([2, 536870913, 4],"float16"), p=1, axis=list[0,1,], keepdim=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4 / 4 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([0., 0., 0., 0.], dtype=float16)
 y: array([1., 1., 1., 1.], dtype=float16)
2025-04-21 21:23:51.330406 test begin: paddle.linalg.norm(x=Tensor([2, 536870913, 4],"float16"), p=1, axis=list[0,1,], keepdim=True, )

[accuracy error] paddle.linalg.norm(x=Tensor([2, 536870913, 4],"float16"), p=1, axis=list[0,1,], keepdim=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4 / 4 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0., 0., 0., 0.]]], dtype=float16)
 y: array([[[1., 1., 1., 1.]]], dtype=float16)
2025-04-21 21:24:01.775113 test begin: paddle.linalg.norm(x=Tensor([2, 536870913, 4],"float16"), p=math.inf, axis=list[0,1,], keepdim=False, )

[accuracy error] paddle.linalg.norm(x=Tensor([2, 536870913, 4],"float16"), p=math.inf, axis=list[0,1,], keepdim=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([1024., 1024., 1024., 1024.], dtype=float16)
 y: array([inf, inf, inf, inf], dtype=float16)
2025-04-21 21:28:22.636700 test begin: paddle.linalg.norm(x=Tensor([2, 536870913, 4],"float16"), p=math.inf, axis=list[0,1,], keepdim=True, )

[accuracy error] paddle.linalg.norm(x=Tensor([2, 536870913, 4],"float16"), p=math.inf, axis=list[0,1,], keepdim=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[[1024., 1024., 1024., 1024.]]], dtype=float16)
 y: array([[[inf, inf, inf, inf]]], dtype=float16)
2025-04-21 21:32:49.462341 test begin: paddle.linalg.norm(x=Tensor([3, 1431655766],"float16"), axis=None, p="fro", )

[accuracy error] paddle.linalg.norm(x=Tensor([3, 1431655766],"float16"), axis=None, p="fro", ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 18912.
Max relative difference: 1.
 x: array(0.4714, dtype=float16)
 y: array(18912., dtype=float16)
2025-04-21 21:33:03.928977 test begin: paddle.linalg.norm(x=Tensor([3, 1431655766],"float16"), p=0, axis=1, keepdim=False, )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[accuracy error] paddle.linalg.norm(x=Tensor([3, 1431655766],"float16"), p=0, axis=1, keepdim=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([2048., 2048., 2048.], dtype=float16)
 y: array([inf, inf, inf], dtype=float16)
2025-04-21 21:34:07.480081 test begin: paddle.linalg.norm(x=Tensor([3, 1431655766],"float16"), p=0, axis=1, keepdim=True, )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[accuracy error] paddle.linalg.norm(x=Tensor([3, 1431655766],"float16"), p=0, axis=1, keepdim=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[2048.],
       [2048.],
       [2048.]], dtype=float16)
 y: array([[inf],
       [inf],
       [inf]], dtype=float16)
2025-04-21 21:35:17.852719 test begin: paddle.linalg.norm(x=Tensor([3, 1431655766],"float16"), p=1, axis=1, keepdim=False, )

[accuracy error] paddle.linalg.norm(x=Tensor([3, 1431655766],"float16"), p=1, axis=1, keepdim=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([1024., 1024., 1024.], dtype=float16)
 y: array([inf, inf, inf], dtype=float16)
2025-04-21 21:44:43.446825 test begin: paddle.linalg.norm(x=Tensor([3, 1431655766],"float16"), p=1, axis=1, keepdim=True, )

[accuracy error] paddle.linalg.norm(x=Tensor([3, 1431655766],"float16"), p=1, axis=1, keepdim=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[1024.],
       [1024.],
       [1024.]], dtype=float16)
 y: array([[inf],
       [inf],
       [inf]], dtype=float16)
2025-04-21 21:54:11.239927 test begin: paddle.linalg.norm(x=Tensor([3, 1431655766],"float16"), p=2, axis=1, keepdim=False, )

[accuracy error] paddle.linalg.norm(x=Tensor([3, 1431655766],"float16"), p=2, axis=1, keepdim=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 3 / 3 (100%)
Max absolute difference: 10896.
Max relative difference: 0.9976
 x: array([22.62, 22.62, 22.62], dtype=float16)
 y: array([10920., 10920., 10920.], dtype=float16)
2025-04-21 22:03:40.446887 test begin: paddle.linalg.norm(x=Tensor([3, 1431655766],"float16"), p=2, axis=1, keepdim=True, )

[accuracy error] paddle.linalg.norm(x=Tensor([3, 1431655766],"float16"), p=2, axis=1, keepdim=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 3 / 3 (100%)
Max absolute difference: 10896.
Max relative difference: 0.9976
 x: array([[22.62],
       [22.62],
       [22.62]], dtype=float16)
 y: array([[10920.],
       [10920.],
       [10920.]], dtype=float16)
2025-04-21 22:13:09.767425 test begin: paddle.linalg.norm(x=Tensor([3, 3, 477218589],"float16"), axis=None, )

[accuracy error] paddle.linalg.norm(x=Tensor([3, 3, 477218589],"float16"), axis=None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 18912.
Max relative difference: 1.
 x: array(0.4763, dtype=float16)
 y: array(18912., dtype=float16)
2025-04-21 22:13:21.086441 test begin: paddle.linalg.norm(x=Tensor([3, 3, 477218589],"float16"), axis=list[1,2,], p=math.inf, )

[accuracy error] paddle.linalg.norm(x=Tensor([3, 3, 477218589],"float16"), axis=list[1,2,], p=math.inf, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([1024., 1024., 1024.], dtype=float16)
 y: array([inf, inf, inf], dtype=float16)
2025-04-21 22:17:19.816179 test begin: paddle.linalg.norm(x=Tensor([3, 477218589, 3],"float16"), axis=1, p=0, )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[accuracy error] paddle.linalg.norm(x=Tensor([3, 477218589, 3],"float16"), axis=1, p=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[2048., 2048., 2048.],
       [2048., 2048., 2048.],
       [2048., 2048., 2048.]], dtype=float16)
 y: array([[inf, inf, inf],
       [inf, inf, inf],
       [inf, inf, inf]], dtype=float16)
2025-04-21 22:17:52.221671 test begin: paddle.linalg.norm(x=Tensor([3, 477218589, 3],"float16"), axis=None, )

[accuracy error] paddle.linalg.norm(x=Tensor([3, 477218589, 3],"float16"), axis=None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 18912.
Max relative difference: 1.
 x: array(0.4763, dtype=float16)
 y: array(18912., dtype=float16)
2025-04-21 22:18:03.252504 test begin: paddle.linalg.norm(x=Tensor([3, 477218589, 3],"float16"), axis=list[1,2,], p=1, )

[accuracy error] paddle.linalg.norm(x=Tensor([3, 477218589, 3],"float16"), axis=list[1,2,], p=1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([1024., 1024., 1024.], dtype=float16)
 y: array([inf, inf, inf], dtype=float16)
2025-04-21 22:21:56.840996 test begin: paddle.linalg.norm(x=Tensor([357913942, 3, 4],"float16"), )

[accuracy error] paddle.linalg.norm(x=Tensor([357913942, 3, 4],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 18912.
Max relative difference: 1.
 x: array(0.7183, dtype=float16)
 y: array(18912., dtype=float16)
2025-04-21 22:22:08.501337 test begin: paddle.linalg.norm(x=Tensor([357913942, 3, 4],"float16"), p=1, axis=list[0,1,], keepdim=False, )

[accuracy error] paddle.linalg.norm(x=Tensor([357913942, 3, 4],"float16"), p=1, axis=list[0,1,], keepdim=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([1024., 1024., 1024., 1024.], dtype=float16)
 y: array([inf, inf, inf, inf], dtype=float16)
2025-04-21 22:25:24.968346 test begin: paddle.linalg.norm(x=Tensor([357913942, 3, 4],"float16"), p=1, axis=list[0,1,], keepdim=True, )

[accuracy error] paddle.linalg.norm(x=Tensor([357913942, 3, 4],"float16"), p=1, axis=list[0,1,], keepdim=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[[1024., 1024., 1024., 1024.]]], dtype=float16)
 y: array([[[inf, inf, inf, inf]]], dtype=float16)
2025-04-21 22:28:45.723197 test begin: paddle.linalg.norm(x=Tensor([477218589, 3, 3],"float16"), axis=None, )

[accuracy error] paddle.linalg.norm(x=Tensor([477218589, 3, 3],"float16"), axis=None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 18912.
Max relative difference: 1.
 x: array(0.4763, dtype=float16)
 y: array(18912., dtype=float16)
2025-04-21 22:28:59.536759 test begin: paddle.linalg.norm(x=Tensor([477218589, 3, 3],"float16"), axis=list[0,2,], p=1, )

[accuracy error] paddle.linalg.norm(x=Tensor([477218589, 3, 3],"float16"), axis=list[0,2,], p=1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([1024., 1024., 1024.], dtype=float16)
 y: array([inf, inf, inf], dtype=float16)
2025-04-21 22:32:56.298714 test begin: paddle.linalg.solve(Tensor([10, 10],"float32"), Tensor([10, 228170138],"float32"), )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_solve(_object*, _object*, _object*)
1   solve_ad_func(paddle::Tensor const&, paddle::Tensor const&)
2   paddle::experimental::solve(paddle::Tensor const&, paddle::Tensor const&)
3   void phi::SolveKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*)
4   phi::funcs::MatrixSolveFunctor<phi::GPUContext, float>::operator()(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*)
5   phi::funcs::TransposeNormal<phi::GPUContext, float>::operator()(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, std::vector<int, std::allocator<int> > const&)
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745246055 (unix time) try "date -d @1745246055" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x342d) received by PID 13357 (TID 0x7f5ef6949700) from PID 13357 ***]

2025-04-21 22:34:21.440499 test begin: paddle.linalg.vector_norm(x=Tensor([1073741824, 4],"float32"), p=2, axis=None, keepdim=True, )

W0421 22:35:46.958079 94706 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 22:35:46.959255 94706 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.linalg.vector_norm(x=Tensor([1073741824, 4],"float32"), p=2, axis=None, keepdim=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 18918.467
Max relative difference: 1.
 x: array([[0.]], dtype=float32)
 y: array([[18918.467]], dtype=float32)
2025-04-21 22:42:34.628157 test begin: paddle.linalg.vector_norm(x=Tensor([3, 1431655765],"float32"), p=2, axis=None, keepdim=False, )

W0421 22:43:58.845544 103514 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 22:43:58.846694 103514 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.linalg.vector_norm(x=Tensor([3, 1431655765],"float32"), p=2, axis=None, keepdim=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 18918.557
Max relative difference: 1.
 x: array(0., dtype=float32)
 y: array(18918.557, dtype=float32)
2025-04-21 22:43:59.711486 test begin: paddle.linalg.vector_norm(x=Tensor([3, 1431655765],"float32"), p=2, axis=None, keepdim=True, )

[accuracy error] paddle.linalg.vector_norm(x=Tensor([3, 1431655765],"float32"), p=2, axis=None, keepdim=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 18918.557
Max relative difference: 1.
 x: array([[0.]], dtype=float32)
 y: array([[18918.557]], dtype=float32)
2025-04-21 22:44:22.353190 test begin: paddle.linalg.vector_norm(x=Tensor([3, 715827883],"float64"), p=0, axis=None, keepdim=False, )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[accuracy error] paddle.linalg.vector_norm(x=Tensor([3, 715827883],"float64"), p=0, axis=None, keepdim=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 2.14748365e+09
Max relative difference: 1.
 x: array(0.)
 y: array(2.147484e+09)
2025-04-21 22:45:34.641334 test begin: paddle.linalg.vector_norm(x=Tensor([3, 715827883],"float64"), p=0, axis=None, keepdim=True, )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[accuracy error] paddle.linalg.vector_norm(x=Tensor([3, 715827883],"float64"), p=0, axis=None, keepdim=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 2.14748365e+09
Max relative difference: 1.
 x: array([[0.]])
 y: array([[2.147484e+09]])
2025-04-21 22:47:14.130276 test begin: paddle.linalg.vector_norm(x=Tensor([536870913, 4],"float64"), p=0, axis=None, keepdim=False, )

W0421 22:48:45.148648 108409 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 22:48:45.149708 108409 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[accuracy error] paddle.linalg.vector_norm(x=Tensor([536870913, 4],"float64"), p=0, axis=None, keepdim=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 2.14748365e+09
Max relative difference: 1.
 x: array(0.)
 y: array(2.147484e+09)
2025-04-21 22:48:45.183403 test begin: paddle.linalg.vector_norm(x=Tensor([536870913, 4],"float64"), p=0, axis=None, keepdim=True, )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[accuracy error] paddle.linalg.vector_norm(x=Tensor([536870913, 4],"float64"), p=0, axis=None, keepdim=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 2.14748365e+09
Max relative difference: 1.
 x: array([[0.]])
 y: array([[2.147484e+09]])
2025-04-21 22:48:54.889427 test begin: paddle.logical_or(Tensor([1],"bool"), Tensor([4294967295],"bool"), )

[paddle error] paddle.logical_or(Tensor([1],"bool"), Tensor([4294967295],"bool"), ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 22:49:58.573086 test begin: paddle.logical_or(Tensor([4294967295],"bool"), Tensor([1],"bool"), )

[paddle error] paddle.logical_or(Tensor([4294967295],"bool"), Tensor([1],"bool"), ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 22:50:03.686030 test begin: paddle.logsumexp(Tensor([2, 107374183, 4, 5],"float16"), list[-1,], False, )

[accuracy error] paddle.logsumexp(Tensor([2, 107374183, 4, 5],"float16"), list[-1,], False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 858791332 / 858993464 (100%)
Max absolute difference: 2.104
Max relative difference: 1.
 x: array([[[1.56 , 1.435, 1.64 , 1.687],
        [1.693, 1.566, 1.567, 1.709],
        [1.8  , 1.673, 1.826, 1.692],...
 y: array([[[1.493 , 1.73  , 1.658 , 1.611 ],
        [1.609 , 1.588 , 1.435 , 1.806 ],
        [1.596 , 1.335 , 1.82  , 1.613 ],...
2025-04-21 22:53:59.038657 test begin: paddle.logsumexp(Tensor([2, 107374183, 4, 5],"float32"), list[-1,], False, )

[accuracy error] paddle.logsumexp(Tensor([2, 107374183, 4, 5],"float32"), list[-1,], False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 858791187 / 858993464 (100%)
Max absolute difference: 2.1022131
Max relative difference: 1.
 x: array([[[1.668956, 1.360504, 1.659272, 1.837293],
        [1.634698, 1.62027 , 1.826287, 1.924385],
        [1.482853, 1.80405 , 1.456157, 1.608512],...
 y: array([[[1.594192, 1.821256, 1.530687, 1.41469 ],
        [1.629744, 1.720242, 1.704597, 1.771286],
        [1.675571, 1.780023, 1.538849, 1.538507],...
2025-04-21 22:56:15.343986 test begin: paddle.logsumexp(Tensor([2, 3, 143165577, 5],"float16"), list[-1,], False, )

[accuracy error] paddle.logsumexp(Tensor([2, 3, 143165577, 5],"float16"), list[-1,], False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 858791329 / 858993462 (100%)
Max absolute difference: 2.104
Max relative difference: 1.
 x: array([[[1.56 , 1.435, 1.64 , ..., 0.   , 0.   , 0.   ],
        [0.   , 0.   , 0.   , ..., 0.   , 0.   , 0.   ],
        [0.   , 0.   , 0.   , ..., 0.   , 0.   , 0.   ]],...
 y: array([[[1.493 , 1.73  , 1.658 , ..., 1.714 , 1.668 , 1.702 ],
        [1.325 , 1.628 , 1.668 , ..., 1.607 , 1.641 , 1.54  ],
        [1.568 , 1.379 , 1.482 , ..., 1.639 , 1.8545, 1.412 ]],...
2025-04-21 22:58:58.274730 test begin: paddle.logsumexp(Tensor([2, 3, 143165577, 5],"float32"), list[-1,], False, )

[accuracy error] paddle.logsumexp(Tensor([2, 3, 143165577, 5],"float32"), list[-1,], False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 858791185 / 858993462 (100%)
Max absolute difference: 2.1022131
Max relative difference: 1.
 x: array([[[1.668956, 1.360504, 1.659272, ..., 0.      , 0.      ,
         0.      ],
        [0.      , 0.      , 0.      , ..., 0.      , 0.      ,...
 y: array([[[1.594192, 1.821256, 1.530687, ..., 1.748791, 1.651627,
         1.693895],
        [1.603388, 1.678923, 1.777451, ..., 1.841437, 1.640115,...
2025-04-21 23:00:18.000359 test begin: paddle.logsumexp(Tensor([2, 3, 143165577, 5],"float32"), tuple(0,1,-1,), False, )

[accuracy error] paddle.logsumexp(Tensor([2, 3, 143165577, 5],"float32"), tuple(0,1,-1,), False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 142965869 / 143165577 (99.9%)
Max absolute difference: 3.7105129
Max relative difference: 1.
 x: array([3.420462, 3.441093, 3.442717, ..., 0.      , 0.      , 0.      ],
      dtype=float32)
 y: array([3.383597, 3.406723, 3.481938, ..., 3.551963, 3.467984, 3.445696],
      dtype=float32)
2025-04-21 23:00:49.199358 test begin: paddle.logsumexp(Tensor([2, 3, 4, 178956971],"float32"), list[2,-3,], False, )

[accuracy error] paddle.logsumexp(Tensor([2, 3, 4, 178956971],"float32"), list[2,-3,], False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 357497590 / 357913942 (99.9%)
Max absolute difference: 2.8996322
Max relative difference: 1.
 x: array([[2.384959, 2.550623, 2.509576, ..., 0.      , 0.      , 0.      ],
       [0.      , 0.      , 0.      , ..., 0.      , 0.      , 0.      ]],
      dtype=float32)
 y: array([[2.528863, 2.57121 , 2.496988, ..., 2.639815, 2.53967 , 2.581868],
       [2.627599, 2.442341, 2.604893, ..., 2.502093, 2.645042, 2.538561]],
      dtype=float32)
2025-04-21 23:01:27.874323 test begin: paddle.logsumexp(Tensor([2, 3, 71582789, 5],"float64"), list[-1,], False, )

[accuracy error] paddle.logsumexp(Tensor([2, 3, 71582789, 5],"float64"), list[-1,], False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 429294792 / 429496734 (100%)
Max absolute difference: 2.100296
Max relative difference: 1.
 x: array([[[1.573683, 1.829085, 1.747226, ..., 0.      , 0.      ,
         0.      ],
        [0.      , 0.      , 0.      , ..., 0.      , 0.      ,...
 y: array([[[1.776291, 1.761332, 1.685537, ..., 1.544433, 1.703657,
         1.402488],
        [1.60317 , 1.681068, 1.640936, ..., 1.690141, 1.671519,...
2025-04-21 23:02:35.183769 test begin: paddle.logsumexp(Tensor([2, 3, 71582789, 5],"float64"), list[-1,], True, )

[accuracy error] paddle.logsumexp(Tensor([2, 3, 71582789, 5],"float64"), list[-1,], True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 429294792 / 429496734 (100%)
Max absolute difference: 2.100296
Max relative difference: 1.
 x: array([[[[1.573683],
         [1.829085],
         [1.747226],...
 y: array([[[[1.776291],
         [1.761332],
         [1.685537],...
2025-04-21 23:03:40.818066 test begin: paddle.logsumexp(Tensor([2, 3, 71582789, 5],"float64"), list[0,-1,], False, )

[accuracy error] paddle.logsumexp(Tensor([2, 3, 71582789, 5],"float64"), list[0,-1,], False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 214567515 / 214748367 (99.9%)
Max absolute difference: 2.73761145
Max relative difference: 1.
 x: array([[2.345486, 2.343599, 2.351561, ..., 0.      , 0.      , 0.      ],
       [0.      , 0.      , 0.      , ..., 0.      , 0.      , 0.      ],
       [0.      , 0.      , 0.      , ..., 0.      , 0.      , 0.      ]])
 y: array([[2.429776, 2.373013, 2.337959, ..., 2.267172, 2.359634, 2.305478],
       [2.373121, 2.379282, 2.401883, ..., 2.429292, 2.370679, 2.221805],
       [2.263713, 2.344402, 2.35349 , ..., 2.460474, 2.437306, 2.420943]])
2025-04-21 23:04:21.342578 test begin: paddle.logsumexp(Tensor([2, 53687092, 4, 5],"float64"), list[-1,], False, )

[accuracy error] paddle.logsumexp(Tensor([2, 53687092, 4, 5],"float64"), list[-1,], False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 429294794 / 429496736 (100%)
Max absolute difference: 2.100296
Max relative difference: 1.
 x: array([[[1.573683, 1.829085, 1.747226, 1.732767],
        [1.549212, 1.755975, 1.731023, 1.673523],
        [1.655974, 1.706696, 1.645537, 1.694804],...
 y: array([[[1.776291, 1.761332, 1.685537, 1.737196],
        [1.780981, 1.752415, 1.645354, 1.68119 ],
        [1.392544, 1.56884 , 1.375748, 1.663556],...
2025-04-21 23:05:20.207674 test begin: paddle.logsumexp(Tensor([2, 53687092, 4, 5],"float64"), list[-1,], True, )

[accuracy error] paddle.logsumexp(Tensor([2, 53687092, 4, 5],"float64"), list[-1,], True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 429294794 / 429496736 (100%)
Max absolute difference: 2.100296
Max relative difference: 1.
 x: array([[[[1.573683],
         [1.829085],
         [1.747226],...
 y: array([[[[1.776291],
         [1.761332],
         [1.685537],...
2025-04-21 23:06:23.645449 test begin: paddle.logsumexp(Tensor([2, 53687092, 4, 5],"float64"), list[0,-1,], False, )

[accuracy error] paddle.logsumexp(Tensor([2, 53687092, 4, 5],"float64"), list[0,-1,], False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 214386490 / 214748368 (99.8%)
Max absolute difference: 2.73265573
Max relative difference: 1.
 x: array([[2.251353, 2.377038, 2.412135, 2.253801],
       [2.428332, 2.357042, 2.492974, 2.286474],
       [2.29807 , 2.185775, 2.343405, 2.359557],...
 y: array([[2.381154, 2.378148, 2.374794, 2.469039],
       [2.47994 , 2.474333, 2.333735, 2.394823],
       [2.166158, 2.321694, 2.12563 , 2.284645],...
2025-04-21 23:07:08.645840 test begin: paddle.logsumexp(Tensor([35791395, 3, 4, 5],"float64"), list[-1,], False, )

[accuracy error] paddle.logsumexp(Tensor([35791395, 3, 4, 5],"float64"), list[-1,], False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 429294799 / 429496740 (100%)
Max absolute difference: 2.100296
Max relative difference: 1.
 x: array([[[1.573683, 1.829085, 1.747226, 1.732767],
        [1.549212, 1.755975, 1.731023, 1.673523],
        [1.655974, 1.706696, 1.645537, 1.694804]],...
 y: array([[[1.776291, 1.761332, 1.685537, 1.737196],
        [1.780981, 1.752415, 1.645354, 1.68119 ],
        [1.392544, 1.56884 , 1.375748, 1.663556]],...
2025-04-21 23:08:13.512240 test begin: paddle.logsumexp(Tensor([35791395, 3, 4, 5],"float64"), list[-1,], True, )

[accuracy error] paddle.logsumexp(Tensor([35791395, 3, 4, 5],"float64"), list[-1,], True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 429294799 / 429496740 (100%)
Max absolute difference: 2.100296
Max relative difference: 1.
 x: array([[[[1.573683],
         [1.829085],
         [1.747226],...
 y: array([[[[1.776291],
         [1.761332],
         [1.685537],...
2025-04-21 23:09:21.477983 test begin: paddle.logsumexp(Tensor([4, 89478486, 6],"float64"), list[-1,], False, )

[accuracy error] paddle.logsumexp(Tensor([4, 89478486, 6],"float64"), list[-1,], False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 357440356 / 357913944 (99.9%)
Max absolute difference: 2.27284361
Max relative difference: 1.
 x: array([[1.694123, 1.822286, 1.73017 , ..., 0.      , 0.      , 0.      ],
       [0.      , 0.      , 0.      , ..., 0.      , 0.      , 0.      ],
       [0.      , 0.      , 0.      , ..., 0.      , 0.      , 0.      ],
       [0.      , 0.      , 0.      , ..., 0.      , 0.      , 0.      ]])
 y: array([[2.002722, 1.904686, 1.829221, ..., 1.808238, 1.843296, 1.876698],
       [1.729663, 1.90647 , 1.676037, ..., 1.97194 , 1.912001, 2.031445],
       [1.901568, 1.844958, 1.751998, ..., 1.768435, 1.778419, 1.880958],
       [1.582694, 1.871961, 1.819284, ..., 1.850601, 1.909002, 1.905953]])
2025-04-21 23:10:23.077290 test begin: paddle.logsumexp(Tensor([4294967295],"float32"), axis=0, )

[paddle error] paddle.logsumexp(Tensor([4294967295],"float32"), axis=0, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 23:10:51.061543 test begin: paddle.logsumexp(Tensor([71582789, 3, 4, 5],"float16"), list[-1,], False, )

[accuracy error] paddle.logsumexp(Tensor([71582789, 3, 4, 5],"float16"), list[-1,], False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 858791336 / 858993468 (100%)
Max absolute difference: 2.104
Max relative difference: 1.
 x: array([[[1.56  , 1.435 , 1.64  , 1.687 ],
        [1.693 , 1.566 , 1.567 , 1.709 ],
        [1.8   , 1.673 , 1.826 , 1.692 ]],...
 y: array([[[1.493 , 1.73  , 1.658 , 1.611 ],
        [1.609 , 1.588 , 1.435 , 1.806 ],
        [1.596 , 1.335 , 1.82  , 1.613 ]],...
2025-04-21 23:13:27.488426 test begin: paddle.logsumexp(Tensor([71582789, 3, 4, 5],"float32"), list[-1,], False, )

[accuracy error] paddle.logsumexp(Tensor([71582789, 3, 4, 5],"float32"), list[-1,], False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 858791189 / 858993468 (100%)
Max absolute difference: 2.1022131
Max relative difference: 1.
 x: array([[[1.668956, 1.360504, 1.659272, 1.837293],
        [1.634698, 1.62027 , 1.826287, 1.924385],
        [1.482853, 1.80405 , 1.456157, 1.608512]],...
 y: array([[[1.594192, 1.821256, 1.530687, 1.41469 ],
        [1.629744, 1.720242, 1.704597, 1.771286],
        [1.675571, 1.780023, 1.538849, 1.538507]],...
2025-04-21 23:14:37.271890 test begin: paddle.logsumexp(Tensor([71582789, 3, 4, 5],"float32"), list[2,-3,], False, )

[accuracy error] paddle.logsumexp(Tensor([71582789, 3, 4, 5],"float32"), list[2,-3,], False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 357705431 / 357913945 (99.9%)
Max absolute difference: 2.9078116
Max relative difference: 1.
 x: array([[2.524585, 2.53452 , 2.675268, 2.531658, 2.413718],
       [2.444861, 2.638997, 2.4588  , 2.539145, 2.517697],
       [2.482764, 2.510729, 2.419181, 2.576341, 2.664435],...
 y: array([[2.536186, 2.498821, 2.510421, 2.658321, 2.408141],
       [2.554788, 2.526939, 2.521978, 2.354941, 2.561193],
       [2.617354, 2.61823 , 2.460978, 2.586197, 2.570446],...
2025-04-21 23:15:21.649749 test begin: paddle.logsumexp(Tensor([71582789, 5, 6],"float64"), list[-1,], False, )

[accuracy error] paddle.logsumexp(Tensor([71582789, 5, 6],"float64"), list[-1,], False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 357677269 / 357913945 (99.9%)
Max absolute difference: 2.27284361
Max relative difference: 1.
 x: array([[1.694123, 1.822286, 1.73017 , 2.019562, 1.956141],
       [1.545625, 1.987407, 1.918942, 1.60348 , 1.664086],
       [1.841773, 1.921236, 1.540668, 1.810423, 1.52675 ],...
 y: array([[2.002722, 1.904686, 1.829221, 2.016604, 1.893223],
       [1.869047, 1.732537, 1.683835, 1.580266, 1.830579],
       [1.695214, 1.865414, 1.72227 , 1.793699, 1.912918],...
2025-04-21 23:16:29.051871 test begin: paddle.masked_fill(Tensor([4294967295],"float16"), Tensor([4294967295],"bool"), -0.7255859375, )

One of the differentiated Tensors does not require grad
[paddle error] paddle.masked_fill(Tensor([4294967295],"float16"), Tensor([4294967295],"bool"), -0.7255859375, ) 
 (InvalidArgument) The expanded size (-1) for non-existing dimensions must be positive for expand_v2 op.
  [Hint: Expected expand_shape[i] >= 0, but received expand_shape[i]:-1 < 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/kernels/gpu/expand_kernel.cu:45)

2025-04-22 07:51:50.303521 test begin: paddle.nanmean(Tensor([1431655765, 3],"float32"), 0, True, )

W0422 07:53:15.839335 81616 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 07:53:15.840452 81616 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([1431655765, 3],"float32"), 0, True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 07:53:18.247036 test begin: paddle.nanmean(Tensor([1431655765, 3],"float32"), None, False, )

[cuda error] paddle.nanmean(Tensor([1431655765, 3],"float32"), None, False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 07:53:44.556067 test begin: paddle.nanmean(Tensor([1431655765, 3],"float32"), None, True, )

[cuda error] paddle.nanmean(Tensor([1431655765, 3],"float32"), None, True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 07:54:10.012000 test begin: paddle.nanmean(Tensor([1431655765, 3],"float32"), tuple(0,1,), False, )

[cuda error] paddle.nanmean(Tensor([1431655765, 3],"float32"), tuple(0,1,), False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 07:54:31.540036 test begin: paddle.nanmean(Tensor([2, 107374183, 4, 5],"float32"), None, False, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745279695 (unix time) try "date -d @1745279695" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x13e85) received by PID 81541 (TID 0x7fd26bf48700) from PID 81541 ***]

2025-04-22 07:55:03.599660 test begin: paddle.nanmean(Tensor([2, 107374183, 4, 5],"float32"), None, True, )

W0422 07:56:34.659188 81890 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 07:56:34.660357 81890 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745279796 (unix time) try "date -d @1745279796" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x13f7b) received by PID 81787 (TID 0x7f24b5abb700) from PID 81787 ***]

2025-04-22 07:57:22.371518 test begin: paddle.nanmean(Tensor([2, 107374183, 4, 5],"float32"), list[0,1,2,3,], False, )

W0422 07:58:52.110272 82071 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 07:58:52.111472 82071 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745279934 (unix time) try "date -d @1745279934" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x14040) received by PID 81984 (TID 0x7fcf107c3700) from PID 81984 ***]

2025-04-22 07:59:00.181166 test begin: paddle.nanmean(Tensor([2, 107374183, 4, 5],"float32"), list[], False, )

W0422 08:00:27.363081 82166 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 08:00:27.364217 82166 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745280029 (unix time) try "date -d @1745280029" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1409f) received by PID 82079 (TID 0x7f6fc0949700) from PID 82079 ***]

2025-04-22 08:01:16.207651 test begin: paddle.nanmean(Tensor([2, 2147483648],"float32"), -1, False, )

W0422 08:02:49.519721 82348 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 08:02:49.521446 82348 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.nanmean(Tensor([2, 2147483648],"float32"), -1, False, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_where(_object*, _object*, _object*)
1   where_ad_func(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&)
2   paddle::experimental::where(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&)
3   void phi::WhereKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*)
4   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
5   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 16.000000GB memory on GPU 0, 76.991150GB memory has been allocated and available memory is only 2.193726GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /host_home/wanghuan29/Paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-04-22 08:02:52.358864 test begin: paddle.nanmean(Tensor([2, 2147483648],"float32"), 1, False, )

[paddle error] paddle.nanmean(Tensor([2, 2147483648],"float32"), 1, False, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_where(_object*, _object*, _object*)
1   where_ad_func(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&)
2   paddle::experimental::where(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&)
3   void phi::WhereKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*)
4   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
5   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 16.000000GB memory on GPU 0, 76.991150GB memory has been allocated and available memory is only 2.193726GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /host_home/wanghuan29/Paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-04-22 08:03:18.748199 test begin: paddle.nanmean(Tensor([2, 2147483648],"float32"), None, False, )

[paddle error] paddle.nanmean(Tensor([2, 2147483648],"float32"), None, False, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_where(_object*, _object*, _object*)
1   where_ad_func(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&)
2   paddle::experimental::where(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&)
3   void phi::WhereKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*)
4   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
5   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 16.000000GB memory on GPU 0, 76.991150GB memory has been allocated and available memory is only 2.193726GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /host_home/wanghuan29/Paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-04-22 08:03:44.468030 test begin: paddle.nanmean(Tensor([2, 2147483648],"float32"), None, True, )

[paddle error] paddle.nanmean(Tensor([2, 2147483648],"float32"), None, True, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_where(_object*, _object*, _object*)
1   where_ad_func(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&)
2   paddle::experimental::where(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&)
3   void phi::WhereKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*)
4   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
5   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 16.000000GB memory on GPU 0, 76.991150GB memory has been allocated and available memory is only 2.193726GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /host_home/wanghuan29/Paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-04-22 08:04:10.006634 test begin: paddle.nanmean(Tensor([2, 2147483648],"float32"), tuple(0,1,), False, )

[paddle error] paddle.nanmean(Tensor([2, 2147483648],"float32"), tuple(0,1,), False, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_where(_object*, _object*, _object*)
1   where_ad_func(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&)
2   paddle::experimental::where(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&)
3   void phi::WhereKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*)
4   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
5   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 16.000000GB memory on GPU 0, 76.991150GB memory has been allocated and available memory is only 2.193726GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /host_home/wanghuan29/Paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-04-22 08:04:34.564959 test begin: paddle.nanmean(Tensor([2, 3, 143165577, 5],"float32"), 2, True, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745280298 (unix time) try "date -d @1745280298" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x14155) received by PID 82261 (TID 0x7f0ea27c3700) from PID 82261 ***]

2025-04-22 08:05:44.631697 test begin: paddle.nanmean(Tensor([2, 3, 143165577, 5],"float32"), None, False, )

W0422 08:07:11.952827 82452 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 08:07:11.953984 82452 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745280434 (unix time) try "date -d @1745280434" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x141bd) received by PID 82365 (TID 0x7f1b51935700) from PID 82365 ***]

2025-04-22 08:08:00.267786 test begin: paddle.nanmean(Tensor([2, 3, 143165577, 5],"float32"), None, True, )

W0422 08:09:28.925279 82547 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 08:09:28.926518 82547 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745280571 (unix time) try "date -d @1745280571" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1421c) received by PID 82460 (TID 0x7fad19dc2700) from PID 82460 ***]

2025-04-22 08:10:15.079895 test begin: paddle.nanmean(Tensor([2, 3, 143165577, 5],"float32"), list[0,1,2,3,], False, )

W0422 08:11:40.437893 82639 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 08:11:40.438987 82639 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745280702 (unix time) try "date -d @1745280702" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x14278) received by PID 82552 (TID 0x7fe1627c3700) from PID 82552 ***]

2025-04-22 08:12:27.419804 test begin: paddle.nanmean(Tensor([2, 3, 143165577, 5],"float32"), list[0,2,], False, )

W0422 08:13:53.605263 82731 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 08:13:53.606483 82731 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745280835 (unix time) try "date -d @1745280835" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x142d4) received by PID 82644 (TID 0x7fae69dc2700) from PID 82644 ***]

2025-04-22 08:14:37.567161 test begin: paddle.nanmean(Tensor([2, 3, 143165577, 5],"float32"), list[], False, )

W0422 08:16:16.188839 82824 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 08:16:16.189918 82824 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745280978 (unix time) try "date -d @1745280978" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x14330) received by PID 82736 (TID 0x7f1e17f48700) from PID 82736 ***]

2025-04-22 08:17:05.119300 test begin: paddle.nanmean(Tensor([2, 3, 143165577, 5],"float32"), tuple(0,2,), False, )

W0422 08:18:30.386272 82918 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 08:18:30.387446 82918 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745281112 (unix time) try "date -d @1745281112" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1438f) received by PID 82831 (TID 0x7f1409f48700) from PID 82831 ***]

2025-04-22 08:19:16.975692 test begin: paddle.nanmean(Tensor([2, 3, 4, 178956971],"float32"), -1, False, )

W0422 08:20:57.458398 83009 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 08:20:57.459988 83009 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745281259 (unix time) try "date -d @1745281259" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x143ea) received by PID 82922 (TID 0x7f474c7c3700) from PID 82922 ***]

2025-04-22 08:21:43.240394 test begin: paddle.nanmean(Tensor([2, 3, 4, 178956971],"float32"), None, False, )

W0422 08:23:12.414908 83101 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 08:23:12.416126 83101 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745281394 (unix time) try "date -d @1745281394" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x14446) received by PID 83014 (TID 0x7f76a14f4700) from PID 83014 ***]

2025-04-22 08:23:54.579152 test begin: paddle.nanmean(Tensor([2, 3, 4, 178956971],"float32"), None, True, )

W0422 08:25:22.155118 83193 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 08:25:22.156342 83193 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745281524 (unix time) try "date -d @1745281524" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x144a2) received by PID 83106 (TID 0x7f6d3c949700) from PID 83106 ***]

2025-04-22 08:26:06.112666 test begin: paddle.nanmean(Tensor([2, 3, 4, 178956971],"float32"), list[0,1,2,3,], False, )

W0422 08:27:31.976565 83288 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 08:27:31.977860 83288 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745281654 (unix time) try "date -d @1745281654" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x14501) received by PID 83201 (TID 0x7fc482949700) from PID 83201 ***]

2025-04-22 08:28:17.252595 test begin: paddle.nanmean(Tensor([2, 3, 4, 178956971],"float32"), list[], False, )

W0422 08:29:43.756064 83378 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 08:29:43.757233 83378 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745281786 (unix time) try "date -d @1745281786" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1455b) received by PID 83291 (TID 0x7fbf2c949700) from PID 83291 ***]

2025-04-22 08:30:28.156263 test begin: paddle.nanmean(Tensor([3, 1431655765],"float32"), axis=None, )

W0422 08:32:00.432543 83469 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 08:32:00.433779 83469 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([3, 1431655765],"float32"), axis=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 08:32:02.229898 test begin: paddle.nanmean(Tensor([3, 1431655765],"float32"), keepdim=True, )

[cuda error] paddle.nanmean(Tensor([3, 1431655765],"float32"), keepdim=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 08:32:25.624800 test begin: paddle.nanmean(Tensor([4294967295],"float32"), axis=0, )

[paddle error] paddle.nanmean(Tensor([4294967295],"float32"), axis=0, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_where(_object*, _object*, _object*)
1   where_ad_func(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&)
2   paddle::experimental::where(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&)
3   void phi::WhereKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*)
4   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
5   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 16.000000GB memory on GPU 0, 76.991150GB memory has been allocated and available memory is only 2.193726GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /host_home/wanghuan29/Paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-04-22 08:32:50.595730 test begin: paddle.nanmean(Tensor([71582789, 3, 4, 5],"float32"), None, False, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745281996 (unix time) try "date -d @1745281996" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x145b6) received by PID 83382 (TID 0x7f00867c3700) from PID 83382 ***]

2025-04-22 08:33:57.375616 test begin: paddle.nanmean(Tensor([71582789, 3, 4, 5],"float32"), None, True, )

W0422 08:35:28.404258 83571 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 08:35:28.405506 83571 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745282130 (unix time) try "date -d @1745282130" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1461c) received by PID 83484 (TID 0x7f172a7c3700) from PID 83484 ***]

2025-04-22 08:36:14.994077 test begin: paddle.nanmean(Tensor([71582789, 3, 4, 5],"float32"), list[0,1,2,3,], False, )

W0422 08:37:39.880937 83661 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 08:37:39.882094 83661 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745282262 (unix time) try "date -d @1745282262" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x14676) received by PID 83574 (TID 0x7fb727744700) from PID 83574 ***]

2025-04-22 08:38:24.748636 test begin: paddle.nanmean(Tensor([71582789, 3, 4, 5],"float32"), list[0,2,], False, )

W0422 08:40:06.248554 83751 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 08:40:06.249760 83751 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745282408 (unix time) try "date -d @1745282408" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x146d0) received by PID 83664 (TID 0x7f450b2b7700) from PID 83664 ***]

2025-04-22 08:40:51.573223 test begin: paddle.nanmean(Tensor([71582789, 3, 4, 5],"float32"), list[], False, )

W0422 08:42:27.613612 83844 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 08:42:27.614776 83844 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745282550 (unix time) try "date -d @1745282550" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1472d) received by PID 83757 (TID 0x7fc027dc2700) from PID 83757 ***]

2025-04-22 08:43:12.688823 test begin: paddle.nanmean(Tensor([71582789, 3, 4, 5],"float32"), tuple(0,2,), False, )

W0422 08:44:39.440094 83936 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 08:44:39.441290 83936 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745282681 (unix time) try "date -d @1745282681" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x14789) received by PID 83849 (TID 0x7fc379dc2700) from PID 83849 ***]

2025-04-22 08:45:23.122015 test begin: paddle.nanmean(Tensor([858993459, 5],"float32"), axis=None, )

W0422 08:46:50.957345 84028 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 08:46:50.958592 84028 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([858993459, 5],"float32"), axis=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 08:46:53.016549 test begin: paddle.nanmean(Tensor([858993459, 5],"float32"), keepdim=True, )

[cuda error] paddle.nanmean(Tensor([858993459, 5],"float32"), keepdim=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 08:47:19.225701 test begin: paddle.nansum(Tensor([1073741824, 4],"float32"), )

[cuda error] paddle.nansum(Tensor([1073741824, 4],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 08:47:39.738185 test begin: paddle.nansum(Tensor([1431655765, 3],"float32"), axis=None, keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([1431655765, 3],"float32"), axis=None, keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 08:48:03.357883 test begin: paddle.nansum(Tensor([1431655765, 3],"float32"), axis=None, keepdim=True, name=None, )

[cuda error] paddle.nansum(Tensor([1431655765, 3],"float32"), axis=None, keepdim=True, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 08:48:31.720159 test begin: paddle.nansum(Tensor([1431655765, 3],"float32"), axis=list[0,], keepdim=True, name=None, )

[cuda error] paddle.nansum(Tensor([1431655765, 3],"float32"), axis=list[0,], keepdim=True, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 08:49:00.670818 test begin: paddle.nansum(Tensor([3, 1431655765],"float32"), axis=None, )

[cuda error] paddle.nansum(Tensor([3, 1431655765],"float32"), axis=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 08:49:31.274693 test begin: paddle.nansum(Tensor([3, 1431655765],"float32"), axis=None, keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([3, 1431655765],"float32"), axis=None, keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 08:49:51.110071 test begin: paddle.nansum(Tensor([3, 1431655765],"float32"), axis=None, keepdim=True, name=None, )

[cuda error] paddle.nansum(Tensor([3, 1431655765],"float32"), axis=None, keepdim=True, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 08:50:13.624152 test begin: paddle.nansum(Tensor([3, 1431655765],"float32"), keepdim=True, )

[cuda error] paddle.nansum(Tensor([3, 1431655765],"float32"), keepdim=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 08:50:36.202517 test begin: paddle.nansum(Tensor([858993459, 5],"float32"), axis=None, )

[cuda error] paddle.nansum(Tensor([858993459, 5],"float32"), axis=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 08:50:57.590562 test begin: paddle.nansum(Tensor([858993459, 5],"float32"), axis=None, keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([858993459, 5],"float32"), axis=None, keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 08:51:16.862091 test begin: paddle.nansum(Tensor([858993459, 5],"float32"), axis=None, keepdim=True, name=None, )

[cuda error] paddle.nansum(Tensor([858993459, 5],"float32"), axis=None, keepdim=True, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 08:51:39.391415 test begin: paddle.nansum(Tensor([858993459, 5],"float32"), keepdim=True, )

[cuda error] paddle.nansum(Tensor([858993459, 5],"float32"), keepdim=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 08:52:02.288670 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([2, 67108864, 32],"float32"), 16, None, )

(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1385)

2025-04-22 08:52:13.397752 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([2, 67108864, 32],"float32"), output_size=16, )

(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1385)

2025-04-22 08:53:41.590678 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 67108864, 32],"float32"), 16, False, None, )

(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1385)

2025-04-22 08:55:01.722653 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 67108864, 32],"float32"), output_size=16, )

(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1385)

2025-04-22 08:56:27.084880 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([29217465, 3, 7, 7],"float32"), output_size=list[2,5,], return_mask=False, name=None, )

(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1385)

2025-04-22 08:57:42.088126 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([29217465, 3, 7, 7],"float32"), output_size=list[3,3,], return_mask=False, name=None, )

(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1385)

2025-04-22 08:59:06.067860 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([29217465, 3, 7, 7],"float32"), output_size=list[None,3,], return_mask=False, name=None, )

(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1385)

2025-04-22 09:00:24.052651 test begin: paddle.nn.functional.adaptive_max_pool2d(x=Tensor([29217465, 3, 7, 7],"float32"), output_size=list[2,5,], )

(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1385)

2025-04-22 09:01:45.174876 test begin: paddle.nn.functional.adaptive_max_pool2d(x=Tensor([29217465, 3, 7, 7],"float32"), output_size=list[None,3,], )

(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1385)

2025-04-22 09:03:05.464653 test begin: paddle.nn.functional.adaptive_max_pool2d(x=Tensor([29217465, 3, 7, 7],"float32"), return_mask=False, output_size=list[3,3,], )

(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1385)

2025-04-22 09:04:22.805175 test begin: paddle.nn.functional.grid_sample(Tensor([56, 3, 6790778, 2],"float32"), Tensor([56, 2, 2, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, )

W0422 09:05:47.478586 92803 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 09:05:47.479846 92803 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745283948 (unix time) try "date -d @1745283948" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x169f1) received by PID 92657 (TID 0x7fd15d935700) from PID 92657 ***]

2025-04-22 09:06:30.064613 test begin: paddle.nn.functional.grid_sample(Tensor([56, 3, 6790778, 2],"float32"), Tensor([56, 2, 6790778, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, )

W0422 09:08:30.380816 95519 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 09:08:30.383361 95519 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745284112 (unix time) try "date -d @1745284112" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1747d) received by PID 95357 (TID 0x7f72e7dc2700) from PID 95357 ***]

2025-04-22 09:09:13.449933 test begin: paddle.nn.functional.grid_sample(Tensor([56, 3, 848848, 16],"float32"), Tensor([56, 16, 16, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, )

W0422 09:10:33.849011 98987 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 09:10:33.850266 98987 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745284234 (unix time) try "date -d @1745284234" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x18209) received by PID 98825 (TID 0x7f80cfdc2700) from PID 98825 ***]

2025-04-22 09:11:18.385715 test begin: paddle.nn.functional.grid_sample(Tensor([56, 3, 848848, 16],"float32"), Tensor([56, 16, 848848, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, )

W0422 09:12:50.927955 101724 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 09:12:50.929077 101724 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745284372 (unix time) try "date -d @1745284372" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x18cba) received by PID 101562 (TID 0x7f14eb34a700) from PID 101562 ***]

2025-04-22 09:13:34.708177 test begin: paddle.nn.functional.grid_sample(Tensor([56, 39790, 32, 32],"float32"), Tensor([56, 32, 32, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, )

W0422 09:15:00.415848 104670 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 09:15:00.417043 104670 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745284501 (unix time) try "date -d @1745284501" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1983c) received by PID 104508 (TID 0x7f765d87e700) from PID 104508 ***]

2025-04-22 09:15:46.077805 test begin: paddle.nn.functional.grid_sample(Tensor([56, 9948, 64, 64],"float32"), Tensor([56, 64, 64, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, )

W0422 09:17:19.776897 107625 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 09:17:19.777999 107625 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745284640 (unix time) try "date -d @1745284640" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1a3c7) received by PID 107463 (TID 0x7f965334a700) from PID 107463 ***]

2025-04-22 09:18:02.756069 test begin: paddle.nn.functional.grid_sample(Tensor([5821, 4, 280, 350],"float32"), Tensor([5821, 280, 350, 2],"float32"), mode="nearest", padding_mode="zeros", align_corners=False, )

W0422 09:19:31.168185 110571 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 09:19:31.169287 110571 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745284772 (unix time) try "date -d @1745284772" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1af49) received by PID 110409 (TID 0x7f8159d0b700) from PID 110409 ***]

2025-04-22 09:20:16.493942 test begin: paddle.nn.functional.grid_sample(Tensor([5821, 4, 280, 350],"float32"), Tensor([5821, 298, 364, 2],"float32"), mode="nearest", padding_mode="zeros", align_corners=False, )

W0422 09:21:57.491060 113454 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 09:21:57.492350 113454 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745284918 (unix time) try "date -d @1745284918" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1ba8c) received by PID 113292 (TID 0x7f66787c3700) from PID 113292 ***]

2025-04-22 09:22:43.839305 test begin: paddle.nn.functional.grid_sample(Tensor([5821, 4, 280, 350],"float32"), Tensor([5821, 368, 416, 2],"float32"), mode="nearest", padding_mode="zeros", align_corners=False, )

W0422 09:24:22.149629 116546 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 09:24:22.150910 116546 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745285064 (unix time) try "date -d @1745285064" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1c6a0) received by PID 116384 (TID 0x7f26db2b7700) from PID 116384 ***]

2025-04-22 09:25:09.040634 test begin: paddle.nn.functional.grid_sample(Tensor([61896, 1, 192, 192],"float32"), Tensor([61896, 1, 12544, 2],"float32"), align_corners=False, )

W0422 09:26:34.782354 119650 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 09:26:34.783529 119650 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.nn.functional.grid_sample(Tensor([61896, 1, 192, 192],"float32"), Tensor([61896, 1, 12544, 2],"float32"), align_corners=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 44016477 / 776423424 (5.67%)
Max absolute difference: 0.9472889
Max relative difference: 1.8853173e+08
 x: array([[[[ 0.014458,  0.165526,  0.073304, ...,  0.082494, -0.012945,
          -0.074179]]],
...
 y: array([[[[ 0.014458,  0.165526,  0.073304, ...,  0.082494, -0.012945,
          -0.074179]]],
...
2025-04-22 09:27:11.883360 test begin: paddle.nn.functional.grid_sample(Tensor([67395, 1, 184, 184],"float32"), Tensor([67395, 1, 12544, 2],"float32"), align_corners=False, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745285255 (unix time) try "date -d @1745285255" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1d2c0) received by PID 119488 (TID 0x7f0e7db85700) from PID 119488 ***]

2025-04-22 09:28:20.817334 test begin: paddle.nn.functional.grid_sample(Tensor([727584, 4, 28, 28],"float32"), Tensor([727584, 28, 28, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )

W0422 09:29:55.193336 123721 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 09:29:55.194541 123721 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745285396 (unix time) try "date -d @1745285396" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1e2ab) received by PID 123563 (TID 0x7f835d87e700) from PID 123563 ***]

2025-04-22 09:30:36.720625 test begin: paddle.nn.functional.grid_sample(Tensor([727584, 4, 28, 28],"float32"), Tensor([727584, 34, 34, 2],"float32"), mode="nearest", padding_mode="zeros", align_corners=False, )

W0422 09:32:10.042701 126598 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 09:32:10.045033 126598 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745285531 (unix time) try "date -d @1745285531" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1ee25) received by PID 126501 (TID 0x7f0ae2949700) from PID 126501 ***]

2025-04-22 09:32:52.530405 test begin: paddle.nn.functional.grid_sample(Tensor([73661, 1, 176, 176],"float32"), Tensor([73661, 1, 12544, 2],"float32"), align_corners=False, )

W0422 09:34:21.122359 129537 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 09:34:21.123515 129537 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.nn.functional.grid_sample(Tensor([73661, 1, 176, 176],"float32"), Tensor([73661, 1, 12544, 2],"float32"), align_corners=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 52382776 / 924003584 (5.67%)
Max absolute difference: 0.9611181
Max relative difference: 20023462.
 x: array([[[[ 0.122453, -0.09488 , -0.24399 , ...,  0.345272,  0.045508,
           0.037625]]],
...
 y: array([[[[ 0.122453, -0.09488 , -0.24399 , ...,  0.345272,  0.045508,
           0.037625]]],
...
2025-04-22 09:35:00.392989 test begin: paddle.nn.functional.grid_sample(Tensor([742742, 3, 32, 32],"float32"), Tensor([742742, 32, 32, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745285730 (unix time) try "date -d @1745285730" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1f9a3) received by PID 129443 (TID 0x7fdab7dc2700) from PID 129443 ***]

2025-04-22 09:36:12.208541 test begin: paddle.nn.functional.grid_sample(x=Tensor([16, 64, 80, 94, 311],"float32"), grid=Tensor([16, 280, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )

W0422 09:37:42.135466 133794 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 09:37:42.136746 133794 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745285863 (unix time) try "date -d @1745285863" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x209fe) received by PID 133630 (TID 0x7f25fbf48700) from PID 133630 ***]

2025-04-22 09:38:26.986061 test begin: paddle.nn.functional.hardshrink(Tensor([2281701379],"float32"), 0.5, None, )

W0422 09:39:51.844058 136736 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 09:39:51.845263 136736 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.nn.functional.hardshrink(Tensor([2281701379],"float32"), 0.5, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 67 / 2281701379 (2.94e-06%)
Max absolute difference: 0.5
Max relative difference: inf
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)
 y: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)
2025-04-22 09:41:08.964787 test begin: paddle.nn.functional.hardshrink(x=Tensor([2281701379],"float32"), )

[accuracy error] paddle.nn.functional.hardshrink(x=Tensor([2281701379],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 67 / 2281701379 (2.94e-06%)
Max absolute difference: 0.5
Max relative difference: inf
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)
 y: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)
2025-04-22 09:42:48.030571 test begin: paddle.nn.functional.layer_norm(Tensor([570425345, 4],"float32"), list[4,], None, None, )

[accuracy error] backward  paddle.nn.functional.layer_norm(Tensor([570425345, 4],"float32"), list[4,], None, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 34 / 2281701380 (1.49e-06%)
Max absolute difference: 0.11776733
Max relative difference: 73.5646
 x: array([[-1.207856, -1.064323,  0.819633,  1.452546],
       [-0.380384,  1.175786, -1.027204,  0.231801],
       [ 2.199755, -2.49479 , -0.728409,  1.023445],...
 y: array([[-1.207856, -1.064323,  0.819633,  1.452546],
       [-0.380384,  1.175786, -1.027204,  0.231801],
       [ 2.199754, -2.49479 , -0.728409,  1.023445],...
2025-04-22 09:46:18.993807 test begin: paddle.nn.functional.layer_norm(Tensor([69633, 128, 256],"float32"), list[256,], weight=Tensor([256],"float32"), bias=Tensor([256],"float32"), epsilon=1e-05, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   LayerNormGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::layer_norm_grad(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, float, int, paddle::Tensor*, paddle::Tensor*, paddle::Tensor*)
4   void phi::LayerNormGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, paddle::optional<phi::DenseTensor> const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, float, int, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::funcs::ln_bwd_fast_kernel_driver<float, float, float, unsigned char>(phi::GPUContext const&, int, int, float, float const*, float const*, float const*, float const*, float const*, float*, float*, float*, unsigned char const*, float, float*)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745286471 (unix time) try "date -d @1745286471" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2157e) received by PID 136574 (TID 0x7f6a46949700) from PID 136574 ***]

2025-04-22 09:48:36.697137 test begin: paddle.nn.functional.layer_norm(Tensor([8, 1114113, 256],"float32"), list[256,], weight=Tensor([256],"float32"), bias=Tensor([256],"float32"), epsilon=1e-05, )

W0422 09:50:14.033169 149487 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 09:50:14.034418 149487 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   LayerNormGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::layer_norm_grad(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, float, int, paddle::Tensor*, paddle::Tensor*, paddle::Tensor*)
4   void phi::LayerNormGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, paddle::optional<phi::DenseTensor> const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, float, int, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::funcs::ln_bwd_fast_kernel_driver<float, float, float, unsigned char>(phi::GPUContext const&, int, int, float, float const*, float const*, float const*, float const*, float const*, float*, float*, float*, unsigned char const*, float, float*)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745286724 (unix time) try "date -d @1745286724" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x24751) received by PID 149329 (TID 0x7fdef2949700) from PID 149329 ***]

2025-04-22 09:52:48.660661 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,3,), x=Tensor([715827883, 2, 3],"float16"), )

W0422 09:54:46.340330 155117 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 09:54:46.341471 155117 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] backward  paddle.nn.functional.layer_norm(normalized_shape=tuple(2,3,), x=Tensor([715827883, 2, 3],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 8 / 4294967298 (1.86e-07%)
Max absolute difference: 0.0625
Max relative difference: 16384.
 x: array([[[ 1.681  , -0.917  ,  1.052  ],
        [ 0.2212 , -1.327  , -0.709  ]],
...
 y: array([[[ 1.681  , -0.918  ,  1.052  ],
        [ 0.221  , -1.327  , -0.7085 ]],
...
2025-04-22 10:16:16.191316 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,3,), x=Tensor([715827883, 2, 3],"float16"), epsilon=1e-05, )

[accuracy error] backward  paddle.nn.functional.layer_norm(normalized_shape=tuple(2,3,), x=Tensor([715827883, 2, 3],"float16"), epsilon=1e-05, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 8 / 4294967298 (1.86e-07%)
Max absolute difference: 0.0625
Max relative difference: 16384.
 x: array([[[ 1.681  , -0.917  ,  1.052  ],
        [ 0.2212 , -1.327  , -0.709  ]],
...
 y: array([[[ 1.681  , -0.918  ,  1.052  ],
        [ 0.221  , -1.327  , -0.7085 ]],
...
2025-04-22 10:43:34.403545 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(3,), x=Tensor([1431655766, 3],"float16"), )

[accuracy error] backward  paddle.nn.functional.layer_norm(normalized_shape=tuple(3,), x=Tensor([1431655766, 3],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 76747 / 4294967298 (0.00179%)
Max absolute difference: 0.25
Max relative difference: 16384.
 x: array([[-0.252 , -0.334 ,  0.587 ],
       [ 1.223 , -0.2734, -0.9487],
       [ 0.8535, -0.9253,  0.0703],...
 y: array([[-0.2524 , -0.3345 ,  0.587  ],
       [ 1.223  , -0.2734 , -0.949  ],
       [ 0.855  , -0.9253 ,  0.07007],...
2025-04-22 11:06:45.621419 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(3,), x=Tensor([1431655766, 3],"float16"), epsilon=1e-05, )

[accuracy error] backward  paddle.nn.functional.layer_norm(normalized_shape=tuple(3,), x=Tensor([1431655766, 3],"float16"), epsilon=1e-05, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 76747 / 4294967298 (0.00179%)
Max absolute difference: 0.25
Max relative difference: 16384.
 x: array([[-0.252 , -0.334 ,  0.587 ],
       [ 1.223 , -0.2734, -0.9487],
       [ 0.8535, -0.9253,  0.0703],...
 y: array([[-0.2524 , -0.3345 ,  0.587  ],
       [ 1.223  , -0.2734 , -0.949  ],
       [ 0.855  , -0.9253 ,  0.07007],...
2025-04-22 11:29:22.819591 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(3,), x=Tensor([760567127, 3],"float32"), )

[accuracy error] backward  paddle.nn.functional.layer_norm(normalized_shape=tuple(3,), x=Tensor([760567127, 3],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2028 / 2281701381 (8.89e-05%)
Max absolute difference: 0.3500824
Max relative difference: 3154.812
 x: array([[-2.157359e-02,  6.661118e-01, -6.445383e-01],
       [-9.656262e+00,  3.706725e-01,  9.285593e+00],
       [-1.330842e+00,  1.355471e+00, -2.462886e-02],...
 y: array([[-2.157364e-02,  6.661117e-01, -6.445383e-01],
       [-9.656258e+00,  3.706822e-01,  9.285576e+00],
       [-1.330841e+00,  1.355471e+00, -2.462978e-02],...
2025-04-22 11:34:28.339835 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(3,), x=Tensor([760567127, 3],"float32"), epsilon=1e-05, )

[accuracy error] backward  paddle.nn.functional.layer_norm(normalized_shape=tuple(3,), x=Tensor([760567127, 3],"float32"), epsilon=1e-05, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2028 / 2281701381 (8.89e-05%)
Max absolute difference: 0.3500824
Max relative difference: 3154.812
 x: array([[-2.157359e-02,  6.661118e-01, -6.445383e-01],
       [-9.656262e+00,  3.706725e-01,  9.285593e+00],
       [-1.330842e+00,  1.355471e+00, -2.462886e-02],...
 y: array([[-2.157364e-02,  6.661117e-01, -6.445383e-01],
       [-9.656258e+00,  3.706822e-01,  9.285576e+00],
       [-1.330841e+00,  1.355471e+00, -2.462978e-02],...
2025-04-22 11:38:04.573575 test begin: paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 171798692, 1, 1],"float16"), list[list[1,3,2,0,],list[2,1,0,3,],], )

[accuracy error] backward  paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 171798692, 1, 1],"float16"), list[list[1,3,2,0,],list[2,1,0,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967300 / 4294967300 (100%)
Max absolute difference: 395.8
Max relative difference: 16.47
 x: array([[[[-419.8],
         [-419.8],
         [-419.8],...
 y: array([[[[-24.03],
         [-24.03],
         [-24.03],...
2025-04-22 11:51:06.217904 test begin: paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 171798692, 1, 1],"float16"), list[list[2,1,0,3,],list[2,0,3,1,],], )

[accuracy error] backward  paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 171798692, 1, 1],"float16"), list[list[2,1,0,3,],list[2,0,3,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967300 / 4294967300 (100%)
Max absolute difference: 395.8
Max relative difference: 16.47
 x: array([[[[-419.8],
         [-419.8],
         [-419.8],...
 y: array([[[[-24.03],
         [-24.03],
         [-24.03],...
2025-04-22 12:03:29.449655 test begin: paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 171798692, 1, 1],"float16"), list[list[2,1,],list[0,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 171798692, 1, 1],"float16"), list[list[2,1,],list[0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 858988742 / 858993460 (100%)
Max absolute difference: 6764.
Max relative difference: 24.28
 x: array([[[[  124.44],
         [ -533.5 ],
         [ -249.5 ],...
 y: array([[[[ 4.957e+00],
         [-2.123e+01],
         [-9.930e+00],...
2025-04-22 12:06:08.807856 test begin: paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 171798692, 1, 1],"float16"), list[list[2,3,0,],list[3,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 171798692, 1, 1],"float16"), list[list[2,3,0,],list[3,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 171793222 / 171798692 (100%)
Max absolute difference: 9424.
Max relative difference: 16.48
 x: array([[ 3614. ],
       [-2368. ],
       [  252. ],...
 y: array([[ 207.   ],
       [-135.6  ],
       [  14.43 ],...
2025-04-22 12:07:01.311995 test begin: paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 171798692, 1, 1],"float16"), list[list[3,1,2,],list[0,3,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 171798692, 1, 1],"float16"), list[list[3,1,2,],list[0,3,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 170787447 / 858993460 (19.9%)
Max absolute difference: 2.
Max relative difference: 0.06726
 x: array([[ 4.754e+00, -2.038e+01, -9.531e+00, ...,  8.797e+00,  2.248e+01,
        -1.394e+01],
       [-8.465e+02,  3.626e+03,  1.696e+03, ..., -1.566e+03, -4.004e+03,...
 y: array([[ 5.094e+00, -2.183e+01, -1.021e+01, ...,  9.430e+00,  2.409e+01,
        -1.494e+01],
       [-8.465e+02,  3.626e+03,  1.696e+03, ..., -1.566e+03, -4.004e+03,...
2025-04-22 12:11:17.766624 test begin: paddle.var(x=Tensor([13, 175515491, 1],"float32"), axis=tuple(1,), keepdim=True, unbiased=False, )

[accuracy error] paddle.var(x=Tensor([13, 175515491, 1],"float32"), axis=tuple(1,), keepdim=True, unbiased=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 13 / 13 (100%)
Max absolute difference: 0.05147997
Max relative difference: 0.6176977
 x: array([[[0.031857]],

       [[0.031864]],...
 y: array([[[0.08333 ]],

       [[0.083336]],...
2025-04-22 12:11:31.983294 test begin: paddle.var(x=Tensor([1431655766, 3],"float16"), )

[accuracy error] paddle.var(x=Tensor([1431655766, 3],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array(nan, dtype=float16)
 y: array(0.0833, dtype=float16)
2025-04-22 12:11:44.693511 test begin: paddle.var(x=Tensor([16, 142606337, 1],"float32"), axis=tuple(1,), keepdim=True, unbiased=False, )

[accuracy error] paddle.var(x=Tensor([16, 142606337, 1],"float32"), axis=tuple(1,), keepdim=True, unbiased=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 16 / 16 (100%)
Max absolute difference: 0.04412567
Max relative difference: 0.529472
 x: array([[[0.039209]],

       [[0.03922 ]],...
 y: array([[[0.083328]],

       [[0.083329]],...
2025-04-22 12:14:51.019309 test begin: paddle.var(x=Tensor([2, 2147483649],"float16"), )

[paddle error] paddle.var(x=Tensor([2, 2147483649],"float16"), ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-22 12:18:27.464815 test begin: paddle.var(x=Tensor([3, 3, 477218589],"float16"), )

[accuracy error] paddle.var(x=Tensor([3, 3, 477218589],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array(nan, dtype=float16)
 y: array(0.0833, dtype=float16)
2025-04-22 13:18:48.937424 test begin: paddle.var(x=Tensor([3, 477218589, 3],"float16"), )

[accuracy error] paddle.var(x=Tensor([3, 477218589, 3],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array(nan, dtype=float16)
 y: array(0.0833, dtype=float16)
2025-04-22 13:44:53.738846 test begin: paddle.var(x=Tensor([3, 477218589, 3],"float16"), axis=list[0,1,], )

[accuracy error] paddle.var(x=Tensor([3, 477218589, 3],"float16"), axis=list[0,1,], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([nan, nan, nan], dtype=float16)
 y: array([0.0833, 0.0833, 0.0833], dtype=float16)
2025-04-22 13:45:08.779884 test begin: paddle.var(x=Tensor([3, 477218589, 3],"float16"), axis=tuple(0,1,), )

[accuracy error] paddle.var(x=Tensor([3, 477218589, 3],"float16"), axis=tuple(0,1,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([nan, nan, nan], dtype=float16)
 y: array([0.0833, 0.0833, 0.0833], dtype=float16)
2025-04-22 13:45:20.439543 test begin: paddle.var(x=Tensor([3, 477218589, 3],"float16"), axis=tuple(0,1,), keepdim=True, )

[accuracy error] paddle.var(x=Tensor([3, 477218589, 3],"float16"), axis=tuple(0,1,), keepdim=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[nan, nan, nan]]], dtype=float16)
 y: array([[[0.0833, 0.0833, 0.0833]]], dtype=float16)
2025-04-22 13:45:32.458300 test begin: paddle.var(x=Tensor([3, 760567127, 1],"float32"), axis=tuple(1,), keepdim=True, unbiased=False, )

[accuracy error] paddle.var(x=Tensor([3, 760567127, 1],"float32"), axis=tuple(1,), keepdim=True, unbiased=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 3 / 3 (100%)
Max absolute difference: 0.05392256
Max relative difference: 0.64706933
 x: array([[[0.02941 ]],

       [[0.029412]],...
 y: array([[[0.083331]],

       [[0.083334]],...
2025-04-22 13:45:49.377876 test begin: paddle.var(x=Tensor([477218589, 3, 3],"float16"), )

[accuracy error] paddle.var(x=Tensor([477218589, 3, 3],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array(nan, dtype=float16)
 y: array(0.0833, dtype=float16)
2025-04-22 13:46:04.374375 test begin: paddle.var(x=Tensor([477218589, 3, 3],"float16"), axis=0, )

[accuracy error] paddle.var(x=Tensor([477218589, 3, 3],"float16"), axis=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[nan, nan, nan],
       [nan, nan, nan],
       [nan, nan, nan]], dtype=float16)
 y: array([[0.0833, 0.0833, 0.0833],
       [0.0833, 0.0833, 0.0833],
       [0.0833, 0.0833, 0.0833]], dtype=float16)
2025-04-22 13:46:19.446967 test begin: paddle.var(x=Tensor([477218589, 3, 3],"float16"), axis=0, unbiased=False, )

[accuracy error] paddle.var(x=Tensor([477218589, 3, 3],"float16"), axis=0, unbiased=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[nan, nan, nan],
       [nan, nan, nan],
       [nan, nan, nan]], dtype=float16)
 y: array([[0.0833, 0.0833, 0.0833],
       [0.0833, 0.0833, 0.0833],
       [0.0833, 0.0833, 0.0833]], dtype=float16)
2025-04-22 13:46:34.553572 test begin: paddle.var(x=Tensor([477218589, 3, 3],"float16"), axis=list[0,1,], )

[accuracy error] paddle.var(x=Tensor([477218589, 3, 3],"float16"), axis=list[0,1,], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([nan, nan, nan], dtype=float16)
 y: array([0.0833, 0.0833, 0.0833], dtype=float16)
2025-04-22 13:46:45.130839 test begin: paddle.var(x=Tensor([477218589, 3, 3],"float16"), axis=tuple(0,1,), )

[accuracy error] paddle.var(x=Tensor([477218589, 3, 3],"float16"), axis=tuple(0,1,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([nan, nan, nan], dtype=float16)
 y: array([0.0833, 0.0833, 0.0833], dtype=float16)
2025-04-22 13:46:56.626045 test begin: paddle.var(x=Tensor([477218589, 3, 3],"float16"), axis=tuple(0,1,), keepdim=True, )

[accuracy error] paddle.var(x=Tensor([477218589, 3, 3],"float16"), axis=tuple(0,1,), keepdim=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[nan, nan, nan]]], dtype=float16)
 y: array([[[0.0833, 0.0833, 0.0833]]], dtype=float16)
2025-04-22 13:48:53.331707 test begin: paddle.nn.functional.normalize(Tensor([1, 128, 32, 557057],"float32"), axis=1, )

W0422 13:50:17.656090 143438 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 13:50:17.657389 143438 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745301121 (unix time) try "date -d @1745301121" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x22fc7) received by PID 143303 (TID 0x7f5a091f7700) from PID 143303 ***]

2025-04-22 13:52:43.996248 test begin: paddle.nn.functional.normalize(Tensor([1, 128, 557057, 32],"float32"), axis=1, )

W0422 13:54:09.463061 148127 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 13:54:09.464164 148127 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745301317 (unix time) try "date -d @1745301317" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x24212) received by PID 147986 (TID 0x7f2818949700) from PID 147986 ***]

2025-04-22 13:56:02.143042 test begin: paddle.nn.functional.normalize(Tensor([1, 2228225, 32, 32],"float32"), axis=1, )

W0422 13:57:25.378444 151785 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 13:57:25.379814 151785 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   void phi::ReduceWrapper<float>(phi::GPUContext const&, int, phi::DenseTensor*, phi::DenseTensor*)
7   void phi::SumKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
8   void phi::SumRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
9   void phi::funcs::ReduceKernel<float, float, phi::kps::AddFunctor, phi::kps::IdentityFunctor<float, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<float, float> const&, std::vector<int, std::allocator<int> > const&)
10  phi::DenseTensor::~DenseTensor()
11  std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
12  paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745301522 (unix time) try "date -d @1745301522" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2506f) received by PID 151663 (TID 0x7efd9f6f8700) from PID 151663 ***]

2025-04-22 13:58:47.735566 test begin: paddle.nn.functional.normalize(Tensor([1, 2281701379],"float32"), axis=1, )

W0422 14:00:12.526693 155546 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 14:00:12.527874 155546 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.nn.functional.normalize(Tensor([1, 2281701379],"float32"), axis=1, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-22 14:00:13.287152 test begin: paddle.nn.functional.normalize(Tensor([1, 256, 16, 557057],"float32"), axis=1, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745301709 (unix time) try "date -d @1745301709" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x25f1f) received by PID 155423 (TID 0x7f428babb700) from PID 155423 ***]

2025-04-22 14:02:32.670046 test begin: paddle.nn.functional.normalize(Tensor([1, 256, 557057, 16],"float32"), axis=1, )

W0422 14:03:56.473577 160276 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 14:03:56.474779 160276 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745301905 (unix time) try "date -d @1745301905" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x27194) received by PID 160148 (TID 0x7f9a5a949700) from PID 160148 ***]

2025-04-22 14:05:49.110388 test begin: paddle.nn.functional.normalize(Tensor([1, 557057, 64, 64],"float32"), axis=1, )

W0422 14:07:30.806025  1692 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 14:07:30.808020  1692 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   void phi::ReduceWrapper<float>(phi::GPUContext const&, int, phi::DenseTensor*, phi::DenseTensor*)
7   void phi::SumKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
8   void phi::SumRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
9   void phi::funcs::ReduceKernel<float, float, phi::kps::AddFunctor, phi::kps::IdentityFunctor<float, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<float, float> const&, std::vector<int, std::allocator<int> > const&)
10  phi::DenseTensor::~DenseTensor()
11  std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
12  paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745302135 (unix time) try "date -d @1745302135" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x602) received by PID 1538 (TID 0x7f53772b7700) from PID 1538 ***]

2025-04-22 14:09:38.428152 test begin: paddle.nn.functional.normalize(Tensor([1, 64, 557057, 64],"float32"), axis=1, )

W0422 14:11:03.731040  6289 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 14:11:03.732144  6289 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745302330 (unix time) try "date -d @1745302330" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1809) received by PID 6153 (TID 0x7f7a6ddc2700) from PID 6153 ***]

2025-04-22 14:12:55.499449 test begin: paddle.nn.functional.normalize(Tensor([1, 64, 64, 557057],"float32"), axis=1, )

W0422 14:14:24.311411 10618 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 14:14:24.312584 10618 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745302560 (unix time) try "date -d @1745302560" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2902) received by PID 10498 (TID 0x7fc44b87e700) from PID 10498 ***]

2025-04-22 14:16:45.373910 test begin: paddle.nn.functional.normalize(Tensor([1, 8912897, 16, 16],"float32"), axis=1, )

W0422 14:18:09.445698 15188 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 14:18:09.446918 15188 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   void phi::ReduceWrapper<float>(phi::GPUContext const&, int, phi::DenseTensor*, phi::DenseTensor*)
7   void phi::SumKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
8   void phi::SumRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
9   void phi::funcs::ReduceKernel<float, float, phi::kps::AddFunctor, phi::kps::IdentityFunctor<float, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<float, float> const&, std::vector<int, std::allocator<int> > const&)
10  phi::DenseTensor::~DenseTensor()
11  std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
12  paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745302766 (unix time) try "date -d @1745302766" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x3ae4) received by PID 15076 (TID 0x7fc829f48700) from PID 15076 ***]

2025-04-22 14:20:10.746312 test begin: paddle.nn.functional.normalize(Tensor([10, 228170138],"float32"), )

W0422 14:21:36.902522 19219 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 14:21:36.903961 19219 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   void phi::ReduceWrapper<float>(phi::GPUContext const&, int, phi::DenseTensor*, phi::DenseTensor*)
7   void phi::SumKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
8   void phi::SumRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
9   void phi::funcs::ReduceKernel<float, float, phi::kps::AddFunctor, phi::kps::IdentityFunctor<float, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<float, float> const&, std::vector<int, std::allocator<int> > const&)
10  phi::DenseTensor::~DenseTensor()
11  std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
12  paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745303099 (unix time) try "date -d @1745303099" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x4aa0) received by PID 19104 (TID 0x7f57547c3700) from PID 19104 ***]

2025-04-22 14:25:44.298338 test begin: paddle.nn.functional.normalize(Tensor([10, 228170138],"float32"), axis=0, )

W0422 14:27:08.377633 27505 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 14:27:08.378894 27505 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745303296 (unix time) try "date -d @1745303296" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x6acf) received by PID 27343 (TID 0x7f9539f48700) from PID 27343 ***]

2025-04-22 14:28:59.755174 test begin: paddle.nn.functional.normalize(Tensor([10, 228170138],"float32"), p=1.5, )

W0422 14:30:27.510129 33799 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 14:30:27.511276 33799 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   void phi::ReduceWrapper<float>(phi::GPUContext const&, int, phi::DenseTensor*, phi::DenseTensor*)
7   void phi::SumKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
8   void phi::SumRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
9   void phi::funcs::ReduceKernel<float, float, phi::kps::AddFunctor, phi::kps::IdentityFunctor<float, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<float, float> const&, std::vector<int, std::allocator<int> > const&)
10  phi::DenseTensor::~DenseTensor()
11  std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
12  paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745303618 (unix time) try "date -d @1745303618" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x837f) received by PID 33663 (TID 0x7fb15df48700) from PID 33663 ***]

2025-04-22 14:34:23.253046 test begin: paddle.nn.functional.normalize(Tensor([10, 429496730],"float16"), )

W0422 14:35:59.339541 44960 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 14:35:59.340680 44960 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.nn.functional.normalize(Tensor([10, 429496730],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2343049501 / 4294967300 (54.6%)
Max absolute difference: 0.02202
Max relative difference: 395.
 x: array([[-0.000479,  0.013954, -0.012375, ..., -0.004112,  0.005707,
         0.005188],
       [-0.01135 , -0.02142 , -0.01987 , ...,  0.006233,  0.01055 ,...
 y: array([[-1.788e-06,  5.275e-05, -4.679e-05, ..., -1.556e-05,  2.158e-05,
         1.961e-05],
       [-4.292e-05, -8.100e-05, -7.510e-05, ...,  2.354e-05,  3.988e-05,...
2025-04-22 14:59:35.538961 test begin: paddle.nn.functional.normalize(Tensor([1006, 2268093],"float32"), )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   void phi::ReduceWrapper<float>(phi::GPUContext const&, int, phi::DenseTensor*, phi::DenseTensor*)
7   void phi::SumKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
8   void phi::SumRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
9   void phi::funcs::ReduceKernel<float, float, phi::kps::AddFunctor, phi::kps::IdentityFunctor<float, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<float, float> const&, std::vector<int, std::allocator<int> > const&)
10  phi::DenseTensor::~DenseTensor()
11  std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
12  paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745305335 (unix time) try "date -d @1745305335" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xae7c) received by PID 44668 (TID 0x7ff5707c3700) from PID 44668 ***]

2025-04-22 15:03:00.775731 test begin: paddle.nn.functional.normalize(Tensor([1006, 4269352],"float16"), )

W0422 15:04:39.225682 99190 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 15:04:39.226895 99190 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.nn.functional.normalize(Tensor([1006, 4269352],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2271796009 / 4294968112 (52.9%)
Max absolute difference: 0.02126
Max relative difference: 39.
 x: array([[-0.01784 ,  0.007614, -0.01671 , ..., -0.000786,  0.01682 ,
        -0.0163  ],
       [-0.00483 , -0.01309 ,  0.01164 , ..., -0.00811 ,  0.007385,...
 y: array([[-6.766e-04,  2.887e-04, -6.342e-04, ..., -2.980e-05,  6.380e-04,
        -6.180e-04],
       [-1.831e-04, -4.964e-04,  4.416e-04, ..., -3.076e-04,  2.801e-04,...
2025-04-22 15:22:46.104160 test begin: paddle.nn.functional.normalize(Tensor([11883862, 192],"float32"), axis=1, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745306715 (unix time) try "date -d @1745306715" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x182f6) received by PID 99062 (TID 0x7f9c28949700) from PID 99062 ***]

2025-04-22 15:25:57.868943 test begin: paddle.nn.functional.normalize(Tensor([12, 190141782],"float32"), axis=-1, )

W0422 15:27:19.704741 141137 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 15:27:19.705965 141137 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   void phi::ReduceWrapper<float>(phi::GPUContext const&, int, phi::DenseTensor*, phi::DenseTensor*)
7   void phi::SumKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
8   void phi::SumRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
9   void phi::funcs::ReduceKernel<float, float, phi::kps::AddFunctor, phi::kps::IdentityFunctor<float, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<float, float> const&, std::vector<int, std::allocator<int> > const&)
10  phi::DenseTensor::~DenseTensor()
11  std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
12  paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745307013 (unix time) try "date -d @1745307013" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x226b3) received by PID 140979 (TID 0x7fea09dc2700) from PID 140979 ***]

2025-04-22 15:30:57.368877 test begin: paddle.nn.functional.normalize(Tensor([17409, 128, 32, 32],"float32"), axis=1, )

W0422 15:32:23.395557 151464 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 15:32:23.396992 151464 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745307223 (unix time) try "date -d @1745307223" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x24e68) received by PID 151144 (TID 0x7faf59b85700) from PID 151144 ***]

2025-04-22 15:34:30.547815 test begin: paddle.nn.functional.normalize(Tensor([2, 16297867, 7, 10],"float32"), axis=1, )

W0422 15:35:55.871129 157847 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 15:35:55.872289 157847 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   void phi::ReduceWrapper<float>(phi::GPUContext const&, int, phi::DenseTensor*, phi::DenseTensor*)
7   void phi::SumKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
8   void phi::SumRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
9   void phi::funcs::ReduceKernel<float, float, phi::kps::AddFunctor, phi::kps::IdentityFunctor<float, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<float, float> const&, std::vector<int, std::allocator<int> > const&)
10  phi::DenseTensor::~DenseTensor()
11  std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
12  paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745307455 (unix time) try "date -d @1745307455" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x26828) received by PID 157736 (TID 0x7f7dc1b85700) from PID 157736 ***]

2025-04-22 15:39:45.058368 test begin: paddle.nn.functional.normalize(Tensor([2, 2147483649],"float16"), p=2, axis=-1, )

W0422 15:41:24.686522  4165 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 15:41:24.687666  4165 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.nn.functional.normalize(Tensor([2, 2147483649],"float16"), p=2, axis=-1, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-22 15:41:25.574737 test begin: paddle.nn.functional.normalize(Tensor([2, 8, 14260634, 10],"float32"), axis=1, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745307872 (unix time) try "date -d @1745307872" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xfb8) received by PID 4024 (TID 0x7f4f87f48700) from PID 4024 ***]

2025-04-22 15:45:20.468392 test begin: paddle.nn.functional.normalize(Tensor([2, 8, 7, 20372334],"float32"), axis=1, )

W0422 15:47:03.131768 14858 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 15:47:03.133780 14858 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745308104 (unix time) try "date -d @1745308104" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x3986) received by PID 14726 (TID 0x7f9a87dc2700) from PID 14726 ***]

2025-04-22 15:49:09.880747 test begin: paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 171798692, 1, 1],"float16"), list[list[3,1,2,0,],list[1,2,3,0,],], )

/usr/local/lib/python3.9/dist-packages/torch/autograd/graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:135.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
W0422 15:50:45.354084 22392 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 15:50:45.355144 22392 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] backward  paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 171798692, 1, 1],"float16"), list[list[3,1,2,0,],list[1,2,3,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967300 / 4294967300 (100%)
Max absolute difference: 86.25
Max relative difference: 0.865
 x: array([[[[-13.46],
         [-13.46],
         [-13.46],...
 y: array([[[[-99.7],
         [-99.7],
         [-99.7],...
2025-04-22 16:02:46.250941 test begin: paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 171798692, 1, 1],"float16"), list[list[3,1,2,],list[1,2,3,],], )

[accuracy error] backward  paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 171798692, 1, 1],"float16"), list[list[3,1,2,],list[1,2,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967300 / 4294967300 (100%)
Max absolute difference: 2816.
Max relative difference: 0.865
 x: array([[[[ 283.  ],
         [ 283.  ],
         [ 283.  ],...
 y: array([[[[ 2096. ],
         [ 2096. ],
         [ 2096. ],...
2025-04-22 16:14:51.623989 test begin: paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 171798692, 1, 1],"float16"), list[list[3,1,2,],list[1,3,2,],], )

[accuracy error] backward  paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 171798692, 1, 1],"float16"), list[list[3,1,2,],list[1,3,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967300 / 4294967300 (100%)
Max absolute difference: 2816.
Max relative difference: 0.865
 x: array([[[[ 283.  ],
         [ 283.  ],
         [ 283.  ],...
 y: array([[[[ 2096. ],
         [ 2096. ],
         [ 2096. ],...
2025-04-22 18:13:49.987117 test begin: paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([5, 171798692, 1, 5],"float16"), list[list[2,3,0,],list[3,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([5, 171798692, 1, 5],"float16"), list[list[2,3,0,],list[3,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 171798478 / 171798692 (100%)
Max absolute difference: 32864.
Max relative difference: 1.
 x: array([[-4096.],
       [    0.],
       [    0.],...
 y: array([[-2506. ],
       [-4972. ],
       [ 4412. ],...
2025-04-23 10:41:27.133937 test begin: paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[0,2,3,],list[0,1,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[0,2,3,],list[0,1,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 842111043 / 858993460 (98%)
Max absolute difference: 2.15
Max relative difference: 1.835
 x: array([[ 0.1866 , -0.01958,  0.0413 , -0.12335, -0.0553 ],
       [ 0.     ,  0.     ,  0.     ,  0.     ,  0.     ],
       [ 0.     ,  0.     ,  0.     ,  0.     ,  0.     ],...
 y: array([[ 0.8213 , -0.1908 , -0.566  ,  0.1477 , -0.2046 ],
       [ 0.5405 , -0.4338 ,  0.2014 , -0.6406 ,  0.0926 ],
       [-0.1823 , -0.6724 ,  0.81   ,  0.2332 , -1.052  ],...
2025-04-23 11:18:44.485219 test begin: paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[2,3,0,],list[1,2,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[2,3,0,],list[1,2,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 842111030 / 858993460 (98%)
Max absolute difference: 2.15
Max relative difference: 1.276
 x: array([[ 0.0698 , -0.03635, -0.07935,  0.2527 ,  0.05655],
       [ 0.     ,  0.     ,  0.     ,  0.     ,  0.     ],
       [ 0.     ,  0.     ,  0.     ,  0.     ,  0.     ],...
 y: array([[ 0.8213 , -0.1908 , -0.566  ,  0.1477 , -0.2046 ],
       [ 0.5405 , -0.4338 ,  0.2014 , -0.6406 ,  0.0926 ],
       [-0.1823 , -0.6724 ,  0.81   ,  0.2332 , -1.052  ],...
2025-04-23 11:21:35.203546 test begin: paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[2,3,0,],list[3,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[2,3,0,],list[3,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 170230604 / 171798692 (99.1%)
Max absolute difference: 4.26
Max relative difference: 1.
 x: array([[0.678],
       [0.   ],
       [0.   ],...
 y: array([[ 0.5737 ],
       [ 0.6206 ],
       [ 0.32   ],...
2025-04-23 11:56:55.147078 test begin: paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[3,2,0,],list[2,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[3,2,0,],list[2,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 842111030 / 858993460 (98%)
Max absolute difference: 2.15
Max relative difference: 1.276
 x: array([[ 0.0698 , -0.03635, -0.07935,  0.2527 ,  0.05655],
       [ 0.     ,  0.     ,  0.     ,  0.     ,  0.     ],
       [ 0.     ,  0.     ,  0.     ,  0.     ,  0.     ],...
 y: array([[ 0.8213 , -0.1908 , -0.566  ,  0.1477 , -0.2046 ],
       [ 0.5405 , -0.4338 ,  0.2014 , -0.6406 ,  0.0926 ],
       [-0.1823 , -0.6724 ,  0.81   ,  0.2332 , -1.052  ],...
2025-04-23 11:59:36.556318 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[0,1,2,],list[2,0,1,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[0,1,2,],list[2,0,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 2576.
Max relative difference: 17.11
 x: array([[2726.]], dtype=float16)
 y: array([[150.6]], dtype=float16)
2025-04-23 11:59:52.212637 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[0,1,3,2,],list[0,2,1,3,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[0,1,3,2,],list[0,2,1,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 2576.
Max relative difference: 17.11
 x: array(2726., dtype=float16)
 y: array(150.6, dtype=float16)
2025-04-23 12:00:03.550279 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[0,1,3,2,],list[1,3,0,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[0,1,3,2,],list[1,3,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 2576.
Max relative difference: 17.11
 x: array(2726., dtype=float16)
 y: array(150.6, dtype=float16)
2025-04-23 11:29:36.251614 test begin: paddle.roll(Tensor([1, 38044, 21, 14, 384],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 38044, 21, 14, 384],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 62978 / 4295015424 (0.00147%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[-3.8452e-01, -4.7412e-01, -5.9570e-02, ...,  3.2495e-01,
           -4.5380e-02,  8.1116e-02],
          [-2.4805e-01,  3.7573e-01,  1.1017e-01, ...,  3.6182e-01,...
 y: array([[[[[-0.3845  , -0.474   , -0.05957 , ...,  0.325   , -0.04538 ,
            0.0811  ],
          [-0.248   ,  0.3757  ,  0.11017 , ...,  0.3618  , -0.1403  ,...
2025-04-23 11:42:48.184782 test begin: paddle.roll(Tensor([1, 57066, 14, 14, 384],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 57066, 14, 14, 384],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 63009 / 4295015424 (0.00147%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[-2.7075e-01,  4.4165e-01,  2.5360e-02, ...,  4.1528e-01,
           -3.7939e-01, -4.3384e-01],
          [ 1.7136e-02, -4.3213e-01,  2.8491e-01, ...,  1.1230e-01,...
 y: array([[[[[-2.7075e-01,  4.4165e-01,  2.5360e-02, ...,  4.1528e-01,
           -3.7939e-01, -4.3384e-01],
          [ 1.7136e-02, -4.3213e-01,  2.8491e-01, ...,  1.1230e-01,...
2025-04-23 11:55:48.326464 test begin: paddle.roll(Tensor([1, 57066, 14, 14, 384],"float16"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 57066, 14, 14, 384],"float16"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 63204 / 4295015424 (0.00147%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[-0.3018  , -0.2277  ,  0.1062  , ..., -0.05338 , -0.3943  ,
            0.1299  ],
          [ 0.08984 , -0.143   , -0.2793  , ...,  0.2294  ,  0.309   ,...
 y: array([[[[[-0.3018  , -0.2277  ,  0.1062  , ..., -0.05338 , -0.3943  ,
            0.1299  ],
          [ 0.08984 , -0.143   , -0.2793  , ...,  0.2294  ,  0.309   ,...
2025-04-23 12:08:56.575348 test begin: paddle.roll(Tensor([1, 57066, 14, 7, 768],"float16"), shifts=tuple(-4,-3,0,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 57066, 14, 7, 768],"float16"), shifts=tuple(-4,-3,0,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 63009 / 4295015424 (0.00147%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[-2.1057e-01,  2.7588e-01,  1.9849e-01, ..., -4.3213e-01,
            9.9564e-03,  9.4910e-02],
          [-3.2812e-01,  2.3462e-01, -2.0508e-01, ...,  4.1528e-01,...
 y: array([[[[[-2.1057e-01,  2.7588e-01,  1.9849e-01, ..., -4.3213e-01,
            9.9564e-03,  9.4910e-02],
          [-3.2812e-01,  2.3462e-01, -2.0508e-01, ...,  4.1528e-01,...
2025-04-23 12:22:19.682905 test begin: paddle.roll(Tensor([1, 57066, 14, 7, 768],"float16"), shifts=tuple(4,3,0,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 57066, 14, 7, 768],"float16"), shifts=tuple(4,3,0,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 62951 / 4295015424 (0.00147%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[-0.4673  ,  0.4473  , -0.0775  , ..., -0.4841  , -0.195   ,
            0.2988  ],
          [ 0.2065  ,  0.2834  , -0.376   , ..., -0.02992 , -0.1382  ,...
 y: array([[[[[-0.4673  ,  0.4473  , -0.0775  , ..., -0.4841  , -0.195   ,
            0.2988  ],
          [ 0.2065  ,  0.2834  , -0.376   , ..., -0.02992 , -0.1382  ,...
2025-04-23 12:35:40.328851 test begin: paddle.roll(Tensor([1, 38044, 14, 21, 384],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), )

W0423 12:37:24.803517 98391 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0423 12:37:24.805080 98391 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.roll(Tensor([1, 38044, 14, 21, 384],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 70858 / 4295015424 (0.00165%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[-0.4768  , -0.3135  , -0.438   , ..., -0.4197  ,  0.146   ,
            0.002043],
          [-0.372   , -0.261   , -0.2974  , ..., -0.3816  , -0.3481  ,...
 y: array([[[[[-0.4768  , -0.3135  , -0.438   , ..., -0.4197  ,  0.146   ,
            0.002043],
          [-0.372   , -0.261   , -0.2974  , ..., -0.3816  , -0.3481  ,...
2025-04-21 14:35:16.379479 test begin: paddle.nn.functional.normalize(Tensor([207427399, 11],"float32"), axis=1, )

W0421 14:36:41.711298 100883 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 14:36:41.712532 100883 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745217471 (unix time) try "date -d @1745217471" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x18977) received by PID 100727 (TID 0x7f560e7c3700) from PID 100727 ***]

2025-04-21 14:38:37.071045 test begin: paddle.nn.functional.normalize(Tensor([2281701379],"float32"), axis=0, )

W0421 14:40:07.454084 104535 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 14:40:07.455313 104535 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.nn.functional.normalize(Tensor([2281701379],"float32"), axis=0, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 14:40:08.278120 test begin: paddle.nn.functional.normalize(Tensor([2281701379],"float32"), axis=0, epsilon=1e-10, )

[paddle error] paddle.nn.functional.normalize(Tensor([2281701379],"float32"), axis=0, epsilon=1e-10, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 14:40:30.740250 test begin: paddle.nn.functional.normalize(Tensor([2281701379],"float32"), axis=0, epsilon=1e-12, )

[paddle error] paddle.nn.functional.normalize(Tensor([2281701379],"float32"), axis=0, epsilon=1e-12, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 14:40:51.099137 test begin: paddle.nn.functional.normalize(Tensor([228170138, 10],"float32"), )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745217743 (unix time) try "date -d @1745217743" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x197e9) received by PID 104425 (TID 0x7f6d332b7700) from PID 104425 ***]

2025-04-21 14:43:07.481430 test begin: paddle.nn.functional.normalize(Tensor([228170138, 10],"float32"), axis=0, )

W0421 14:44:32.416666 109311 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 14:44:32.417809 109311 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   void phi::ReduceWrapper<float>(phi::GPUContext const&, int, phi::DenseTensor*, phi::DenseTensor*)
7   void phi::SumKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
8   void phi::SumRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
9   void phi::funcs::ReduceKernel<float, float, phi::kps::AddFunctor, phi::kps::IdentityFunctor<float, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<float, float> const&, std::vector<int, std::allocator<int> > const&)
10  phi::DenseTensor::~DenseTensor()
11  std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
12  paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745218082 (unix time) try "date -d @1745218082" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1aa9a) received by PID 109210 (TID 0x7f28207c3700) from PID 109210 ***]

2025-04-21 14:48:47.032484 test begin: paddle.nn.functional.normalize(Tensor([228170138, 10],"float32"), p=1.5, )

W0421 14:50:13.327579 115181 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 14:50:13.328763 115181 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745218292 (unix time) try "date -d @1745218292" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1c185) received by PID 115077 (TID 0x7fe98e949700) from PID 115077 ***]

2025-04-21 14:52:19.693188 test begin: paddle.nn.functional.normalize(Tensor([253522376, 9],"float32"), axis=1, )

W0421 14:53:56.531908 118469 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 14:53:56.533185 118469 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745218530 (unix time) try "date -d @1745218530" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1ce52) received by PID 118354 (TID 0x7fa1cc7c3700) from PID 118354 ***]

2025-04-21 14:56:15.653299 test begin: paddle.nn.functional.normalize(Tensor([2970966, 768],"float32"), axis=-1, )

W0421 14:57:40.601234 122314 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 14:57:40.602463 122314 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745218757 (unix time) try "date -d @1745218757" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1dd50) received by PID 122192 (TID 0x7f469ff48700) from PID 122192 ***]

2025-04-21 15:00:01.541330 test begin: paddle.nn.functional.normalize(Tensor([325957340, 7],"float32"), axis=0, )

W0421 15:01:48.639912 126345 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 15:01:48.641232 126345 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   void phi::ReduceWrapper<float>(phi::GPUContext const&, int, phi::DenseTensor*, phi::DenseTensor*)
7   void phi::SumKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
8   void phi::SumRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
9   void phi::funcs::ReduceKernel<float, float, phi::kps::AddFunctor, phi::kps::IdentityFunctor<float, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<float, float> const&, std::vector<int, std::allocator<int> > const&)
10  phi::DenseTensor::~DenseTensor()
11  std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
12  paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745219182 (unix time) try "date -d @1745219182" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1ed1a) received by PID 126234 (TID 0x7f40707c3700) from PID 126234 ***]

2025-04-21 15:07:09.883559 test begin: paddle.nn.functional.normalize(Tensor([325957340, 7],"float32"), axis=1, )

W0421 15:08:36.687214 133281 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 15:08:36.688314 133281 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745219387 (unix time) try "date -d @1745219387" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x20824) received by PID 133156 (TID 0x7fdeb7dc2700) from PID 133156 ***]

2025-04-21 15:10:34.074799 test begin: paddle.nn.functional.normalize(Tensor([34817, 256, 16, 16],"float32"), axis=1, )

W0421 15:12:09.301632 136667 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 15:12:09.302836 136667 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745219605 (unix time) try "date -d @1745219605" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2155b) received by PID 136539 (TID 0x7f23ef4f4700) from PID 136539 ***]

2025-04-21 15:14:11.393650 test begin: paddle.nn.functional.normalize(Tensor([35651585, 64],"float32"), axis=-1, )

W0421 15:15:54.836890 140444 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 15:15:54.838030 140444 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745219846 (unix time) try "date -d @1745219846" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x22435) received by PID 140341 (TID 0x7f42fff48700) from PID 140341 ***]

2025-04-21 15:18:08.235997 test begin: paddle.nn.functional.normalize(Tensor([35651585, 64],"float32"), axis=1, )

W0421 15:19:49.519296 144444 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 15:19:49.520524 144444 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745220077 (unix time) try "date -d @1745220077" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x233b8) received by PID 144312 (TID 0x7f48d10b7700) from PID 144312 ***]

2025-04-21 15:22:03.470114 test begin: paddle.nn.functional.normalize(Tensor([4, 570425345],"float32"), axis=0, )

W0421 15:23:48.806149 148406 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 15:23:48.807264 148406 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745220307 (unix time) try "date -d @1745220307" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2433f) received by PID 148287 (TID 0x7fb231935700) from PID 148287 ***]

2025-04-21 15:25:55.601840 test begin: paddle.nn.functional.normalize(Tensor([4074467, 8, 7, 10],"float32"), axis=1, )

W0421 15:27:21.403636 152330 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 15:27:21.404821 152330 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745220517 (unix time) try "date -d @1745220517" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x25288) received by PID 152200 (TID 0x7fa4e36f8700) from PID 152200 ***]

2025-04-21 15:29:21.602561 test begin: paddle.nn.functional.normalize(Tensor([4194305, 1024],"float16"), p=2, axis=-1, )

W0421 15:31:01.682938 155943 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 15:31:01.684109 155943 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<phi::dtype::float16, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745221198 (unix time) try "date -d @1745221198" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x260b9) received by PID 155833 (TID 0x7effdd87e700) from PID 155833 ***]

2025-04-21 15:44:54.541341 test begin: paddle.nn.functional.normalize(Tensor([4456449, 512],"float32"), )

W0421 15:46:19.529654  7850 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 15:46:19.530807  7850 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745221656 (unix time) try "date -d @1745221656" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1e3d) received by PID 7741 (TID 0x7f39e74f4700) from PID 7741 ***]

2025-04-21 15:48:21.287802 test begin: paddle.nn.functional.normalize(Tensor([45, 50704476],"float32"), axis=0, )

W0421 15:49:53.680811 11316 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 15:49:53.681851 11316 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745221874 (unix time) try "date -d @1745221874" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2bda) received by PID 11226 (TID 0x7efb32949700) from PID 11226 ***]

2025-04-21 15:51:20.257025 test begin: paddle.nn.functional.normalize(Tensor([456340276, 5],"float32"), axis=0, )

W0421 15:52:52.054497 14774 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 15:52:52.055752 14774 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   void phi::ReduceWrapper<float>(phi::GPUContext const&, int, phi::DenseTensor*, phi::DenseTensor*)
7   void phi::SumKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
8   void phi::SumRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
9   void phi::funcs::ReduceKernel<float, float, phi::kps::AddFunctor, phi::kps::IdentityFunctor<float, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<float, float> const&, std::vector<int, std::allocator<int> > const&)
10  phi::DenseTensor::~DenseTensor()
11  std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
12  paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745222286 (unix time) try "date -d @1745222286" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x3936) received by PID 14646 (TID 0x7fcb49f48700) from PID 14646 ***]

2025-04-21 15:58:12.604609 test begin: paddle.nn.functional.normalize(Tensor([570425345, 4],"float32"), axis=0, )

W0421 15:59:48.461874 21286 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 15:59:48.463841 21286 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   void phi::ReduceWrapper<float>(phi::GPUContext const&, int, phi::DenseTensor*, phi::DenseTensor*)
7   void phi::SumKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
8   void phi::SumRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
9   void phi::funcs::ReduceKernel<float, float, phi::kps::AddFunctor, phi::kps::IdentityFunctor<float, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<float, float> const&, std::vector<int, std::allocator<int> > const&)
10  phi::DenseTensor::~DenseTensor()
11  std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
12  paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745222733 (unix time) try "date -d @1745222733" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x52c4) received by PID 21188 (TID 0x7f0a9a7c3700) from PID 21188 ***]

2025-04-21 16:06:16.010487 test begin: paddle.nn.functional.normalize(Tensor([60, 38028357],"float32"), axis=0, )

W0421 16:07:47.383066 29696 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 16:07:47.384223 29696 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745222960 (unix time) try "date -d @1745222960" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x7381) received by PID 29569 (TID 0x7f69ab34a700) from PID 29569 ***]

2025-04-21 16:10:05.945038 test begin: paddle.nn.functional.normalize(Tensor([760567127, 3],"float32"), axis=0, )

W0421 16:11:38.146472 33744 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 16:11:38.147933 33744 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   void phi::ReduceWrapper<float>(phi::GPUContext const&, int, phi::DenseTensor*, phi::DenseTensor*)
7   void phi::SumKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
8   void phi::SumRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
9   void phi::funcs::ReduceKernel<float, float, phi::kps::AddFunctor, phi::kps::IdentityFunctor<float, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<float, float> const&, std::vector<int, std::allocator<int> > const&)
10  phi::DenseTensor::~DenseTensor()
11  std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
12  paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745223497 (unix time) try "date -d @1745223497" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x8353) received by PID 33619 (TID 0x7fb08bf48700) from PID 33619 ***]

2025-04-21 16:19:02.041290 test begin: paddle.nn.functional.normalize(Tensor([80, 28521268],"float32"), axis=-1, )

W0421 16:20:25.936622 42325 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 16:20:25.937820 42325 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   void phi::ReduceWrapper<float>(phi::GPUContext const&, int, phi::DenseTensor*, phi::DenseTensor*)
7   void phi::SumKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
8   void phi::SumRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
9   void phi::funcs::ReduceKernel<float, float, phi::kps::AddFunctor, phi::kps::IdentityFunctor<float, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<float, float> const&, std::vector<int, std::allocator<int> > const&)
10  phi::DenseTensor::~DenseTensor()
11  std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
12  paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745223707 (unix time) try "date -d @1745223707" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xa4f0) received by PID 42224 (TID 0x7f8fbd2b7700) from PID 42224 ***]

2025-04-21 16:22:27.972915 test begin: paddle.nn.functional.normalize(Tensor([8388609, 512],"float16"), )

W0421 16:24:25.325989 45727 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 16:24:25.327158 45727 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<phi::dtype::float16, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745224387 (unix time) try "date -d @1745224387" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xb21d) received by PID 45597 (TID 0x7f4b49dc2700) from PID 45597 ***]

2025-04-21 16:33:48.394552 test begin: paddle.nn.functional.normalize(Tensor([8705, 64, 64, 64],"float32"), axis=1, )

W0421 16:35:14.453285 57409 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 16:35:14.454408 57409 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745224583 (unix time) try "date -d @1745224583" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xdfc4) received by PID 57284 (TID 0x7f4412949700) from PID 57284 ***]

2025-04-21 16:37:04.974406 test begin: paddle.nn.functional.normalize(x=Tensor([1, 2281701379],"float32"), axis=-1, )

W0421 16:38:30.407920 60616 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 16:38:30.408994 60616 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.nn.functional.normalize(x=Tensor([1, 2281701379],"float32"), axis=-1, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 16:38:31.269537 test begin: paddle.nn.functional.normalize(x=Tensor([1073741825, 4],"float16"), )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<phi::dtype::float16, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745225281 (unix time) try "date -d @1745225281" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xec43) received by PID 60483 (TID 0x7feb23abb700) from PID 60483 ***]

2025-04-21 16:48:46.791690 test begin: paddle.nn.functional.normalize(x=Tensor([143165577, 5, 6],"float16"), )

W0421 16:50:25.959733 72057 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 16:50:25.960795 72057 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<phi::dtype::float16, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745225906 (unix time) try "date -d @1745225906" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1190e) received by PID 71950 (TID 0x7f845ddc2700) from PID 71950 ***]

2025-04-21 16:59:07.753764 test begin: paddle.nn.functional.normalize(x=Tensor([2, 1140850690],"float32"), )

W0421 17:00:31.370251 81911 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 17:00:31.371449 81911 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   void phi::ReduceWrapper<float>(phi::GPUContext const&, int, phi::DenseTensor*, phi::DenseTensor*)
7   void phi::SumKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
8   void phi::SumRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
9   void phi::funcs::ReduceKernel<float, float, phi::kps::AddFunctor, phi::kps::IdentityFunctor<float, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<float, float> const&, std::vector<int, std::allocator<int> > const&)
10  phi::DenseTensor::~DenseTensor()
11  std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
12  paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745226542 (unix time) try "date -d @1745226542" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x13f94) received by PID 81812 (TID 0x7f034d87e700) from PID 81812 ***]

2025-04-21 17:09:44.744350 test begin: paddle.nn.functional.normalize(x=Tensor([2, 2147483649],"float16"), )

W0421 17:11:25.689186 92709 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 17:11:25.690266 92709 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.nn.functional.normalize(x=Tensor([2, 2147483649],"float16"), ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 17:11:26.749326 test begin: paddle.nn.functional.normalize(x=Tensor([20452226, 5, 6, 7],"float16"), )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<phi::dtype::float16, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745227198 (unix time) try "date -d @1745227198" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x169a7) received by PID 92583 (TID 0x7f2206949700) from PID 92583 ***]

2025-04-21 17:20:44.424259 test begin: paddle.nn.functional.normalize(x=Tensor([20452226, 5, 6, 7],"float16"), p=1, )

W0421 17:22:23.351817 103305 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 17:22:23.352973 103305 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<phi::dtype::float16, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745227835 (unix time) try "date -d @1745227835" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x19304) received by PID 103172 (TID 0x7f79776f8700) from PID 103172 ***]

2025-04-21 17:31:17.324598 test begin: paddle.nn.functional.normalize(x=Tensor([20452226, 5, 6, 7],"float16"), p=4, )

W0421 17:33:11.463725 113779 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 17:33:11.464862 113779 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.nn.functional.normalize(x=Tensor([20452226, 5, 6, 7],"float16"), p=4, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[[[ 0.0712  ,  0.2798  ,  0.0352  , ..., -0.709   ,  0.366   ,
           0.836   ],
         [ 0.4944  ,  0.1259  , -0.1443  , ...,  0.8633  ,  0.8003  ,...
 y: array([[[[ 0.0712  ,  0.2798  ,  0.0352  , ..., -0.709   ,  0.366   ,
           0.836   ],
         [ 0.4944  ,  0.1257  , -0.1443  , ...,  0.8633  ,  0.801   ,...
2025-04-21 17:35:05.975079 test begin: paddle.nn.functional.normalize(x=Tensor([20452226, 5, 6, 7],"float16"), p=4, axis=3, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<phi::dtype::float16, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745228621 (unix time) try "date -d @1745228621" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1bb03) received by PID 113411 (TID 0x7f28df34a700) from PID 113411 ***]

2025-04-21 17:44:22.358522 test begin: paddle.nn.functional.normalize(x=Tensor([2147483649, 2],"float16"), p=1.2, )

W0421 17:46:01.604278 125720 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 17:46:01.605434 125720 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.nn.functional.normalize(x=Tensor([2147483649, 2],"float16"), p=1.2, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 17:46:02.457566 test begin: paddle.nn.functional.normalize(x=Tensor([2970966, 768],"float32"), axis=-1, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745228955 (unix time) try "date -d @1745228955" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1eabc) received by PID 125628 (TID 0x7ff1dbdc2700) from PID 125628 ***]

2025-04-21 17:49:21.430537 test begin: paddle.nn.functional.normalize(x=Tensor([4, 1073741825],"float16"), p=1.2, )

W0421 17:51:15.172621 130815 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 17:51:15.173820 130815 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<phi::dtype::float16, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16>)
6   void phi::ReduceWrapper<phi::dtype::float16>(phi::GPUContext const&, int, phi::DenseTensor*, phi::DenseTensor*)
7   void phi::SumKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
8   void phi::SumRawKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
9   void phi::funcs::ReduceKernel<phi::dtype::float16, phi::dtype::float16, phi::kps::AddFunctor, phi::kps::IdentityFunctor<phi::dtype::float16, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<phi::dtype::float16, float> const&, std::vector<int, std::allocator<int> > const&)
10  phi::DenseTensor::~DenseTensor()
11  std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
12  paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745229975 (unix time) try "date -d @1745229975" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1fe97) received by PID 130711 (TID 0x7fd51f935700) from PID 130711 ***]

2025-04-21 18:06:56.934325 test begin: paddle.nn.functional.normalize(x=Tensor([4, 178956971, 6],"float16"), )

W0421 18:08:35.265317 147813 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:08:35.266402 147813 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.nn.functional.normalize(x=Tensor([4, 178956971, 6],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2338823101 / 4294967304 (54.5%)
Max absolute difference: 0.02197
Max relative difference: 255.
 x: array([[[ 0.01837 ,  0.001366, -0.001853, -0.00946 , -0.01362 ,
         -0.01049 ],
        [-0.01511 ,  0.0184  , -0.01069 ,  0.001364, -0.006474,...
 y: array([[[ 1.0759e-04,  7.9870e-06, -1.0848e-05, -5.5432e-05,
         -7.9751e-05, -6.1452e-05],
        [-8.8513e-05,  1.0777e-04, -6.2644e-05,  7.9870e-06,...
2025-04-21 18:30:07.344388 test begin: paddle.nn.functional.normalize(x=Tensor([4, 25565282, 6, 7],"float16"), )

[accuracy error] paddle.nn.functional.normalize(x=Tensor([4, 25565282, 6, 7],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2318901012 / 4294967376 (54%)
Max absolute difference: 0.02176
Max relative difference: 96.
 x: array([[[[ 1.8372e-02,  1.3657e-03, -1.8530e-03, ..., -1.3618e-02,
          -1.0490e-02, -1.5106e-02],
         [ 1.8402e-02, -1.0689e-02,  1.3638e-03, ..., -2.3956e-03,...
 y: array([[[[ 2.8467e-04,  2.1160e-05, -2.8729e-05, ..., -2.1112e-04,
          -1.6260e-04, -2.3413e-04],
         [ 2.8515e-04, -1.6558e-04,  2.1160e-05, ..., -3.7134e-05,...
2025-04-21 18:49:23.319482 test begin: paddle.nn.functional.normalize(x=Tensor([4, 25565282, 6, 7],"float16"), p=1, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<phi::dtype::float16, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16>)
6   void phi::ReduceWrapper<phi::dtype::float16>(phi::GPUContext const&, int, phi::DenseTensor*, phi::DenseTensor*)
7   void phi::SumKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
8   void phi::SumRawKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
9   void phi::funcs::ReduceKernel<phi::dtype::float16, phi::dtype::float16, phi::kps::AddFunctor, phi::kps::IdentityFunctor<phi::dtype::float16, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<phi::dtype::float16, float> const&, std::vector<int, std::allocator<int> > const&)
10  phi::DenseTensor::~DenseTensor()
11  std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
12  paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745233077 (unix time) try "date -d @1745233077" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x24106) received by PID 147718 (TID 0x7fe5d96f8700) from PID 147718 ***]

2025-04-21 18:58:01.980584 test begin: paddle.nn.functional.normalize(x=Tensor([4, 25565282, 6, 7],"float16"), p=4, )

W0421 18:59:42.850168 40678 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:59:42.851315 40678 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.nn.functional.normalize(x=Tensor([4, 25565282, 6, 7],"float16"), p=4, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 3957744176 / 4294967376 (92.1%)
Max absolute difference: 0.1277
Max relative difference: 9.
 x: array([[[[ 4.1077e-02,  8.7646e-02, -1.0974e-01, ..., -1.2103e-01,
           8.2825e-02, -1.4612e-01],
         [-1.0059e-01, -6.7017e-02,  9.5463e-04, ..., -1.0248e-01,...
 y: array([[[[ 5.8098e-03,  1.2390e-02, -1.5526e-02, ..., -1.7120e-02,
           1.1711e-02, -2.0660e-02],
         [-1.4229e-02, -9.4757e-03,  1.3494e-04, ..., -1.4496e-02,...
2025-04-21 19:13:59.070048 test begin: paddle.nn.functional.normalize(x=Tensor([4, 25565282, 6, 7],"float16"), p=4, axis=3, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<phi::dtype::float16, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745234526 (unix time) try "date -d @1745234526" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x9e78) received by PID 40568 (TID 0x7f266134a700) from PID 40568 ***]

2025-04-21 19:22:50.065970 test begin: paddle.nn.functional.normalize(x=Tensor([4, 5, 214748365],"float16"), )

W0421 19:24:29.790114 65873 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 19:24:29.791311 65873 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<phi::dtype::float16, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745235132 (unix time) try "date -d @1745235132" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x100e1) received by PID 65761 (TID 0x7fbd1734a700) from PID 65761 ***]

2025-04-21 19:32:18.290300 test begin: paddle.nn.functional.normalize(x=Tensor([4, 5, 30678338, 7],"float16"), )

W0421 19:33:54.882243 76172 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 19:33:54.883363 76172 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<phi::dtype::float16, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745235712 (unix time) try "date -d @1745235712" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1290f) received by PID 76047 (TID 0x7fe301dc2700) from PID 76047 ***]

2025-04-21 19:42:35.643211 test begin: paddle.nn.functional.normalize(x=Tensor([4, 5, 30678338, 7],"float16"), p=1, )

W0421 19:44:20.051275 86413 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 19:44:20.052415 86413 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<phi::dtype::float16, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745236335 (unix time) try "date -d @1745236335" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x15117) received by PID 86295 (TID 0x7f8d0787e700) from PID 86295 ***]

2025-04-21 19:52:59.939124 test begin: paddle.nn.functional.normalize(x=Tensor([4, 5, 30678338, 7],"float16"), p=4, )

W0421 19:54:40.065888 96295 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 19:54:40.066987 96295 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.nn.functional.normalize(x=Tensor([4, 5, 30678338, 7],"float16"), p=4, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[[[-7.6318e-01,  4.7144e-01,  6.9092e-01, ...,  7.6660e-01,
          -2.9160e-02,  6.1230e-01],
         [-8.0664e-01, -6.4990e-01, -8.1934e-01, ..., -7.4561e-01,...
 y: array([[[[-7.6318e-01,  4.7144e-01,  6.9092e-01, ...,  7.6660e-01,
          -2.9160e-02,  6.1230e-01],
         [-8.0664e-01, -6.4990e-01, -8.1934e-01, ..., -7.4561e-01,...
2025-04-21 19:56:27.308300 test begin: paddle.nn.functional.normalize(x=Tensor([4, 5, 30678338, 7],"float16"), p=4, axis=3, )

[accuracy error] paddle.nn.functional.normalize(x=Tensor([4, 5, 30678338, 7],"float16"), p=4, axis=3, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 6 / 4294967320 (1.4e-07%)
Max absolute difference: 0.0679
Max relative difference: 0.3333
 x: array([[[[-7.7002e-01,  3.8696e-01,  5.9863e-01, ...,  7.1875e-01,
          -1.6891e-02,  4.2383e-01],
         [-6.7578e-01, -4.7437e-01, -6.7090e-01, ..., -6.2354e-01,...
 y: array([[[[-7.7002e-01,  3.8696e-01,  5.9863e-01, ...,  7.1875e-01,
          -1.6891e-02,  4.2383e-01],
         [-6.7578e-01, -4.7437e-01, -6.7090e-01, ..., -6.2354e-01,...
2025-04-21 20:09:16.205523 test begin: paddle.nn.functional.normalize(x=Tensor([4, 5, 6, 35791395],"float16"), )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<phi::dtype::float16, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745237860 (unix time) try "date -d @1745237860" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x177b1) received by PID 96177 (TID 0x7faf0a7c3700) from PID 96177 ***]

2025-04-21 20:18:24.338450 test begin: paddle.nn.functional.normalize(x=Tensor([4, 5, 6, 35791395],"float16"), p=1, )

W0421 20:20:07.942306 122025 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 20:20:07.944110 122025 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<phi::dtype::float16, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745238489 (unix time) try "date -d @1745238489" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1dc29) received by PID 121897 (TID 0x7efb692b7700) from PID 121897 ***]

2025-04-21 20:28:52.874032 test begin: paddle.nn.functional.normalize(x=Tensor([4, 5, 6, 35791395],"float16"), p=4, )

W0421 20:30:35.752776 132069 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 20:30:35.753980 132069 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.nn.functional.normalize(x=Tensor([4, 5, 6, 35791395],"float16"), p=4, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[[[ 0.991   ,  0.4092  , -0.0641  , ...,  0.1813  , -0.10913 ,
          -0.785   ],
         [-0.5884  ,  0.66    , -0.05713 , ...,  0.1282  ,  0.7583  ,...
 y: array([[[[ 0.991   ,  0.4092  , -0.0641  , ...,  0.1813  , -0.10913 ,
          -0.785   ],
         [-0.5884  ,  0.66    , -0.05713 , ...,  0.1282  ,  0.7583  ,...
2025-04-21 20:32:22.497145 test begin: paddle.nn.functional.normalize(x=Tensor([4, 5, 6, 35791395],"float16"), p=4, axis=3, )

[accuracy error] paddle.nn.functional.normalize(x=Tensor([4, 5, 6, 35791395],"float16"), p=4, axis=3, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 3962163947 / 4294967400 (92.3%)
Max absolute difference: 0.1294
Max relative difference: 10.
 x: array([[[[ 0.10596 ,  0.02838 , -0.00967 , ...,  0.01996 , -0.01323 ,
          -0.1245  ],
         [-0.08673 ,  0.0748  , -0.00833 , ...,  0.0161  ,  0.1167  ,...
 y: array([[[[ 1.3786e-02,  3.6907e-03, -1.2569e-03, ...,  2.5959e-03,
          -1.7204e-03, -1.6190e-02],
         [-1.1284e-02,  9.7351e-03, -1.0834e-03, ...,  2.0943e-03,...
2025-04-21 20:47:45.286989 test begin: paddle.nn.functional.normalize(x=Tensor([4294967297],"float16"), axis=0, )

[paddle error] paddle.nn.functional.normalize(x=Tensor([4294967297],"float16"), axis=0, ) 
 (InvalidArgument) The 0-th dimension of input tensor is expected to be equal with the 0-th dimension of output tensor 1 or 1, but received 4294967297.
  [Hint: Expected in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1 == true, but received in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1:0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/kernels/funcs/dims_simplifier.h:144)

2025-04-21 20:48:10.593539 test begin: paddle.nn.functional.normalize(x=Tensor([570425345, 4],"float32"), )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745239908 (unix time) try "date -d @1745239908" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2037c) received by PID 131964 (TID 0x7f97cbd0b700) from PID 131964 ***]

2025-04-21 20:52:33.155057 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 1],"float32"), 2.0, 1e-06, False, None, )

W0421 20:53:48.488207 156030 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 20:53:48.489468 156030 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 1],"float32"), 2.0, 1e-06, False, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 100 / 100 (100%)
Max absolute difference: 48.21631
Max relative difference: 0.02576358
 x: array([2065.175 , 1501.1304, 1491.7052, 1363.9744, 2601.484 , 1978.5906,
       1537.8335, 2260.7922, 2710.217 , 1830.6055, 1359.6326, 2030.2363,
       2260.3035, 1386.0581, 2453.7986, 1364.8324, 1473.4572, 1684.1083,...
 y: array([2100.1887, 1535.0221, 1523.3774, 1390.2961, 2642.6416, 2018.6515,
       1578.444 , 2307.5574, 2749.2488, 1876.0459, 1381.4191, 2066.1907,
       2307.1226, 1418.4573, 2502.015 , 1391.5856, 1500.7543, 1725.812 ,...
2025-04-21 20:54:01.410628 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), -1, 1e-06, False, None, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745240080 (unix time) try "date -d @1745240080" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2610f) received by PID 155919 (TID 0x7f1a176f8700) from PID 155919 ***]

2025-04-21 20:55:22.305355 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), -1, 1e-06, True, None, )

W0421 20:57:02.925722 159056 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 20:57:02.927667 159056 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745240235 (unix time) try "date -d @1745240235" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x26cee) received by PID 158958 (TID 0x7f1253277700) from PID 158958 ***]

2025-04-21 20:57:59.642391 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), -math.inf, 1e-06, False, None, )

W0421 20:59:27.187682 161717 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 20:59:27.189230 161717 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745240369 (unix time) try "date -d @1745240369" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x27734) received by PID 161588 (TID 0x7f51be949700) from PID 161588 ***]

2025-04-21 21:00:12.102856 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), -math.inf, 1e-06, True, None, )

W0421 21:01:48.850859   333 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 21:01:48.852480   333 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745240511 (unix time) try "date -d @1745240511" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x27fb8) received by PID 163768 (TID 0x7ff0fddc2700) from PID 163768 ***]

2025-04-21 21:02:37.470315 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), 0, 1e-06, False, None, )

W0421 21:03:52.170105  2621 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 21:03:52.171257  2621 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), 0, 1e-06, False, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 100 / 100 (100%)
Max absolute difference: 6039798.
Max relative difference: 0.2647059
 x: array([16777216., 16777216., 16777216., 16777216., 16777216., 16777216.,
       16777216., 16777216., 16777216., 16777216., 16777216., 16777216.,
       16777216., 16777216., 16777216., 16777216., 16777216., 16777216.,...
 y: array([22817014., 22817014., 22817014., 22817014., 22817014., 22817014.,
       22817014., 22817014., 22817014., 22817014., 22817014., 22817014.,
       22817014., 22817014., 22817014., 22817014., 22817014., 22817014.,...
2025-04-21 21:03:54.017689 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), 0, 1e-06, True, None, )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), 0, 1e-06, True, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 100 / 100 (100%)
Max absolute difference: 6039798.
Max relative difference: 0.2647059
 x: array([[16777216.],
       [16777216.],
       [16777216.],...
 y: array([[22817014.],
       [22817014.],
       [22817014.],...
2025-04-21 21:04:07.217685 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), 1, 1e-06, False, None, )

[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), 1, 1e-06, False, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 100 / 100 (100%)
Max absolute difference: 4.8137093
Max relative difference: 0.2109701
 x: array([27.63073, 27.63073, 27.63073, 27.63073, 27.63073, 27.63073,
       27.63073, 27.63073, 27.63073, 27.63073, 27.63073, 27.63073,
       27.63073, 27.63073, 27.63073, 27.63073, 27.63073, 27.63073,...
 y: array([22.81702, 22.81702, 22.81702, 22.81702, 22.81702, 22.81702,
       22.81702, 22.81702, 22.81702, 22.81702, 22.81702, 22.81702,
       22.81702, 22.81702, 22.81702, 22.81702, 22.81702, 22.81702,...
2025-04-21 21:04:44.003932 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), 1, 1e-06, True, None, )

[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), 1, 1e-06, True, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 100 / 100 (100%)
Max absolute difference: 4.8137093
Max relative difference: 0.2109701
 x: array([[27.63073],
       [27.63073],
       [27.63073],...
 y: array([[22.81702],
       [22.81702],
       [22.81702],...
2025-04-21 21:05:19.364222 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), 2, 1e-06, False, None, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745240752 (unix time) try "date -d @1745240752" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x9dc) received by PID 2524 (TID 0x7f8c95f48700) from PID 2524 ***]

2025-04-21 21:06:35.206872 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), 2, 1e-06, True, None, )

W0421 21:08:14.190057  6586 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 21:08:14.191354  6586 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745240907 (unix time) try "date -d @1745240907" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1940) received by PID 6464 (TID 0x7f3f91dc2700) from PID 6464 ***]

2025-04-21 21:09:12.124520 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), 2.0, 1e-06, False, None, )

W0421 21:10:37.548049  9324 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 21:10:37.549237  9324 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745241050 (unix time) try "date -d @1745241050" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x22f8) received by PID 8952 (TID 0x7f4daf2b7700) from PID 8952 ***]

2025-04-21 21:11:36.813668 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), math.inf, 1e-06, False, None, )

W0421 21:13:03.084005 12009 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 21:13:03.085054 12009 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745241185 (unix time) try "date -d @1745241185" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2e31) received by PID 11825 (TID 0x7f79472b7700) from PID 11825 ***]

2025-04-21 21:13:48.295467 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), math.inf, 1e-06, True, None, )

W0421 21:15:10.936118 14433 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 21:15:10.937301 14433 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745241313 (unix time) try "date -d @1745241313" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x37f8) received by PID 14328 (TID 0x7faa76949700) from PID 14328 ***]

2025-04-21 21:15:54.035329 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), -1, 1e-06, False, None, )

W0421 21:17:34.895314 16807 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 21:17:34.896507 16807 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745241489 (unix time) try "date -d @1745241489" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x4128) received by PID 16680 (TID 0x7f593f87e700) from PID 16680 ***]

2025-04-21 21:18:51.860089 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), -1, 1e-06, True, None, )

W0421 21:20:38.684791 19382 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 21:20:38.685937 19382 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745241673 (unix time) try "date -d @1745241673" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x4b47) received by PID 19271 (TID 0x7fec86949700) from PID 19271 ***]

2025-04-21 21:22:00.161537 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), -math.inf, 1e-06, False, None, )

W0421 21:23:46.558362 22864 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 21:23:46.559553 22864 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745241833 (unix time) try "date -d @1745241833" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x58ce) received by PID 22734 (TID 0x7f7a11b85700) from PID 22734 ***]

2025-04-21 21:24:35.767543 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), -math.inf, 1e-06, True, None, )

W0421 21:26:29.481889 25418 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 21:26:29.483043 25418 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745241996 (unix time) try "date -d @1745241996" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x62d3) received by PID 25299 (TID 0x7fb7027c3700) from PID 25299 ***]

2025-04-21 21:27:17.293272 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), 0, 1e-06, False, None, )

W0421 21:28:46.053865 28407 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 21:28:46.054962 28407 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), 0, 1e-06, False, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([2048., 2048., 2048., 2048., 2048., 2048., 2048., 2048., 2048.,
       2048., 2048., 2048., 2048., 2048., 2048., 2048., 2048., 2048.,
       2048., 2048., 2048., 2048., 2048., 2048., 2048., 2048., 2048.,...
 y: array([inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,
       inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,
       inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,...
2025-04-21 21:28:50.071745 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), 0, 1e-06, True, None, )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), 0, 1e-06, True, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[2048.],
       [2048.],
       [2048.],...
 y: array([[inf],
       [inf],
       [inf],...
2025-04-21 21:29:03.546322 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), 1, 1e-06, False, None, )

[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), 1, 1e-06, False, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 100 / 100 (100%)
Max absolute difference: 43.53
Max relative difference: 1.
 x: array([0.003906, 0.003906, 0.003906, 0.003906, 0.003906, 0.003906,
       0.003906, 0.003906, 0.003906, 0.003906, 0.003906, 0.003906,
       0.003906, 0.003906, 0.003906, 0.003906, 0.003906, 0.003906,...
 y: array([43.53, 43.53, 43.53, 43.53, 43.53, 43.53, 43.53, 43.53, 43.53,
       43.53, 43.53, 43.53, 43.53, 43.53, 43.53, 43.53, 43.53, 43.53,
       43.53, 43.53, 43.53, 43.53, 43.53, 43.53, 43.53, 43.53, 43.53,...
2025-04-21 21:29:57.207693 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), 1, 1e-06, True, None, )

[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), 1, 1e-06, True, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 100 / 100 (100%)
Max absolute difference: 43.53
Max relative difference: 1.
 x: array([[0.003906],
       [0.003906],
       [0.003906],...
 y: array([[43.53],
       [43.53],
       [43.53],...
2025-04-21 21:30:49.953090 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), 2, 1e-06, False, None, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745242309 (unix time) try "date -d @1745242309" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x6d81) received by PID 28033 (TID 0x7f61e6949700) from PID 28033 ***]

2025-04-21 21:32:31.473994 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), 2, 1e-06, True, None, )

W0421 21:34:28.720633 33357 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 21:34:28.721788 33357 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745242503 (unix time) try "date -d @1745242503" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x81c7) received by PID 33223 (TID 0x7f9eae949700) from PID 33223 ***]

2025-04-21 21:35:46.648461 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), math.inf, 1e-06, False, None, )

W0421 21:37:27.408576 36665 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 21:37:27.409693 36665 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745242654 (unix time) try "date -d @1745242654" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x8eb5) received by PID 36533 (TID 0x7fdf27d0b700) from PID 36533 ***]

2025-04-21 21:38:13.887974 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), math.inf, 1e-06, True, None, )

W0421 21:39:55.227231 39228 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 21:39:55.228310 39228 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745242802 (unix time) try "date -d @1745242802" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x98c0) received by PID 39104 (TID 0x7f38296f8700) from PID 39104 ***]

2025-04-21 21:40:48.253578 test begin: paddle.nn.functional.pairwise_distance(Tensor([1431655766, 3],"float16"), Tensor([1431655766, 3],"float16"), 1, 1e-06, False, None, )

W0421 21:42:29.002229 41576 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 21:42:29.003763 41576 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745243219 (unix time) try "date -d @1745243219" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xa1fb) received by PID 41467 (TID 0x7f4eaf34a700) from PID 41467 ***]

2025-04-21 21:47:42.730998 test begin: paddle.nn.functional.pairwise_distance(Tensor([1431655766, 3],"float16"), Tensor([1431655766, 3],"float16"), 2, 1e-06, False, None, )

W0421 21:49:23.462716 48574 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 21:49:23.463793 48574 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745243625 (unix time) try "date -d @1745243625" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xbd56) received by PID 48470 (TID 0x7f2038949700) from PID 48470 ***]

2025-04-21 21:54:29.823917 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 1140850690],"float32"), Tensor([2, 1140850690],"float32"), -math.inf, 1e-06, False, None, )

W0421 21:55:54.045889 55244 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 21:55:54.047029 55244 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745243813 (unix time) try "date -d @1745243813" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xd751) received by PID 55121 (TID 0x7fa6e4949700) from PID 55121 ***]

2025-04-21 21:57:39.190383 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 1140850690],"float32"), Tensor([2, 1140850690],"float32"), 0, 1e-06, False, None, )

W0421 21:58:53.019352 58492 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 21:58:53.020546 58492 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([2, 1140850690],"float32"), Tensor([2, 1140850690],"float32"), 0, 1e-06, False, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2 / 2 (100%)
Max absolute difference: 1.1240735e+09
Max relative difference: 0.9852941
 x: array([16777216., 16777216.], dtype=float32)
 y: array([1.140851e+09, 1.140851e+09], dtype=float32)
2025-04-21 21:59:53.022345 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 1140850690],"float32"), Tensor([2, 1140850690],"float32"), 1, 1e-06, False, None, )

[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([2, 1140850690],"float32"), Tensor([2, 1140850690],"float32"), 1, 1e-06, False, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2 / 2 (100%)
Max absolute difference: 1108.851
Max relative difference: 0.97195077
 x: array([32., 32.], dtype=float32)
 y: array([1140.851, 1140.851], dtype=float32)
2025-04-21 22:07:29.232622 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 1140850690],"float32"), Tensor([2, 1140850690],"float32"), 2, 1e-06, False, None, )

[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([2, 1140850690],"float32"), Tensor([2, 1140850690],"float32"), 2, 1e-06, False, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2 / 2 (100%)
Max absolute difference: 0.0282522
Max relative difference: 0.83644617
 x: array([0.005524, 0.005524], dtype=float32)
 y: array([0.033776, 0.033776], dtype=float32)
2025-04-21 22:15:03.911576 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 1140850690],"float32"), Tensor([2, 1140850690],"float32"), math.inf, 1e-06, False, None, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745244990 (unix time) try "date -d @1745244990" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xe400) received by PID 58368 (TID 0x7f1f8d87e700) from PID 58368 ***]

2025-04-21 22:17:15.681315 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 2147483649],"float16"), -math.inf, 1e-06, False, None, )

W0421 22:18:56.827438 77680 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 22:18:56.828707 77680 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 2147483649],"float16"), -math.inf, 1e-06, False, None, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 22:18:57.734926 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 2147483649],"float16"), 0, 1e-06, False, None, )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[paddle error] paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 2147483649],"float16"), 0, 1e-06, False, None, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 22:19:03.554325 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 2147483649],"float16"), 1, 1e-06, False, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 2147483649],"float16"), 1, 1e-06, False, None, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 22:19:25.034674 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 2147483649],"float16"), 2, 1e-06, False, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 2147483649],"float16"), 2, 1e-06, False, None, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 22:19:44.258826 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 2147483649],"float16"), math.inf, 1e-06, False, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 2147483649],"float16"), math.inf, 1e-06, False, None, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 22:20:07.322529 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 22:21:27.469748 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), -1, 1e-06, False, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), -1, 1e-06, False, None, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 22:21:50.616506 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), -1, 1e-06, True, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), -1, 1e-06, True, None, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 22:22:15.463318 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), -math.inf, 1e-06, False, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), -math.inf, 1e-06, False, None, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 22:22:38.494677 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), -math.inf, 1e-06, True, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), -math.inf, 1e-06, True, None, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 22:23:01.674333 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), 0, 1e-06, False, None, )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[paddle error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), 0, 1e-06, False, None, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 22:23:10.171202 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), 0, 1e-06, True, None, )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[paddle error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), 0, 1e-06, True, None, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 22:23:16.561852 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), 1, 1e-06, False, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), 1, 1e-06, False, None, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 22:23:38.448802 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), 1, 1e-06, True, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), 1, 1e-06, True, None, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 22:23:58.724544 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), 2, 1e-06, False, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), 2, 1e-06, False, None, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 22:24:22.254447 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), 2, 1e-06, True, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), 2, 1e-06, True, None, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 22:24:47.276911 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), math.inf, 1e-06, False, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), math.inf, 1e-06, False, None, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 22:25:18.020794 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), math.inf, 1e-06, True, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), math.inf, 1e-06, True, None, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-21 22:27:03.991834 test begin: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([22817014, 100],"float32"), -1, 1e-06, False, None, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745245647 (unix time) try "date -d @1745245647" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x12ef4) received by PID 77556 (TID 0x7f1a8e7c3700) from PID 77556 ***]

2025-04-21 22:28:11.271617 test begin: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([22817014, 100],"float32"), -1, 1e-06, True, None, )

W0421 22:29:36.898361 88442 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 22:29:36.899544 88442 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745245777 (unix time) try "date -d @1745245777" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x158fd) received by PID 88317 (TID 0x7fbbb7f48700) from PID 88317 ***]

2025-04-21 22:30:21.224919 test begin: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([22817014, 100],"float32"), -math.inf, 1e-06, False, None, )

W0421 22:31:47.148643 90589 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 22:31:47.149796 90589 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745245908 (unix time) try "date -d @1745245908" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x16162) received by PID 90466 (TID 0x7f417a7c3700) from PID 90466 ***]

2025-04-21 22:32:29.536693 test begin: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([22817014, 100],"float32"), -math.inf, 1e-06, True, None, )

W0421 22:34:14.005885 92728 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 22:34:14.007722 92728 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745246055 (unix time) try "date -d @1745246055" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x169c3) received by PID 92611 (TID 0x7fe289abb700) from PID 92611 ***]

2025-04-21 22:35:00.329464 test begin: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([22817014, 100],"float32"), 1, 1e-06, False, None, )

W0421 22:36:26.419304 95173 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 22:36:26.420449 95173 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745246187 (unix time) try "date -d @1745246187" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1734d) received by PID 95053 (TID 0x7fde07dc2700) from PID 95053 ***]

2025-04-21 22:37:08.461275 test begin: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([22817014, 100],"float32"), 1, 1e-06, True, None, )

W0421 22:38:46.420646 97366 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 22:38:46.422475 97366 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745246327 (unix time) try "date -d @1745246327" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x17bf1) received by PID 97265 (TID 0x7f1d39935700) from PID 97265 ***]

2025-04-21 22:39:30.065207 test begin: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([22817014, 100],"float32"), 2, 1e-06, False, None, )

W0421 22:40:54.888119 100320 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 22:40:54.889362 100320 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745246455 (unix time) try "date -d @1745246455" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1876c) received by PID 100204 (TID 0x7fbf45f48700) from PID 100204 ***]

2025-04-21 22:41:39.977704 test begin: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([22817014, 100],"float32"), 2, 1e-06, True, None, )

W0421 22:43:04.580518 102635 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 22:43:04.581687 102635 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745246585 (unix time) try "date -d @1745246585" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1906e) received by PID 102510 (TID 0x7f6259f48700) from PID 102510 ***]

2025-04-21 22:43:46.199480 test begin: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([22817014, 100],"float32"), math.inf, 1e-06, False, None, )

W0421 22:45:12.571404 104820 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 22:45:12.572543 104820 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745246713 (unix time) try "date -d @1745246713" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x198f2) received by PID 104690 (TID 0x7f690e7c3700) from PID 104690 ***]

2025-04-21 22:45:58.753170 test begin: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([22817014, 100],"float32"), math.inf, 1e-06, True, None, )

W0421 22:47:32.711123 107311 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 22:47:32.712926 107311 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745246854 (unix time) try "date -d @1745246854" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1a2cf) received by PID 107215 (TID 0x7f8e42949700) from PID 107215 ***]

2025-04-21 22:50:44.169484 test begin: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), -1, 1e-06, False, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), -1, 1e-06, False, None, ) 
 (InvalidArgument) The 0-th dimension of input tensor is expected to be equal with the 0-th dimension of output tensor 1 or 1, but received 4294967297.
  [Hint: Expected in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1 == true, but received in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1:0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/kernels/funcs/dims_simplifier.h:144)

2025-04-21 22:52:28.786955 test begin: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), -1, 1e-06, True, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), -1, 1e-06, True, None, ) 
 (InvalidArgument) The 0-th dimension of input tensor is expected to be equal with the 0-th dimension of output tensor 1 or 1, but received 4294967297.
  [Hint: Expected in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1 == true, but received in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1:0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/kernels/funcs/dims_simplifier.h:144)

2025-04-21 22:52:49.761059 test begin: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), -math.inf, 1e-06, False, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), -math.inf, 1e-06, False, None, ) 
 (InvalidArgument) The 0-th dimension of input tensor is expected to be equal with the 0-th dimension of output tensor 1 or 1, but received 4294967297.
  [Hint: Expected in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1 == true, but received in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1:0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/kernels/funcs/dims_simplifier.h:144)

2025-04-21 22:53:18.646947 test begin: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), -math.inf, 1e-06, True, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), -math.inf, 1e-06, True, None, ) 
 (InvalidArgument) The 0-th dimension of input tensor is expected to be equal with the 0-th dimension of output tensor 1 or 1, but received 4294967297.
  [Hint: Expected in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1 == true, but received in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1:0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/kernels/funcs/dims_simplifier.h:144)

2025-04-21 22:53:41.808098 test begin: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), 0, 1e-06, False, None, )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[paddle error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), 0, 1e-06, False, None, ) 
 (InvalidArgument) The 0-th dimension of input tensor is expected to be equal with the 0-th dimension of output tensor 1 or 1, but received 4294967297.
  [Hint: Expected in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1 == true, but received in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1:0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/kernels/funcs/dims_simplifier.h:144)

2025-04-21 22:53:51.392185 test begin: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), 0, 1e-06, True, None, )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[paddle error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), 0, 1e-06, True, None, ) 
 (InvalidArgument) The 0-th dimension of input tensor is expected to be equal with the 0-th dimension of output tensor 1 or 1, but received 4294967297.
  [Hint: Expected in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1 == true, but received in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1:0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/kernels/funcs/dims_simplifier.h:144)

2025-04-21 22:54:01.886678 test begin: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), 1, 1e-06, False, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), 1, 1e-06, False, None, ) 
 (InvalidArgument) The 0-th dimension of input tensor is expected to be equal with the 0-th dimension of output tensor 1 or 1, but received 4294967297.
  [Hint: Expected in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1 == true, but received in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1:0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/kernels/funcs/dims_simplifier.h:144)

2025-04-21 22:54:25.578687 test begin: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), 1, 1e-06, True, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), 1, 1e-06, True, None, ) 
 (InvalidArgument) The 0-th dimension of input tensor is expected to be equal with the 0-th dimension of output tensor 1 or 1, but received 4294967297.
  [Hint: Expected in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1 == true, but received in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1:0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/kernels/funcs/dims_simplifier.h:144)

2025-04-21 22:54:54.270151 test begin: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), 2, 1e-06, False, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), 2, 1e-06, False, None, ) 
 (InvalidArgument) The 0-th dimension of input tensor is expected to be equal with the 0-th dimension of output tensor 1 or 1, but received 4294967297.
  [Hint: Expected in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1 == true, but received in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1:0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/kernels/funcs/dims_simplifier.h:144)

2025-04-21 22:55:15.858339 test begin: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), 2, 1e-06, True, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), 2, 1e-06, True, None, ) 
 (InvalidArgument) The 0-th dimension of input tensor is expected to be equal with the 0-th dimension of output tensor 1 or 1, but received 4294967297.
  [Hint: Expected in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1 == true, but received in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1:0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/kernels/funcs/dims_simplifier.h:144)

2025-04-21 22:55:41.777493 test begin: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), math.inf, 1e-06, False, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), math.inf, 1e-06, False, None, ) 
 (InvalidArgument) The 0-th dimension of input tensor is expected to be equal with the 0-th dimension of output tensor 1 or 1, but received 4294967297.
  [Hint: Expected in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1 == true, but received in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1:0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/kernels/funcs/dims_simplifier.h:144)

2025-04-21 22:56:12.786358 test begin: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), math.inf, 1e-06, True, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), math.inf, 1e-06, True, None, ) 
 (InvalidArgument) The 0-th dimension of input tensor is expected to be equal with the 0-th dimension of output tensor 1 or 1, but received 4294967297.
  [Hint: Expected in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1 == true, but received in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1:0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/kernels/funcs/dims_simplifier.h:144)

2025-04-21 22:56:36.052949 test begin: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([42949673, 100],"float16"), -1, 1e-06, False, None, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745247426 (unix time) try "date -d @1745247426" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1abd6) received by PID 109526 (TID 0x7efce0c3e700) from PID 109526 ***]

2025-04-21 22:57:51.981490 test begin: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([42949673, 100],"float16"), -1, 1e-06, True, None, )

W0421 22:59:36.251097 119294 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 22:59:36.252313 119294 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745247584 (unix time) try "date -d @1745247584" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1d16b) received by PID 119147 (TID 0x7f0685744700) from PID 119147 ***]

2025-04-21 23:00:24.902292 test begin: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([42949673, 100],"float16"), -math.inf, 1e-06, False, None, )

W0421 23:02:06.684144 121670 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 23:02:06.685266 121670 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745247738 (unix time) try "date -d @1745247738" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1dacc) received by PID 121548 (TID 0x7f706df48700) from PID 121548 ***]

2025-04-21 23:03:03.219729 test begin: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([42949673, 100],"float16"), -math.inf, 1e-06, True, None, )

W0421 23:04:46.177554 124074 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 23:04:46.178728 124074 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745247897 (unix time) try "date -d @1745247897" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1e443) received by PID 123971 (TID 0x7f15907c3700) from PID 123971 ***]

2025-04-21 23:05:37.301568 test begin: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([42949673, 100],"float16"), 1, 1e-06, False, None, )

W0421 23:07:19.770934 126981 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 23:07:19.772172 126981 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745248051 (unix time) try "date -d @1745248051" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1ef9e) received by PID 126878 (TID 0x7f3fdddc2700) from PID 126878 ***]

2025-04-21 23:08:18.310997 test begin: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([42949673, 100],"float16"), 1, 1e-06, True, None, )

W0421 23:10:05.255625 129692 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 23:10:05.256732 129692 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745248216 (unix time) try "date -d @1745248216" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1fa35) received by PID 129589 (TID 0x7f299934a700) from PID 129589 ***]

2025-04-21 23:10:59.676490 test begin: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([42949673, 100],"float16"), 2, 1e-06, False, None, )

W0421 23:12:50.259585 132186 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 23:12:50.260751 132186 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745248381 (unix time) try "date -d @1745248381" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x203df) received by PID 132063 (TID 0x7f6ad734a700) from PID 132063 ***]

2025-04-21 23:13:46.879342 test begin: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([42949673, 100],"float16"), 2, 1e-06, True, None, )

W0421 23:15:31.489473 135320 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 23:15:31.490664 135320 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745248543 (unix time) try "date -d @1745248543" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x21014) received by PID 135188 (TID 0x7f1f8ff48700) from PID 135188 ***]

2025-04-21 23:16:25.716535 test begin: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([42949673, 100],"float16"), math.inf, 1e-06, False, None, )

W0421 23:18:22.612895 138101 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 23:18:22.614133 138101 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745248714 (unix time) try "date -d @1745248714" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x21af6) received by PID 137974 (TID 0x7fe04df48700) from PID 137974 ***]

2025-04-21 23:19:19.525483 test begin: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([42949673, 100],"float16"), math.inf, 1e-06, True, None, )

W0421 23:21:02.287948 141465 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 23:21:02.289124 141465 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745248873 (unix time) try "date -d @1745248873" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2270a) received by PID 141066 (TID 0x7f2deddc2700) from PID 141066 ***]

2025-04-21 23:21:59.637279 test begin: paddle.nn.functional.pairwise_distance(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), 2, 1e-06, False, None, )

W0421 23:23:41.772435 144237 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 23:23:41.773526 144237 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), 2, 1e-06, False, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 0.0297
Max relative difference: 1.
 x: array([0., 0., 0., 0., 0.], dtype=float16)
 y: array([0.0297, 0.0297, 0.0297, 0.0297, 0.0297], dtype=float16)
2025-04-21 23:29:33.700536 test begin: paddle.nn.functional.pairwise_distance(Tensor([760567127, 3],"float32"), Tensor([760567127, 3],"float32"), 1, 1e-06, False, None, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745249490 (unix time) try "date -d @1745249490" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x232e8) received by PID 144104 (TID 0x7efbdfdc2700) from PID 144104 ***]

2025-04-21 23:32:12.686640 test begin: paddle.nn.functional.pairwise_distance(Tensor([760567127, 3],"float32"), Tensor([760567127, 3],"float32"), 2, 1e-06, False, None, )

W0421 23:33:53.295926 154490 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 23:33:53.297021 154490 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745249666 (unix time) try "date -d @1745249666" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x25b11) received by PID 154385 (TID 0x7f4cdc949700) from PID 154385 ***]

2025-04-21 23:35:10.744396 test begin: paddle.nn.functional.pairwise_distance(Tensor([858993460, 5],"float16"), Tensor([858993460, 5],"float16"), 2, 1e-06, False, None, )

W0421 23:37:11.246402 157121 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 23:37:11.248257 157121 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745249990 (unix time) try "date -d @1745249990" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x26541) received by PID 156993 (TID 0x7f63876f8700) from PID 156993 ***]

2025-04-21 23:40:38.154466 test begin: paddle.nn.functional.pairwise_distance(x=Tensor([100, 22817014],"float32"), y=Tensor([100, 1],"float32"), p=2.0, epsilon=1e-06, keepdim=False, )

W0421 23:42:10.877012 161705 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 23:42:10.878479 161705 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.nn.functional.pairwise_distance(x=Tensor([100, 22817014],"float32"), y=Tensor([100, 1],"float32"), p=2.0, epsilon=1e-06, keepdim=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 100 / 100 (100%)
Max absolute difference: 48.53125
Max relative difference: 0.02573007
 x: array([1767.3383, 2623.0974, 1361.3134, 1611.9344, 1839.8414, 1700.7185,
       1527.0048, 1359.2773, 1562.8896, 2106.136 , 1734.7719, 1587.5815,
       1980.57  , 1692.1252, 1451.4907, 1927.5496, 2340.9438, 1602.9636,...
 y: array([1814.013 , 2663.437 , 1385.7931, 1652.8401, 1885.2067, 1743.0941,
       1566.0406, 1379.0582, 1603.5205, 2142.537 , 1779.0387, 1628.3284,
       2020.4258, 1734.1992, 1474.7008, 1972.5996, 2389.0452, 1643.7521,...
2025-04-21 23:42:24.038501 test begin: paddle.nn.functional.pairwise_distance(x=Tensor([100, 22817014],"float32"), y=Tensor([100, 22817014],"float32"), p=2.0, epsilon=1e-06, keepdim=False, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745250179 (unix time) try "date -d @1745250179" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2774e) received by PID 161614 (TID 0x7ff5967c3700) from PID 161614 ***]

2025-04-21 23:48:04.682262 test begin: paddle.nn.functional.rrelu(Tensor([2, 3, 4, 178956971],"float16"), 0.1, 0.3, training=False, )

[accuracy error] backward  paddle.nn.functional.rrelu(Tensor([2, 3, 4, 178956971],"float16"), 0.1, 0.3, training=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4034604539 / 4294967304 (93.9%)
Max absolute difference: 0.5
Max relative difference: 1.
 x: array([[[[-0.2188 ,  0.04358,  0.05325, ...,  0.     ,  0.     ,
           0.     ],
         [ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,...
 y: array([[[[-0.2188  ,  0.04358 ,  0.05328 , ...,  0.011375,  0.1852  ,
          -0.03387 ],
         [ 0.02335 , -0.2954  ,  0.1957  , ..., -0.3303  ,  0.3606  ,...
2025-04-22 00:10:34.789746 test begin: paddle.nn.functional.rrelu(Tensor([2, 3, 4, 178956971],"float16"), 0.3, 0.300000009, training=True, )

[accuracy error] paddle.nn.functional.rrelu(Tensor([2, 3, 4, 178956971],"float16"), 0.3, 0.300000009, training=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4108806266 / 4294967304 (95.7%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[ 0.4573  , -0.1006  , -0.11206 , ..., -0.0673  ,  0.1937  ,
          -0.11664 ],
         [-0.01126 ,  0.07654 ,  0.2354  , ...,  0.3298  ,  0.2632  ,...
 y: array([[[[ 0.4573 , -0.1006 , -0.11206, ...,  0.     ,  0.     ,
           0.     ],
         [ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,...
2025-04-22 00:21:16.526250 test begin: paddle.nn.functional.rrelu(Tensor([2, 3, 4, 95070891],"float32"), 0.1, 0.3, training=False, )

[cuda error] paddle.nn.functional.rrelu(Tensor([2, 3, 4, 95070891],"float32"), 0.1, 0.3, training=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 00:23:25.994779 test begin: paddle.nn.functional.rrelu(Tensor([2, 3, 4, 95070891],"float32"), 0.3, 0.300000009, training=True, )

[accuracy error] paddle.nn.functional.rrelu(Tensor([2, 3, 4, 95070891],"float32"), 0.3, 0.300000009, training=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2182827342 / 2281701384 (95.7%)
Max absolute difference: 0.5
Max relative difference: inf
 x: array([[[[-0.145894, -0.009817,  0.473089, ...,  0.280772,  0.463544,
          -0.050277],
         [-0.083114, -0.113541,  0.378774, ...,  0.183144, -0.108373,...
 y: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
2025-04-22 00:25:12.385840 test begin: paddle.nn.functional.rrelu(Tensor([2, 3, 76056713, 5],"float32"), 0.1, 0.3, training=False, )

[cuda error] paddle.nn.functional.rrelu(Tensor([2, 3, 76056713, 5],"float32"), 0.1, 0.3, training=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 00:26:52.609196 test begin: paddle.nn.functional.rrelu(Tensor([2, 3, 76056713, 5],"float32"), 0.3, 0.300000009, training=True, )

[accuracy error] paddle.nn.functional.rrelu(Tensor([2, 3, 76056713, 5],"float32"), 0.3, 0.300000009, training=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2182827348 / 2281701390 (95.7%)
Max absolute difference: 0.5
Max relative difference: inf
 x: array([[[[-0.145894, -0.009817,  0.473089,  0.112825, -0.052741],
         [ 0.440384, -0.051618,  0.238116,  0.406068,  0.389283],
         [-0.01048 , -0.148289,  0.070099,  0.038623, -0.141245],...
 y: array([[[[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],...
2025-04-22 00:28:41.747677 test begin: paddle.nn.functional.rrelu(Tensor([2, 57042535, 4, 5],"float32"), 0.1, 0.3, training=False, )

[cuda error] paddle.nn.functional.rrelu(Tensor([2, 57042535, 4, 5],"float32"), 0.1, 0.3, training=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 00:30:36.413494 test begin: paddle.nn.functional.rrelu(Tensor([2, 57042535, 4, 5],"float32"), 0.3, 0.300000009, training=True, )

[accuracy error] paddle.nn.functional.rrelu(Tensor([2, 57042535, 4, 5],"float32"), 0.3, 0.300000009, training=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2182827358 / 2281701400 (95.7%)
Max absolute difference: 0.5
Max relative difference: inf
 x: array([[[[-1.458936e-01, -9.816518e-03,  4.730890e-01,  1.128255e-01,
          -5.274147e-02],
         [ 4.403844e-01, -5.161766e-02,  2.381161e-01,  4.060684e-01,...
 y: array([[[[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],...
2025-04-22 00:32:32.995522 test begin: paddle.nn.functional.rrelu(Tensor([38028357, 3, 4, 5],"float32"), 0.1, 0.3, training=False, )

[cuda error] paddle.nn.functional.rrelu(Tensor([38028357, 3, 4, 5],"float32"), 0.1, 0.3, training=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 00:34:30.228201 test begin: paddle.nn.functional.rrelu(Tensor([38028357, 3, 4, 5],"float32"), 0.3, 0.300000009, training=True, )

[accuracy error] paddle.nn.functional.rrelu(Tensor([38028357, 3, 4, 5],"float32"), 0.3, 0.300000009, training=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2182827377 / 2281701420 (95.7%)
Max absolute difference: 0.5
Max relative difference: inf
 x: array([[[[-0.145894, -0.009817,  0.473089,  0.112825, -0.052741],
         [ 0.440384, -0.051618,  0.238116,  0.406068,  0.389283],
         [-0.01048 , -0.148289,  0.070099,  0.038623, -0.141245],...
 y: array([[[[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],...
2025-04-22 00:36:16.257589 test begin: paddle.nn.functional.rrelu(Tensor([71582789, 3, 4, 5],"float16"), 0.1, 0.3, training=False, )

[accuracy error] backward  paddle.nn.functional.rrelu(Tensor([71582789, 3, 4, 5],"float16"), 0.1, 0.3, training=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4034584952 / 4294967340 (93.9%)
Max absolute difference: 0.5
Max relative difference: 1.
 x: array([[[[-0.387   , -0.06854 , -0.08594 ,  0.4644  , -0.03262 ],
         [ 0.1013  , -0.012535, -0.0256  , -0.427   , -0.05197 ],
         [ 0.3394  , -0.02002 , -0.06885 ,  0.0783  ,  0.2367  ],...
 y: array([[[[-3.8696e-01, -6.8542e-02, -8.5999e-02,  4.6436e-01,
          -3.2623e-02],
         [ 1.0132e-01, -1.2535e-02, -2.5604e-02, -4.2700e-01,...
2025-04-22 00:58:41.827087 test begin: paddle.nn.functional.rrelu(Tensor([71582789, 3, 4, 5],"float16"), 0.3, 0.300000009, training=True, )

[accuracy error] paddle.nn.functional.rrelu(Tensor([71582789, 3, 4, 5],"float16"), 0.3, 0.300000009, training=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4108806269 / 4294967340 (95.7%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[ 4.5728e-01, -1.0059e-01, -1.1206e-01,  4.9512e-01,
           3.7476e-01],
         [ 1.3196e-01, -1.1383e-01, -1.0236e-01,  3.9001e-02,...
 y: array([[[[ 4.5728e-01, -1.0059e-01, -1.1206e-01,  4.9512e-01,
           3.7476e-01],
         [ 1.3196e-01, -1.1383e-01, -1.0236e-01,  3.9001e-02,...
2025-04-22 01:37:03.042617 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=False, reduction="mean", name=None, )

[paddle error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=False, reduction="mean", name=None, ) 
 The positive distance or negative distance should be greater than 0, The distance functions should be checked.
2025-04-22 01:49:23.407969 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=False, reduction="none", name=None, )

[paddle error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=False, reduction="none", name=None, ) 
 The positive distance or negative distance should be greater than 0, The distance functions should be checked.
2025-04-22 02:01:51.474944 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=False, reduction="sum", name=None, )

[paddle error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=False, reduction="sum", name=None, ) 
 The positive distance or negative distance should be greater than 0, The distance functions should be checked.
2025-04-22 02:14:15.504472 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=True, reduction="mean", name=None, )

[paddle error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=True, reduction="mean", name=None, ) 
 The positive distance or negative distance should be greater than 0, The distance functions should be checked.
2025-04-22 02:32:31.491680 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([858993460, 5],"float16"), Tensor([858993460, 5],"float16"), Tensor([858993460, 5],"float16"), margin=0.3, swap=False, reduction="sum", name=None, )

[paddle error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([858993460, 5],"float16"), Tensor([858993460, 5],"float16"), Tensor([858993460, 5],"float16"), margin=0.3, swap=False, reduction="sum", name=None, ) 
 The positive distance or negative distance should be greater than 0, The distance functions should be checked.
2025-04-22 02:33:05.496931 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=False, reduction="mean", )

[paddle error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=False, reduction="mean", ) 
 The positive distance or negative distance should be greater than 0, The distance functions should be checked.
2025-04-22 02:45:21.206554 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=False, reduction="none", )

[paddle error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=False, reduction="none", ) 
 The positive distance or negative distance should be greater than 0, The distance functions should be checked.
2025-04-22 02:57:39.621667 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", )

[paddle error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", ) 
 The positive distance or negative distance should be greater than 0, The distance functions should be checked.
2025-04-22 03:10:02.371340 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=True, reduction="mean", )

[paddle error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=True, reduction="mean", ) 
 The positive distance or negative distance should be greater than 0, The distance functions should be checked.
2025-04-22 03:28:23.263127 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([858993460, 5],"float16"), positive=Tensor([858993460, 5],"float16"), negative=Tensor([858993460, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", )

[paddle error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([858993460, 5],"float16"), positive=Tensor([858993460, 5],"float16"), negative=Tensor([858993460, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", ) 
 The positive distance or negative distance should be greater than 0, The distance functions should be checked.
2025-04-22 03:29:02.313907 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([10, 15],"float32"),Tensor([15],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([10, 15],"float32"),Tensor([15],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [150, 15], input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:165 != input_axis_dim:2281701379.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4456)

2025-04-22 03:29:07.989292 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([10, 3],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([10, 3],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [30, 0], input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:30 != input_axis_dim:2281701379.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4456)

2025-04-22 03:29:11.778851 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [128, 256, 16, 16, 256, 256, 16, 16], input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:960 != input_axis_dim:2281701379.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4456)

2025-04-22 03:29:16.032858 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([2, 2],"float32"),Tensor([2],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([2, 2],"float32"),Tensor([2],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [4, 2], input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:6 != input_axis_dim:2281701379.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4456)

2025-04-22 03:29:19.274314 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([3, 2, 4, 4, 4],"float32"),Tensor([3],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([3, 2, 4, 4, 4],"float32"),Tensor([3],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [384, 3], input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:387 != input_axis_dim:2281701379.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4456)

2025-04-22 03:29:22.036298 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([3, 2, 4, 4],"float32"),Tensor([3],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([3, 2, 4, 4],"float32"),Tensor([3],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [96, 3], input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:99 != input_axis_dim:2281701379.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4456)

2025-04-22 03:29:24.832141 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([3, 2, 4],"float32"),Tensor([3],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([3, 2, 4],"float32"),Tensor([3],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [24, 3], input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:27 != input_axis_dim:2281701379.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4456)

2025-04-22 03:29:27.606342 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [384, 768, 48, 48, 768, 768, 48, 48], input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:2880 != input_axis_dim:2281701379.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4456)

2025-04-22 03:29:31.406361 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [512, 1024, 64, 64, 1024, 1024, 64, 64], input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:3840 != input_axis_dim:2281701379.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4456)

2025-04-22 03:29:34.639829 test begin: paddle.ones_like(Tensor([1, 10, 114085069, 2],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 10, 114085069, 2],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0.],
         [0., 0.],
         [0., 0.],...
 y: array([[[[1., 1.],
         [1., 1.],
         [1., 1.],...
2025-04-22 03:31:49.763352 test begin: paddle.ones_like(Tensor([1, 10, 8, 28521268],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 10, 8, 28521268],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701440 / 2281701440 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 03:33:55.964008 test begin: paddle.ones_like(Tensor([1, 1024, 2228225],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 1024, 2228225],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281702400 / 2281702400 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 03:36:08.891420 test begin: paddle.ones_like(Tensor([1, 1048576, 128, 32],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 1048576, 128, 32],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967296 / 4294967296 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 03:41:03.686330 test begin: paddle.ones_like(Tensor([1, 12, 9, 21126865],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 12, 9, 21126865],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701420 / 2281701420 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 03:43:47.491473 test begin: paddle.ones_like(Tensor([1, 12, 95070891, 2],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 12, 95070891, 2],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701384 / 2281701384 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0.],
         [0., 0.],
         [0., 0.],...
 y: array([[[[1., 1.],
         [1., 1.],
         [1., 1.],...
2025-04-22 03:46:16.116062 test begin: paddle.ones_like(Tensor([1, 126761188, 9, 2],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 126761188, 9, 2],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701384 / 2281701384 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0.],
         [0., 0.],
         [0., 0.],...
 y: array([[[[1., 1.],
         [1., 1.],
         [1., 1.],...
2025-04-22 03:48:54.280249 test begin: paddle.ones_like(Tensor([1, 128, 1114113, 16],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 128, 1114113, 16],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281703424 / 2281703424 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 03:51:23.949130 test begin: paddle.ones_like(Tensor([1, 128, 17825793],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 128, 17825793],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701504 / 2281701504 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 03:53:47.691570 test begin: paddle.ones_like(Tensor([1, 128, 2097152, 16],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 128, 2097152, 16],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967296 / 4294967296 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 03:58:47.813050 test begin: paddle.ones_like(Tensor([1, 128, 8, 2228225],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 128, 8, 2228225],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281702400 / 2281702400 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 04:00:52.540722 test begin: paddle.ones_like(Tensor([1, 128, 8, 4194304],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 128, 8, 4194304],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967296 / 4294967296 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 04:05:58.900874 test begin: paddle.ones_like(Tensor([1, 142606337, 8, 2],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 142606337, 8, 2],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701392 / 2281701392 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0.],
         [0., 0.],
         [0., 0.],...
 y: array([[[[1., 1.],
         [1., 1.],
         [1., 1.],...
2025-04-22 04:08:06.486623 test begin: paddle.ones_like(Tensor([1, 144, 200, 79226],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 144, 200, 79226],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281708800 / 2281708800 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 04:11:06.245361 test begin: paddle.ones_like(Tensor([1, 144, 7922575, 2],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 144, 7922575, 2],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701600 / 2281701600 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0.],
         [0., 0.],
         [0., 0.],...
 y: array([[[[1., 1.],
         [1., 1.],
         [1., 1.],...
2025-04-22 04:13:43.134494 test begin: paddle.ones_like(Tensor([1, 15, 15, 10140896],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 15, 15, 10140896],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701600 / 2281701600 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 04:16:17.033571 test begin: paddle.ones_like(Tensor([1, 15, 76056713, 2],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 15, 76056713, 2],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701390 / 2281701390 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0.],
         [0., 0.],
         [0., 0.],...
 y: array([[[[1., 1.],
         [1., 1.],
         [1., 1.],...
2025-04-22 04:18:30.121829 test begin: paddle.ones_like(Tensor([1, 16777216, 16, 16],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 16777216, 16, 16],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967296 / 4294967296 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 04:23:21.847325 test begin: paddle.ones_like(Tensor([1, 17825793, 128],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 17825793, 128],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701504 / 2281701504 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 04:26:15.791923 test begin: paddle.ones_like(Tensor([1, 17825793, 8, 16],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 17825793, 8, 16],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701504 / 2281701504 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 04:28:20.980720 test begin: paddle.ones_like(Tensor([1, 2281701379, 1],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 2281701379, 1],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0.],
        [0.],
        [0.],...
 y: array([[[1.],
        [1.],
        [1.],...
2025-04-22 04:31:02.967619 test begin: paddle.ones_like(Tensor([1, 2281701379],"int32"), )

[accuracy error] paddle.ones_like(Tensor([1, 2281701379],"int32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 1
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0]], dtype=int32)
 y: array([[1, 1, 1, ..., 1, 1, 1]], dtype=int32)
2025-04-22 04:35:57.493134 test begin: paddle.ones_like(Tensor([1, 262144, 128, 128],"float16"), )

[accuracy error] paddle.ones_like(Tensor([1, 262144, 128, 128],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967296 / 4294967296 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 04:49:17.234571 test begin: paddle.ones_like(Tensor([1, 262144, 128, 128],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 262144, 128, 128],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967296 / 4294967296 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 04:54:51.288921 test begin: paddle.ones_like(Tensor([1, 32, 1048576, 128],"float16"), )

[accuracy error] paddle.ones_like(Tensor([1, 32, 1048576, 128],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967296 / 4294967296 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 05:07:52.224774 test begin: paddle.ones_like(Tensor([1, 32, 1048576, 128],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 32, 1048576, 128],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967296 / 4294967296 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 05:13:29.131345 test begin: paddle.ones_like(Tensor([1, 32, 128, 1048576],"float16"), )

[accuracy error] paddle.ones_like(Tensor([1, 32, 128, 1048576],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967296 / 4294967296 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 05:26:22.524969 test begin: paddle.ones_like(Tensor([1, 32, 128, 1048576],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 32, 128, 1048576],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967296 / 4294967296 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 05:31:43.898517 test begin: paddle.ones_like(Tensor([1, 32, 16, 8388608],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 32, 16, 8388608],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967296 / 4294967296 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 05:36:34.300418 test begin: paddle.ones_like(Tensor([1, 32, 16777216, 8],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 32, 16777216, 8],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967296 / 4294967296 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 05:42:10.361486 test begin: paddle.ones_like(Tensor([1, 32, 4194304, 32],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 32, 4194304, 32],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967296 / 4294967296 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 05:47:31.300609 test begin: paddle.ones_like(Tensor([1, 32, 8, 16777216],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 32, 8, 16777216],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967296 / 4294967296 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 05:52:26.171016 test begin: paddle.ones_like(Tensor([1, 32, 8388608, 16],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 32, 8388608, 16],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967296 / 4294967296 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 05:57:52.025693 test begin: paddle.ones_like(Tensor([1, 33554432, 8, 16],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 33554432, 8, 16],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967296 / 4294967296 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 06:02:10.954380 test begin: paddle.ones_like(Tensor([1, 4096, 557057],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 4096, 557057],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281705472 / 2281705472 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 06:04:14.158722 test begin: paddle.ones_like(Tensor([1, 4294967295],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 4294967295],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967295 / 4294967295 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[0., 0., 0., ..., 0., 0., 0.]], dtype=float32)
 y: array([[1., 1., 1., ..., 1., 1., 1.]], dtype=float32)
2025-04-22 06:09:08.956670 test begin: paddle.ones_like(Tensor([1, 5704254, 200, 2],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 5704254, 200, 2],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701600 / 2281701600 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0.],
         [0., 0.],
         [0., 0.],...
 y: array([[[[1., 1.],
         [1., 1.],
         [1., 1.],...
2025-04-22 06:11:00.782442 test begin: paddle.ones_like(Tensor([1, 58, 39339679],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 58, 39339679],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701382 / 2281701382 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 06:13:16.965466 test begin: paddle.ones_like(Tensor([1, 64, 67108864],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 64, 67108864],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967296 / 4294967296 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 06:18:27.833334 test begin: paddle.ones_like(Tensor([1, 67108864, 8, 8],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 67108864, 8, 8],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967296 / 4294967296 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 06:24:14.907020 test begin: paddle.ones_like(Tensor([1, 76056713, 15, 2],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 76056713, 15, 2],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701390 / 2281701390 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0.],
         [0., 0.],
         [0., 0.],...
 y: array([[[[1., 1.],
         [1., 1.],
         [1., 1.],...
2025-04-22 06:26:05.535922 test begin: paddle.ones_like(Tensor([1, 8912897, 256],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 8912897, 256],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701632 / 2281701632 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 06:28:11.751188 test begin: paddle.ones_like(Tensor([10, 214748365],"float64"), )

[accuracy error] paddle.ones_like(Tensor([10, 214748365],"float64"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2147483650 / 2147483650 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[1., 1., 1., ..., 1., 1., 1.],
       [1., 1., 1., ..., 1., 1., 1.],
       [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 06:33:35.726399 test begin: paddle.ones_like(Tensor([10563433, 12, 9, 2],"float32"), )

[accuracy error] paddle.ones_like(Tensor([10563433, 12, 9, 2],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701528 / 2281701528 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0.],
         [0., 0.],
         [0., 0.],...
 y: array([[[[1., 1.],
         [1., 1.],
         [1., 1.],...
2025-04-22 06:35:57.131887 test begin: paddle.ones_like(Tensor([1073741824, 2, 2],"int32"), )

[accuracy error] paddle.ones_like(Tensor([1073741824, 2, 2],"int32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967296 / 4294967296 (100%)
Max absolute difference: 1
Max relative difference: 1.
 x: array([[[0, 0],
        [0, 0]],
...
 y: array([[[1, 1],
        [1, 1]],
...
2025-04-22 06:43:03.904291 test begin: paddle.ones_like(Tensor([139265, 128, 128],"float32"), )

[accuracy error] paddle.ones_like(Tensor([139265, 128, 128],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281717760 / 2281717760 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 06:44:57.371824 test begin: paddle.ones_like(Tensor([139265, 128, 8, 16],"float32"), )

[accuracy error] paddle.ones_like(Tensor([139265, 128, 8, 16],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281717760 / 2281717760 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 06:47:06.327602 test begin: paddle.ones_like(Tensor([14260634, 10, 8, 2],"float32"), )

[accuracy error] paddle.ones_like(Tensor([14260634, 10, 8, 2],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701440 / 2281701440 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0.],
         [0., 0.],
         [0., 0.],...
 y: array([[[[1., 1.],
         [1., 1.],
         [1., 1.],...
2025-04-22 06:49:50.720939 test begin: paddle.ones_like(Tensor([2228225, 1024, 1],"float32"), )

[accuracy error] paddle.ones_like(Tensor([2228225, 1024, 1],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281702400 / 2281702400 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0.],
        [0.],
        [0.],...
 y: array([[[1.],
        [1.],
        [1.],...
2025-04-22 06:51:40.720103 test begin: paddle.ones_like(Tensor([2270350, 1005],"int32"), )

[accuracy error] paddle.ones_like(Tensor([2270350, 1005],"int32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701750 / 2281701750 (100%)
Max absolute difference: 1
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[1, 1, 1, ..., 1, 1, 1],
       [1, 1, 1, ..., 1, 1, 1],
       [1, 1, 1, ..., 1, 1, 1],...
2025-04-22 06:54:50.933388 test begin: paddle.ones_like(Tensor([2272611, 1004],"int32"), )

[accuracy error] paddle.ones_like(Tensor([2272611, 1004],"int32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701444 / 2281701444 (100%)
Max absolute difference: 1
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[1, 1, 1, ..., 1, 1, 1],
       [1, 1, 1, ..., 1, 1, 1],
       [1, 1, 1, ..., 1, 1, 1],...
2025-04-22 06:57:52.379968 test begin: paddle.ones_like(Tensor([2274877, 1003],"int32"), )

[accuracy error] paddle.ones_like(Tensor([2274877, 1003],"int32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701631 / 2281701631 (100%)
Max absolute difference: 1
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[1, 1, 1, ..., 1, 1, 1],
       [1, 1, 1, ..., 1, 1, 1],
       [1, 1, 1, ..., 1, 1, 1],...
2025-04-22 07:01:11.628971 test begin: paddle.ones_like(Tensor([2277148, 1002],"int32"), )

[accuracy error] paddle.ones_like(Tensor([2277148, 1002],"int32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281702296 / 2281702296 (100%)
Max absolute difference: 1
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[1, 1, 1, ..., 1, 1, 1],
       [1, 1, 1, ..., 1, 1, 1],
       [1, 1, 1, ..., 1, 1, 1],...
2025-04-22 07:04:31.587291 test begin: paddle.ones_like(Tensor([2279422, 1001],"int32"), )

[accuracy error] paddle.ones_like(Tensor([2279422, 1001],"int32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701422 / 2281701422 (100%)
Max absolute difference: 1
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[1, 1, 1, ..., 1, 1, 1],
       [1, 1, 1, ..., 1, 1, 1],
       [1, 1, 1, ..., 1, 1, 1],...
2025-04-22 07:07:41.351166 test begin: paddle.ones_like(Tensor([2281701379],"int32"), )

[accuracy error] paddle.ones_like(Tensor([2281701379],"int32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 1
Max relative difference: 1.
 x: array([0, 0, 0, ..., 0, 0, 0], dtype=int32)
 y: array([1, 1, 1, ..., 1, 1, 1], dtype=int32)
2025-04-22 07:10:37.308417 test begin: paddle.ones_like(Tensor([39339679, 58, 1],"float32"), )

[accuracy error] paddle.ones_like(Tensor([39339679, 58, 1],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701382 / 2281701382 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0.],
        [0.],
        [0.],...
 y: array([[[1.],
        [1.],
        [1.],...
2025-04-22 07:14:05.805687 test begin: paddle.ones_like(Tensor([39613, 144, 200, 2],"float32"), )

[accuracy error] paddle.ones_like(Tensor([39613, 144, 200, 2],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281708800 / 2281708800 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0.],
         [0., 0.],
         [0., 0.],...
 y: array([[[[1., 1.],
         [1., 1.],
         [1., 1.],...
2025-04-22 07:16:31.691502 test begin: paddle.ones_like(Tensor([4, 280, 376, 25, 217],"float32"), )

[accuracy error] paddle.ones_like(Tensor([4, 280, 376, 25, 217],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2284576000 / 2284576000 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[[0., 0., 0., ..., 0., 0., 0.],
          [0., 0., 0., ..., 0., 0., 0.],
          [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[[1., 1., 1., ..., 1., 1., 1.],
          [1., 1., 1., ..., 1., 1., 1.],
          [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 07:18:34.601799 test begin: paddle.ones_like(Tensor([4, 280, 376, 5419, 1],"float32"), )

[accuracy error] paddle.ones_like(Tensor([4, 280, 376, 5419, 1],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2282049280 / 2282049280 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[[0.],
          [0.],
          [0.],...
 y: array([[[[[1.],
          [1.],
          [1.],...
2025-04-22 07:20:49.698466 test begin: paddle.ones_like(Tensor([4, 280, 81490, 25, 1],"float32"), )

[accuracy error] paddle.ones_like(Tensor([4, 280, 81490, 25, 1],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281720000 / 2281720000 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[[0.],
          [0.],
          [0.],...
 y: array([[[[[1.],
          [1.],
          [1.],...
2025-04-22 07:23:04.878450 test begin: paddle.ones_like(Tensor([4, 60684, 376, 25, 1],"float32"), )

[accuracy error] paddle.ones_like(Tensor([4, 60684, 376, 25, 1],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281718400 / 2281718400 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[[0.],
          [0.],
          [0.],...
 y: array([[[[[1.],
          [1.],
          [1.],...
2025-04-22 07:25:22.578940 test begin: paddle.ones_like(Tensor([5070448, 15, 15, 2],"float32"), )

[accuracy error] paddle.ones_like(Tensor([5070448, 15, 15, 2],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701600 / 2281701600 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0.],
         [0., 0.],
         [0., 0.],...
 y: array([[[[1., 1.],
         [1., 1.],
         [1., 1.],...
2025-04-22 07:27:45.388775 test begin: paddle.ones_like(Tensor([557057, 4096, 1],"float32"), )

[accuracy error] paddle.ones_like(Tensor([557057, 4096, 1],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281705472 / 2281705472 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0.],
        [0.],
        [0.],...
 y: array([[[1.],
        [1.],
        [1.],...
2025-04-22 07:29:50.828442 test begin: paddle.ones_like(Tensor([69633, 128, 256],"float32"), )

[accuracy error] paddle.ones_like(Tensor([69633, 128, 256],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281734144 / 2281734144 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],...
2025-04-22 07:31:55.391678 test begin: paddle.ones_like(Tensor([867, 280, 376, 25, 1],"float32"), )

[accuracy error] paddle.ones_like(Tensor([867, 280, 376, 25, 1],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281944000 / 2281944000 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[[0.],
          [0.],
          [0.],...
 y: array([[[[[1.],
          [1.],
          [1.],...
2025-04-22 07:34:24.631485 test begin: paddle.ones_like(x=Tensor([253522376, 3, 3],"bool"), )

[accuracy error] paddle.ones_like(x=Tensor([253522376, 3, 3],"bool"), ) 
 
Arrays are not equal

Mismatched elements: 2281701384 / 2281701384 (100%)
 x: array([[[False, False, False],
        [False, False, False],
        [False, False, False]],...
 y: array([[[ True,  True,  True],
        [ True,  True,  True],
        [ True,  True,  True]],...
2025-04-22 07:35:56.743969 test begin: paddle.ones_like(x=Tensor([253522376, 3, 3],"float32"), )

[accuracy error] paddle.ones_like(x=Tensor([253522376, 3, 3],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701384 / 2281701384 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]],...
 y: array([[[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]],...
2025-04-22 07:38:03.976489 test begin: paddle.ones_like(x=Tensor([253522376, 3, 3],"int32"), )

[accuracy error] paddle.ones_like(x=Tensor([253522376, 3, 3],"int32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701384 / 2281701384 (100%)
Max absolute difference: 1
Max relative difference: 1.
 x: array([[[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]],...
 y: array([[[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]],...
2025-04-22 07:40:35.178325 test begin: paddle.ones_like(x=Tensor([3, 253522376, 3],"bool"), )

[accuracy error] paddle.ones_like(x=Tensor([3, 253522376, 3],"bool"), ) 
 
Arrays are not equal

Mismatched elements: 2281701384 / 2281701384 (100%)
 x: array([[[False, False, False],
        [False, False, False],
        [False, False, False],...
 y: array([[[ True,  True,  True],
        [ True,  True,  True],
        [ True,  True,  True],...
2025-04-22 07:41:10.518416 test begin: paddle.ones_like(x=Tensor([3, 253522376, 3],"float32"), )

[accuracy error] paddle.ones_like(x=Tensor([3, 253522376, 3],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701384 / 2281701384 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],...
 y: array([[[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],...
2025-04-22 07:43:05.439895 test begin: paddle.ones_like(x=Tensor([3, 253522376, 3],"int32"), )

[accuracy error] paddle.ones_like(x=Tensor([3, 253522376, 3],"int32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701384 / 2281701384 (100%)
Max absolute difference: 1
Max relative difference: 1.
 x: array([[[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],...
 y: array([[[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],...
2025-04-22 07:46:11.280527 test begin: paddle.ones_like(x=Tensor([3, 3, 253522376],"bool"), )

[accuracy error] paddle.ones_like(x=Tensor([3, 3, 253522376],"bool"), ) 
 
Arrays are not equal

Mismatched elements: 2281701384 / 2281701384 (100%)
 x: array([[[False, False, False, ..., False, False, False],
        [False, False, False, ..., False, False, False],
        [False, False, False, ..., False, False, False]],...
 y: array([[[ True,  True,  True, ...,  True,  True,  True],
        [ True,  True,  True, ...,  True,  True,  True],
        [ True,  True,  True, ...,  True,  True,  True]],...
2025-04-22 07:46:54.658027 test begin: paddle.ones_like(x=Tensor([3, 3, 253522376],"float32"), )

[accuracy error] paddle.ones_like(x=Tensor([3, 3, 253522376],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701384 / 2281701384 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],...
 y: array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.]],...
2025-04-22 07:49:23.254947 test begin: paddle.ones_like(x=Tensor([3, 3, 253522376],"int32"), )

[accuracy error] paddle.ones_like(x=Tensor([3, 3, 253522376],"int32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701384 / 2281701384 (100%)
Max absolute difference: 1
Max relative difference: 1.
 x: array([[[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]],...
 y: array([[[1, 1, 1, ..., 1, 1, 1],
        [1, 1, 1, ..., 1, 1, 1],
        [1, 1, 1, ..., 1, 1, 1]],...
2025-04-22 07:52:28.303130 test begin: paddle.outer(Tensor([2281701379],"float32"), Tensor([1],"float32"), )

[paddle error] paddle.outer(Tensor([2281701379],"float32"), Tensor([1],"float32"), ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/dense_tensor.cc:117)

2025-04-22 14:02:50.757109 test begin: paddle.roll(Tensor([1, 114131, 7, 7, 768],"float16"), shifts=tuple(-4,0,0,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 114131, 7, 7, 768],"float16"), shifts=tuple(-4,0,0,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 10261 / 4294977792 (0.000239%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[ 3.8599e-01,  4.9268e-01, -3.0615e-01, ..., -3.8013e-01,
            7.2205e-02, -2.1643e-01],
          [-4.3506e-01, -4.2554e-01, -2.6636e-01, ...,  4.7516e-02,...
 y: array([[[[[ 3.8599e-01,  4.9268e-01, -3.0615e-01, ..., -3.8013e-01,
            7.2205e-02, -2.1643e-01],
          [-4.3506e-01, -4.2554e-01, -2.6636e-01, ...,  4.7516e-02,...
2025-04-22 14:15:46.215935 test begin: paddle.roll(Tensor([1, 114131, 7, 7, 768],"float16"), shifts=tuple(4,0,0,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 114131, 7, 7, 768],"float16"), shifts=tuple(4,0,0,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 10286 / 4294977792 (0.000239%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[-3.7744e-01,  3.3228e-01,  1.6772e-01, ..., -2.1729e-01,
            3.6548e-01,  7.8186e-02],
          [-3.1982e-02,  2.3950e-01, -2.6886e-02, ..., -1.5857e-01,...
 y: array([[[[[-3.7744e-01,  3.3228e-01,  1.6772e-01, ..., -2.1729e-01,
            3.6548e-01,  7.8186e-02],
          [-3.1982e-02,  2.3950e-01, -2.6886e-02, ..., -1.5857e-01,...
2025-04-22 14:28:45.239931 test begin: paddle.roll(Tensor([1, 16, 14, 14, 1369569],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 16, 14, 14, 1369569],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 3189 / 4294968384 (7.42e-05%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[-0.3857  ,  0.1131  , -0.2017  , ...,  0.3677  ,  0.4524  ,
           -0.2448  ],
          [-0.349   ,  0.4436  , -0.3225  , ..., -0.2717  ,  0.4966  ,...
 y: array([[[[[-0.3857  ,  0.1131  , -0.2017  , ...,  0.3677  ,  0.4524  ,
           -0.2448  ],
          [-0.349   ,  0.4436  , -0.3225  , ..., -0.2717  ,  0.4966  ,...
2025-04-22 14:41:19.702589 test begin: paddle.roll(Tensor([1, 16, 14, 14, 1369569],"float16"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 16, 14, 14, 1369569],"float16"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 3197 / 4294968384 (7.44e-05%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[ 9.6680e-02, -6.8237e-02, -1.7639e-01, ..., -2.8125e-01,
           -4.3188e-01, -3.8647e-01],
          [ 4.4580e-01, -1.3721e-01, -3.0713e-01, ...,  2.0496e-01,...
 y: array([[[[[ 9.6680e-02, -6.8237e-02, -1.7639e-01, ..., -2.8125e-01,
           -4.3188e-01, -3.8647e-01],
          [ 4.4580e-01, -1.3721e-01, -3.0713e-01, ...,  2.0496e-01,...
2025-04-22 14:53:55.299438 test begin: paddle.roll(Tensor([1, 16, 14, 21, 913046],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 16, 14, 21, 913046],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 3194 / 4294968384 (7.44e-05%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[ 4.8340e-01, -9.2529e-02, -2.2961e-01, ...,  1.3664e-02,
            7.2327e-02,  4.7632e-01],
          [ 4.3262e-01, -4.1797e-01, -3.8867e-01, ..., -1.2866e-01,...
 y: array([[[[[ 4.8340e-01, -9.2529e-02, -2.2961e-01, ...,  1.3664e-02,
            7.2327e-02,  4.7632e-01],
          [ 4.3262e-01, -4.1797e-01, -3.8867e-01, ..., -1.2866e-01,...
2025-04-22 15:06:42.465207 test begin: paddle.roll(Tensor([1, 16, 14, 21, 913046],"float16"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 16, 14, 21, 913046],"float16"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 3205 / 4294968384 (7.46e-05%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[ 4.4580e-01, -1.3721e-01, -3.0713e-01, ...,  2.8369e-01,
            3.0615e-01, -2.1448e-01],
          [-4.5068e-01,  2.1619e-01,  2.5488e-01, ...,  7.8674e-02,...
 y: array([[[[[ 4.4580e-01, -1.3721e-01, -3.0713e-01, ...,  2.8369e-01,
            3.0615e-01, -2.1448e-01],
          [-4.5068e-01,  2.1619e-01,  2.5488e-01, ...,  7.8674e-02,...
2025-04-22 15:19:39.906803 test begin: paddle.roll(Tensor([1, 16, 14, 24967, 768],"float16"), shifts=tuple(-4,-3,0,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 16, 14, 24967, 768],"float16"), shifts=tuple(-4,-3,0,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 305184 / 4295122944 (0.00711%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[ 3.0908e-01, -2.7664e-02, -2.5488e-01, ..., -3.4351e-01,
            2.2937e-01,  2.1875e-01],
          [ 4.6558e-01,  4.8682e-01,  5.7526e-02, ..., -1.9214e-01,...
 y: array([[[[[ 3.0908e-01, -2.7664e-02, -2.5488e-01, ..., -3.4351e-01,
            2.2937e-01,  2.1875e-01],
          [ 4.6558e-01,  4.8682e-01,  5.7526e-02, ..., -1.9214e-01,...
2025-04-22 15:32:25.428693 test begin: paddle.roll(Tensor([1, 16, 14, 24967, 768],"float16"), shifts=tuple(4,3,0,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 16, 14, 24967, 768],"float16"), shifts=tuple(4,3,0,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 305071 / 4295122944 (0.0071%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[-7.8430e-02,  3.7939e-01, -2.7710e-01, ...,  4.4336e-01,
            4.3091e-01,  2.1960e-01],
          [-1.7700e-02, -1.7151e-01, -3.0493e-01, ..., -3.6572e-01,...
 y: array([[[[[-7.8430e-02,  3.7939e-01, -2.7710e-01, ...,  4.4336e-01,
            4.3091e-01,  2.1960e-01],
          [-1.7700e-02, -1.7151e-01, -3.0493e-01, ..., -3.6572e-01,...
2025-04-22 15:45:32.891908 test begin: paddle.roll(Tensor([1, 16, 14, 49933, 384],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 16, 14, 49933, 384],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 137707 / 4295036928 (0.00321%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[-1.9556e-01, -1.9568e-01, -4.5972e-01, ..., -3.4277e-01,
            3.5083e-01,  3.2861e-01],
          [-1.8921e-01, -3.2300e-01,  3.9624e-01, ...,  9.0332e-02,...
 y: array([[[[[-1.9556e-01, -1.9568e-01, -4.5972e-01, ..., -3.4277e-01,
            3.5083e-01,  3.2861e-01],
          [-1.8921e-01, -3.2300e-01,  3.9624e-01, ...,  9.0332e-02,...
2025-04-22 15:58:34.015929 test begin: paddle.roll(Tensor([1, 16, 14, 49933, 384],"float16"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 16, 14, 49933, 384],"float16"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 137654 / 4295036928 (0.0032%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[ 1.0626e-01,  4.4702e-01, -2.7725e-02, ...,  7.2571e-02,
            7.5378e-02,  1.9824e-01],
          [ 6.2408e-02,  4.3945e-01,  4.6973e-01, ...,  3.9380e-01,...
 y: array([[[[[ 1.0626e-01,  4.4702e-01, -2.7725e-02, ...,  7.2571e-02,
            7.5378e-02,  1.9824e-01],
          [ 6.2408e-02,  4.3945e-01,  4.6973e-01, ...,  3.9380e-01,...
2025-04-22 16:11:08.298038 test begin: paddle.roll(Tensor([1, 16, 14, 7, 2739138],"float16"), shifts=tuple(-4,-3,0,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 16, 14, 7, 2739138],"float16"), shifts=tuple(-4,-3,0,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2128 / 4294968384 (4.95e-05%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[-4.9194e-01,  3.7939e-01,  1.2891e-01, ..., -2.6416e-01,
            7.9773e-02, -2.1997e-01],
          [ 4.8340e-01, -9.2529e-02, -2.2961e-01, ...,  3.6768e-01,...
 y: array([[[[[-4.9194e-01,  3.7939e-01,  1.2891e-01, ..., -2.6416e-01,
            7.9773e-02, -2.1997e-01],
          [ 4.8340e-01, -9.2529e-02, -2.2961e-01, ...,  3.6768e-01,...
2025-04-22 16:23:57.808917 test begin: paddle.roll(Tensor([1, 16, 14, 7, 2739138],"float16"), shifts=tuple(4,3,0,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 16, 14, 7, 2739138],"float16"), shifts=tuple(4,3,0,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2135 / 4294968384 (4.97e-05%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[-1.8225e-01,  3.0444e-01, -9.1614e-02, ...,  4.4360e-01,
            2.3987e-01, -1.3611e-01],
          [-3.3789e-01,  3.0884e-01,  4.9341e-01, ...,  4.7290e-01,...
 y: array([[[[[-1.8225e-01,  3.0444e-01, -9.1614e-02, ...,  4.4360e-01,
            2.3987e-01, -1.3611e-01],
          [-3.3789e-01,  3.0884e-01,  4.9341e-01, ...,  4.7290e-01,...
2025-04-22 16:36:43.209819 test begin: paddle.roll(Tensor([1, 16, 21, 14, 913046],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 16, 21, 14, 913046],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 3188 / 4294968384 (7.42e-05%)
Max absolute difference: 0.4998
Max relative difference: 0.
 x: array([[[[[-3.7329e-01, -8.1055e-02, -2.3950e-01, ...,  1.9714e-01,
           -1.7126e-01,  1.1307e-02],
          [-4.4604e-01,  4.2114e-01, -2.7563e-01, ...,  2.6562e-01,...
 y: array([[[[[-3.7329e-01, -8.1055e-02, -2.3950e-01, ...,  1.9714e-01,
           -1.7126e-01,  1.1307e-02],
          [-4.4604e-01,  4.2114e-01, -2.7563e-01, ...,  2.6562e-01,...
2025-04-22 16:49:24.902235 test begin: paddle.roll(Tensor([1, 16, 21, 14, 913046],"float16"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 16, 21, 14, 913046],"float16"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 3207 / 4294968384 (7.47e-05%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[ 1.9446e-01, -1.0107e-01,  3.7354e-01, ...,  1.1249e-01,
           -4.0991e-01, -1.2830e-01],
          [-3.0664e-01, -3.4131e-01,  2.5049e-01, ...,  1.6650e-01,...
 y: array([[[[[ 1.9446e-01, -1.0107e-01,  3.7354e-01, ...,  1.1249e-01,
           -4.0991e-01, -1.2830e-01],
          [-3.0664e-01, -3.4131e-01,  2.5049e-01, ...,  1.6650e-01,...
2025-04-22 17:02:23.230347 test begin: paddle.roll(Tensor([1, 16, 21, 33289, 384],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 16, 21, 33289, 384],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 221748 / 4295079936 (0.00516%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[ 0.315   , -0.00864 ,  0.388   , ..., -0.2542  ,  0.4312  ,
           -0.2583  ],
          [-0.2064  ,  0.4136  , -0.1725  , ..., -0.4617  , -0.4211  ,...
 y: array([[[[[ 0.315   , -0.00864 ,  0.388   , ..., -0.2542  ,  0.4312  ,
           -0.2583  ],
          [-0.2064  ,  0.4136  , -0.1725  , ..., -0.4617  , -0.4211  ,...
2025-04-22 17:14:55.854432 test begin: paddle.roll(Tensor([1, 16, 21, 33289, 384],"float16"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 16, 21, 33289, 384],"float16"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 221981 / 4295079936 (0.00517%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[-2.2156e-01,  4.7632e-01,  3.7720e-01, ...,  3.6646e-01,
           -2.8784e-01,  3.4961e-01],
          [-4.0918e-01,  9.6375e-02, -1.7493e-01, ...,  1.2079e-01,...
 y: array([[[[[-2.2156e-01,  4.7632e-01,  3.7720e-01, ...,  3.6646e-01,
           -2.8784e-01,  3.4961e-01],
          [-4.0918e-01,  9.6375e-02, -1.7493e-01, ...,  1.2079e-01,...
2025-04-22 17:27:41.293853 test begin: paddle.roll(Tensor([1, 16, 24967, 14, 768],"float16"), shifts=tuple(-4,0,-3,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 16, 24967, 14, 768],"float16"), shifts=tuple(-4,0,-3,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 154818 / 4295122944 (0.0036%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[ 0.4734  ,  0.3752  , -0.0784  , ...,  0.4355  , -0.0692  ,
            0.2947  ],
          [-0.08954 , -0.2969  ,  0.3457  , ...,  0.2703  ,  0.2866  ,...
 y: array([[[[[ 0.4734  ,  0.3752  , -0.0784  , ...,  0.4355  , -0.0692  ,
            0.2947  ],
          [-0.08954 , -0.2969  ,  0.3457  , ...,  0.2703  ,  0.2866  ,...
2025-04-22 17:40:43.930353 test begin: paddle.roll(Tensor([1, 16, 24967, 14, 768],"float16"), shifts=tuple(4,0,3,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 16, 24967, 14, 768],"float16"), shifts=tuple(4,0,3,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 154797 / 4295122944 (0.0036%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[-0.1122  , -0.2505  , -0.2417  , ..., -0.478   , -0.0186  ,
           -0.4646  ],
          [ 0.1715  ,  0.07526 , -0.3315  , ..., -0.3337  ,  0.3972  ,...
 y: array([[[[[-0.1122  , -0.2505  , -0.2417  , ..., -0.478   , -0.0186  ,
           -0.4646  ],
          [ 0.1715  ,  0.07526 , -0.3315  , ..., -0.3337  ,  0.3972  ,...
2025-04-22 17:53:50.996123 test begin: paddle.roll(Tensor([1, 16, 33289, 21, 384],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 16, 33289, 21, 384],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 134042 / 4295079936 (0.00312%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[ 0.3574  , -0.1582  ,  0.1431  , ...,  0.4043  , -0.236   ,
           -0.4482  ],
          [-0.4573  ,  0.0783  ,  0.2734  , ..., -0.0357  ,  0.2478  ,...
 y: array([[[[[ 0.3574  , -0.1582  ,  0.1431  , ...,  0.4043  , -0.236   ,
           -0.4482  ],
          [-0.4573  ,  0.0783  ,  0.2734  , ..., -0.0357  ,  0.2478  ,...
2025-04-22 18:07:40.707595 test begin: paddle.roll(Tensor([1, 16, 33289, 21, 384],"float16"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 16, 33289, 21, 384],"float16"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 134331 / 4295079936 (0.00313%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[-8.4473e-02,  3.4576e-02,  1.0956e-01, ..., -1.0419e-01,
           -2.9688e-01,  2.9150e-01],
          [ 3.9746e-01, -2.7417e-01, -2.6587e-01, ...,  2.8809e-01,...
 y: array([[[[[-8.4473e-02,  3.4576e-02,  1.0956e-01, ..., -1.0419e-01,
           -2.9688e-01,  2.9150e-01],
          [ 3.9746e-01, -2.7417e-01, -2.6587e-01, ...,  2.8809e-01,...
2025-04-23 09:58:26.443475 test begin: paddle.roll(Tensor([1, 16, 49933, 14, 384],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), )

W0423 10:00:13.843345 41512 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0423 10:00:13.845433 41512 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.roll(Tensor([1, 16, 49933, 14, 384],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 84057 / 4295036928 (0.00196%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[-2.5635e-01, -3.2520e-01,  3.4863e-01, ...,  9.4666e-02,
            2.1021e-01,  2.0557e-01],
          [ 2.5000e-01,  4.8804e-01, -6.1676e-02, ..., -6.6711e-02,...
 y: array([[[[[-2.5635e-01, -3.2520e-01,  3.4863e-01, ...,  9.4666e-02,
            2.1021e-01,  2.0557e-01],
          [ 2.5000e-01,  4.8804e-01, -6.1676e-02, ..., -6.6711e-02,...
2025-04-23 10:12:48.145186 test begin: paddle.roll(Tensor([1, 16, 49933, 14, 384],"float16"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 16, 49933, 14, 384],"float16"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 84266 / 4295036928 (0.00196%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[-0.2102  ,  0.4233  ,  0.2666  , ...,  0.1061  , -0.3772  ,
           -0.12286 ],
          [ 0.1818  ,  0.2272  , -0.209   , ..., -0.0426  , -0.367   ,...
 y: array([[[[[-0.2102  ,  0.4233  ,  0.2666  , ...,  0.1061  , -0.3772  ,
           -0.12286 ],
          [ 0.1818  ,  0.2272  , -0.209   , ..., -0.0426  , -0.367   ,...
2025-04-23 10:25:45.720898 test begin: paddle.roll(Tensor([1, 16, 49933, 7, 768],"float16"), shifts=tuple(-4,-3,0,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 16, 49933, 7, 768],"float16"), shifts=tuple(-4,-3,0,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 84057 / 4295036928 (0.00196%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[ 7.3120e-02,  1.9470e-01, -3.3569e-01, ...,  3.6926e-02,
            1.1304e-01,  1.8738e-01],
          [-4.7437e-01,  1.5820e-01, -5.5618e-03, ...,  9.4666e-02,...
 y: array([[[[[ 7.3120e-02,  1.9470e-01, -3.3569e-01, ...,  3.6926e-02,
            1.1304e-01,  1.8738e-01],
          [-4.7437e-01,  1.5820e-01, -5.5618e-03, ...,  9.4666e-02,...
2025-04-23 10:38:38.266521 test begin: paddle.roll(Tensor([1, 16, 49933, 7, 768],"float16"), shifts=tuple(-4,0,0,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 16, 49933, 7, 768],"float16"), shifts=tuple(-4,0,0,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 68243 / 4295036928 (0.00159%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[ 0.010056, -0.09595 , -0.2747  , ..., -0.466   ,  0.1086  ,
           -0.4006  ],
          [-0.2135  , -0.0845  ,  0.219   , ...,  0.3535  ,  0.1389  ,...
 y: array([[[[[ 0.010056, -0.09595 , -0.2747  , ..., -0.466   ,  0.1086  ,
           -0.4006  ],
          [-0.2135  , -0.0845  ,  0.219   , ...,  0.3535  ,  0.1389  ,...
2025-04-23 10:51:36.845969 test begin: paddle.roll(Tensor([1, 16, 49933, 7, 768],"float16"), shifts=tuple(4,0,0,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 16, 49933, 7, 768],"float16"), shifts=tuple(4,0,0,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 68207 / 4295036928 (0.00159%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[-5.1941e-02, -3.1860e-01,  1.1163e-01, ..., -4.6289e-01,
            2.1729e-01,  5.7861e-02],
          [ 2.5879e-01, -1.4673e-01, -4.5068e-01, ...,  2.4927e-01,...
 y: array([[[[[-5.1941e-02, -3.1860e-01,  1.1163e-01, ..., -4.6289e-01,
            2.1729e-01,  5.7861e-02],
          [ 2.5879e-01, -1.4673e-01, -4.5068e-01, ...,  2.4927e-01,...
2025-04-23 11:04:26.264769 test begin: paddle.roll(Tensor([1, 16, 49933, 7, 768],"float16"), shifts=tuple(4,3,0,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 16, 49933, 7, 768],"float16"), shifts=tuple(4,3,0,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 84011 / 4295036928 (0.00196%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[-0.2976  ,  0.2556  , -0.2778  , ..., -0.106   , -0.4653  ,
           -0.3772  ],
          [ 0.04333 ,  0.3113  , -0.1255  , ...,  0.1296  , -0.4268  ,...
 y: array([[[[[-0.2976  ,  0.2556  , -0.2778  , ..., -0.106   , -0.4653  ,
           -0.3772  ],
          [ 0.04333 ,  0.3113  , -0.1255  , ...,  0.1296  , -0.4268  ,...
2025-04-23 11:17:29.329011 test begin: paddle.roll(Tensor([1, 16, 7, 14, 2739138],"float16"), shifts=tuple(-4,0,-3,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 16, 7, 14, 2739138],"float16"), shifts=tuple(-4,0,-3,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2135 / 4294968384 (4.97e-05%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[ 2.3169e-01, -4.4263e-01,  3.7598e-01, ..., -1.6699e-01,
           -4.1846e-01, -8.8806e-02],
          [-3.7964e-01, -2.4463e-01, -3.5449e-01, ...,  1.5454e-01,...
 y: array([[[[[ 2.3169e-01, -4.4263e-01,  3.7598e-01, ..., -1.6699e-01,
           -4.1846e-01, -8.8806e-02],
          [-3.7964e-01, -2.4463e-01, -3.5449e-01, ...,  1.5454e-01,...
2025-04-23 11:30:22.630266 test begin: paddle.roll(Tensor([1, 16, 7, 14, 2739138],"float16"), shifts=tuple(4,0,3,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 16, 7, 14, 2739138],"float16"), shifts=tuple(4,0,3,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2133 / 4294968384 (4.97e-05%)
Max absolute difference: 0.4998
Max relative difference: 0.
 x: array([[[[[-3.3716e-01,  3.8232e-01, -4.1235e-01, ..., -3.0615e-01,
            4.1577e-01,  1.2286e-01],
          [ 1.0046e-01,  2.6560e-04, -3.2666e-01, ...,  3.4912e-01,...
 y: array([[[[[-3.3716e-01,  3.8232e-01, -4.1235e-01, ..., -3.0615e-01,
            4.1577e-01,  1.2286e-01],
          [ 1.0046e-01,  2.6560e-04, -3.2666e-01, ...,  3.4912e-01,...
2025-04-23 11:43:22.386452 test begin: paddle.roll(Tensor([1, 16, 7, 49933, 768],"float16"), shifts=tuple(-4,0,-3,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 16, 7, 49933, 768],"float16"), shifts=tuple(-4,0,-3,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 70497 / 4295036928 (0.00164%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[-2.3352e-01, -3.8116e-02,  1.3318e-01, ..., -1.7432e-01,
            2.0111e-02, -8.9539e-02],
          [-1.4551e-01,  4.6777e-01, -1.3232e-01, ..., -8.9417e-02,...
 y: array([[[[[-2.3352e-01, -3.8116e-02,  1.3318e-01, ..., -1.7432e-01,
            2.0111e-02, -8.9539e-02],
          [-1.4551e-01,  4.6777e-01, -1.3232e-01, ..., -8.9417e-02,...
2025-04-23 11:56:37.788025 test begin: paddle.roll(Tensor([1, 16, 7, 49933, 768],"float16"), shifts=tuple(-4,0,0,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 16, 7, 49933, 768],"float16"), shifts=tuple(-4,0,0,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 68243 / 4295036928 (0.00159%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[ 1.0056e-02, -9.5947e-02, -2.7466e-01, ..., -4.6606e-01,
            1.0858e-01, -4.0063e-01],
          [-2.1350e-01, -8.4473e-02,  2.1899e-01, ...,  3.5352e-01,...
 y: array([[[[[ 1.0056e-02, -9.5947e-02, -2.7466e-01, ..., -4.6606e-01,
            1.0858e-01, -4.0063e-01],
          [-2.1350e-01, -8.4473e-02,  2.1899e-01, ...,  3.5352e-01,...
2025-04-23 12:09:52.999349 test begin: paddle.roll(Tensor([1, 16, 7, 49933, 768],"float16"), shifts=tuple(4,0,0,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 16, 7, 49933, 768],"float16"), shifts=tuple(4,0,0,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 68207 / 4295036928 (0.00159%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[-0.05194 , -0.3186  ,  0.11163 , ..., -0.463   ,  0.2173  ,
            0.05786 ],
          [ 0.2588  , -0.1467  , -0.4507  , ...,  0.2493  ,  0.1144  ,...
 y: array([[[[[-0.05194 , -0.3186  ,  0.11163 , ..., -0.463   ,  0.2173  ,
            0.05786 ],
          [ 0.2588  , -0.1467  , -0.4507  , ...,  0.2493  ,  0.1144  ,...
2025-04-23 12:22:58.039437 test begin: paddle.roll(Tensor([1, 16, 7, 49933, 768],"float16"), shifts=tuple(4,0,3,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 16, 7, 49933, 768],"float16"), shifts=tuple(4,0,3,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 70472 / 4295036928 (0.00164%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[ 0.2607  , -0.4182  ,  0.4932  , ..., -0.1433  ,  0.10516 ,
           -0.1322  ],
          [ 0.3008  ,  0.4155  ,  0.08575 , ..., -0.4868  , -0.3438  ,...
 y: array([[[[[ 0.2607  , -0.4182  ,  0.4932  , ..., -0.1433  ,  0.10516 ,
           -0.1322  ],
          [ 0.3008  ,  0.4155  ,  0.08575 , ..., -0.4868  , -0.3438  ,...
2025-04-23 12:35:54.480814 test begin: paddle.roll(Tensor([1, 16, 7, 7, 5478275],"float16"), shifts=tuple(-4,0,0,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 16, 7, 7, 5478275],"float16"), shifts=tuple(-4,0,0,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 297 / 4294967600 (6.92e-06%)
Max absolute difference: 0.499
Max relative difference: 0.
 x: array([[[[[ 3.9844e-01,  3.3716e-01, -4.1113e-01, ...,  2.8955e-01,
            4.2310e-01,  2.0764e-01],
          [-8.8072e-04,  2.6270e-01, -1.3440e-01, ...,  4.0283e-01,...
 y: array([[[[[ 3.9844e-01,  3.3716e-01, -4.1113e-01, ...,  2.8955e-01,
            4.2310e-01,  2.0764e-01],
          [-8.8072e-04,  2.6270e-01, -1.3440e-01, ...,  4.0283e-01,...
2025-04-23 12:48:50.234905 test begin: paddle.roll(Tensor([1, 16, 7, 7, 5478275],"float16"), shifts=tuple(4,0,0,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 16, 7, 7, 5478275],"float16"), shifts=tuple(4,0,0,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 298 / 4294967600 (6.94e-06%)
Max absolute difference: 0.4995
Max relative difference: 0.
 x: array([[[[[ 4.0820e-01,  4.5898e-01, -3.3057e-01, ...,  4.5288e-01,
            3.8110e-01,  4.3311e-01],
          [ 2.9346e-01,  4.7485e-01,  4.3579e-01, ..., -3.6224e-02,...
 y: array([[[[[ 4.0820e-01,  4.5898e-01, -3.3057e-01, ...,  4.5288e-01,
            3.8110e-01,  4.3311e-01],
          [ 2.9346e-01,  4.7485e-01,  4.3579e-01, ..., -3.6224e-02,...
2025-04-23 13:01:56.099925 test begin: paddle.roll(Tensor([1, 38044, 14, 21, 384],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 38044, 14, 21, 384],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 70902 / 4295015424 (0.00165%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[-1.4612e-01,  4.5459e-01,  3.5889e-01, ...,  3.9551e-01,
            1.2878e-01,  1.2878e-01],
          [ 3.9331e-01,  4.7546e-02,  1.0876e-01, ..., -2.9102e-01,...
 y: array([[[[[-1.4612e-01,  4.5459e-01,  3.5889e-01, ...,  3.9551e-01,
            1.2878e-01,  1.2878e-01],
          [ 3.9331e-01,  4.7546e-02,  1.0876e-01, ..., -2.9102e-01,...
2025-04-23 13:15:12.924122 test begin: paddle.roll(Tensor([1, 38044, 14, 21, 384],"float16"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 38044, 14, 21, 384],"float16"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 71171 / 4295015424 (0.00166%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[-4.7632e-01,  4.5557e-01,  2.6685e-01, ..., -1.1896e-01,
           -4.6021e-01,  3.3960e-01],
          [-1.1060e-01,  1.4087e-01,  4.3457e-01, ...,  4.9780e-01,...
 y: array([[[[[-4.7632e-01,  4.5557e-01,  2.6685e-01, ..., -1.1896e-01,
           -4.6021e-01,  3.3960e-01],
          [-1.1060e-01,  1.4087e-01,  4.3457e-01, ...,  4.9780e-01,...
2025-04-21 15:53:58.772184 test begin: paddle.Tensor.argmax(Tensor([1, 1, 2281701379],"float32"), axis=-2, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745222058 (unix time) try "date -d @1745222058" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x18a08) received by PID 100872 (TID 0x7fd9d87c3700) from PID 100872 ***]

2025-04-21 15:56:11.821097 test begin: paddle.Tensor.argmax(Tensor([221848, 1, 10285],"float32"), axis=-2, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745222256 (unix time) try "date -d @1745222256" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x4643) received by PID 17987 (TID 0x7f42e0949700) from PID 17987 ***]

2025-04-21 15:58:21.615918 test begin: paddle.Tensor.argmax(Tensor([4, 285212673, 2],"float32"), axis=-1, )

W0421 15:59:51.884058 21465 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 15:59:51.884941 21465 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745222393 (unix time) try "date -d @1745222393" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x536c) received by PID 21356 (TID 0x7fb40fdc2700) from PID 21356 ***]

2025-04-21 16:00:35.285028 test begin: paddle.Tensor.argmax(Tensor([67908, 1, 33600],"float32"), axis=-2, )

W0421 16:02:15.182240 23603 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 16:02:15.183156 23603 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745222536 (unix time) try "date -d @1745222536" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x5bb6) received by PID 23478 (TID 0x7fa409dc2700) from PID 23478 ***]

2025-04-21 16:03:02.909694 test begin: paddle.Tensor.argmax(Tensor([7225, 157920, 2],"float32"), axis=-1, )

W0421 16:04:20.995751 26301 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 16:04:20.996745 26301 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745222662 (unix time) try "date -d @1745222662" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x664c) received by PID 26188 (TID 0x7f3e75dc2700) from PID 26188 ***]

2025-04-21 16:05:03.421175 test begin: paddle.Tensor.argmax(Tensor([75245, 1, 30324],"float32"), axis=-2, )

W0421 16:06:27.009019 28413 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 16:06:27.010361 28413 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745222788 (unix time) try "date -d @1745222788" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x6e86) received by PID 28294 (TID 0x7f9a0fb85700) from PID 28294 ***]

2025-04-21 16:07:09.246708 test begin: paddle.Tensor.argmax(Tensor([83837, 1, 27216],"float32"), axis=-2, )

W0421 16:08:30.671422 30546 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 16:08:30.672313 30546 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745222911 (unix time) try "date -d @1745222911" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x76d2) received by PID 30418 (TID 0x7f94a27c3700) from PID 30418 ***]

2025-04-21 16:09:15.339618 test begin: paddle.Tensor.argmax(Tensor([93991, 1, 24276],"float32"), axis=-2, )

W0421 16:10:47.345818 32872 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 16:10:47.346904 32872 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745223048 (unix time) try "date -d @1745223048" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x7fef) received by PID 32751 (TID 0x7ff8d10b7700) from PID 32751 ***]

2025-04-21 16:11:31.472295 test begin: paddle.Tensor.bmm(Tensor([5203, 146200, 3],"float32"), Tensor([5203, 3, 2],"float32"), )

/usr/local/lib/python3.9/dist-packages/torch/autograd/graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:135.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
W0421 16:13:16.376549 35449 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 16:13:16.378155 35449 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] backward  paddle.Tensor.bmm(Tensor([5203, 146200, 3],"float32"), Tensor([5203, 3, 2],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 168 / 31218 (0.538%)
Max absolute difference: 0.07085419
Max relative difference: 6.7199173
 x: array([[[-12.567126, -19.851595],
        [-51.392117,  32.57916 ],
        [  9.664671,  51.3757  ]],...
 y: array([[[-12.552541, -19.867094],
        [-51.403656,  32.574333],
        [  9.657479,  51.397842]],...
2025-04-21 16:15:44.599938 test begin: paddle.Tensor.bmm(Tensor([5309, 143264, 3],"float32"), Tensor([5309, 3, 2],"float32"), )

[accuracy error] backward  paddle.Tensor.bmm(Tensor([5309, 143264, 3],"float32"), Tensor([5309, 3, 2],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 147 / 31854 (0.461%)
Max absolute difference: 0.07386017
Max relative difference: 1.9421962
 x: array([[[  2.34946 ,  13.964616],
        [-34.371964,  15.67006 ],
        [-17.031147, -53.952232]],...
 y: array([[[  2.33274 ,  13.961933],
        [-34.382046,  15.670734],
        [-17.055918, -53.965797]],...
2025-04-21 16:18:36.123958 test begin: paddle.Tensor.bmm(Tensor([6339, 120000, 3],"float32"), Tensor([6339, 3, 2],"float32"), )

[accuracy error] backward  paddle.Tensor.bmm(Tensor([6339, 120000, 3],"float32"), Tensor([6339, 3, 2],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 142 / 38034 (0.373%)
Max absolute difference: 0.0535965
Max relative difference: 8.691459
 x: array([[[ -5.910995,  16.324394],
        [ -3.663386,  23.855751],
        [-13.553939,   4.408708]],...
 y: array([[[ -5.916528,  16.334894],
        [ -3.656208,  23.869493],
        [-13.545221,   4.413946]],...
2025-04-21 16:21:28.119281 test begin: paddle.Tensor.bmm(Tensor([7012, 108472, 3],"float32"), Tensor([7012, 3, 2],"float32"), )

[accuracy error] backward  paddle.Tensor.bmm(Tensor([7012, 108472, 3],"float32"), Tensor([7012, 3, 2],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 128 / 42072 (0.304%)
Max absolute difference: 0.05138397
Max relative difference: 15.550651
 x: array([[[-7.306623e+01,  7.970851e+00],
        [ 5.157165e+01,  2.603329e+01],
        [ 3.455878e+00,  3.188984e-02]],...
 y: array([[[-7.309784e+01,  7.966678e+00],
        [ 5.158191e+01,  2.603543e+01],
        [ 3.453684e+00,  3.619798e-02]],...
2025-04-21 16:24:20.783163 test begin: paddle.Tensor.chunk(Tensor([1, 1, 1, 2281701379],"float32"), 4, axis=-1, )

[paddle error] paddle.Tensor.chunk(Tensor([1, 1, 1, 2281701379],"float32"), 4, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 4, input(X)'s shape = [1, 1, 1, 2281701379], Attr(dim) = 3.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:3 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 16:24:44.861873 test begin: paddle.Tensor.chunk(Tensor([1, 1, 10164, 224489],"float32"), 2, axis=-1, )

[paddle error] paddle.Tensor.chunk(Tensor([1, 1, 10164, 224489],"float32"), 2, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 2, input(X)'s shape = [1, 1, 10164, 224489], Attr(dim) = 3.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 16:25:06.089938 test begin: paddle.Tensor.chunk(Tensor([1, 1, 11109, 205393],"float32"), 2, axis=-1, )

[paddle error] paddle.Tensor.chunk(Tensor([1, 1, 11109, 205393],"float32"), 2, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 2, input(X)'s shape = [1, 1, 11109, 205393], Attr(dim) = 3.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 16:25:25.797032 test begin: paddle.Tensor.chunk(Tensor([1, 1, 12096, 188633],"float32"), 2, axis=-1, )

[paddle error] paddle.Tensor.chunk(Tensor([1, 1, 12096, 188633],"float32"), 2, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 2, input(X)'s shape = [1, 1, 12096, 188633], Attr(dim) = 3.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 16:25:54.417535 test begin: paddle.Tensor.chunk(Tensor([1, 1, 2281701379],"float32"), 2, axis=-1, )

[paddle error] paddle.Tensor.chunk(Tensor([1, 1, 2281701379],"float32"), 2, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 2, input(X)'s shape = [1, 1, 2281701379], Attr(dim) = 2.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 16:26:18.168041 test begin: paddle.Tensor.chunk(Tensor([1, 10, 1, 228170138],"float32"), 4, axis=-1, )

[paddle error] paddle.Tensor.chunk(Tensor([1, 10, 1, 228170138],"float32"), 4, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 4, input(X)'s shape = [1, 10, 1, 228170138], Attr(dim) = 3.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:2 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 16:26:41.755433 test begin: paddle.Tensor.chunk(Tensor([1, 101, 1, 22591103],"float32"), 4, axis=-1, )

[paddle error] paddle.Tensor.chunk(Tensor([1, 101, 1, 22591103],"float32"), 4, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 4, input(X)'s shape = [1, 101, 1, 22591103], Attr(dim) = 3.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:3 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 16:27:02.538751 test begin: paddle.Tensor.chunk(Tensor([1, 101, 22591103],"float32"), 2, axis=-1, )

[paddle error] paddle.Tensor.chunk(Tensor([1, 101, 22591103],"float32"), 2, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 2, input(X)'s shape = [1, 101, 22591103], Attr(dim) = 2.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 16:27:23.366482 test begin: paddle.Tensor.chunk(Tensor([1, 102, 1, 22369622],"float32"), 4, axis=-1, )

[paddle error] paddle.Tensor.chunk(Tensor([1, 102, 1, 22369622],"float32"), 4, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 4, input(X)'s shape = [1, 102, 1, 22369622], Attr(dim) = 3.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:2 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 16:27:44.795149 test begin: paddle.Tensor.chunk(Tensor([1, 103, 1, 22152441],"float32"), 4, axis=-1, )

[paddle error] paddle.Tensor.chunk(Tensor([1, 103, 1, 22152441],"float32"), 4, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 4, input(X)'s shape = [1, 103, 1, 22152441], Attr(dim) = 3.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 16:28:05.471939 test begin: paddle.Tensor.chunk(Tensor([1, 2281701379],"float32"), 2, axis=1, )

[paddle error] paddle.Tensor.chunk(Tensor([1, 2281701379],"float32"), 2, axis=1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 2, input(X)'s shape = [1, 2281701379], Attr(dim) = 1.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 16:28:26.460032 test begin: paddle.Tensor.chunk(Tensor([13, 5484860, 32],"float32"), 8, axis=1, )

[paddle error] paddle.Tensor.chunk(Tensor([13, 5484860, 32],"float32"), 8, axis=1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 8, input(X)'s shape = [13, 5484860, 32], Attr(dim) = 1.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:4 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 16:28:47.458441 test begin: paddle.Tensor.chunk(Tensor([1901418, 300, 4],"float32"), 4, )

[paddle error] paddle.Tensor.chunk(Tensor([1901418, 300, 4],"float32"), 4, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 4, input(X)'s shape = [1901418, 300, 4], Attr(dim) = 0.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:2 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 16:29:08.338617 test begin: paddle.Tensor.chunk(Tensor([475355, 300, 16],"float32"), 2, )

[paddle error] paddle.Tensor.chunk(Tensor([475355, 300, 16],"float32"), 2, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 2, input(X)'s shape = [475355, 300, 16], Attr(dim) = 0.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 16:29:29.766518 test begin: paddle.Tensor.chunk(Tensor([5704254, 100, 4],"float32"), 4, )

[paddle error] paddle.Tensor.chunk(Tensor([5704254, 100, 4],"float32"), 4, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 4, input(X)'s shape = [5704254, 100, 4], Attr(dim) = 0.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:2 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 16:29:48.353680 test begin: paddle.Tensor.chunk(Tensor([75304, 300, 101],"float32"), 16, )

[paddle error] paddle.Tensor.chunk(Tensor([75304, 300, 101],"float32"), 16, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 16, input(X)'s shape = [75304, 300, 101], Attr(dim) = 0.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:8 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 16:30:07.919784 test begin: paddle.Tensor.chunk(Tensor([950709, 300, 8],"float32"), 8, )

[paddle error] paddle.Tensor.chunk(Tensor([950709, 300, 8],"float32"), 8, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 8, input(X)'s shape = [950709, 300, 8], Attr(dim) = 0.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:5 != 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:4508)

2025-04-21 18:36:24.604861 test begin: paddle.Tensor.cumsum(Tensor([1, 10, 228170138],"float32"), 1, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745231813 (unix time) try "date -d @1745231813" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x8a0c) received by PID 35340 (TID 0x7f1a712b7700) from PID 35340 ***]

2025-04-21 18:36:59.053126 test begin: paddle.Tensor.cumsum(Tensor([1, 10, 228170138],"float32"), 2, )

W0421 18:38:42.073171 15699 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:38:42.074414 15699 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.Tensor.cumsum(Tensor([1, 10, 228170138],"float32"), 2, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1312782 / 2281701380 (0.0575%)
Max absolute difference: 0.30126953
Max relative difference: 9452.421
 x: array([[[ 6.886307e-02,  1.914744e-01,  4.531510e-01, ...,
          7.469743e+03,  7.469785e+03,  7.469973e+03],
        [ 3.061995e-01, -1.270555e-01,  1.173105e-01, ...,...
 y: array([[[ 6.886307e-02,  1.914744e-01,  4.531510e-01, ...,
          7.469489e+03,  7.469531e+03,  7.469718e+03],
        [ 3.061995e-01, -1.270555e-01,  1.173105e-01, ...,...
2025-04-21 18:40:47.046506 test begin: paddle.Tensor.cumsum(Tensor([1, 11408507, 200],"float32"), 1, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745232082 (unix time) try "date -d @1745232082" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x3cd6) received by PID 15574 (TID 0x7ff7410f7700) from PID 15574 ***]

2025-04-21 18:41:29.066177 test begin: paddle.Tensor.cumsum(Tensor([1, 11408507, 200],"float32"), 2, )

W0421 18:43:09.485674 21232 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:43:09.487025 21232 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745232189 (unix time) try "date -d @1745232189" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x5264) received by PID 21092 (TID 0x7fc7ff2b7700) from PID 21092 ***]

2025-04-21 18:43:16.067125 test begin: paddle.Tensor.cumsum(Tensor([1, 12, 190141782],"float32"), 1, )

W0421 18:44:36.857290 23328 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:44:36.858395 23328 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745232277 (unix time) try "date -d @1745232277" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x5a92) received by PID 23186 (TID 0x7fe0d5abb700) from PID 23186 ***]

2025-04-21 18:45:26.185465 test begin: paddle.Tensor.cumsum(Tensor([1, 12, 190141782],"float32"), 2, )

W0421 18:46:51.806095 26311 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:46:51.807250 26311 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.Tensor.cumsum(Tensor([1, 12, 190141782],"float32"), 2, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 668173 / 2281701384 (0.0293%)
Max absolute difference: 0.37304688
Max relative difference: 7158.6313
 x: array([[[-1.139702e-01,  1.690529e-01,  4.930175e-02, ...,
         -1.040672e+03, -1.040445e+03, -1.040933e+03],
        [ 2.227859e-01,  6.381426e-01,  9.999467e-01, ...,...
 y: array([[[-1.139702e-01,  1.690529e-01,  4.930175e-02, ...,
         -1.040721e+03, -1.040495e+03, -1.040983e+03],
        [ 2.227859e-01,  6.381426e-01,  9.999467e-01, ...,...
2025-04-21 18:48:31.331111 test begin: paddle.Tensor.cumsum(Tensor([1, 144, 15845149],"float32"), 1, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745232534 (unix time) try "date -d @1745232534" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x663d) received by PID 26173 (TID 0x7f775c7c3700) from PID 26173 ***]

2025-04-21 18:49:38.405637 test begin: paddle.Tensor.cumsum(Tensor([1, 144, 15845149],"float32"), 2, )

W0421 18:51:03.014539 30763 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:51:03.015661 30763 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745232663 (unix time) try "date -d @1745232663" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x77b5) received by PID 30645 (TID 0x7f11f787e700) from PID 30645 ***]

2025-04-21 18:51:45.517195 test begin: paddle.Tensor.cumsum(Tensor([1, 15845149, 144],"float32"), 1, )

W0421 18:53:23.270018 33151 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:53:23.271217 33151 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745232803 (unix time) try "date -d @1745232803" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x80f7) received by PID 33015 (TID 0x7f97d1f48700) from PID 33015 ***]

2025-04-21 18:54:04.694896 test begin: paddle.Tensor.cumsum(Tensor([1, 15845149, 144],"float32"), 2, )

W0421 18:55:44.426985 35928 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:55:44.428154 35928 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745232944 (unix time) try "date -d @1745232944" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x8be7) received by PID 35815 (TID 0x7f173b4f4700) from PID 35815 ***]

2025-04-21 18:55:50.533851 test begin: paddle.Tensor.cumsum(Tensor([1, 18, 126761188],"float32"), 1, )

W0421 18:57:13.608711 38036 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:57:13.609938 38036 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745233034 (unix time) try "date -d @1745233034" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x930d) received by PID 37645 (TID 0x7f31f2949700) from PID 37645 ***]

2025-04-21 18:57:56.403856 test begin: paddle.Tensor.cumsum(Tensor([1, 18, 126761188],"float32"), 2, )

W0421 18:59:22.480808 40531 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 18:59:22.482028 40531 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745233162 (unix time) try "date -d @1745233162" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x9ddf) received by PID 40415 (TID 0x7f5e5e7c3700) from PID 40415 ***]

2025-04-21 19:00:08.768011 test begin: paddle.Tensor.cumsum(Tensor([1, 192, 11883862],"float32"), 1, )

W0421 19:01:32.314905 42868 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 19:01:32.316036 42868 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745233292 (unix time) try "date -d @1745233292" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xa6f7) received by PID 42743 (TID 0x7f2cd587e700) from PID 42743 ***]

2025-04-21 19:02:17.529073 test begin: paddle.Tensor.cumsum(Tensor([1, 192, 11883862],"float32"), 2, )

W0421 19:03:40.154704 45085 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 19:03:40.155901 45085 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745233420 (unix time) try "date -d @1745233420" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xafb6) received by PID 44982 (TID 0x7f6876949700) from PID 44982 ***]

2025-04-21 19:04:20.949673 test begin: paddle.Tensor.cumsum(Tensor([1, 2281701379],"float32"), axis=-1, )

W0421 19:05:51.402632 47268 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 19:05:51.403843 47268 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] backward  paddle.Tensor.cumsum(Tensor([1, 2281701379],"float32"), axis=-1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 87560 / 2281701379 (0.00384%)
Max absolute difference: 0.15917969
Max relative difference: 72546.46
 x: array([[-9.754627e+03, -9.754830e+03, -9.754831e+03, ..., -1.163476e+00,
        -7.643354e-01, -3.421055e-01]], dtype=float32)
 y: array([[-9.754734e+03, -9.754938e+03, -9.754938e+03, ..., -1.163476e+00,
        -7.643354e-01, -3.421055e-01]], dtype=float32)
2025-04-21 19:08:57.545453 test begin: paddle.Tensor.cumsum(Tensor([1, 253522376, 9],"float32"), 1, )

[cuda error] paddle.Tensor.cumsum(Tensor([1, 253522376, 9],"float32"), 1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 19:11:50.921027 test begin: paddle.Tensor.cumsum(Tensor([1, 253522376, 9],"float32"), 2, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745233935 (unix time) try "date -d @1745233935" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xb827) received by PID 47143 (TID 0x7fd11a949700) from PID 47143 ***]

2025-04-21 19:13:01.345973 test begin: paddle.Tensor.cumsum(Tensor([1, 285212673, 8],"float32"), 1, )

W0421 19:17:04.451072 55730 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 19:17:04.452280 55730 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.Tensor.cumsum(Tensor([1, 285212673, 8],"float32"), 1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 19:17:06.003468 test begin: paddle.Tensor.cumsum(Tensor([1, 285212673, 8],"float32"), 2, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745234254 (unix time) try "date -d @1745234254" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xd944) received by PID 55620 (TID 0x7fcb856f8700) from PID 55620 ***]

2025-04-21 19:18:19.610424 test begin: paddle.Tensor.cumsum(Tensor([1, 91268056, 25],"float32"), 1, )

W0421 19:20:59.423058 61505 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 19:20:59.424196 61505 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745234459 (unix time) try "date -d @1745234459" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xefbf) received by PID 61375 (TID 0x7f76af87e700) from PID 61375 ***]

2025-04-21 19:21:43.743064 test begin: paddle.Tensor.cumsum(Tensor([1, 91268056, 25],"float32"), 2, )

W0421 19:23:08.442902 64887 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 19:23:08.444099 64887 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745234588 (unix time) try "date -d @1745234588" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xfcfb) received by PID 64763 (TID 0x7f85cfdc2700) from PID 64763 ***]

2025-04-21 19:23:50.524684 test begin: paddle.Tensor.cumsum(Tensor([114085069, 20],"float32"), axis=-1, )

W0421 19:25:17.041294 67377 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 19:25:17.042455 67377 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745234717 (unix time) try "date -d @1745234717" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x105ae) received by PID 66990 (TID 0x7ff1b5abb700) from PID 66990 ***]

2025-04-21 19:26:02.966852 test begin: paddle.Tensor.cumsum(Tensor([1140850690, 2],"float32"), axis=-1, )

W0421 19:27:47.131507 69537 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 19:27:47.132699 69537 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745234868 (unix time) try "date -d @1745234868" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x10f3a) received by PID 69434 (TID 0x7fca47177700) from PID 69434 ***]

2025-04-21 19:28:29.719460 test begin: paddle.Tensor.cumsum(Tensor([162978670, 14],"int32"), -1, )

W0421 19:29:33.859030 71849 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 19:29:33.859999 71849 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745234974 (unix time) try "date -d @1745234974" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x11833) received by PID 71731 (TID 0x7fd2187c3700) from PID 71731 ***]

2025-04-21 19:30:16.501602 test begin: paddle.Tensor.cumsum(Tensor([21126865, 12, 9],"float32"), 1, )

W0421 19:31:40.060703 73925 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 19:31:40.061762 73925 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745235100 (unix time) try "date -d @1745235100" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x12058) received by PID 73816 (TID 0x7f59c52b7700) from PID 73816 ***]

2025-04-21 19:32:21.805002 test begin: paddle.Tensor.cumsum(Tensor([21126865, 12, 9],"float32"), 2, )

W0421 19:33:50.527324 76287 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 19:33:50.528487 76287 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745235231 (unix time) try "date -d @1745235231" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1298d) received by PID 76173 (TID 0x7fc2e187e700) from PID 76173 ***]

2025-04-21 19:34:37.794515 test begin: paddle.Tensor.cumsum(Tensor([2228225, 1024],"int32"), axis=-1, )

W0421 19:35:39.960114 78596 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 19:35:39.961063 78596 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745235340 (unix time) try "date -d @1745235340" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x132a0) received by PID 78496 (TID 0x7fb4e3abb700) from PID 78496 ***]

2025-04-21 19:36:25.618483 test begin: paddle.Tensor.cumsum(Tensor([2281701379],"float32"), -1, )

W0421 19:38:07.848713 80557 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 19:38:07.850728 80557 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.Tensor.cumsum(Tensor([2281701379],"float32"), -1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 275634 / 2281701379 (0.0121%)
Max absolute difference: 0.08300781
Max relative difference: 2520.4978
 x: array([-4.359789e-01, -3.827131e-01, -1.008153e-01, ..., -9.218370e+03,
       -9.218374e+03, -9.218173e+03], dtype=float32)
 y: array([-4.359789e-01, -3.827131e-01, -1.008153e-01, ..., -9.218312e+03,
       -9.218315e+03, -9.218114e+03], dtype=float32)
2025-04-21 19:39:54.193030 test begin: paddle.Tensor.cumsum(Tensor([228170138, 10],"int64"), axis=1, )

[accuracy error] paddle.Tensor.cumsum(Tensor([228170138, 10],"int64"), axis=1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 134217100 / 2281701380 (5.88%)
Max absolute difference: 549136
Max relative difference: 1.
 x: array([[ -41000,  -60647,    -422, ...,  -78154, -110595, -167047],
       [  63693,   23455,   82940, ...,  -33818,   12068,  -23286],
       [  43832,  -21267,  -36629, ...,  -47297,  -73061,  -69729],...
 y: array([[ -41000,  -60647,    -422, ...,  -78154, -110595, -167047],
       [  63693,   23455,   82940, ...,  -33818,   12068,  -23286],
       [  43832,  -21267,  -36629, ...,  -47297,  -73061,  -69729],...
2025-04-21 19:46:25.740666 test begin: paddle.Tensor.cumsum(Tensor([22817014, 100],"int64"), axis=1, )

[accuracy error] paddle.Tensor.cumsum(Tensor([22817014, 100],"int64"), axis=1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 134217427 / 2281701400 (5.88%)
Max absolute difference: 1764095
Max relative difference: 1.
 x: array([[ -41000,  -60647,    -422, ..., -438910, -409200, -451803],
       [ -45598,   13629,   78435, ...,  546495,  603241,  542346],
       [ -61849,   -4042,  -34175, ...,   41687,   77472,  107029],...
 y: array([[ -41000,  -60647,    -422, ..., -438910, -409200, -451803],
       [ -45598,   13629,   78435, ...,  546495,  603241,  542346],
       [ -61849,   -4042,  -34175, ...,   41687,   77472,  107029],...
2025-04-21 19:50:37.246443 test begin: paddle.Tensor.cumsum(Tensor([285212673, 4, 2],"int64"), axis=0, )

[cuda error] paddle.Tensor.cumsum(Tensor([285212673, 4, 2],"int64"), axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 19:53:02.977771 test begin: paddle.Tensor.cumsum(Tensor([285212673, 4, 2],"int64"), axis=1, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745236406 (unix time) try "date -d @1745236406" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x13a2b) received by PID 80427 (TID 0x7f74d1f48700) from PID 80427 ***]

2025-04-21 19:54:12.926593 test begin: paddle.Tensor.cumsum(Tensor([285212673, 4, 2],"int64"), axis=2, )

W0421 19:56:08.923771 97749 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 19:56:08.925066 97749 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745236571 (unix time) try "date -d @1745236571" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x17d64) received by PID 97636 (TID 0x7f8ae47c3700) from PID 97636 ***]

2025-04-21 19:56:57.937280 test begin: paddle.Tensor.cumsum(Tensor([28521268, 10, 8],"float32"), 1, )

W0421 19:58:23.065812 100224 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 19:58:23.067018 100224 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745236703 (unix time) try "date -d @1745236703" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1871e) received by PID 100126 (TID 0x7f4f314f4700) from PID 100126 ***]

2025-04-21 19:59:09.832405 test begin: paddle.Tensor.cumsum(Tensor([28521268, 10, 8],"float32"), 2, )

W0421 20:00:39.608767 102409 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 20:00:39.609912 102409 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745236840 (unix time) try "date -d @1745236840" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x18f87) received by PID 102279 (TID 0x7fed13b85700) from PID 102279 ***]

2025-04-21 20:01:20.818580 test begin: paddle.Tensor.cumsum(Tensor([3, 380283564, 2],"int64"), axis=0, )

W0421 20:02:39.083901 105078 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 20:02:39.085491 105078 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745236961 (unix time) try "date -d @1745236961" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x199fb) received by PID 104955 (TID 0x7fd1947c3700) from PID 104955 ***]

2025-04-21 20:03:25.310189 test begin: paddle.Tensor.cumsum(Tensor([3, 380283564, 2],"int64"), axis=1, )

W0421 20:05:46.828881 107169 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 20:05:46.829803 107169 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.Tensor.cumsum(Tensor([3, 380283564, 2],"int64"), axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 20:05:48.665515 test begin: paddle.Tensor.cumsum(Tensor([3, 380283564, 2],"int64"), axis=2, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745237190 (unix time) try "date -d @1745237190" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1a22e) received by PID 107054 (TID 0x7f94f34f4700) from PID 107054 ***]

2025-04-21 20:07:16.370983 test begin: paddle.Tensor.cumsum(Tensor([3, 4, 190141782],"int64"), axis=0, )

W0421 20:08:46.284584 110776 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 20:08:46.285422 110776 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745237328 (unix time) try "date -d @1745237328" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1b040) received by PID 110656 (TID 0x7f32d5b85700) from PID 110656 ***]

2025-04-21 20:09:33.111876 test begin: paddle.Tensor.cumsum(Tensor([3, 4, 190141782],"int64"), axis=1, )

W0421 20:10:55.069265 113230 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 20:10:55.070127 113230 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745237456 (unix time) try "date -d @1745237456" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1b9e7) received by PID 113127 (TID 0x7f4d2e7c3700) from PID 113127 ***]

2025-04-21 20:11:41.386147 test begin: paddle.Tensor.cumsum(Tensor([3, 760567127],"int64"), axis=0, )

W0421 20:13:01.781761 115340 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 20:13:01.783038 115340 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745237583 (unix time) try "date -d @1745237583" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1c22c) received by PID 115244 (TID 0x7f8c0f34a700) from PID 115244 ***]

2025-04-21 20:13:45.881635 test begin: paddle.Tensor.cumsum(Tensor([4294967297],"float16"), -1, )

W0421 20:15:25.689455 117419 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 20:15:25.691453 117419 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.Tensor.cumsum(Tensor([4294967297],"float16"), -1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294964304 / 4294967297 (100%)
Max absolute difference: 19200.
Max relative difference: 1.
 x: array([-0.3586,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],
      dtype=float16)
 y: array([-3.586e-01, -6.855e-01, -8.188e-01, ...,  1.189e+04,  1.189e+04,
        1.189e+04], dtype=float16)
2025-04-21 20:27:35.054641 test begin: paddle.Tensor.cumsum(Tensor([5, 456340276],"int64"), axis=0, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745238550 (unix time) try "date -d @1745238550" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1ca29) received by PID 117289 (TID 0x7fc408949700) from PID 117289 ***]

2025-04-21 20:29:55.189412 test begin: paddle.Tensor.cumsum(Tensor([5070448, 18, 25],"float32"), 1, )

W0421 20:31:20.446812 133567 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 20:31:20.448007 133567 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745238680 (unix time) try "date -d @1745238680" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2093f) received by PID 133439 (TID 0x7fbfc5f48700) from PID 133439 ***]

2025-04-21 20:32:04.842667 test begin: paddle.Tensor.cumsum(Tensor([5070448, 18, 25],"float32"), 2, )

W0421 20:33:29.349409 135674 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 20:33:29.350504 135674 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745238809 (unix time) try "date -d @1745238809" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x21186) received by PID 135558 (TID 0x7f0a030f7700) from PID 135558 ***]

2025-04-21 20:34:14.018683 test begin: paddle.Tensor.cumsum(Tensor([570425345, 4],"int64"), axis=0, )

W0421 20:38:15.407524 137798 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 20:38:15.408447 137798 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.Tensor.cumsum(Tensor([570425345, 4],"int64"), axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 20:38:17.741058 test begin: paddle.Tensor.cumsum(Tensor([570425345, 4],"int64"), axis=1, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745239130 (unix time) try "date -d @1745239130" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x219d0) received by PID 137680 (TID 0x7fbe10949700) from PID 137680 ***]

2025-04-21 20:39:31.710028 test begin: paddle.Tensor.cumsum(Tensor([760567127, 3],"float32"), axis=-1, )

W0421 20:41:48.755821 143026 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 20:41:48.758124 143026 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745239309 (unix time) try "date -d @1745239309" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x22e46) received by PID 142918 (TID 0x7f212ff48700) from PID 142918 ***]

2025-04-21 20:42:36.187986 test begin: paddle.Tensor.cumsum(Tensor([79226, 144, 200],"float32"), 1, )

W0421 20:44:00.172200 145919 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 20:44:00.173349 145919 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745239440 (unix time) try "date -d @1745239440" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x23987) received by PID 145799 (TID 0x7f06bb2b7700) from PID 145799 ***]

2025-04-21 20:44:45.594153 test begin: paddle.Tensor.cumsum(Tensor([79226, 144, 200],"float32"), 2, )

W0421 20:46:12.445624 148066 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 20:46:12.446780 148066 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745239572 (unix time) try "date -d @1745239572" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x241e3) received by PID 147939 (TID 0x7fddc5dc2700) from PID 147939 ***]

2025-04-21 20:46:59.211328 test begin: paddle.Tensor.cumsum(Tensor([82527, 192, 144],"float32"), 1, )

W0421 20:48:28.190515 150226 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 20:48:28.191768 150226 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745239708 (unix time) try "date -d @1745239708" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x24a4f) received by PID 150095 (TID 0x7f98454f4700) from PID 150095 ***]

2025-04-21 20:49:15.071627 test begin: paddle.Tensor.cumsum(Tensor([82527, 192, 144],"float32"), 2, )

W0421 20:50:43.395469 152663 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 20:50:43.396682 152663 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745239843 (unix time) try "date -d @1745239843" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x253e3) received by PID 152547 (TID 0x7f72bb6f8700) from PID 152547 ***]

2025-04-21 20:51:26.921819 test begin: paddle.Tensor.diff(Tensor([2281701379],"float32"), )

W0421 20:52:41.527375 155057 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 20:52:41.528120 155057 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.Tensor.diff(Tensor([2281701379],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2235848351 / 2281701378 (98%)
Max absolute difference: 0.99996024
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)
 y: array([ 0.401145,  0.247601, -0.628788, ...,  0.070783, -0.049194,
        0.292945], dtype=float32)
2025-04-21 20:54:23.907904 test begin: paddle.Tensor.diff(Tensor([4294967297],"float16"), )

[accuracy error] paddle.Tensor.diff(Tensor([4294967297],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208561242 / 4294967296 (98%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float16)
 y: array([-0.198  ,  0.0958 , -0.3772 , ..., -0.5376 , -0.04736,  0.742  ],
      dtype=float16)
2025-04-21 21:08:00.402865 test begin: paddle.Tensor.diff(x=Tensor([10, 429496730],"float16"), axis=0, prepend=Tensor([4, 429496730],"float16"), append=Tensor([4, 429496730],"float16"), )

[accuracy error] paddle.Tensor.diff(x=Tensor([10, 429496730],"float16"), axis=0, prepend=Tensor([4, 429496730],"float16"), append=Tensor([4, 429496730],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 7154553728 / 7301444410 (98%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[ 0.02197,  0.3347 ,  0.2622 , ...,  0.11694, -0.0647 ,  0.6045 ],
       [ 0.1726 , -0.259  , -0.385  , ...,  0.755  ,  0.829  , -0.2346 ],
       [-0.03662, -0.4172 , -0.5713 , ..., -0.516  , -0.4512 ,  0.1606 ],...
2025-04-21 21:29:22.695929 test begin: paddle.Tensor.diff(x=Tensor([10, 4],"float16"), axis=0, prepend=Tensor([1073741825, 4],"float16"), append=Tensor([4, 4],"float16"), )

[accuracy error] paddle.Tensor.diff(x=Tensor([10, 4],"float16"), axis=0, prepend=Tensor([1073741825, 4],"float16"), append=Tensor([4, 4],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208560925 / 4294967352 (98%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.],...
 y: array([[-0.5664 , -0.2346 , -0.6494 ,  0.2568 ],
       [ 0.7056 ,  0.03595,  0.2505 ,  0.1594 ],
       [-0.1008 ,  0.0279 ,  0.3335 ,  0.0531 ],...
2025-04-21 21:41:48.990351 test begin: paddle.Tensor.diff(x=Tensor([10, 4],"float16"), axis=0, prepend=Tensor([4, 4],"float16"), append=Tensor([1073741825, 4],"float16"), )

[accuracy error] paddle.Tensor.diff(x=Tensor([10, 4],"float16"), axis=0, prepend=Tensor([4, 4],"float16"), append=Tensor([1073741825, 4],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208560925 / 4294967352 (98%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.],...
 y: array([[-0.5664 , -0.2346 , -0.6494 ,  0.2568 ],
       [ 0.7056 ,  0.03595,  0.2505 ,  0.1594 ],
       [-0.1008 ,  0.0279 ,  0.3335 ,  0.0531 ],...
2025-04-21 21:54:38.685782 test begin: paddle.Tensor.diff(x=Tensor([1073741825, 4],"float16"), axis=0, prepend=Tensor([4, 4],"float16"), append=Tensor([4, 4],"float16"), )

[accuracy error] paddle.Tensor.diff(x=Tensor([1073741825, 4],"float16"), axis=0, prepend=Tensor([4, 4],"float16"), append=Tensor([4, 4],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208560902 / 4294967328 (98%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.],...
 y: array([[-0.5664 , -0.2346 , -0.6494 ,  0.2568 ],
       [ 0.7056 ,  0.03595,  0.2505 ,  0.1594 ],
       [-0.1008 ,  0.0279 ,  0.3335 ,  0.0531 ],...
2025-04-21 22:06:56.748882 test begin: paddle.Tensor.diff(x=Tensor([10],"float16"), prepend=Tensor([4294967297],"float16"), )

[accuracy error] paddle.Tensor.diff(x=Tensor([10],"float16"), prepend=Tensor([4294967297],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208561252 / 4294967306 (98%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float16)
 y: array([-0.198 ,  0.0958, -0.3772, ...,  0.529 ,  0.3618, -0.5356],
      dtype=float16)
2025-04-21 22:19:29.288492 test begin: paddle.Tensor.diff(x=Tensor([10],"float16"), prepend=Tensor([4294967297],"float16"), append=Tensor([4],"float16"), )

[accuracy error] paddle.Tensor.diff(x=Tensor([10],"float16"), prepend=Tensor([4294967297],"float16"), append=Tensor([4],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208561256 / 4294967310 (98%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float16)
 y: array([-0.198 ,  0.0958, -0.3772, ..., -0.198 ,  0.0958, -0.3772],
      dtype=float16)
2025-04-21 22:32:05.100079 test begin: paddle.Tensor.diff(x=Tensor([10],"float16"), prepend=Tensor([4],"float16"), append=Tensor([4294967297],"float16"), )

[accuracy error] paddle.Tensor.diff(x=Tensor([10],"float16"), prepend=Tensor([4],"float16"), append=Tensor([4294967297],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208561256 / 4294967310 (98%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float16)
 y: array([-0.198  ,  0.0958 , -0.3772 , ..., -0.5376 , -0.04736,  0.742  ],
      dtype=float16)
2025-04-21 22:44:44.705951 test begin: paddle.Tensor.diff(x=Tensor([2281701379],"float32"), )

[accuracy error] paddle.Tensor.diff(x=Tensor([2281701379],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2235848351 / 2281701378 (98%)
Max absolute difference: 0.99996024
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)
 y: array([ 0.401145,  0.247601, -0.628788, ...,  0.070783, -0.049194,
        0.292945], dtype=float32)
2025-04-21 22:46:54.548362 test begin: paddle.Tensor.diff(x=Tensor([2281701379],"int32"), )

[accuracy error] paddle.Tensor.diff(x=Tensor([2281701379],"int32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281683995 / 2281701378 (100%)
Max absolute difference: 131069
Max relative difference: 1.
 x: array([0, 0, 0, ..., 0, 0, 0], dtype=int32)
 y: array([108370, -23183, -45700, ...,  71331, -36713, -71807], dtype=int32)
2025-04-21 22:50:37.540765 test begin: paddle.Tensor.diff(x=Tensor([2281701379],"int64"), )

[accuracy error] paddle.Tensor.diff(x=Tensor([2281701379],"int64"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281683915 / 2281701378 (100%)
Max absolute difference: 131067
Max relative difference: 1.
 x: array([0, 0, 0, ..., 0, 0, 0])
 y: array([-66379,  82179,   -325, ..., -57107, -65155,  49850])
2025-04-21 22:56:50.449662 test begin: paddle.Tensor.diff(x=Tensor([4, 1073741825],"float16"), )

[cuda error] paddle.Tensor.diff(x=Tensor([4, 1073741825],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 22:57:07.354519 test begin: paddle.Tensor.diff(x=Tensor([4, 4, 268435457],"float16"), )

[cuda error] paddle.Tensor.diff(x=Tensor([4, 4, 268435457],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 22:57:17.168946 test begin: paddle.Tensor.diff(x=Tensor([4294967297],"float16"), )

[accuracy error] paddle.Tensor.diff(x=Tensor([4294967297],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208561242 / 4294967296 (98%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float16)
 y: array([-0.198  ,  0.0958 , -0.3772 , ..., -0.5376 , -0.04736,  0.742  ],
      dtype=float16)
2025-04-21 23:10:13.310647 test begin: paddle.Tensor.diff(x=Tensor([4294967297],"float16"), prepend=Tensor([4294967297],"float16"), )

[accuracy error] paddle.Tensor.diff(x=Tensor([4294967297],"float16"), prepend=Tensor([4294967297],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 8417122485 / 8589934593 (98%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float16)
 y: array([-0.198  ,  0.0958 , -0.3772 , ..., -0.5376 , -0.04736,  0.742  ],
      dtype=float16)
2025-04-21 23:36:49.193289 test begin: paddle.Tensor.diff(x=Tensor([4294967297],"float16"), prepend=Tensor([4],"float16"), )

[accuracy error] paddle.Tensor.diff(x=Tensor([4294967297],"float16"), prepend=Tensor([4],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208561246 / 4294967300 (98%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float16)
 y: array([-0.198  ,  0.0958 , -0.3772 , ..., -0.5376 , -0.04736,  0.742  ],
      dtype=float16)
2025-04-21 23:49:37.640482 test begin: paddle.Tensor.diff(x=Tensor([4294967297],"float16"), prepend=Tensor([4],"float16"), append=Tensor([4],"float16"), )

[accuracy error] paddle.Tensor.diff(x=Tensor([4294967297],"float16"), prepend=Tensor([4],"float16"), append=Tensor([4],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208561250 / 4294967304 (98%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float16)
 y: array([-0.198 ,  0.0958, -0.3772, ..., -0.198 ,  0.0958, -0.3772],
      dtype=float16)
2025-04-22 00:02:35.235897 test begin: paddle.Tensor.diff(x=Tensor([67108865, 4, 4, 4],"float16"), )

[cuda error] paddle.Tensor.diff(x=Tensor([67108865, 4, 4, 4],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 00:02:48.308528 test begin: paddle.Tensor.diff(x=Tensor([67108865, 4, 4, 4],"float16"), axis=-2, )

[cuda error] paddle.Tensor.diff(x=Tensor([67108865, 4, 4, 4],"float16"), axis=-2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 00:02:55.976622 test begin: paddle.Tensor.diff(x=Tensor([67108865, 4, 4, 4],"float16"), axis=2, )

[cuda error] paddle.Tensor.diff(x=Tensor([67108865, 4, 4, 4],"float16"), axis=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-22 00:03:03.317260 test begin: paddle.Tensor.expand_as(Tensor([1, 1],"int32"), Tensor([2281701379, 1],"int64"), )

[paddle error] paddle.Tensor.expand_as(Tensor([1, 1],"int32"), Tensor([2281701379, 1],"int64"), ) 
 (InvalidArgument) When the value in shape is negative for expand_as_v2 op, only -1 is supported, but the value received is -2013265917.
  [Hint: Expected target_shape[i] == -1, but received target_shape[i]:-2013265917 != -1:-1.] (at /host_home/wanghuan29/Paddle/paddle/phi/kernels/gpu/expand_as_kernel.cu:76)

2025-04-22 00:03:22.362111 test begin: paddle.Tensor.expand_as(Tensor([2281701379, 1],"int32"), Tensor([2281701379, 1],"int64"), )

[paddle error] paddle.Tensor.expand_as(Tensor([2281701379, 1],"int32"), Tensor([2281701379, 1],"int64"), ) 
 (InvalidArgument) When the value in shape is negative for expand_as_v2 op, only -1 is supported, but the value received is -2013265917.
  [Hint: Expected target_shape[i] == -1, but received target_shape[i]:-2013265917 != -1:-1.] (at /host_home/wanghuan29/Paddle/paddle/phi/kernels/gpu/expand_as_kernel.cu:76)

2025-04-22 00:03:41.254325 test begin: paddle.Tensor.flatten(Tensor([1, 1, 12, 190141782],"float32"), start_axis=2, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 1, 12, 190141782],"float32"), start_axis=2, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, 1, -2013265912], shape[2] = -2013265912.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265912 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:04:02.188971 test begin: paddle.Tensor.flatten(Tensor([1, 1, 16, 142606337],"float32"), start_axis=2, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 1, 16, 142606337],"float32"), start_axis=2, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, 1, -2013265904], shape[2] = -2013265904.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265904 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:04:20.733531 test begin: paddle.Tensor.flatten(Tensor([1, 1, 20, 114085069],"float32"), start_axis=2, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 1, 20, 114085069],"float32"), start_axis=2, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, 1, -2013265916], shape[2] = -2013265916.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265916 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:04:37.121841 test begin: paddle.Tensor.flatten(Tensor([1, 1, 2281701379],"float32"), stop_axis=-2, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 1, 2281701379],"float32"), stop_axis=-2, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265917], shape[1] = -2013265917.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265917 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:04:54.997098 test begin: paddle.Tensor.flatten(Tensor([1, 1, 2281701379],"int32"), )

[paddle error] paddle.Tensor.flatten(Tensor([1, 1, 2281701379],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265917], shape[0] = -2013265917.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265917 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:05:03.937015 test begin: paddle.Tensor.flatten(Tensor([1, 1, 2281701379],"int64"), stop_axis=-2, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 1, 2281701379],"int64"), stop_axis=-2, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265917], shape[1] = -2013265917.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265917 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:05:28.530025 test begin: paddle.Tensor.flatten(Tensor([1, 1, 35651585, 64],"float32"), start_axis=2, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 1, 35651585, 64],"float32"), start_axis=2, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, 1, -2013265856], shape[2] = -2013265856.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265856 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:05:45.975833 test begin: paddle.Tensor.flatten(Tensor([1, 1, 6, 380283564],"float32"), start_axis=2, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 1, 6, 380283564],"float32"), start_axis=2, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, 1, -2013265912], shape[2] = -2013265912.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265912 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:06:03.199467 test begin: paddle.Tensor.flatten(Tensor([1, 1, 8, 285212673],"float32"), start_axis=2, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 1, 8, 285212673],"float32"), start_axis=2, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, 1, -2013265912], shape[2] = -2013265912.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265912 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:06:21.121482 test begin: paddle.Tensor.flatten(Tensor([1, 10, 228170138],"float32"), 1, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 10, 228170138],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265916], shape[1] = -2013265916.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265916 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:06:39.498401 test begin: paddle.Tensor.flatten(Tensor([1, 10, 228170138],"int32"), )

[paddle error] paddle.Tensor.flatten(Tensor([1, 10, 228170138],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265916], shape[0] = -2013265916.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265916 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:06:51.708605 test begin: paddle.Tensor.flatten(Tensor([1, 101, 22591103],"int32"), )

[paddle error] paddle.Tensor.flatten(Tensor([1, 101, 22591103],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265893], shape[0] = -2013265893.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265893 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:07:03.646922 test begin: paddle.Tensor.flatten(Tensor([1, 102, 22369622],"int32"), )

[paddle error] paddle.Tensor.flatten(Tensor([1, 102, 22369622],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265852], shape[0] = -2013265852.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265852 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:07:12.711826 test begin: paddle.Tensor.flatten(Tensor([1, 103, 22152441],"int32"), )

[paddle error] paddle.Tensor.flatten(Tensor([1, 103, 22152441],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265873], shape[0] = -2013265873.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265873 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:07:22.794045 test begin: paddle.Tensor.flatten(Tensor([1, 11408507, 200],"float32"), 1, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 11408507, 200],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265896], shape[1] = -2013265896.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265896 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:07:43.977934 test begin: paddle.Tensor.flatten(Tensor([1, 12, 190141782],"float32"), 1, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 12, 190141782],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265912], shape[1] = -2013265912.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265912 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:08:01.844723 test begin: paddle.Tensor.flatten(Tensor([1, 142606337, 4, 4],"float32"), start_axis=1, stop_axis=3, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 142606337, 4, 4],"float32"), start_axis=1, stop_axis=3, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265904], shape[1] = -2013265904.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265904 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:08:18.074784 test begin: paddle.Tensor.flatten(Tensor([1, 144, 15845149],"float32"), 1, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 144, 15845149],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265840], shape[1] = -2013265840.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265840 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:08:34.058083 test begin: paddle.Tensor.flatten(Tensor([1, 15845149, 144],"float32"), 1, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 15845149, 144],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265840], shape[1] = -2013265840.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265840 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:08:50.421225 test begin: paddle.Tensor.flatten(Tensor([1, 18, 126761188],"float32"), 1, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 18, 126761188],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265912], shape[1] = -2013265912.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265912 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:09:10.329292 test begin: paddle.Tensor.flatten(Tensor([1, 192, 11883862],"float32"), 1, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 192, 11883862],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265792], shape[1] = -2013265792.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265792 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:09:28.744088 test begin: paddle.Tensor.flatten(Tensor([1, 2281701379, 1],"int32"), )

[paddle error] paddle.Tensor.flatten(Tensor([1, 2281701379, 1],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265917], shape[0] = -2013265917.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265917 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:09:40.377943 test begin: paddle.Tensor.flatten(Tensor([1, 2281701379],"float32"), 1, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 2281701379],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265917], shape[1] = -2013265917.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265917 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:10:04.824475 test begin: paddle.Tensor.flatten(Tensor([1, 2281701379],"int32"), )

[paddle error] paddle.Tensor.flatten(Tensor([1, 2281701379],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265917], shape[0] = -2013265917.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265917 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:10:13.445729 test begin: paddle.Tensor.flatten(Tensor([1, 2281701379],"int64"), )

[paddle error] paddle.Tensor.flatten(Tensor([1, 2281701379],"int64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265917], shape[0] = -2013265917.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265917 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:10:38.234998 test begin: paddle.Tensor.flatten(Tensor([1, 253522376, 9],"float32"), 1, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 253522376, 9],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265912], shape[1] = -2013265912.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265912 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:10:55.727247 test begin: paddle.Tensor.flatten(Tensor([1, 268435457, 4, 4],"float16"), start_axis=1, stop_axis=3, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 268435457, 4, 4],"float16"), start_axis=1, stop_axis=3, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 268435457, 4, 4], X's size = 4294967312, 'shape' is [1, 16], the capacity of 'shape' is 16.
  [Hint: Expected capacity == in_size, but received capacity:16 != in_size:4294967312.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2257)

2025-04-22 00:11:16.498158 test begin: paddle.Tensor.flatten(Tensor([1, 285212673, 8],"float32"), 1, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 285212673, 8],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265912], shape[1] = -2013265912.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265912 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:11:36.828646 test begin: paddle.Tensor.flatten(Tensor([1, 6, 178956971, 4],"float16"), start_axis=1, stop_axis=3, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 6, 178956971, 4],"float16"), start_axis=1, stop_axis=3, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 6, 178956971, 4], X's size = 4294967304, 'shape' is [1, 8], the capacity of 'shape' is 8.
  [Hint: Expected capacity == in_size, but received capacity:8 != in_size:4294967304.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2257)

2025-04-22 00:11:52.193212 test begin: paddle.Tensor.flatten(Tensor([1, 6, 4, 178956971],"float16"), start_axis=1, stop_axis=3, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 6, 4, 178956971],"float16"), start_axis=1, stop_axis=3, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 6, 4, 178956971], X's size = 4294967304, 'shape' is [1, 8], the capacity of 'shape' is 8.
  [Hint: Expected capacity == in_size, but received capacity:8 != in_size:4294967304.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2257)

2025-04-22 00:12:10.918562 test begin: paddle.Tensor.flatten(Tensor([1, 6, 4, 95070891],"float32"), start_axis=1, stop_axis=3, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 6, 4, 95070891],"float32"), start_axis=1, stop_axis=3, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265912], shape[1] = -2013265912.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265912 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:12:30.591409 test begin: paddle.Tensor.flatten(Tensor([1, 6, 95070891, 4],"float32"), start_axis=1, stop_axis=3, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 6, 95070891, 4],"float32"), start_axis=1, stop_axis=3, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265912], shape[1] = -2013265912.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265912 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:12:46.925012 test begin: paddle.Tensor.flatten(Tensor([1, 91268056, 25],"float32"), 1, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 91268056, 25],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265896], shape[1] = -2013265896.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265896 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:13:03.486801 test begin: paddle.Tensor.flatten(Tensor([10, 228170138],"float32"), 0, 1, )

[paddle error] paddle.Tensor.flatten(Tensor([10, 228170138],"float32"), 0, 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265916], shape[0] = -2013265916.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265916 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:13:22.808331 test begin: paddle.Tensor.flatten(Tensor([100, 22817014],"float32"), 0, 1, )

[paddle error] paddle.Tensor.flatten(Tensor([100, 22817014],"float32"), 0, 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265896], shape[0] = -2013265896.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265896 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:13:40.083142 test begin: paddle.Tensor.flatten(Tensor([12, 18397, 76, 136],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([12, 18397, 76, 136],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013150592], shape[0] = -2013150592.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013150592 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:13:59.489968 test begin: paddle.Tensor.flatten(Tensor([12, 19, 10007463, 1],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([12, 19, 10007463, 1],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265732], shape[0] = -2013265732.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265732 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:14:18.971038 test begin: paddle.Tensor.flatten(Tensor([12, 19, 34, 294338],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([12, 19, 34, 294338],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013259120], shape[0] = -2013259120.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013259120 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:14:39.066305 test begin: paddle.Tensor.flatten(Tensor([12, 2796203, 68, 1],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([12, 2796203, 68, 1],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265648], shape[0] = -2013265648.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265648 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:14:59.273636 test begin: paddle.Tensor.flatten(Tensor([12, 294338, 19, 34],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([12, 294338, 19, 34],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013259120], shape[0] = -2013259120.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013259120 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:15:16.813357 test begin: paddle.Tensor.flatten(Tensor([12, 38, 5003732, 1],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([12, 38, 5003732, 1],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265504], shape[0] = -2013265504.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265504 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:15:34.990917 test begin: paddle.Tensor.flatten(Tensor([12, 38, 68, 73585],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([12, 38, 68, 73585],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013243616], shape[0] = -2013243616.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013243616 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:15:54.234287 test begin: paddle.Tensor.flatten(Tensor([12, 4, 1398102, 34],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([12, 4, 1398102, 34],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013264832], shape[0] = -2013264832.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013264832 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:16:12.430050 test begin: paddle.Tensor.flatten(Tensor([12, 4, 19, 2501866],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([12, 4, 19, 2501866],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265504], shape[0] = -2013265504.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265504 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:16:32.511124 test begin: paddle.Tensor.flatten(Tensor([12, 4, 349526, 136],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([12, 4, 349526, 136],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013261568], shape[0] = -2013261568.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013261568 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:16:51.532188 test begin: paddle.Tensor.flatten(Tensor([12, 4, 38, 1250933],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([12, 4, 38, 1250933],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265504], shape[0] = -2013265504.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265504 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:17:12.224918 test begin: paddle.Tensor.flatten(Tensor([12, 4, 699051, 68],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([12, 4, 699051, 68],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013264832], shape[0] = -2013264832.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013264832 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:17:30.611570 test begin: paddle.Tensor.flatten(Tensor([12, 4, 76, 625467],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([12, 4, 76, 625467],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013263680], shape[0] = -2013263680.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013263680 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:17:48.040940 test begin: paddle.Tensor.flatten(Tensor([12, 5592406, 34, 1],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([12, 5592406, 34, 1],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265648], shape[0] = -2013265648.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265648 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:18:06.935037 test begin: paddle.Tensor.flatten(Tensor([12, 73585, 38, 68],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([12, 73585, 38, 68],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013243616], shape[0] = -2013243616.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013243616 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:18:25.789897 test begin: paddle.Tensor.flatten(Tensor([2, 3, 15845149, 6, 4],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([2, 3, 15845149, 6, 4],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265840], shape[0] = -2013265840.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265840 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:18:45.101833 test begin: paddle.Tensor.flatten(Tensor([2, 3, 6, 15845149, 4],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([2, 3, 6, 15845149, 4],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265840], shape[0] = -2013265840.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265840 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:19:04.649662 test begin: paddle.Tensor.flatten(Tensor([2, 3, 6, 6, 10563433],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([2, 3, 6, 6, 10563433],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265768], shape[0] = -2013265768.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265768 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:19:25.816964 test begin: paddle.Tensor.flatten(Tensor([2, 7922575, 6, 6, 4],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([2, 7922575, 6, 6, 4],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265696], shape[0] = -2013265696.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265696 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:19:44.737191 test begin: paddle.Tensor.flatten(Tensor([220753, 4, 38, 68],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([220753, 4, 38, 68],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013264288], shape[0] = -2013264288.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013264288 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:20:03.889311 test begin: paddle.Tensor.flatten(Tensor([22152441, 103, 1],"int32"), )

[paddle error] paddle.Tensor.flatten(Tensor([22152441, 103, 1],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265873], shape[0] = -2013265873.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265873 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:20:14.642866 test begin: paddle.Tensor.flatten(Tensor([22369622, 102, 1],"int32"), )

[paddle error] paddle.Tensor.flatten(Tensor([22369622, 102, 1],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265852], shape[0] = -2013265852.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265852 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:20:24.627515 test begin: paddle.Tensor.flatten(Tensor([22591103, 101, 1],"int32"), )

[paddle error] paddle.Tensor.flatten(Tensor([22591103, 101, 1],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265893], shape[0] = -2013265893.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265893 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:20:33.854470 test begin: paddle.Tensor.flatten(Tensor([2277148, 1002],"int32"), )

[paddle error] paddle.Tensor.flatten(Tensor([2277148, 1002],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265000], shape[0] = -2013265000.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265000 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:20:42.485307 test begin: paddle.Tensor.flatten(Tensor([2277148, 1002],"int64"), )

[paddle error] paddle.Tensor.flatten(Tensor([2277148, 1002],"int64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265000], shape[0] = -2013265000.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265000 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:21:06.720088 test begin: paddle.Tensor.flatten(Tensor([2279422, 1001],"int32"), )

[paddle error] paddle.Tensor.flatten(Tensor([2279422, 1001],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265874], shape[0] = -2013265874.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265874 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:21:15.453024 test begin: paddle.Tensor.flatten(Tensor([2279422, 1001],"int64"), )

[paddle error] paddle.Tensor.flatten(Tensor([2279422, 1001],"int64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265874], shape[0] = -2013265874.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265874 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:21:36.822413 test begin: paddle.Tensor.flatten(Tensor([2281701379, 1, 1],"int32"), )

[paddle error] paddle.Tensor.flatten(Tensor([2281701379, 1, 1],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265917], shape[0] = -2013265917.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265917 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:21:45.750543 test begin: paddle.Tensor.flatten(Tensor([2281701379],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([2281701379],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265917], shape[0] = -2013265917.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265917 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:22:06.026952 test begin: paddle.Tensor.flatten(Tensor([2281701379],"int32"), )

[paddle error] paddle.Tensor.flatten(Tensor([2281701379],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265917], shape[0] = -2013265917.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265917 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:22:17.138534 test begin: paddle.Tensor.flatten(Tensor([228170138, 10, 1],"int32"), )

[paddle error] paddle.Tensor.flatten(Tensor([228170138, 10, 1],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265916], shape[0] = -2013265916.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265916 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:22:25.983980 test begin: paddle.Tensor.flatten(Tensor([228170138, 10],"float32"), 0, 1, )

[paddle error] paddle.Tensor.flatten(Tensor([228170138, 10],"float32"), 0, 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265916], shape[0] = -2013265916.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265916 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:22:48.925557 test begin: paddle.Tensor.flatten(Tensor([22817014, 100],"float32"), 0, 1, )

[paddle error] paddle.Tensor.flatten(Tensor([22817014, 100],"float32"), 0, 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265896], shape[0] = -2013265896.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265896 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:23:06.477337 test begin: paddle.Tensor.flatten(Tensor([3532046, 19, 34, 1],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([3532046, 19, 34, 1],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265580], shape[0] = -2013265580.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265580 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:23:25.855674 test begin: paddle.Tensor.flatten(Tensor([4294967297],"bfloat16"), )

[paddle error] paddle.Tensor.flatten(Tensor([4294967297],"bfloat16"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [4294967297], X's size = 4294967297, 'shape' is [1], the capacity of 'shape' is 1.
  [Hint: Expected capacity == in_size, but received capacity:1 != in_size:4294967297.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2257)

2025-04-22 00:24:02.204789 test begin: paddle.Tensor.flatten(Tensor([5281717, 3, 6, 6, 4],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([5281717, 3, 6, 6, 4],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265552], shape[0] = -2013265552.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265552 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:24:18.457865 test begin: paddle.Tensor.flatten(Tensor([55189, 4, 76, 136],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([55189, 4, 76, 136],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013233280], shape[0] = -2013233280.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013233280 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:24:37.008853 test begin: paddle.Tensor.flatten(Tensor([883012, 38, 68, 1],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([883012, 38, 68, 1],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013264288], shape[0] = -2013264288.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013264288 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:24:56.861967 test begin: paddle.Tensor.flatten(Tensor([883012, 4, 19, 34],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([883012, 4, 19, 34],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013264288], shape[0] = -2013264288.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013264288 <= 0:0.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/unary.cc:2198)

2025-04-22 00:25:14.976252 test begin: paddle.Tensor.gcd(x=Tensor([2281701379],"int32"), y=Tensor([1],"int32"), )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.gcd(x=Tensor([2281701379],"int32"), y=Tensor([1],"int32"), ) 
 (InvalidArgument) The [0] th of Inputs(X) and Inputs(Y) should be same. But received X's shape is [1], Y's shape is [2281701379]
  [Hint: Expected x_dims[i] == y_dims[i], but received x_dims[i]:1 != y_dims[i]:2281701379.] (at /host_home/wanghuan29/Paddle/paddle/phi/infermeta/multiary.cc:5638)

2025-04-22 00:25:35.013146 test begin: paddle.Tensor.inner(x=Tensor([2, 5, 3, 143165577],"float16"), y=Tensor([3, 2, 5, 143165577],"float16"), )

[accuracy error] paddle.Tensor.inner(x=Tensor([2, 5, 3, 143165577],"float16"), y=Tensor([3, 2, 5, 143165577],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 74 / 900 (8.22%)
Max absolute difference: 16.
Max relative difference: 1.656
 x: array([[[[[[        inf, -1.8138e+02,  1.0550e+03,  8.0100e+02,
             1.3930e+03],
           [ 9.0812e+01, -4.0475e+02,  3.3075e+02, -1.6060e+03,...
 y: array([[[[[[        inf, -1.8175e+02,  1.0550e+03,  8.0250e+02,
             1.3940e+03],
           [ 9.2500e+01, -4.0600e+02,  3.2900e+02, -1.6030e+03,...
2025-04-22 00:25:57.353559 test begin: paddle.Tensor.kthvalue(Tensor([1140851, 200, 10],"float32"), k=200, axis=1, )

element 1 of tensors does not require grad and does not have a grad_fn
[accuracy error] paddle.Tensor.kthvalue(Tensor([1140851, 200, 10],"float32"), k=200, axis=1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 15 / 11408510 (0.000131%)
Max absolute difference: 178
Max relative difference: 77.
 x: array([[ 21, 171,   0, ..., 136,  27, 172],
       [142,  78, 101, ..., 148, 174, 101],
       [ 73, 161,  54, ...,  45,  97, 177],...
 y: array([[ 21, 171,   0, ..., 136,  27, 172],
       [142,  78, 101, ..., 148, 174, 101],
       [ 73, 161,  54, ...,  45,  97, 177],...
2025-04-22 00:26:02.108229 test begin: paddle.Tensor.kthvalue(Tensor([2, 114085069, 10],"float32"), k=200, axis=1, )

element 1 of tensors does not require grad and does not have a grad_fn
[accuracy error] paddle.Tensor.kthvalue(Tensor([2, 114085069, 10],"float32"), k=200, axis=1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4 / 20 (20%)
Max absolute difference: 50013121
Max relative difference: 5.70794593
 x: array([[101699714,   1420769,  92659773,  62462248,  95600982,  75349196,
         13852815,  46219289,  63773759,  52305093],
       [ 49780052,  44700739,  14378140,  95316232,  19449914,  20545622,
         34660381,  39941195,  74607049,  88330982]])
 y: array([[101699714,   1420769,  92659773,  62462248,  95600982,  75349196,
         13852815,  46219289,  63773759,  52305093],
       [ 73729292,  94713860,   2143449,  95316232,  20996175,  20545622,
         34660381,  39941195,  74607049,  88330982]])
2025-04-22 00:26:08.153140 test begin: paddle.Tensor.kthvalue(Tensor([2, 200, 5704254],"float32"), k=200, axis=1, )

element 1 of tensors does not require grad and does not have a grad_fn
[accuracy error] paddle.Tensor.kthvalue(Tensor([2, 200, 5704254],"float32"), k=200, axis=1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 16 / 11408508 (0.00014%)
Max absolute difference: 160
Max relative difference: 19.66666667
 x: array([[ 83, 108, 139, ...,  61, 122,  52],
       [ 83, 184, 199, ...,  35, 137, 129]])
 y: array([[ 83, 108, 139, ...,  61, 122,  52],
       [ 83, 184, 199, ...,  35, 137, 129]])
2025-04-22 00:26:12.942804 test begin: paddle.Tensor.lerp(x=Tensor([214748365, 5, 4],"float16"), y=Tensor([214748365, 5, 4],"float16"), weight=0.5, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([214748365, 5, 4],"float16"), y=Tensor([214748365, 5, 4],"float16"), weight=0.5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4121379383 / 4294967300 (96%)
Max absolute difference: 0.25
Max relative difference: 1.
 x: array([[[ 0.1292 , -0.06775, -0.2125 , -0.2114 ],
        [ 0.     ,  0.     ,  0.     ,  0.     ],
        [ 0.     ,  0.     ,  0.     ,  0.     ],...
 y: array([[[ 1.2915e-01, -6.7749e-02, -2.1252e-01, -2.1143e-01],
        [ 1.2164e-01, -2.1667e-01, -8.3191e-02,  1.6040e-01],
        [-1.1848e-02,  6.6650e-02,  2.0947e-01,  1.4514e-01],...
2025-04-22 00:48:26.361621 test begin: paddle.Tensor.lerp(x=Tensor([2281701379],"float32"), y=Tensor([2281701379],"float32"), weight=0.5, )

W0422 00:50:12.774487 49048 backward.cc:441] While running Node (LerpGradNode) raises a std::exception: paddle::memory::allocation::BadAlloc
[paddle error] paddle.Tensor.lerp(x=Tensor([2281701379],"float32"), y=Tensor([2281701379],"float32"), weight=0.5, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   LerpGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::lerp_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor*, paddle::Tensor*)
4   void phi::LerpGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*)
5   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
6   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 8.500000GB memory on GPU 0, 72.385681GB memory has been allocated and available memory is only 6.799194GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /host_home/wanghuan29/Paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-04-22 00:50:14.274697 test begin: paddle.Tensor.lerp(x=Tensor([4, 1073741825],"float16"), y=Tensor([1],"float16"), weight=0.2, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([4, 1073741825],"float16"), y=Tensor([1],"float16"), weight=0.2, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4186477994 / 4294967300 (97.5%)
Max absolute difference: 0.4
Max relative difference: 1.
 x: array([[ 0.2067, -0.1084, -0.34  , ...,  0.    ,  0.    ,  0.    ],
       [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],
       [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],...
 y: array([[ 0.2067 , -0.1084 , -0.34   , ...,  0.3962 , -0.1209 ,  0.156  ],
       [ 0.3137 ,  0.07367,  0.2605 , ...,  0.2067 ,  0.3801 , -0.2148 ],
       [ 0.0925 ,  0.2576 ,  0.2064 , ..., -0.1539 , -0.3599 ,  0.1619 ],...
2025-04-22 01:11:16.331083 test begin: paddle.Tensor.lerp(x=Tensor([4, 1073741825],"float16"), y=Tensor([4, 1073741825],"float16"), weight=0.5, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([4, 1073741825],"float16"), y=Tensor([4, 1073741825],"float16"), weight=0.5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4121379383 / 4294967300 (96%)
Max absolute difference: 0.25
Max relative difference: 1.
 x: array([[ 0.1292 , -0.06775, -0.2125 , ...,  0.     ,  0.     ,  0.     ],
       [ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,  0.     ],
       [ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,  0.     ],...
 y: array([[ 0.1292 , -0.06775, -0.2125 , ...,  0.2477 , -0.07556,  0.09753],
       [ 0.196  ,  0.04605,  0.1628 , ...,  0.1292 ,  0.2375 , -0.1343 ],
       [ 0.05783,  0.161  ,  0.129  , ..., -0.0962 , -0.2249 ,  0.10114],...
2025-04-22 01:32:58.655070 test begin: paddle.Tensor.lerp(x=Tensor([4, 268435457, 4],"float16"), y=Tensor([4, 268435457, 4],"float16"), weight=0.5, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([4, 268435457, 4],"float16"), y=Tensor([4, 268435457, 4],"float16"), weight=0.5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4121379383 / 4294967312 (96%)
Max absolute difference: 0.25
Max relative difference: 1.
 x: array([[[-0.1808  ,  0.0789  ,  0.1052  ,  0.161   ],
        [-0.2104  ,  0.1631  ,  0.04474 ,  0.10156 ],
        [ 0.1355  ,  0.002161,  0.1477  ,  0.246   ],...
 y: array([[[-0.1808  ,  0.0789  ,  0.1052  ,  0.161   ],
        [-0.2104  ,  0.1631  ,  0.04474 ,  0.10156 ],
        [ 0.1355  ,  0.002161,  0.1477  ,  0.246   ],...
2025-04-22 01:54:15.910081 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 214748365],"float16"), y=Tensor([4, 5, 214748365],"float16"), weight=0.5, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([4, 5, 214748365],"float16"), y=Tensor([4, 5, 214748365],"float16"), weight=0.5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4121379383 / 4294967300 (96%)
Max absolute difference: 0.25
Max relative difference: 1.
 x: array([[[ 0.1292 , -0.06775, -0.2125 , ...,  0.     ,  0.     ,
          0.     ],
        [ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,...
 y: array([[[ 0.1292  , -0.06775 , -0.2125  , ..., -0.2062  , -0.03705 ,
         -0.1019  ],
        [-0.144   ,  0.1221  ,  0.04    , ...,  0.1967  ,  0.1791  ,...
2025-04-22 02:15:19.178699 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 4, 53687092],"float16"), y=Tensor([4, 5, 4, 53687092],"float16"), weight=0.0, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([4, 5, 4, 53687092],"float16"), y=Tensor([4, 5, 4, 53687092],"float16"), weight=0.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208170565 / 4294967360 (98%)
Max absolute difference: 0.5
Max relative difference: 1.
 x: array([[[[-0.221  , -0.05887,  0.1616 , ...,  0.     ,  0.     ,
           0.     ],
         [ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,...
 y: array([[[[-0.221   , -0.05887 ,  0.1616  , ..., -0.3994  ,  0.0613  ,
           0.10315 ],
         [-0.08746 , -0.3962  , -0.1434  , ...,  0.10803 ,  0.04727 ,...
2025-04-22 02:36:12.426487 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 4, 53687092],"float16"), y=Tensor([4, 5, 4, 53687092],"float16"), weight=0.5, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([4, 5, 4, 53687092],"float16"), y=Tensor([4, 5, 4, 53687092],"float16"), weight=0.5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4121379383 / 4294967360 (96%)
Max absolute difference: 0.25
Max relative difference: 1.
 x: array([[[[-0.1105 , -0.02943,  0.0808 , ...,  0.     ,  0.     ,
           0.     ],
         [ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,...
 y: array([[[[-0.1105  , -0.02943 ,  0.0808  , ..., -0.1997  ,  0.03065 ,
           0.05157 ],
         [-0.04373 , -0.1981  , -0.0717  , ...,  0.05402 ,  0.02364 ,...
2025-04-22 02:57:16.242199 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 4, 53687092],"float16"), y=Tensor([4, 5, 4, 53687092],"float16"), weight=1.0, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([4, 5, 4, 53687092],"float16"), y=Tensor([4, 5, 4, 53687092],"float16"), weight=1.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208170565 / 4294967360 (98%)
Max absolute difference: 0.5
Max relative difference: 1.
 x: array([[[[-0.221  , -0.05887,  0.1616 , ...,  0.     ,  0.     ,
           0.     ],
         [ 0.     ,  0.     ,  0.     , ...,  0.     ,  0.     ,...
 y: array([[[[-0.221   , -0.05887 ,  0.1616  , ..., -0.3994  ,  0.0613  ,
           0.10315 ],
         [-0.08746 , -0.3962  , -0.1434  , ...,  0.10803 ,  0.04727 ,...
2025-04-22 03:26:36.568636 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 71582789, 3],"float16"), y=Tensor([4, 5, 71582789, 3],"float16"), weight=0.0, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([4, 5, 71582789, 3],"float16"), y=Tensor([4, 5, 71582789, 3],"float16"), weight=0.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208170565 / 4294967340 (98%)
Max absolute difference: 0.5
Max relative difference: 1.
 x: array([[[[-0.2424 , -0.2532 ,  0.4976 ],
         [-0.02371, -0.3147 ,  0.415  ],
         [-0.4695 , -0.06042, -0.2703 ],...
 y: array([[[[-2.4243e-01, -2.5317e-01,  4.9756e-01],
         [-2.3712e-02, -3.1470e-01,  4.1504e-01],
         [-4.6948e-01, -6.0425e-02, -2.7026e-01],...
2025-04-22 03:48:41.498263 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 71582789, 3],"float16"), y=Tensor([4, 5, 71582789, 3],"float16"), weight=0.5, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([4, 5, 71582789, 3],"float16"), y=Tensor([4, 5, 71582789, 3],"float16"), weight=0.5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4121379383 / 4294967340 (96%)
Max absolute difference: 0.25
Max relative difference: 1.
 x: array([[[[-0.1212  , -0.1266  ,  0.2488  ],
         [-0.011856, -0.1573  ,  0.2075  ],
         [-0.2347  , -0.03021 , -0.1351  ],...
 y: array([[[[-1.2122e-01, -1.2659e-01,  2.4878e-01],
         [-1.1856e-02, -1.5735e-01,  2.0752e-01],
         [-2.3474e-01, -3.0212e-02, -1.3513e-01],...
2025-04-22 04:10:36.304824 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 71582789, 3],"float16"), y=Tensor([4, 5, 71582789, 3],"float16"), weight=1.0, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([4, 5, 71582789, 3],"float16"), y=Tensor([4, 5, 71582789, 3],"float16"), weight=1.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208170565 / 4294967340 (98%)
Max absolute difference: 0.5
Max relative difference: 1.
 x: array([[[[-0.2424 , -0.2532 ,  0.4976 ],
         [-0.02371, -0.3147 ,  0.415  ],
         [-0.4695 , -0.06042, -0.2703 ],...
 y: array([[[[-2.4243e-01, -2.5317e-01,  4.9756e-01],
         [-2.3712e-02, -3.1470e-01,  4.1504e-01],
         [-4.6948e-01, -6.0425e-02, -2.7026e-01],...
2025-04-22 04:39:43.707673 test begin: paddle.Tensor.lerp(x=Tensor([4, 89478486, 4, 3],"float16"), y=Tensor([4, 89478486, 4, 3],"float16"), weight=0.0, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([4, 89478486, 4, 3],"float16"), y=Tensor([4, 89478486, 4, 3],"float16"), weight=0.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208170565 / 4294967328 (98%)
Max absolute difference: 0.5
Max relative difference: 1.
 x: array([[[[-0.4673  , -0.3005  , -0.013626],
         [ 0.2607  , -0.2922  , -0.2372  ],
         [ 0.03625 , -0.1852  ,  0.2773  ],...
 y: array([[[[-4.6729e-01, -3.0054e-01, -1.3626e-02],
         [ 2.6074e-01, -2.9224e-01, -2.3718e-01],
         [ 3.6255e-02, -1.8518e-01,  2.7734e-01],...
2025-04-22 05:00:58.674291 test begin: paddle.Tensor.lerp(x=Tensor([4, 89478486, 4, 3],"float16"), y=Tensor([4, 89478486, 4, 3],"float16"), weight=0.5, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([4, 89478486, 4, 3],"float16"), y=Tensor([4, 89478486, 4, 3],"float16"), weight=0.5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4121379383 / 4294967328 (96%)
Max absolute difference: 0.25
Max relative difference: 1.
 x: array([[[[-0.2336  , -0.1503  , -0.006813],
         [ 0.1304  , -0.1461  , -0.1186  ],
         [ 0.01813 , -0.0926  ,  0.1387  ],...
 y: array([[[[-2.3364e-01, -1.5027e-01, -6.8130e-03],
         [ 1.3037e-01, -1.4612e-01, -1.1859e-01],
         [ 1.8127e-02, -9.2590e-02,  1.3867e-01],...
2025-04-22 05:21:52.898508 test begin: paddle.Tensor.lerp(x=Tensor([4, 89478486, 4, 3],"float16"), y=Tensor([4, 89478486, 4, 3],"float16"), weight=1.0, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([4, 89478486, 4, 3],"float16"), y=Tensor([4, 89478486, 4, 3],"float16"), weight=1.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208170565 / 4294967328 (98%)
Max absolute difference: 0.5
Max relative difference: 1.
 x: array([[[[-0.4673  , -0.3005  , -0.013626],
         [ 0.2607  , -0.2922  , -0.2372  ],
         [ 0.03625 , -0.1852  ,  0.2773  ],...
 y: array([[[[-4.6729e-01, -3.0054e-01, -1.3626e-02],
         [ 2.6074e-01, -2.9224e-01, -2.3718e-01],
         [ 3.6255e-02, -1.8518e-01,  2.7734e-01],...
2025-04-22 05:50:27.960430 test begin: paddle.Tensor.lerp(x=Tensor([4294967297],"float16"), y=Tensor([4294967297],"float16"), weight=0.5, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([4294967297],"float16"), y=Tensor([4294967297],"float16"), weight=0.5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4121379383 / 4294967297 (96%)
Max absolute difference: 0.25
Max relative difference: 1.
 x: array([-0.2114,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],
      dtype=float16)
 y: array([-0.2114 ,  0.12164, -0.2167 , ...,  0.0935 ,  0.1273 , -0.154  ],
      dtype=float16)
2025-04-22 06:11:32.306010 test begin: paddle.Tensor.lerp(x=Tensor([71582789, 5, 4, 3],"float16"), y=Tensor([71582789, 5, 4, 3],"float16"), weight=0.0, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([71582789, 5, 4, 3],"float16"), y=Tensor([71582789, 5, 4, 3],"float16"), weight=0.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208170565 / 4294967340 (98%)
Max absolute difference: 0.5
Max relative difference: 1.
 x: array([[[[-0.2424  , -0.2532  ,  0.4976  ],
         [-0.02371 , -0.3147  ,  0.415   ],
         [-0.4695  , -0.06042 , -0.2703  ],...
 y: array([[[[-2.4243e-01, -2.5317e-01,  4.9756e-01],
         [-2.3712e-02, -3.1470e-01,  4.1504e-01],
         [-4.6948e-01, -6.0425e-02, -2.7026e-01],...
2025-04-22 06:32:16.206666 test begin: paddle.Tensor.lerp(x=Tensor([71582789, 5, 4, 3],"float16"), y=Tensor([71582789, 5, 4, 3],"float16"), weight=0.5, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([71582789, 5, 4, 3],"float16"), y=Tensor([71582789, 5, 4, 3],"float16"), weight=0.5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4121379383 / 4294967340 (96%)
Max absolute difference: 0.25
Max relative difference: 1.
 x: array([[[[-0.1212  , -0.1266  ,  0.2488  ],
         [-0.011856, -0.1573  ,  0.2075  ],
         [-0.2347  , -0.03021 , -0.1351  ],...
 y: array([[[[-1.2122e-01, -1.2659e-01,  2.4878e-01],
         [-1.1856e-02, -1.5735e-01,  2.0752e-01],
         [-2.3474e-01, -3.0212e-02, -1.3513e-01],...
2025-04-22 06:53:06.546967 test begin: paddle.Tensor.lerp(x=Tensor([71582789, 5, 4, 3],"float16"), y=Tensor([71582789, 5, 4, 3],"float16"), weight=1.0, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([71582789, 5, 4, 3],"float16"), y=Tensor([71582789, 5, 4, 3],"float16"), weight=1.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208170565 / 4294967340 (98%)
Max absolute difference: 0.5
Max relative difference: 1.
 x: array([[[[-0.2424  , -0.2532  ,  0.4976  ],
         [-0.02371 , -0.3147  ,  0.415   ],
         [-0.4695  , -0.06042 , -0.2703  ],...
 y: array([[[[-2.4243e-01, -2.5317e-01,  4.9756e-01],
         [-2.3712e-02, -3.1470e-01,  4.1504e-01],
         [-4.6948e-01, -6.0425e-02, -2.7026e-01],...
2025-04-22 07:21:38.483319 test begin: paddle.Tensor.lerp(x=Tensor([858993460, 5],"float16"), y=Tensor([1],"float16"), weight=0.2, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([858993460, 5],"float16"), y=Tensor([1],"float16"), weight=0.2, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4186477994 / 4294967300 (97.5%)
Max absolute difference: 0.4
Max relative difference: 1.
 x: array([[ 0.2067, -0.1084, -0.34  , -0.3384,  0.    ],
       [ 0.    ,  0.    ,  0.    ,  0.    ,  0.    ],
       [ 0.    ,  0.    ,  0.    ,  0.    ,  0.    ],...
 y: array([[ 0.2067 , -0.1084 , -0.34   , -0.3384 ,  0.1946 ],
       [-0.3467 , -0.133  ,  0.2566 , -0.01895,  0.1066 ],
       [ 0.3352 ,  0.2322 ,  0.0819 ,  0.067  , -0.0621 ],...
2025-04-22 07:42:25.026678 test begin: paddle.Tensor.lerp(x=Tensor([858993460, 5],"float16"), y=Tensor([858993460, 5],"float16"), weight=0.5, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([858993460, 5],"float16"), y=Tensor([858993460, 5],"float16"), weight=0.5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4121379383 / 4294967300 (96%)
Max absolute difference: 0.25
Max relative difference: 1.
 x: array([[ 0.1292 , -0.06775, -0.2125 , -0.2114 ,  0.     ],
       [ 0.     ,  0.     ,  0.     ,  0.     ,  0.     ],
       [ 0.     ,  0.     ,  0.     ,  0.     ,  0.     ],...
 y: array([[ 0.1292 , -0.06775, -0.2125 , -0.2114 ,  0.12164],
       [-0.2167 , -0.0832 ,  0.1604 , -0.01185,  0.06665],
       [ 0.2095 ,  0.1451 ,  0.05118,  0.04187, -0.03882],...
2025-04-22 08:03:18.122642 test begin: paddle.Tensor.logit(x=Tensor([143165577, 3, 2, 5],"float16"), )

[accuracy error] backward  paddle.Tensor.logit(x=Tensor([143165577, 3, 2, 5],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[[  0.999  ,   3.463  ,  -2.635  ,   0.     ,   0.     ],
         [  0.     ,   0.     ,   0.0593 ,   1.198  ,   0.     ]],
...
 y: array([[[[  0.999  ,   3.463  ,  -2.635  ,       nan,       nan],
         [      nan,       nan,   0.0593 ,   1.198  ,       nan]],
...
2025-04-22 08:11:51.806710 test begin: paddle.Tensor.logit(x=Tensor([2147483649, 2],"float16"), )

[accuracy error] backward  paddle.Tensor.logit(x=Tensor([2147483649, 2],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[-2.018, -4.547],
       [ 1.522,  0.   ],
       [ 0.   ,  0.   ],...
 y: array([[-2.018, -4.547],
       [ 1.522,    nan],
       [   nan,    nan],...
2025-04-22 08:20:35.466910 test begin: paddle.Tensor.logit(x=Tensor([2281701379],"float32"), )

[accuracy error] backward  paddle.Tensor.logit(x=Tensor([2281701379],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([ 0.      ,  0.781529, -0.297888, ...,  0.      ,  0.      ,
        0.      ], dtype=float32)
 y: array([      nan,  0.781529, -0.297888, ...,       nan,       nan,
             nan], dtype=float32)
2025-04-22 08:22:52.691363 test begin: paddle.Tensor.logit(x=Tensor([4, 1073741825],"float16"), )

[accuracy error] backward  paddle.Tensor.logit(x=Tensor([4, 1073741825],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[ 1.226 , -1.457 , -2.66  , ...,  0.    ,  0.    ,  6.76  ],
       [ 0.    ,  0.5737,  0.    , ...,  0.    ,  2.639 , -3.303 ],
       [ 0.    ,  0.    ,  0.    , ...,  0.    , -1.957 ,  0.9146],...
 y: array([[ 1.226 , -1.457 , -2.66  , ...,     nan,     nan,  6.76  ],
       [    nan,  0.5737,     nan, ...,     nan,  2.639 , -3.303 ],
       [    nan,     nan,     nan, ...,     nan, -1.957 ,  0.9146],...
2025-04-22 13:02:40.743873 test begin: paddle.sum(Tensor([3, 4, 357913942],"int32"), axis=0, keepdim=True, )

[paddle error] paddle.sum(Tensor([3, 4, 357913942],"int32"), axis=0, keepdim=True, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_sum(_object*, _object*, _object*)
1   sum_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, phi::DataType, bool)
2   paddle::experimental::sum(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, phi::DataType, bool)
3   void phi::SumKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
4   void phi::SumRawKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
5   void phi::funcs::ReduceKernel<long, long, phi::kps::AddFunctor, phi::kps::IdentityFunctor<long, long>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<long, long> const&, std::vector<int, std::allocator<int> > const&)
6   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
7   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
8   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 10.666667GB memory on GPU 0, 69.377869GB memory has been allocated and available memory is only 9.807007GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /host_home/wanghuan29/Paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-04-22 13:04:34.871309 test begin: paddle.sum(Tensor([3, 715827883, 2],"int32"), axis=0, keepdim=True, )

[paddle error] paddle.sum(Tensor([3, 715827883, 2],"int32"), axis=0, keepdim=True, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_sum(_object*, _object*, _object*)
1   sum_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, phi::DataType, bool)
2   paddle::experimental::sum(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, phi::DataType, bool)
3   void phi::SumKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
4   void phi::SumRawKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
5   void phi::funcs::ReduceKernel<long, long, phi::kps::AddFunctor, phi::kps::IdentityFunctor<long, long>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<long, long> const&, std::vector<int, std::allocator<int> > const&)
6   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
7   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
8   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 10.666667GB memory on GPU 0, 69.377869GB memory has been allocated and available memory is only 9.807007GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /host_home/wanghuan29/Paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-04-22 13:04:53.153891 test begin: paddle.sum(Tensor([3, 715827883, 2],"int32"), axis=1, keepdim=True, )

[accuracy error] paddle.sum(Tensor([3, 715827883, 2],"int32"), axis=1, keepdim=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 6 / 6 (100%)
Max absolute difference: 1190787246
Max relative difference: 3.41294574
 x: array([[[  48673454,    9084979]],

       [[-443613831, -775538790]],...
 y: array([[[  -20171798,  1199872225]],

       [[ -381106381, -1243132490]],...
2025-04-22 13:57:12.889120 test begin: paddle.take_along_axis(Tensor([13, 4, 7, 14],"float32"), axis=-1, indices=Tensor([13, 4, 7, 6268411],"int64"), )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745301492 (unix time) try "date -d @1745301492" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x25d4a) received by PID 154954 (TID 0x7ff2775fe700) from PID 154954 ***]

2025-04-22 13:58:59.862183 test begin: paddle.take_along_axis(Tensor([2, 302, 768],"bfloat16"), axis=1, indices=Tensor([2, 1485483, 768],"int64"), )

W0422 13:59:54.625188 155774 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 13:59:54.626302 155774 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745301595 (unix time) try "date -d @1745301595" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2600d) received by PID 155661 (TID 0x7fdb1334a700) from PID 155661 ***]

2025-04-22 14:00:41.693144 test begin: paddle.take_along_axis(Tensor([52, 4, 7, 14],"float32"), axis=-1, indices=Tensor([52, 4, 7, 1567103],"int64"), )

W0422 14:01:55.299080 157929 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 14:01:55.300071 157929 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745301715 (unix time) try "date -d @1745301715" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2687b) received by PID 157819 (TID 0x7f7b2e7c3700) from PID 157819 ***]

2025-04-22 14:02:01.487272 test begin: paddle.take_along_axis(Tensor([8, 63, 768],"float32"), axis=1, indices=Tensor([8, 371371, 768],"int64"), )

W0422 14:03:03.201447 158983 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 14:03:03.203033 158983 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745301783 (unix time) try "date -d @1745301783" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x26c8b) received by PID 158859 (TID 0x7f6f0bdc2700) from PID 158859 ***]

2025-04-22 14:45:54.786451 test begin: paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[2,0,1,3,],list[3,1,0,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[2,0,1,3,],list[3,1,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 2654.
Max relative difference: 0.961
 x: array(5416., dtype=float16)
 y: array(2762., dtype=float16)
2025-04-22 14:46:08.488085 test begin: paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[2,0,1,],list[0,1,3,],], )

[accuracy error] paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[2,0,1,],list[0,1,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 2654.
Max relative difference: 0.961
 x: array([[5416.]], dtype=float16)
 y: array([[2762.]], dtype=float16)
2025-04-22 14:46:20.250005 test begin: paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[2,1,0,3,],list[2,0,1,3,],], )

[accuracy error] paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[2,1,0,3,],list[2,0,1,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 2648.
Max relative difference: 0.958
 x: array(5412., dtype=float16)
 y: array(2764., dtype=float16)
2025-04-22 14:46:30.714881 test begin: paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[2,1,0,3,],list[2,0,3,1,],], )

[accuracy error] paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[2,1,0,3,],list[2,0,3,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 2648.
Max relative difference: 0.958
 x: array(5412., dtype=float16)
 y: array(2764., dtype=float16)
2025-04-22 14:46:40.776555 test begin: paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[2,1,0,3,],list[3,1,2,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[2,1,0,3,],list[3,1,2,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 2648.
Max relative difference: 0.958
 x: array(5412., dtype=float16)
 y: array(2764., dtype=float16)
2025-04-22 14:46:51.356598 test begin: paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[2,1,],list[0,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 171798692, 5, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[2,1,],list[0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 2506.
Max relative difference: 22.4
 x: array([[[[ 1564.]]],

...
 y: array([[[[  608. ]]],

...
2025-04-22 14:47:07.847028 test begin: paddle.roll(Tensor([3567, 16, 7, 14, 768],"float16"), shifts=tuple(-4,0,-3,), axis=tuple(1,2,3,), )

W0422 14:48:45.970788 69545 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 14:48:45.971880 69545 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.roll(Tensor([3567, 16, 7, 14, 768],"float16"), shifts=tuple(-4,0,-3,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 491899 / 4295467008 (0.0115%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[-8.7280e-02, -2.1948e-01, -6.4148e-02, ...,  3.3936e-01,
           -3.6572e-01,  3.1885e-01],
          [ 7.4036e-02, -2.2107e-01, -4.9146e-01, ...,  2.9541e-01,...
 y: array([[[[[-8.7280e-02, -2.1948e-01, -6.4148e-02, ...,  3.3936e-01,
           -3.6572e-01,  3.1885e-01],
          [ 7.4036e-02, -2.2107e-01, -4.9146e-01, ...,  2.9541e-01,...
2025-04-22 15:00:57.836207 test begin: paddle.roll(Tensor([3567, 16, 7, 14, 768],"float16"), shifts=tuple(4,0,3,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([3567, 16, 7, 14, 768],"float16"), shifts=tuple(4,0,3,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 492027 / 4295467008 (0.0115%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[ 3.0688e-01, -4.3286e-01,  3.5596e-01, ..., -4.3091e-01,
            1.9995e-01,  4.0479e-01],
          [-2.0288e-01,  3.9136e-01, -2.6709e-01, ..., -2.5342e-01,...
 y: array([[[[[ 3.0688e-01, -4.3286e-01,  3.5596e-01, ..., -4.3091e-01,
            1.9995e-01,  4.0479e-01],
          [-2.0288e-01,  3.9136e-01, -2.6709e-01, ..., -2.5342e-01,...
2025-04-22 15:17:52.275777 test begin: paddle.roll(x=Tensor([1431655766, 3],"float16"), shifts=tuple(-1,1,), axis=tuple(0,1,), )

[accuracy error] paddle.roll(x=Tensor([1431655766, 3],"float16"), shifts=tuple(-1,1,), axis=tuple(0,1,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 3 / 4294967298 (6.98e-08%)
Max absolute difference: 0.09973
Max relative difference: 0.
 x: array([[-0.3254  , -0.4841  , -0.0925  ],
       [-0.401   ,  0.0683  ,  0.4817  ],
       [-0.3813  ,  0.2544  , -0.4695  ],...
 y: array([[-0.3254, -0.4841, -0.0925],
       [-0.401 ,  0.0683,  0.4817],
       [-0.3813,  0.2544, -0.4695],...
2025-04-22 15:30:40.711771 test begin: paddle.roll(x=Tensor([3, 1431655766],"float16"), shifts=-1, axis=0, )

[accuracy error] paddle.roll(x=Tensor([3, 1431655766],"float16"), shifts=-1, axis=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2 / 4294967298 (4.66e-08%)
Max absolute difference: 0.465
Max relative difference: 0.
 x: array([[ 0.3364  , -0.01016 ,  0.2355  , ..., -0.1581  , -0.3337  ,
        -0.4028  ],
       [-0.06238 ,  0.4968  , -0.1044  , ...,  0.2076  , -0.0631  ,...
 y: array([[ 0.3364  , -0.01016 ,  0.2355  , ..., -0.1581  , -0.3337  ,
        -0.4028  ],
       [-0.06238 ,  0.4968  , -0.1044  , ...,  0.2076  , -0.0631  ,...
2025-04-22 15:43:40.098199 test begin: paddle.roll(x=Tensor([3, 1431655766],"float16"), shifts=0, axis=None, )

[accuracy error] paddle.roll(x=Tensor([3, 1431655766],"float16"), shifts=0, axis=None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2 / 4294967298 (4.66e-08%)
Max absolute difference: 0.3147
Max relative difference: 0.
 x: array([[-0.014496,  0.02347 , -0.09973 , ..., -0.148   ,  0.465   ,
        -0.2206  ],
       [ 0.3364  , -0.01016 ,  0.2355  , ..., -0.1581  , -0.3337  ,...
 y: array([[-0.014496,  0.02347 , -0.09973 , ..., -0.148   ,  0.465   ,
        -0.2206  ],
       [ 0.3364  , -0.01016 ,  0.2355  , ..., -0.1581  , -0.3337  ,...
2025-04-22 15:56:42.650367 test begin: paddle.roll(x=Tensor([3, 1431655766],"float16"), shifts=1, axis=None, )

[accuracy error] paddle.roll(x=Tensor([3, 1431655766],"float16"), shifts=1, axis=None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2 / 4294967298 (4.66e-08%)
Max absolute difference: 0.2076
Max relative difference: 0.
 x: array([[-0.3147  , -0.014496,  0.02347 , ...,  0.4834  , -0.148   ,
         0.465   ],
       [-0.2206  ,  0.3364  , -0.01016 , ...,  0.054   , -0.1581  ,...
 y: array([[-0.3147  , -0.014496,  0.02347 , ...,  0.4834  , -0.148   ,
         0.465   ],
       [-0.2206  ,  0.3364  , -0.01016 , ...,  0.054   , -0.1581  ,...
2025-04-22 16:09:11.632287 test begin: paddle.roll(x=Tensor([3, 1431655766],"float16"), shifts=list[-1,1,], axis=list[0,1,], )

[accuracy error] paddle.roll(x=Tensor([3, 1431655766],"float16"), shifts=list[-1,1,], axis=list[0,1,], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 3 / 4294967298 (6.98e-08%)
Max absolute difference: 0.465
Max relative difference: 0.
 x: array([[-0.4028  ,  0.3364  , -0.01016 , ...,  0.054   , -0.1581  ,
        -0.3337  ],
       [-0.3147  , -0.06238 ,  0.4968  , ...,  0.3748  ,  0.2076  ,...
 y: array([[-0.4028  ,  0.3364  , -0.01016 , ...,  0.054   , -0.1581  ,
        -0.3337  ],
       [-0.3147  , -0.06238 ,  0.4968  , ...,  0.3748  ,  0.2076  ,...
2025-04-22 16:21:51.337014 test begin: paddle.roll(x=Tensor([3, 1431655766],"float16"), shifts=tuple(-1,1,), axis=tuple(0,1,), )

[accuracy error] paddle.roll(x=Tensor([3, 1431655766],"float16"), shifts=tuple(-1,1,), axis=tuple(0,1,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 3 / 4294967298 (6.98e-08%)
Max absolute difference: 0.465
Max relative difference: 0.
 x: array([[-0.4028  ,  0.3364  , -0.01016 , ...,  0.054   , -0.1581  ,
        -0.3337  ],
       [-0.3147  , -0.06238 ,  0.4968  , ...,  0.3748  ,  0.2076  ,...
 y: array([[-0.4028  ,  0.3364  , -0.01016 , ...,  0.054   , -0.1581  ,
        -0.3337  ],
       [-0.3147  , -0.06238 ,  0.4968  , ...,  0.3748  ,  0.2076  ,...
2025-04-22 16:34:32.402241 test begin: paddle.rsqrt(x=Tensor([2, 1073741825, 2],"float16"), )

[accuracy error] backward  paddle.rsqrt(x=Tensor([2, 1073741825, 2],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[       nan, -3.809e+01],
        [       nan,        nan],
        [       nan,        nan],...
 y: array([[[       nan, -3.806e+01],
        [       nan,        nan],
        [       nan,        nan],...
2025-04-22 16:43:23.060864 test begin: paddle.rsqrt(x=Tensor([2, 3, 715827883],"float16"), )

[accuracy error] backward  paddle.rsqrt(x=Tensor([2, 3, 715827883],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[[       nan,  1.455e+01,        nan, ...,  8.765e-02,
          7.598e-01,        nan],
        [-1.184e-01, -2.979e-02,  3.132e-01, ...,        nan,...
 y: array([[[        nan,  1.4539e+01,         nan, ...,  8.7646e-02,
          7.5977e-01,         nan],
        [-1.1847e-01, -2.9770e-02,  3.1323e-01, ...,         nan,...
2025-04-22 16:53:42.141395 test begin: paddle.rsqrt(x=Tensor([2, 715827883, 3],"float16"), )

[accuracy error] backward  paddle.rsqrt(x=Tensor([2, 715827883, 3],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[[       nan,  1.455e+01,        nan],
        [       nan,        nan,        nan],
        [-7.707e+00, -1.982e-02,        nan],...
 y: array([[[       nan,  1.454e+01,        nan],
        [       nan,        nan,        nan],
        [-7.707e+00, -1.981e-02,        nan],...
2025-04-22 17:03:58.432948 test begin: paddle.rsqrt(x=Tensor([477218589, 3, 3],"float16"), )

[accuracy error] backward  paddle.rsqrt(x=Tensor([477218589, 3, 3],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[[       nan,  2.959e+01,        nan],
        [       nan,        nan,        nan],
        [ 1.283e+01,  5.288e-01,        nan]],...
 y: array([[[        nan,  2.9578e+01,         nan],
        [        nan,         nan,         nan],
        [ 1.2828e+01,  5.2881e-01,         nan]],...
2025-04-22 17:14:14.833299 test begin: paddle.rsqrt(x=Tensor([715827883, 3, 2],"float16"), )

[accuracy error] backward  paddle.rsqrt(x=Tensor([715827883, 3, 2],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[[      nan,  14.55   ],
        [      nan,       nan],
        [      nan,       nan]],...
 y: array([[[     nan,  14.54  ],
        [     nan,      nan],
        [     nan,      nan]],...
2025-04-22 17:24:32.602334 test begin: paddle.trunc(input=Tensor([119304648, 6, 6],"float16"), )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745313895 (unix time) try "date -d @1745313895" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x10f30) received by PID 69424 (TID 0x7f768d2b7700) from PID 69424 ***]

2025-04-22 17:25:37.094899 test begin: paddle.roll(Tensor([1, 57066, 7, 14, 768],"float16"), shifts=tuple(-4,0,-3,), axis=tuple(1,2,3,), )

W0422 17:27:18.033164 44293 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 17:27:18.034389 44293 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.roll(Tensor([1, 57066, 7, 14, 768],"float16"), shifts=tuple(-4,0,-3,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 49455 / 4295015424 (0.00115%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[ 4.6558e-01, -3.1763e-01,  3.1763e-01, ..., -1.0399e-02,
            2.0801e-01,  2.1164e-02],
          [ 5.5450e-02,  3.4326e-01, -4.6924e-01, ...,  4.3384e-01,...
 y: array([[[[[ 4.6558e-01, -3.1763e-01,  3.1763e-01, ..., -1.0399e-02,
            2.0801e-01,  2.1164e-02],
          [ 5.5450e-02,  3.4326e-01, -4.6924e-01, ...,  4.3384e-01,...
2025-04-22 17:39:30.315466 test begin: paddle.roll(Tensor([1, 57066, 7, 14, 768],"float16"), shifts=tuple(4,0,3,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([1, 57066, 7, 14, 768],"float16"), shifts=tuple(4,0,3,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 49403 / 4295015424 (0.00115%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[-7.1960e-02,  2.5391e-01, -5.9723e-02, ..., -4.2090e-01,
            4.1211e-01,  3.5791e-01],
          [ 1.4313e-02,  4.5044e-01, -1.8835e-01, ...,  2.8223e-01,...
 y: array([[[[[-7.1960e-02,  2.5391e-01, -5.9723e-02, ..., -4.2090e-01,
            4.1211e-01,  3.5791e-01],
          [ 1.4313e-02,  4.5044e-01, -1.8835e-01, ...,  2.8223e-01,...
2025-04-22 17:52:06.015834 test begin: paddle.roll(Tensor([1431655766, 3],"float16"), shifts=1, )

[accuracy error] paddle.roll(Tensor([1431655766, 3],"float16"), shifts=1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2 / 4294967298 (4.66e-08%)
Max absolute difference: 0.4795
Max relative difference: 0.
 x: array([[ 0.3247 , -0.0662 , -0.201  ],
       [-0.2283 ,  0.1256 ,  0.2141 ],
       [-0.01804, -0.1508 , -0.3145 ],...
 y: array([[ 0.3247 , -0.0662 , -0.201  ],
       [-0.2283 ,  0.1256 ,  0.2141 ],
       [-0.01804, -0.1508 , -0.3145 ],...
2025-04-22 18:05:04.426888 test begin: paddle.roll(Tensor([1431655766, 3],"float16"), shifts=1, axis=0, )

[accuracy error] paddle.roll(Tensor([1431655766, 3],"float16"), shifts=1, axis=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2 / 4294967298 (4.66e-08%)
Max absolute difference: 0.3489
Max relative difference: 0.
 x: array([[ 0.4795 , -0.1799 ,  0.3247 ],
       [-0.0662 , -0.201  , -0.2283 ],
       [ 0.1256 ,  0.2141 , -0.01804],...
 y: array([[ 0.4795 , -0.1799 ,  0.3247 ],
       [-0.0662 , -0.201  , -0.2283 ],
       [ 0.1256 ,  0.2141 , -0.01804],...
2025-04-23 09:58:29.483733 test begin: paddle.roll(Tensor([2378, 16, 14, 21, 384],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), )

W0423 10:00:22.492288 41910 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0423 10:00:22.494221 41910 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.roll(Tensor([2378, 16, 14, 21, 384],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 513428 / 4295467008 (0.012%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[-3.0640e-01, -1.7261e-01,  2.8198e-01, ...,  3.2568e-01,
            4.6295e-02,  4.4897e-01],
          [ 4.0161e-02,  1.9995e-01, -1.4572e-02, ...,  4.1187e-01,...
 y: array([[[[[-3.0640e-01, -1.7261e-01,  2.8198e-01, ...,  3.2568e-01,
            4.6295e-02,  4.4897e-01],
          [ 4.0161e-02,  1.9995e-01, -1.4572e-02, ...,  4.1187e-01,...
2025-04-23 10:13:11.476001 test begin: paddle.roll(Tensor([2378, 16, 14, 21, 384],"float16"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([2378, 16, 14, 21, 384],"float16"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 513744 / 4295467008 (0.012%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[ 1.0681e-01,  1.3916e-01,  3.8818e-01, ...,  2.5659e-01,
            3.6841e-01,  4.0015e-01],
          [ 5.4398e-03, -2.8198e-01, -3.4424e-01, ..., -2.3120e-01,...
 y: array([[[[[ 1.0681e-01,  1.3916e-01,  3.8818e-01, ...,  2.5659e-01,
            3.6841e-01,  4.0015e-01],
          [ 5.4398e-03, -2.8198e-01, -3.4424e-01, ..., -2.3120e-01,...
2025-04-23 10:26:29.756481 test begin: paddle.roll(Tensor([2378, 16, 21, 14, 384],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([2378, 16, 21, 14, 384],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 505528 / 4295467008 (0.0118%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[-3.3398e-01, -2.9907e-01,  1.5674e-01, ..., -4.6295e-02,
           -2.4182e-01,  3.8452e-01],
          [ 7.1472e-02, -2.2925e-01,  2.6535e-02, ...,  4.7754e-01,...
 y: array([[[[[-3.3398e-01, -2.9907e-01,  1.5674e-01, ..., -4.6295e-02,
           -2.4182e-01,  3.8452e-01],
          [ 7.1472e-02, -2.2925e-01,  2.6535e-02, ...,  4.7754e-01,...
2025-04-23 10:39:45.677858 test begin: paddle.roll(Tensor([2378, 16, 21, 14, 384],"float16"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([2378, 16, 21, 14, 384],"float16"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 505828 / 4295467008 (0.0118%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[ 2.2791e-01,  1.9580e-01,  6.5430e-02, ...,  1.1279e-01,
            4.0308e-01,  4.6826e-01],
          [-6.6345e-02, -2.0520e-01, -3.6230e-01, ..., -1.2952e-01,...
 y: array([[[[[ 2.2791e-01,  1.9580e-01,  6.5430e-02, ...,  1.1279e-01,
            4.0308e-01,  4.6826e-01],
          [-6.6345e-02, -2.0520e-01, -3.6230e-01, ..., -1.2952e-01,...
2025-04-23 10:53:06.305832 test begin: paddle.roll(Tensor([3, 1431655766],"float16"), shifts=1, )

[accuracy error] paddle.roll(Tensor([3, 1431655766],"float16"), shifts=1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2 / 4294967298 (4.66e-08%)
Max absolute difference: 0.354
Max relative difference: 0.
 x: array([[ 0.285  , -0.3381 ,  0.4277 , ..., -0.3286 , -0.257  , -0.3481 ],
       [-0.0401 , -0.00597, -0.03305, ...,  0.2861 , -0.4685 , -0.02556],
       [-0.1509 ,  0.4475 ,  0.336  , ..., -0.2052 , -0.354  ,  0.2964 ]],
      dtype=float16)
 y: array([[ 0.285  , -0.3381 ,  0.4277 , ..., -0.3286 , -0.257  , -0.3481 ],
       [-0.0401 , -0.00597, -0.03305, ...,  0.2861 , -0.4685 , -0.02556],
       [-0.1509 ,  0.4475 ,  0.336  , ..., -0.2052 ,  0.     ,  0.     ]],
      dtype=float16)
2025-04-23 11:06:17.434886 test begin: paddle.roll(Tensor([3, 1431655766],"float16"), shifts=1, axis=0, )

[accuracy error] paddle.roll(Tensor([3, 1431655766],"float16"), shifts=1, axis=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2 / 4294967298 (4.66e-08%)
Max absolute difference: 0.1509
Max relative difference: 0.
 x: array([[ 0.4475 ,  0.336  , -0.4153 , ..., -0.354  ,  0.2964 ,  0.285  ],
       [-0.3381 ,  0.4277 ,  0.2844 , ..., -0.257  , -0.3481 , -0.0401 ],
       [-0.00597, -0.03305, -0.1935 , ..., -0.4685 , -0.02556, -0.1509 ]],
      dtype=float16)
 y: array([[ 0.4475 ,  0.336  , -0.4153 , ..., -0.354  ,  0.2964 ,  0.285  ],
       [-0.3381 ,  0.4277 ,  0.2844 , ..., -0.257  , -0.3481 , -0.0401 ],
       [-0.00597, -0.03305, -0.1935 , ..., -0.4685 ,  0.     ,  0.     ]],
      dtype=float16)
2025-04-23 11:19:35.411068 test begin: paddle.roll(Tensor([3567, 16, 14, 14, 384],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([3567, 16, 14, 14, 384],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 505613 / 4295467008 (0.0118%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[ 4.4861e-03, -2.6831e-01,  4.1235e-01, ...,  2.9541e-01,
            1.8884e-01, -2.3755e-01],
          [-4.6362e-01, -4.7192e-01, -1.0887e-02, ..., -1.0071e-01,...
 y: array([[[[[ 4.4861e-03, -2.6831e-01,  4.1235e-01, ...,  2.9541e-01,
            1.8884e-01, -2.3755e-01],
          [-4.6362e-01, -4.7192e-01, -1.0887e-02, ..., -1.0071e-01,...
2025-04-23 11:32:53.417390 test begin: paddle.roll(Tensor([3567, 16, 14, 14, 384],"float16"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([3567, 16, 14, 14, 384],"float16"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 505874 / 4295467008 (0.0118%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[ 2.6831e-01, -4.1724e-01,  7.2205e-02, ..., -4.4312e-01,
            2.8174e-01, -3.8428e-01],
          [ 2.8467e-01,  4.6387e-01, -3.4912e-01, ...,  1.7419e-01,...
 y: array([[[[[ 2.6831e-01, -4.1724e-01,  7.2205e-02, ..., -4.4312e-01,
            2.8174e-01, -3.8428e-01],
          [ 2.8467e-01,  4.6387e-01, -3.4912e-01, ...,  1.7419e-01,...
2025-04-23 11:46:13.078264 test begin: paddle.roll(Tensor([3567, 16, 14, 7, 768],"float16"), shifts=tuple(-4,-3,0,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([3567, 16, 14, 7, 768],"float16"), shifts=tuple(-4,-3,0,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 505613 / 4295467008 (0.0118%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[ 3.8647e-01,  1.9702e-01, -3.1677e-02, ...,  3.8818e-01,
           -4.8950e-01, -2.1545e-02],
          [-4.8242e-01, -3.3569e-01,  2.2156e-02, ...,  2.9541e-01,...
 y: array([[[[[ 3.8647e-01,  1.9702e-01, -3.1677e-02, ...,  3.8818e-01,
           -4.8950e-01, -2.1545e-02],
          [-4.8242e-01, -3.3569e-01,  2.2156e-02, ...,  2.9541e-01,...
2025-04-23 11:59:34.951196 test begin: paddle.roll(Tensor([3567, 16, 14, 7, 768],"float16"), shifts=tuple(4,3,0,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([3567, 16, 14, 7, 768],"float16"), shifts=tuple(4,3,0,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 505622 / 4295467008 (0.0118%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[ 4.2920e-01,  2.2485e-01, -2.7100e-01, ..., -1.1859e-01,
           -9.6436e-02,  2.4329e-01],
          [ 1.7920e-03,  2.4719e-01,  2.1094e-01, ...,  3.1226e-01,...
 y: array([[[[[ 4.2920e-01,  2.2485e-01, -2.7100e-01, ..., -1.1859e-01,
           -9.6436e-02,  2.4329e-01],
          [ 1.7920e-03,  2.4719e-01,  2.1094e-01, ...,  3.1226e-01,...
2025-04-23 12:13:03.157510 test begin: paddle.trunc(input=Tensor([19884108, 6, 6, 6],"float16"), )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745381604 (unix time) try "date -d @1745381604" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xa1ce) received by PID 41422 (TID 0x7f55bdd0b700) from PID 41422 ***]

2025-04-23 12:14:02.630240 test begin: paddle.trunc(input=Tensor([3, 6, 6, 6, 6628036],"float16"), )

W0423 12:15:53.009574 72967 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0423 12:15:53.011639 72967 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745381754 (unix time) try "date -d @1745381754" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x11a4e) received by PID 72270 (TID 0x7fe367277700) from PID 72270 ***]

2025-04-23 12:16:35.636512 test begin: paddle.trunc(input=Tensor([3, 6, 6, 6628036, 6],"float16"), )

W0423 12:18:21.371754 98800 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0423 12:18:21.373513 98800 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745381902 (unix time) try "date -d @1745381902" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x17f2b) received by PID 98091 (TID 0x7fc1d3f48700) from PID 98091 ***]

2025-04-23 12:19:00.931424 test begin: paddle.trunc(input=Tensor([3, 6, 6628036, 6, 6],"float16"), )

W0423 12:20:47.350725 122873 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0423 12:20:47.352988 122873 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745382048 (unix time) try "date -d @1745382048" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1dd6b) received by PID 122219 (TID 0x7f50267c3700) from PID 122219 ***]

2025-04-23 12:21:26.539462 test begin: paddle.trunc(input=Tensor([3, 6628036, 6, 6, 6],"float16"), )

W0423 12:23:28.456614 147926 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0423 12:23:28.458398 147926 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745382209 (unix time) try "date -d @1745382209" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x23e33) received by PID 146995 (TID 0x7ff1d5b85700) from PID 146995 ***]

2025-04-23 12:51:15.871962 test begin: paddle.trunc(input=Tensor([380283564, 6],"int32"), )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745383941 (unix time) try "date -d @1745383941" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x34d6) received by PID 13526 (TID 0x7fbe8134a700) from PID 13526 ***]

2025-04-23 12:53:06.423959 test begin: paddle.trunc(input=Tensor([6, 119304648, 6],"float16"), )

W0423 12:54:50.642627 90750 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0423 12:54:50.644596 90750 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745384091 (unix time) try "date -d @1745384091" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x15fee) received by PID 90094 (TID 0x7f452b6f8700) from PID 90094 ***]

2025-04-21 14:35:22.004256 test begin: paddle.Tensor.logit(x=Tensor([4, 107374183, 2, 5],"float16"), )

W0421 14:37:14.969381 101437 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 14:37:14.970548 101437 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] backward  paddle.Tensor.logit(x=Tensor([4, 107374183, 2, 5],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[[ 0.0000e+00,  3.6987e-01, -2.7422e+00,  1.0805e+01,
           5.7812e+00],
         [ 0.0000e+00,  2.0586e+00,  3.6373e-03,  0.0000e+00,...
 y: array([[[[        nan,  3.6987e-01, -2.7422e+00,  1.0805e+01,
           5.7812e+00],
         [        nan,  2.0586e+00,  3.6373e-03,         nan,...
2025-04-21 14:45:33.003236 test begin: paddle.Tensor.logit(x=Tensor([4, 3, 2, 178956971],"float16"), )

[accuracy error] backward  paddle.Tensor.logit(x=Tensor([4, 3, 2, 178956971],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[[ 0.0000e+00, -9.7949e-01,  2.0435e-01, ...,  0.0000e+00,
           2.4531e+00, -1.0039e+00],
         [-2.1270e+00,  0.0000e+00, -3.8428e-01, ..., -9.2285e-01,...
 y: array([[[[        nan, -9.7949e-01,  2.0435e-01, ...,         nan,
           2.4531e+00, -1.0039e+00],
         [-2.1270e+00,         nan, -3.8428e-01, ..., -9.2285e-01,...
2025-04-21 14:54:49.461484 test begin: paddle.Tensor.logit(x=Tensor([4, 3, 357913942],"float16"), )

[accuracy error] backward  paddle.Tensor.logit(x=Tensor([4, 3, 357913942],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[ 0.0000e+00, -9.7949e-01,  2.0435e-01, ..., -9.2285e-01,
          1.8291e+00,  1.3984e+00],
        [ 1.2861e+00,  0.0000e+00,  1.7275e+00, ...,  1.9648e+00,...
 y: array([[[        nan, -9.7949e-01,  2.0435e-01, ..., -9.2285e-01,
          1.8291e+00,  1.3984e+00],
        [ 1.2861e+00,         nan,  1.7275e+00, ...,  1.9648e+00,...
2025-04-21 15:03:53.060512 test begin: paddle.Tensor.logit(x=Tensor([4, 3, 71582789, 5],"float16"), )

[accuracy error] backward  paddle.Tensor.logit(x=Tensor([4, 3, 71582789, 5],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[[ 0.0000e+00, -2.1211e+00, -2.0117e+00, -1.0633e+01,
          -7.5244e-01],
         [ 0.0000e+00, -1.5361e+00,  3.1641e-01,  0.0000e+00,...
 y: array([[[[        nan, -2.1211e+00, -2.0117e+00, -1.0633e+01,
          -7.5244e-01],
         [        nan, -1.5361e+00,  3.1641e-01,         nan,...
2025-04-21 15:13:00.209084 test begin: paddle.Tensor.logit(x=Tensor([4, 536870913, 2],"float16"), )

[accuracy error] backward  paddle.Tensor.logit(x=Tensor([4, 536870913, 2],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[  0.     ,  -0.9795 ],
        [  0.2043 ,   3.264  ],
        [  0.07117,   0.     ],...
 y: array([[[      nan,  -0.9795 ],
        [  0.2043 ,   3.264  ],
        [  0.07117,       nan],...
2025-04-21 15:22:05.824569 test begin: paddle.Tensor.logit(x=Tensor([4294967297],"float16"), )

[accuracy error] backward  paddle.Tensor.logit(x=Tensor([4294967297],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([ 0.   , -1.465,  1.326, ...,  0.   ,  0.   ,  0.   ], dtype=float16)
 y: array([   nan, -1.465,  1.326, ...,    nan,    nan,    nan], dtype=float16)
2025-04-21 15:30:49.488735 test begin: paddle.Tensor.logit(x=Tensor([715827883, 3, 2],"float16"), )

[accuracy error] backward  paddle.Tensor.logit(x=Tensor([715827883, 3, 2],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[ 0.    ,  2.543 ],
        [-1.194 ,  5.72  ],
        [ 2.227 ,  0.    ]],...
 y: array([[[    nan,  2.543 ],
        [-1.194 ,  5.72  ],
        [ 2.227 ,     nan]],...
2025-04-21 15:39:33.128576 test begin: paddle.Tensor.lu(Tensor([3, 760567127],"float32"), )

CUDA runtime error: an illegal memory access was encountered (700) in magma_sgetrf_batched at /opt/conda/conda-bld/magma-cuda124_1721936943789/work/src/sgetrf_batched.cpp:217
CUDA runtime error: an illegal memory access was encountered (700) in magma_sgetrf_batched at /opt/conda/conda-bld/magma-cuda124_1721936943789/work/src/sgetrf_batched.cpp:218
CUDA runtime error: an illegal memory access was encountered (700) in magma_sgetrf_batched at /opt/conda/conda-bld/magma-cuda124_1721936943789/work/src/sgetrf_batched.cpp:219
/usr/local/lib/python3.9/dist-packages/torch/_tensor.py:857: UserWarning: torch.lu is deprecated in favor of torch.linalg.lu_factor / torch.linalg.lu_factor_ex and will be removed in a future PyTorch release.
LU, pivots = torch.lu(A, compute_pivots)
should be replaced with
LU, pivots = torch.linalg.lu_factor(A, compute_pivots)
and
LU, pivots, info = torch.lu(A, compute_pivots, get_infos=True)
should be replaced with
LU, pivots, info = torch.linalg.lu_factor_ex(A, compute_pivots) (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:1990.)
  LU, pivots, infos = torch._lu_with_info(
[torch error] paddle.Tensor.lu(Tensor([3, 760567127],"float32"), ) 
 CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

=========================================================================================
   WARNING batched routines are designed for small sizes. It might be better to use the
   Native/Hybrid classical routines if you want good performance.
=========================================================================================
2025-04-21 15:40:52.474938 test begin: paddle.Tensor.lu(Tensor([760567127, 3],"float32"), )

CUDA runtime error: an illegal memory access was encountered (700) in magma_sgetrf_batched at /opt/conda/conda-bld/magma-cuda124_1721936943789/work/src/sgetrf_batched.cpp:217
CUDA runtime error: an illegal memory access was encountered (700) in magma_sgetrf_batched at /opt/conda/conda-bld/magma-cuda124_1721936943789/work/src/sgetrf_batched.cpp:218
CUDA runtime error: an illegal memory access was encountered (700) in magma_sgetrf_batched at /opt/conda/conda-bld/magma-cuda124_1721936943789/work/src/sgetrf_batched.cpp:219
/usr/local/lib/python3.9/dist-packages/torch/_tensor.py:857: UserWarning: torch.lu is deprecated in favor of torch.linalg.lu_factor / torch.linalg.lu_factor_ex and will be removed in a future PyTorch release.
LU, pivots = torch.lu(A, compute_pivots)
should be replaced with
LU, pivots = torch.linalg.lu_factor(A, compute_pivots)
and
LU, pivots, info = torch.lu(A, compute_pivots, get_infos=True)
should be replaced with
LU, pivots, info = torch.linalg.lu_factor_ex(A, compute_pivots) (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:1990.)
  LU, pivots, infos = torch._lu_with_info(
[torch error] paddle.Tensor.lu(Tensor([760567127, 3],"float32"), ) 
 CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

=========================================================================================
   WARNING batched routines are designed for small sizes. It might be better to use the
   Native/Hybrid classical routines if you want good performance.
=========================================================================================
2025-04-21 15:42:16.624024 test begin: paddle.Tensor.nansum(Tensor([477218589, 3, 3],"float16"), )

W0421 15:43:50.803990  4835 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 15:43:50.805162  4835 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745221431 (unix time) try "date -d @1745221431" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x128f) received by PID 4751 (TID 0x7f7808949700) from PID 4751 ***]

2025-04-21 15:44:33.082627 test begin: paddle.Tensor.nansum(Tensor([477218589, 3, 3],"float16"), axis=-1, )

W0421 15:46:10.185017  7113 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 15:46:10.186249  7113 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745221730 (unix time) try "date -d @1745221730" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1b58) received by PID 7000 (TID 0x7f531e949700) from PID 7000 ***]

2025-04-21 15:49:30.062938 test begin: paddle.Tensor.nansum(Tensor([477218589, 3, 3],"float16"), axis=0, keepdim=True, )

W0421 15:51:01.524228 12859 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 15:51:01.525552 12859 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745221861 (unix time) try "date -d @1745221861" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x31c3) received by PID 12739 (TID 0x7f4c40949700) from PID 12739 ***]

2025-04-21 15:51:45.802496 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1140850690],"int64"), 2, axis=0, )

W0421 15:53:18.198853 15096 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 15:53:18.199694 15096 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.Tensor.repeat_interleave(Tensor([2, 1140850690],"int64"), 2, axis=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 268433400 / 4563402760 (5.88%)
Max absolute difference: 65535
Max relative difference: 1.
 x: array([[ 50947,  28931, -44516, ...,  17944, -30482,  40612],
       [ 50947,  28931, -44516, ...,  17944, -30482,  40612],
       [-45656, -27001,  21239, ..., -19727, -43050, -32735],
       [-45656, -27001,  21239, ...,      0,      0,      0]])
 y: array([[ 50947,  28931, -44516, ...,  17944, -30482,  40612],
       [ 50947,  28931, -44516, ...,  17944, -30482,  40612],
       [-45656, -27001,  21239, ..., -19727, -43050, -32735],
       [-45656, -27001,  21239, ..., -19727, -43050, -32735]])
2025-04-21 16:02:20.872582 test begin: paddle.Tensor.repeat_interleave(Tensor([2281701379, 1],"int64"), 1, axis=0, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_repeat_interleave(_object*, _object*, _object*)
1   repeat_interleave_ad_func(paddle::Tensor const&, int, int)
2   paddle::experimental::repeat_interleave(paddle::Tensor const&, int, int)
3   void phi::RepeatInterleaveKernel<long, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, int, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745222579 (unix time) try "date -d @1745222579" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x3a7f) received by PID 14975 (TID 0x7f73bbdc2700) from PID 14975 ***]

2025-04-21 16:03:43.063012 test begin: paddle.Tensor.repeat_interleave(Tensor([228170138, 10],"int64"), 2, axis=0, )

W0421 16:05:28.832824 27320 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 16:05:28.834776 27320 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.Tensor.repeat_interleave(Tensor([228170138, 10],"int64"), 2, axis=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 268433476 / 4563402760 (5.88%)
Max absolute difference: 65535
Max relative difference: 1.
 x: array([[ 43577,  39816, -25582, ..., -23003,  12125, -50691],
       [ 43577,  39816, -25582, ..., -23003,  12125, -50691],
       [ 56971,  46858,  34877, ..., -47200,   9853,  50076],...
 y: array([[ 43577,  39816, -25582, ..., -23003,  12125, -50691],
       [ 43577,  39816, -25582, ..., -23003,  12125, -50691],
       [ 56971,  46858,  34877, ..., -47200,   9853,  50076],...
2025-04-21 16:17:08.016882 test begin: paddle.Tensor.repeat_interleave(Tensor([22817014, 1, 10, 10],"int64"), 2, axis=0, )

[accuracy error] paddle.Tensor.repeat_interleave(Tensor([22817014, 1, 10, 10],"int64"), 2, axis=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 268433516 / 4563402800 (5.88%)
Max absolute difference: 65535
Max relative difference: 1.
 x: array([[[[ 43577,  39816, -25582, ..., -23003,  12125, -50691],
         [ 56971,  46858,  34877, ..., -47200,   9853,  50076],
         [ -6930, -26640,  51699, ...,  13724, -38356,  23913],...
 y: array([[[[ 43577,  39816, -25582, ..., -23003,  12125, -50691],
         [ 56971,  46858,  34877, ..., -47200,   9853,  50076],
         [ -6930, -26640,  51699, ...,  13724, -38356,  23913],...
2025-04-21 20:58:21.102578 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[0,1,2,],list[2,0,1,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[0,1,2,],list[2,0,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 1282.
Max relative difference: 3.26
 x: array([[  956. ],
       [-1676. ],
       [-1096. ],...
 y: array([[ 224.5 ],
       [-393.5 ],
       [-257.5 ],...
2025-04-21 20:58:32.707834 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[0,1,3,2,],list[1,3,0,2,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[0,1,3,2,],list[1,3,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 249.6
Max relative difference: 3.258
 x: array(-326.2, dtype=float16)
 y: array(-76.6, dtype=float16)
2025-04-21 20:58:42.363038 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[0,1,3,2,],list[2,3,0,1,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[0,1,3,2,],list[2,3,0,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 249.6
Max relative difference: 3.258
 x: array(-326.2, dtype=float16)
 y: array(-76.6, dtype=float16)
2025-04-21 20:58:51.976474 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[0,2,3,],list[0,1,2,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[0,2,3,],list[0,1,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 249.6
Max relative difference: 3.258
 x: array([[-326.2]], dtype=float16)
 y: array([[-76.6]], dtype=float16)
2025-04-21 20:59:02.442728 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[0,3,1,2,],list[3,2,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[0,3,1,2,],list[3,2,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 249.6
Max relative difference: 3.258
 x: array(-326.2, dtype=float16)
 y: array(-76.6, dtype=float16)
2025-04-21 20:59:12.968970 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[1,0,2,3,],list[3,0,1,2,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[1,0,2,3,],list[3,0,1,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 249.6
Max relative difference: 3.258
 x: array(-326.2, dtype=float16)
 y: array(-76.6, dtype=float16)
2025-04-21 20:59:23.377138 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[1,0,3,2,],list[2,3,0,1,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[1,0,3,2,],list[2,3,0,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 249.6
Max relative difference: 3.258
 x: array(-326.2, dtype=float16)
 y: array(-76.6, dtype=float16)
2025-04-21 20:59:33.543971 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[1,2,0,3,],list[0,2,1,3,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[1,2,0,3,],list[0,2,1,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 249.6
Max relative difference: 3.258
 x: array(-326.2, dtype=float16)
 y: array(-76.6, dtype=float16)
2025-04-21 20:59:43.429647 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[1,2,0,],list[1,2,3,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[1,2,0,],list[1,2,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 1282.
Max relative difference: 3.26
 x: array([[  956. ],
       [-1676. ],
       [-1096. ],...
 y: array([[ 224.5 ],
       [-393.5 ],
       [-257.5 ],...
2025-04-21 20:59:54.693722 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[1,2,0,],list[1,3,2,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[1,2,0,],list[1,3,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 1282.
Max relative difference: 3.26
 x: array([[  956. ],
       [-1676. ],
       [-1096. ],...
 y: array([[ 224.5 ],
       [-393.5 ],
       [-257.5 ],...
2025-04-21 21:00:06.140469 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[1,2,3,0,],list[1,3,0,2,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[1,2,3,0,],list[1,3,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 249.6
Max relative difference: 3.258
 x: array(-326.2, dtype=float16)
 y: array(-76.6, dtype=float16)
2025-04-21 21:00:17.949816 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[1,3,2,0,],list[1,3,2,0,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[1,3,2,0,],list[1,3,2,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 249.6
Max relative difference: 3.258
 x: array(-326.2, dtype=float16)
 y: array(-76.6, dtype=float16)
2025-04-21 21:00:27.478065 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[1,3,2,0,],list[3,0,1,2,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[1,3,2,0,],list[3,0,1,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 249.6
Max relative difference: 3.258
 x: array(-326.2, dtype=float16)
 y: array(-76.6, dtype=float16)
2025-04-21 21:00:37.140821 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[1,3,],list[1,0,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[1,3,],list[1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 249.6
Max relative difference: 3.258
 x: array([[[[-326.2]]]], dtype=float16)
 y: array([[[[-76.6]]]], dtype=float16)
2025-04-21 21:00:46.947311 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[2,0,1,3,],list[3,1,0,2,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[2,0,1,3,],list[3,1,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 249.6
Max relative difference: 3.258
 x: array(-326.2, dtype=float16)
 y: array(-76.6, dtype=float16)
2025-04-21 21:01:00.837110 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[2,0,1,],list[0,1,3,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[2,0,1,],list[0,1,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 1282.
Max relative difference: 3.26
 x: array([[  956. ],
       [-1676. ],
       [-1096. ],...
 y: array([[ 224.5 ],
       [-393.5 ],
       [-257.5 ],...
2025-04-21 21:01:11.286590 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[2,1,0,3,],list[2,0,1,3,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[2,1,0,3,],list[2,0,1,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 249.6
Max relative difference: 3.258
 x: array(-326.2, dtype=float16)
 y: array(-76.6, dtype=float16)
2025-04-21 21:01:21.750503 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[2,1,0,3,],list[3,1,2,0,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[2,1,0,3,],list[3,1,2,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 249.6
Max relative difference: 3.258
 x: array(-326.2, dtype=float16)
 y: array(-76.6, dtype=float16)
2025-04-21 21:01:31.686873 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[2,3,0,],list[1,2,0,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[2,3,0,],list[1,2,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 249.6
Max relative difference: 3.258
 x: array([[-326.2]], dtype=float16)
 y: array([[-76.6]], dtype=float16)
2025-04-21 21:01:41.404548 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[2,3,1,],list[1,0,2,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[2,3,1,],list[1,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 249.6
Max relative difference: 3.258
 x: array([[-326.2]], dtype=float16)
 y: array([[-76.6]], dtype=float16)
2025-04-21 21:01:51.339082 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[2,3,],list[1,3,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[2,3,],list[1,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 249.6
Max relative difference: 3.258
 x: array([[[[-326.2]]]], dtype=float16)
 y: array([[[[-76.6]]]], dtype=float16)
2025-04-21 21:02:01.397632 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[3,0,2,1,],list[2,1,0,3,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[3,0,2,1,],list[2,1,0,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 249.6
Max relative difference: 3.258
 x: array(-326.2, dtype=float16)
 y: array(-76.6, dtype=float16)
2025-04-21 21:02:12.124648 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[3,0,],list[2,1,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[3,0,],list[2,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 249.6
Max relative difference: 3.258
 x: array([[[[-326.2]]]], dtype=float16)
 y: array([[[[-76.6]]]], dtype=float16)
2025-04-21 21:02:22.698293 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[3,1,0,2,],list[2,3,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[3,1,0,2,],list[2,3,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 249.6
Max relative difference: 3.258
 x: array(-326.2, dtype=float16)
 y: array(-76.6, dtype=float16)
2025-04-21 21:02:33.237205 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[3,1,0,2,],list[3,1,0,2,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[3,1,0,2,],list[3,1,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 249.6
Max relative difference: 3.258
 x: array(-326.2, dtype=float16)
 y: array(-76.6, dtype=float16)
2025-04-21 21:02:42.239279 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[3,1,0,],list[3,2,1,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[3,1,0,],list[3,2,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 249.6
Max relative difference: 3.258
 x: array([[-326.2]], dtype=float16)
 y: array([[-76.6]], dtype=float16)
2025-04-21 21:02:51.934820 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[3,1,2,0,],list[2,3,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[3,1,2,0,],list[2,3,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 249.6
Max relative difference: 3.258
 x: array(-326.2, dtype=float16)
 y: array(-76.6, dtype=float16)
2025-04-21 21:03:01.620353 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[3,1,2,],list[2,3,1,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[3,1,2,],list[2,3,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 249.6
Max relative difference: 3.258
 x: array([[-326.2]], dtype=float16)
 y: array([[-76.6]], dtype=float16)
2025-04-21 21:03:11.103077 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[3,1,2,],list[3,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[3,1,2,],list[3,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 249.6
Max relative difference: 3.258
 x: array([[-326.2]], dtype=float16)
 y: array([[-76.6]], dtype=float16)
2025-04-21 21:03:21.447761 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[3,2,0,],list[2,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[3,2,0,],list[2,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 249.6
Max relative difference: 3.258
 x: array([[-326.2]], dtype=float16)
 y: array([[-76.6]], dtype=float16)
2025-04-21 21:03:31.982059 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[3,2,1,0,],list[0,1,3,2,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[3,2,1,0,],list[0,1,3,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 249.6
Max relative difference: 3.258
 x: array(-326.2, dtype=float16)
 y: array(-76.6, dtype=float16)
2025-04-21 21:03:41.915539 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[3,2,1,],list[2,0,1,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[3,2,1,],list[2,0,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 249.6
Max relative difference: 3.258
 x: array([[-326.2]], dtype=float16)
 y: array([[-76.6]], dtype=float16)
2025-04-21 21:03:51.999872 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[3,2,],list[3,1,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[3,2,],list[3,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 249.6
Max relative difference: 3.258
 x: array([[[[-326.2]]]], dtype=float16)
 y: array([[[[-76.6]]]], dtype=float16)
2025-04-22 00:19:02.862305 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[0,1,2,],list[2,0,1,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[0,1,2,],list[2,0,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 1147.
Max relative difference: 2.922
 x: array([[  879. ],
       [-1540. ],
       [-1007.5],...
 y: array([[ 224.1 ],
       [-392.8 ],
       [-257.  ],...
2025-04-22 00:19:15.453164 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[0,1,3,2,],list[0,2,1,3,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[0,1,3,2,],list[0,2,1,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 940.
Max relative difference: 0.2769
 x: array(-2456., dtype=float16)
 y: array(-3396., dtype=float16)
2025-04-22 00:19:26.194930 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[0,1,3,2,],list[1,3,0,2,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[0,1,3,2,],list[1,3,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 223.6
Max relative difference: 2.918
 x: array(-300.2, dtype=float16)
 y: array(-76.6, dtype=float16)
2025-04-22 00:19:37.306687 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[0,1,3,2,],list[2,3,0,1,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[0,1,3,2,],list[2,3,0,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 223.5
Max relative difference: 2.922
 x: array(-300., dtype=float16)
 y: array(-76.5, dtype=float16)
2025-04-22 00:19:48.015070 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[0,1,3,],list[0,3,1,],], )

[accuracy error] backward  paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[0,1,3,],list[0,3,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4 / 4294967300 (9.31e-08%)
Max absolute difference: 0.1981
Max relative difference: 1.
 x: array([[[[ 0.03845 ],
         [-0.1084  ],
         [-0.1032  ],...
 y: array([[[[ 0.03845 ],
         [-0.1084  ],
         [-0.1032  ],...
2025-04-22 00:35:33.511170 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[0,2,3,],list[0,2,1,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[0,2,3,],list[0,2,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 940.
Max relative difference: 0.2769
 x: array([[-2456.]], dtype=float16)
 y: array([[-3396.]], dtype=float16)
2025-04-22 00:35:45.810170 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[0,3,2,1,],list[2,1,3,0,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[0,3,2,1,],list[2,1,3,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 940.
Max relative difference: 0.2769
 x: array(-2456., dtype=float16)
 y: array(-3396., dtype=float16)
2025-04-22 00:36:02.355188 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[1,0,3,2,],list[2,3,0,1,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[1,0,3,2,],list[2,3,0,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 223.5
Max relative difference: 2.922
 x: array(-300., dtype=float16)
 y: array(-76.5, dtype=float16)
2025-04-22 00:36:12.263471 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[1,0,3,2,],list[3,0,1,2,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[1,0,3,2,],list[3,0,1,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 940.
Max relative difference: 0.2769
 x: array(-2456., dtype=float16)
 y: array(-3396., dtype=float16)
2025-04-22 00:36:30.156165 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[1,0,3,],list[2,3,0,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[1,0,3,],list[2,3,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 574.
Max relative difference: 8.17
 x: array([[-200.2 ,   14.71, -405.  ,  -66.56,  357.2 ]], dtype=float16)
 y: array([[  27.94, -360.  ,  169.  , -312.2 ,  399.  ]], dtype=float16)
2025-04-22 00:36:46.704037 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[1,2,0,3,],list[0,2,1,3,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[1,2,0,3,],list[0,2,1,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 223.5
Max relative difference: 2.922
 x: array(-300., dtype=float16)
 y: array(-76.5, dtype=float16)
2025-04-22 00:36:56.053090 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[1,2,0,],list[1,2,3,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[1,2,0,],list[1,2,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 1148.
Max relative difference: 2.922
 x: array([[  880.  ],
       [-1542.  ],
       [-1009.  ],...
 y: array([[ 224.5 ],
       [-393.5 ],
       [-257.5 ],...
2025-04-22 00:37:05.331870 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[1,2,0,],list[1,3,2,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[1,2,0,],list[1,3,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 1148.
Max relative difference: 2.922
 x: array([[  880.  ],
       [-1542.  ],
       [-1009.  ],...
 y: array([[ 224.5 ],
       [-393.5 ],
       [-257.5 ],...
2025-04-22 00:37:16.150577 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[1,2,3,0,],list[1,3,0,2,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[1,2,3,0,],list[1,3,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 223.6
Max relative difference: 2.918
 x: array(-300.2, dtype=float16)
 y: array(-76.6, dtype=float16)
2025-04-22 00:37:27.500627 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[1,2,3,0,],list[3,2,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[1,2,3,0,],list[3,2,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 940.
Max relative difference: 0.2769
 x: array(-2456., dtype=float16)
 y: array(-3396., dtype=float16)
2025-04-22 00:37:44.344019 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[1,3,0,],list[3,0,2,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[1,3,0,],list[3,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 574.
Max relative difference: 8.17
 x: array([[-200.2 ,   14.71, -405.  ,  -66.56,  357.2 ]], dtype=float16)
 y: array([[  27.94, -360.  ,  169.  , -312.2 ,  399.  ]], dtype=float16)
2025-04-22 00:38:01.008406 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[1,3,2,0,],list[1,3,2,0,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[1,3,2,0,],list[1,3,2,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 223.6
Max relative difference: 2.918
 x: array(-300.2, dtype=float16)
 y: array(-76.6, dtype=float16)
2025-04-22 00:38:10.976232 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[1,3,2,0,],list[2,1,0,3,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[1,3,2,0,],list[2,1,0,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 940.
Max relative difference: 0.2769
 x: array(-2456., dtype=float16)
 y: array(-3396., dtype=float16)
2025-04-22 00:38:25.183567 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[1,3,2,0,],list[3,0,1,2,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[1,3,2,0,],list[3,0,1,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 223.6
Max relative difference: 2.918
 x: array(-300.2, dtype=float16)
 y: array(-76.6, dtype=float16)
2025-04-22 00:38:34.161010 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[2,1,0,3,],list[2,0,1,3,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[2,1,0,3,],list[2,0,1,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 223.5
Max relative difference: 2.922
 x: array(-300., dtype=float16)
 y: array(-76.5, dtype=float16)
2025-04-22 00:38:44.518718 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[2,1,0,3,],list[2,0,3,1,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[2,1,0,3,],list[2,0,3,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 940.
Max relative difference: 0.2769
 x: array(-2456., dtype=float16)
 y: array(-3396., dtype=float16)
2025-04-22 00:39:03.913238 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[2,1,0,3,],list[3,1,2,0,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[2,1,0,3,],list[3,1,2,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 223.6
Max relative difference: 2.918
 x: array(-300.2, dtype=float16)
 y: array(-76.6, dtype=float16)
2025-04-22 00:39:14.148077 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[2,1,],list[0,2,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[2,1,],list[0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 25 / 25 (100%)
Max absolute difference: 2948.
Max relative difference: 8.17
 x: array([[[[  586.5  ],
         [  -43.1  ],
         [ 1187.   ],...
 y: array([[[[  -81.9 ],
         [ 1055.  ],
         [ -495.2 ],...
2025-04-22 00:39:23.465504 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[2,3,0,],list[3,1,0,],], )

[accuracy error] backward  paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[2,3,0,],list[3,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4 / 4294967300 (9.31e-08%)
Max absolute difference: 0.1981
Max relative difference: 1.
 x: array([[[[ 0.03845 ],
         [-0.1084  ],
         [-0.1032  ],...
 y: array([[[[ 0.03845 ],
         [-0.1084  ],
         [-0.1032  ],...
2025-04-22 00:54:42.454551 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[2,3,1,],list[1,0,2,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[2,3,1,],list[1,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 223.6
Max relative difference: 2.918
 x: array([[-300.2]], dtype=float16)
 y: array([[-76.6]], dtype=float16)
2025-04-22 00:54:53.869134 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[3,1,0,2,],list[3,1,0,2,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[3,1,0,2,],list[3,1,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 223.6
Max relative difference: 2.918
 x: array(-300.2, dtype=float16)
 y: array(-76.6, dtype=float16)
2025-04-22 00:55:02.970959 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[3,1,0,],list[3,2,1,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[3,1,0,],list[3,2,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 223.5
Max relative difference: 2.922
 x: array([[-300.]], dtype=float16)
 y: array([[-76.5]], dtype=float16)
2025-04-22 00:55:12.731107 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[3,1,2,0,],list[1,2,3,0,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[3,1,2,0,],list[1,2,3,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 940.
Max relative difference: 0.2769
 x: array(-2456., dtype=float16)
 y: array(-3396., dtype=float16)
2025-04-22 00:55:30.566681 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[3,1,2,],list[0,3,2,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[3,1,2,],list[0,3,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 574.
Max relative difference: 8.17
 x: array([[-200.2 ,   14.71, -405.  ,  -66.56,  357.2 ]], dtype=float16)
 y: array([[  27.94, -360.  ,  169.  , -312.2 ,  399.  ]], dtype=float16)
2025-04-22 00:55:48.166600 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[3,1,2,],list[1,2,3,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[3,1,2,],list[1,2,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 940.
Max relative difference: 0.2769
 x: array([[-2456.]], dtype=float16)
 y: array([[-3396.]], dtype=float16)
2025-04-22 00:56:06.515029 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[3,1,2,],list[1,3,2,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[3,1,2,],list[1,3,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 940.
Max relative difference: 0.2769
 x: array([[-2456.]], dtype=float16)
 y: array([[-3396.]], dtype=float16)
2025-04-22 00:56:23.222748 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[3,2,0,],list[3,2,0,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[3,2,0,],list[3,2,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 574.
Max relative difference: 8.17
 x: array([[-200.2 ,   14.71, -405.  ,  -66.56,  357.2 ]], dtype=float16)
 y: array([[  27.94, -360.  ,  169.  , -312.2 ,  399.  ]], dtype=float16)
2025-04-22 00:56:39.365835 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[3,2,1,0,],list[0,1,3,2,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[3,2,1,0,],list[0,1,3,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 223.6
Max relative difference: 2.918
 x: array(-300.2, dtype=float16)
 y: array(-76.6, dtype=float16)
2025-04-22 01:40:59.586026 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([858993460, 5, 1, 1],"float16"), list[list[0,3,1,2,],list[3,2,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([858993460, 5, 1, 1],"float16"), list[list[0,3,1,2,],list[3,2,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 63.94
Max relative difference: 0.835
 x: array(-140.5, dtype=float16)
 y: array(-76.56, dtype=float16)
2025-04-22 02:24:29.707188 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([858993460, 5, 1, 1],"float16"), list[list[1,3,2,0,],list[1,3,2,0,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([858993460, 5, 1, 1],"float16"), list[list[1,3,2,0,],list[1,3,2,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 63.94
Max relative difference: 0.835
 x: array(-140.5, dtype=float16)
 y: array(-76.56, dtype=float16)
2025-04-22 02:33:39.029909 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([858993460, 5, 1, 1],"float16"), list[list[2,0,1,3,],list[3,1,0,2,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([858993460, 5, 1, 1],"float16"), list[list[2,0,1,3,],list[3,1,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 63.94
Max relative difference: 0.835
 x: array(-140.5, dtype=float16)
 y: array(-76.56, dtype=float16)
2025-04-22 03:07:37.111788 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([858993460, 5, 1, 1],"float16"), list[list[2,3,0,],list[1,2,0,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([858993460, 5, 1, 1],"float16"), list[list[2,3,0,],list[1,2,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 63.94
Max relative difference: 0.835
 x: array([[-140.5]], dtype=float16)
 y: array([[-76.56]], dtype=float16)
2025-04-22 03:16:58.410575 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([858993460, 5, 1, 1],"float16"), list[list[3,0,2,1,],list[2,1,0,3,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([858993460, 5, 1, 1],"float16"), list[list[3,0,2,1,],list[2,1,0,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 63.94
Max relative difference: 0.835
 x: array(-140.5, dtype=float16)
 y: array(-76.56, dtype=float16)
2025-04-22 03:17:10.114827 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([858993460, 5, 1, 1],"float16"), list[list[3,1,0,2,],list[2,3,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([858993460, 5, 1, 1],"float16"), list[list[3,1,0,2,],list[2,3,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 63.94
Max relative difference: 0.835
 x: array(-140.5, dtype=float16)
 y: array(-76.56, dtype=float16)
2025-04-22 03:17:19.453728 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([858993460, 5, 1, 1],"float16"), list[list[3,1,0,2,],list[3,1,0,2,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([858993460, 5, 1, 1],"float16"), list[list[3,1,0,2,],list[3,1,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 63.94
Max relative difference: 0.835
 x: array(-140.5, dtype=float16)
 y: array(-76.56, dtype=float16)
2025-04-22 03:26:26.594934 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([858993460, 5, 1, 1],"float16"), list[list[3,1,2,0,],list[2,3,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([858993460, 5, 1, 1],"float16"), list[list[3,1,2,0,],list[2,3,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 63.94
Max relative difference: 0.835
 x: array(-140.5, dtype=float16)
 y: array(-76.56, dtype=float16)
2025-04-22 03:26:40.955388 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([858993460, 5, 1, 1],"float16"), list[list[3,1,2,],list[1,2,3,],], )

[accuracy error] backward  paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([858993460, 5, 1, 1],"float16"), list[list[3,1,2,],list[1,2,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 3502073776 / 4294967300 (81.5%)
Max absolute difference: 0.2467
Max relative difference: 1.
 x: array([[[[ 0.03845]],

        [[-0.0674 ]],...
 y: array([[[[ 0.03845 ]],

        [[-0.0674  ]],...
2025-04-22 03:42:02.557945 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([858993460, 5, 1, 1],"float16"), list[list[3,1,2,],list[1,3,2,],], )

[accuracy error] backward  paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([858993460, 5, 1, 1],"float16"), list[list[3,1,2,],list[1,3,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 3502073776 / 4294967300 (81.5%)
Max absolute difference: 0.2467
Max relative difference: 1.
 x: array([[[[ 0.03845]],

        [[-0.0674 ]],...
 y: array([[[[ 0.03845 ]],

        [[-0.0674  ]],...
2025-04-22 03:57:26.783067 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([858993460, 5, 1, 1],"float16"), list[list[3,1,2,],list[3,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([858993460, 5, 1, 1],"float16"), list[list[3,1,2,],list[3,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 63.94
Max relative difference: 0.835
 x: array([[-140.5]], dtype=float16)
 y: array([[-76.56]], dtype=float16)
2025-04-22 03:57:39.108294 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([858993460, 5, 1, 1],"float16"), list[list[3,2,0,],list[2,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 1, 5],"float16"), Tensor([858993460, 5, 1, 1],"float16"), list[list[3,2,0,],list[2,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 63.94
Max relative difference: 0.835
 x: array([[-140.5]], dtype=float16)
 y: array([[-76.56]], dtype=float16)
2025-04-22 06:24:28.014765 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float16"), Tensor([171798692, 5, 1, 5],"float16"), list[list[3,1,2,],list[1,2,3,],], )

[accuracy error] backward  paddle.tensordot(Tensor([1, 1, 5, 5],"float16"), Tensor([171798692, 5, 1, 5],"float16"), list[list[3,1,2,],list[1,2,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 3279427154 / 4294967300 (76.4%)
Max absolute difference: 0.2467
Max relative difference: 1.
 x: array([[[[ 0.0358 , -0.01191,  0.02092,  0.0085 ,  0.     ]],

        [[ 0.     ,  0.     ,  0.     ,  0.     ,  0.     ]],...
 y: array([[[[ 0.0358  , -0.01191 ,  0.02092 ,  0.0085  ,  0.0347  ]],

        [[-0.06274 , -0.0426  ,  0.008316,  0.0325  ,  0.002798]],...
2025-04-22 07:03:48.588952 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float16"), Tensor([5, 171798692, 1, 5],"float16"), list[list[0,3,1,2,],list[3,2,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 5, 5],"float16"), Tensor([5, 171798692, 1, 5],"float16"), list[list[0,3,1,2,],list[3,2,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 2238.
Max relative difference: 4.824
 x: array(-1775., dtype=float16)
 y: array(463.8, dtype=float16)
2025-04-22 09:07:36.443105 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float16"), Tensor([5, 171798692, 1, 5],"float16"), list[list[3,1,0,2,],list[2,3,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 5, 5],"float16"), Tensor([5, 171798692, 1, 5],"float16"), list[list[3,1,0,2,],list[2,3,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 2238.
Max relative difference: 4.824
 x: array(-1775., dtype=float16)
 y: array(463.8, dtype=float16)
2025-04-22 10:18:53.568887 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float16"), Tensor([5, 5, 1, 171798692],"float16"), list[list[0,2,3,],list[0,2,1,],], )

[accuracy error] backward  paddle.tensordot(Tensor([1, 1, 5, 5],"float16"), Tensor([5, 5, 1, 171798692],"float16"), list[list[0,2,3,],list[0,2,1,],], ) 
 Unable to allocate 8.00 GiB for an array with shape (4294967300,) and data type float16
2025-04-22 13:35:30.438917 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float16"), Tensor([5, 5, 34359739, 5],"float16"), list[list[0,1,3,2,],list[2,3,0,1,],], )

[accuracy error] backward  paddle.tensordot(Tensor([1, 1, 5, 5],"float16"), Tensor([5, 5, 34359739, 5],"float16"), list[list[0,1,3,2,],list[2,3,0,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 25 (4%)
Max absolute difference: 0.2969
Max relative difference: 0.01945
 x: array([[[[-685.   ,  211.9  , -276.8  ,   35.06 , -869.5  ],
         [ 745.   ,   14.96 ,  145.6  , -348.   ,  370.5  ],
         [ 400.2  ,   31.84 , -282.5  , -414.   ,  410.8  ],...
 y: array([[[[-685.   ,  211.9  , -276.8  ,   35.06 , -869.5  ],
         [ 745.   ,   15.26 ,  145.6  , -348.   ,  370.5  ],
         [ 400.2  ,   31.84 , -282.5  , -414.   ,  410.8  ],...
2025-04-22 13:44:17.593139 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float16"), Tensor([5, 5, 34359739, 5],"float16"), list[list[1,0,3,2,],list[2,3,0,1,],], )

[accuracy error] backward  paddle.tensordot(Tensor([1, 1, 5, 5],"float16"), Tensor([5, 5, 34359739, 5],"float16"), list[list[1,0,3,2,],list[2,3,0,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 25 (4%)
Max absolute difference: 0.2969
Max relative difference: 0.01945
 x: array([[[[-685.   ,  211.9  , -276.8  ,   35.06 , -869.5  ],
         [ 745.   ,   14.96 ,  145.6  , -348.   ,  370.5  ],
         [ 400.2  ,   31.84 , -282.5  , -414.   ,  410.8  ],...
 y: array([[[[-685.   ,  211.9  , -276.8  ,   35.06 , -869.5  ],
         [ 745.   ,   15.26 ,  145.6  , -348.   ,  370.5  ],
         [ 400.2  ,   31.84 , -282.5  , -414.   ,  410.8  ],...
2025-04-22 14:02:01.207858 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float16"), Tensor([5, 5, 34359739, 5],"float16"), list[list[1,3,0,],list[3,0,2,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 5, 5],"float16"), Tensor([5, 5, 34359739, 5],"float16"), list[list[1,3,0,],list[3,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 25 / 25 (100%)
Max absolute difference: 4424.
Max relative difference: 132.4
 x: array([[-4680. ,  2356. ,  3948. , -1070. ,  1610. ],
       [-1682. ,  2076. ,   269.8, -1008.5,  -271.8],
       [ -650. ,  -608. ,   978. , -1068. ,   442.8],...
 y: array([[-4.228e+03,  2.324e+03,  2.690e+03, -4.484e+03,  2.988e+02],
       [ 1.685e+02, -1.101e+03, -1.784e+03,  1.150e+03,  1.109e+03],
       [-1.431e+03,  2.060e+03,  2.570e+03, -3.384e+03, -1.301e+03],...
2025-04-22 14:02:13.455199 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float16"), Tensor([5, 5, 34359739, 5],"float16"), list[list[1,3,2,0,],list[2,1,0,3,],], )

[accuracy error] backward  paddle.tensordot(Tensor([1, 1, 5, 5],"float16"), Tensor([5, 5, 34359739, 5],"float16"), list[list[1,3,2,0,],list[2,1,0,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 25 (4%)
Max absolute difference: 0.2969
Max relative difference: 0.01945
 x: array([[[[-685.   ,  745.   ,  400.2  ,   -2.693, -499.8  ],
         [ 211.9  ,   14.96 ,   31.84 ,  549.5  , -262.5  ],
         [-276.8  ,  145.6  , -282.5  ,  373.   , -215.6  ],...
 y: array([[[[-685.   ,  745.   ,  400.2  ,   -2.693, -499.8  ],
         [ 211.9  ,   15.26 ,   31.84 ,  549.5  , -262.5  ],
         [-276.8  ,  145.6  , -282.5  ,  373.   , -215.6  ],...
2025-04-22 14:02:28.590358 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float16"), Tensor([5, 5, 34359739, 5],"float16"), list[list[1,3,2,0,],list[3,0,1,2,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 5, 5],"float16"), Tensor([5, 5, 34359739, 5],"float16"), list[list[1,3,2,0,],list[3,0,1,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 2330.
Max relative difference: 10.516
 x: array(2552., dtype=float16)
 y: array(221.6, dtype=float16)
2025-04-22 14:39:01.146824 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float16"), Tensor([5, 5, 34359739, 5],"float16"), list[list[3,1,0,],list[3,2,1,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 5, 5],"float16"), Tensor([5, 5, 34359739, 5],"float16"), list[list[3,1,0,],list[3,2,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 25 (4%)
Max absolute difference: 0.7812
Max relative difference: 0.01221
 x: array([[ -287.8  ,  2340.   , -1794.   , -1431.   ,  -567.   ],
       [ -959.5  ,   511.5  ,   645.5  ,   531.5  , -1164.   ],
       [ 1261.   ,   -63.22 , -1215.   ,  -571.   ,  1275.   ],...
 y: array([[ -287.8  ,  2340.   , -1794.   , -1431.   ,  -567.   ],
       [ -959.5  ,   512.   ,   645.5  ,   531.5  , -1164.   ],
       [ 1261.   ,   -64.   , -1215.   ,  -571.   ,  1275.   ],...
2025-04-22 14:56:18.299457 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float16"), Tensor([5, 5, 34359739, 5],"float16"), list[list[3,2,1,0,],list[0,1,3,2,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 5, 5],"float16"), Tensor([5, 5, 34359739, 5],"float16"), list[list[3,2,1,0,],list[0,1,3,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 2330.
Max relative difference: 10.516
 x: array(2552., dtype=float16)
 y: array(221.6, dtype=float16)
2025-04-22 14:56:31.045528 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 17179870, 5],"float64"), list[list[1,3,0,],list[3,0,2,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 17179870, 5],"float64"), list[list[1,3,0,],list[3,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 24 / 25 (96%)
Max absolute difference: 2798.42158005
Max relative difference: 22.59783617
 x: array([[ -391.549253, -2447.701982,  -469.538156,  -876.011275,
        -2433.023356],
       [ 1644.434107,   -68.742975,  -471.832884,   435.790429,...
 y: array([[  793.115993, -4375.851604, -1138.805871, -2081.685315,
        -1810.794317],
       [  678.780795, -1693.549706,   358.385954,  1636.018461,...
2025-04-22 14:58:04.203173 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 17179870, 5],"float64"), list[list[1,3,2,0,],list[3,0,1,2,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 17179870, 5],"float64"), list[list[1,3,2,0,],list[3,0,1,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 307.6417933
Max relative difference: 0.11532816
 x: array(-2975.175815)
 y: array(-2667.534021)
2025-04-22 14:58:24.485125 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 17179870, 5],"float64"), list[list[3,2,1,0,],list[0,1,3,2,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 17179870, 5],"float64"), list[list[3,2,1,0,],list[0,1,3,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 307.6417933
Max relative difference: 0.11532816
 x: array(-2975.175815)
 y: array(-2667.534021)
2025-04-22 14:58:49.033899 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 85899346, 1, 5],"float64"), list[list[0,3,1,2,],list[3,2,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 85899346, 1, 5],"float64"), list[list[0,3,1,2,],list[3,2,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 1994.37719393
Max relative difference: 0.51032586
 x: array(-1913.669301)
 y: array(-3908.046495)
2025-04-22 14:59:24.088508 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 85899346, 1, 5],"float64"), list[list[3,1,0,2,],list[2,3,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 85899346, 1, 5],"float64"), list[list[3,1,0,2,],list[2,3,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 1994.37719393
Max relative difference: 0.51032586
 x: array(-1913.669301)
 y: array(-3908.046495)
2025-04-22 15:33:28.402800 test begin: paddle.tensordot(Tensor([1, 1, 858993460, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[0,3,2,1,],list[2,1,3,0,],], )

[accuracy error] backward  paddle.tensordot(Tensor([1, 1, 858993460, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[0,3,2,1,],list[2,1,3,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967300 / 4294967300 (100%)
Max absolute difference: 870.
Max relative difference: 8.164
 x: array([[[[ 303.5 ,  -22.3 ,  614.  ,  100.94, -541.5 ],
         [ 303.5 ,  -22.3 ,  614.  ,  100.94, -541.5 ],
         [ 303.5 ,  -22.3 ,  614.  ,  100.94, -541.5 ],...
 y: array([[[[ -42.34,  546.  , -256.2 ,  473.2 , -604.5 ],
         [ -42.34,  546.  , -256.2 ,  473.2 , -604.5 ],
         [ -42.34,  546.  , -256.2 ,  473.2 , -604.5 ],...
2025-04-22 15:47:14.795589 test begin: paddle.tensordot(Tensor([1, 1, 858993460, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[1,2,0,],list[1,3,2,],], )

[accuracy error] backward  paddle.tensordot(Tensor([1, 1, 858993460, 5],"float16"), Tensor([1, 5, 858993460, 1],"float16"), list[list[1,2,0,],list[1,3,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967300 / 4294967300 (100%)
Max absolute difference: 1020.
Max relative difference: 2.922
 x: array([[[[-1369. , -1304. ,  1326. ,  -557. ,   455.2],
         [-1369. , -1304. ,  1326. ,  -557. ,   455.2],
         [-1369. , -1304. ,  1326. ,  -557. ,   455.2],...
 y: array([[[[-349.2, -332.5,  338.2, -142.1,  116.1],
         [-349.2, -332.5,  338.2, -142.1,  116.1],
         [-349.2, -332.5,  338.2, -142.1,  116.1],...
2025-04-22 16:00:20.951047 test begin: paddle.tensordot(Tensor([1, 5, 85899346, 5],"float64"), Tensor([1, 1, 1, 5],"float64"), list[list[0,1,3,2,],list[1,3,0,2,],], )

[accuracy error] paddle.tensordot(Tensor([1, 5, 85899346, 5],"float64"), Tensor([1, 1, 1, 5],"float64"), list[list[0,1,3,2,],list[1,3,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 5338.88319186
Max relative difference: 0.61982647
 x: array(-3274.629505)
 y: array(-8613.512697)
2025-04-22 16:00:48.801465 test begin: paddle.tensordot(Tensor([1, 5, 85899346, 5],"float64"), Tensor([1, 1, 1, 5],"float64"), list[list[0,1,3,2,],list[2,3,0,1,],], )

[accuracy error] paddle.tensordot(Tensor([1, 5, 85899346, 5],"float64"), Tensor([1, 1, 1, 5],"float64"), list[list[0,1,3,2,],list[2,3,0,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 5338.88319186
Max relative difference: 0.61982647
 x: array(-3274.629505)
 y: array(-8613.512697)
2025-04-22 16:01:08.882731 test begin: paddle.tensordot(Tensor([1, 5, 85899346, 5],"float64"), Tensor([1, 1, 1, 5],"float64"), list[list[1,0,3,2,],list[3,0,1,2,],], )

[accuracy error] paddle.tensordot(Tensor([1, 5, 85899346, 5],"float64"), Tensor([1, 1, 1, 5],"float64"), list[list[1,0,3,2,],list[3,0,1,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 5338.88319186
Max relative difference: 0.61982647
 x: array(-3274.629505)
 y: array(-8613.512697)
2025-04-22 16:01:30.500400 test begin: paddle.tensordot(Tensor([1, 5, 85899346, 5],"float64"), Tensor([1, 1, 1, 5],"float64"), list[list[1,3,2,0,],list[3,0,1,2,],], )

[accuracy error] paddle.tensordot(Tensor([1, 5, 85899346, 5],"float64"), Tensor([1, 1, 1, 5],"float64"), list[list[1,3,2,0,],list[3,0,1,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 5338.88319186
Max relative difference: 0.61982647
 x: array(-3274.629505)
 y: array(-8613.512697)
2025-04-22 16:01:52.836972 test begin: paddle.tensordot(Tensor([1, 5, 85899346, 5],"float64"), Tensor([1, 1, 1, 5],"float64"), list[list[3,0,2,1,],list[2,1,0,3,],], )

[accuracy error] paddle.tensordot(Tensor([1, 5, 85899346, 5],"float64"), Tensor([1, 1, 1, 5],"float64"), list[list[3,0,2,1,],list[2,1,0,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 5338.88319186
Max relative difference: 0.61982647
 x: array(-3274.629505)
 y: array(-8613.512697)
2025-04-22 16:02:15.996095 test begin: paddle.tensordot(Tensor([1, 5, 85899346, 5],"float64"), Tensor([1, 1, 1, 5],"float64"), list[list[3,1,0,2,],list[2,3,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([1, 5, 85899346, 5],"float64"), Tensor([1, 1, 1, 5],"float64"), list[list[3,1,0,2,],list[2,3,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 5338.88319186
Max relative difference: 0.61982647
 x: array(-3274.629505)
 y: array(-8613.512697)
2025-04-22 16:02:38.126268 test begin: paddle.tensordot(Tensor([1, 5, 85899346, 5],"float64"), Tensor([1, 1, 1, 5],"float64"), list[list[3,1,2,0,],list[2,3,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([1, 5, 85899346, 5],"float64"), Tensor([1, 1, 1, 5],"float64"), list[list[3,1,2,0,],list[2,3,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 5338.88319186
Max relative difference: 0.61982647
 x: array(-3274.629505)
 y: array(-8613.512697)
2025-04-22 16:02:58.803330 test begin: paddle.tensordot(Tensor([1, 5, 85899346, 5],"float64"), Tensor([1, 1, 1, 5],"float64"), list[list[3,1,2,],list[0,3,2,],], )

[accuracy error] paddle.tensordot(Tensor([1, 5, 85899346, 5],"float64"), Tensor([1, 1, 1, 5],"float64"), list[list[3,1,2,],list[0,3,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 5338.88319186
Max relative difference: 0.61982647
 x: array([[-3274.629505]])
 y: array([[-8613.512697]])
2025-04-22 16:03:17.889348 test begin: paddle.tensordot(Tensor([1, 5, 85899346, 5],"float64"), Tensor([1, 1, 1, 5],"float64"), list[list[3,1,2,],list[1,3,2,],], )

[accuracy error] paddle.tensordot(Tensor([1, 5, 85899346, 5],"float64"), Tensor([1, 1, 1, 5],"float64"), list[list[3,1,2,],list[1,3,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 5338.88319186
Max relative difference: 0.61982647
 x: array([[-3274.629505]])
 y: array([[-8613.512697]])
2025-04-22 16:03:45.939776 test begin: paddle.tensordot(Tensor([1, 5, 85899346, 5],"float64"), Tensor([1, 1, 1, 5],"float64"), list[list[3,1,2,],list[2,3,1,],], )

[accuracy error] paddle.tensordot(Tensor([1, 5, 85899346, 5],"float64"), Tensor([1, 1, 1, 5],"float64"), list[list[3,1,2,],list[2,3,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 5338.88319186
Max relative difference: 0.61982647
 x: array([[-3274.629505]])
 y: array([[-8613.512697]])
2025-04-22 16:04:08.370821 test begin: paddle.tensordot(Tensor([1, 5, 85899346, 5],"float64"), Tensor([1, 1, 1, 5],"float64"), list[list[3,2,0,],list[2,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([1, 5, 85899346, 5],"float64"), Tensor([1, 1, 1, 5],"float64"), list[list[3,2,0,],list[2,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 25 / 25 (100%)
Max absolute difference: 2117.89359706
Max relative difference: 0.74316452
 x: array([[-1598.112503, -1941.36645 , -2024.510083,  1472.528554,
        -1377.30534 ],
       [ -748.197166,  -908.900264,  -947.826078,   689.401834,...
 y: array([[-3269.940302, -3972.281291, -4142.40368 ,  3012.979659,
        -2818.14092 ],
       [-2034.112977, -2471.0142  , -2576.841258,  1874.266946,...
2025-04-22 16:04:29.984218 test begin: paddle.tensordot(Tensor([1, 5, 85899346, 5],"float64"), Tensor([1, 1, 1, 5],"float64"), list[list[3,2,1,0,],list[0,1,3,2,],], )

[accuracy error] paddle.tensordot(Tensor([1, 5, 85899346, 5],"float64"), Tensor([1, 1, 1, 5],"float64"), list[list[3,2,1,0,],list[0,1,3,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 5338.88319186
Max relative difference: 0.61982647
 x: array(-3274.629505)
 y: array(-8613.512697)
2025-04-22 16:04:54.482202 test begin: paddle.tensordot(Tensor([1, 5, 85899346, 5],"float64"), Tensor([1, 1, 1, 5],"float64"), list[list[3,2,1,],list[2,0,1,],], )

[accuracy error] paddle.tensordot(Tensor([1, 5, 85899346, 5],"float64"), Tensor([1, 1, 1, 5],"float64"), list[list[3,2,1,],list[2,0,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 5361.74414392
Max relative difference: 0.43004026
 x: array([[-5609.556916, -6814.417368, -7106.260992,  5168.74295 ,
        -4834.498623]])
 y: array([[ -9842.023044, -11955.962614, -12468.005136,   9068.610584,
         -8482.17561 ]])
2025-04-22 16:05:13.194115 test begin: paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,1,2,0,],list[1,2,3,0,],], )

[accuracy error] paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,1,2,0,],list[1,2,3,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 1338.
Max relative difference: 5.94
 x: array(1563., dtype=float16)
 y: array(225.2, dtype=float16)
2025-04-22 16:05:26.605483 test begin: paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,1,2,0,],list[2,3,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,1,2,0,],list[2,3,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 1338.
Max relative difference: 5.94
 x: array(1563., dtype=float16)
 y: array(225.2, dtype=float16)
2025-04-22 16:05:36.719598 test begin: paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,2,1,0,],list[0,1,3,2,],], )

[accuracy error] paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,2,1,0,],list[0,1,3,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 1341.
Max relative difference: 6.05
 x: array(1563., dtype=float16)
 y: array(221.6, dtype=float16)
2025-04-22 16:05:47.676043 test begin: paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([171798692, 1, 1, 1],"float16"), list[list[0,1,3,2,],list[1,3,0,2,],], )

[accuracy error] backward  paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([171798692, 1, 1, 1],"float16"), list[list[0,1,3,2,],list[1,3,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967300 / 4294967300 (100%)
Max absolute difference: 476.
Max relative difference: 0.695
 x: array([[[[-209.1],
         [-209.1],
         [-209.1],...
 y: array([[[[-685.],
         [-685.],
         [-685.],...
2025-04-22 16:17:55.542865 test begin: paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([171798692, 1, 1, 1],"float16"), list[list[0,1,3,2,],list[2,3,0,1,],], )

[accuracy error] backward  paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([171798692, 1, 1, 1],"float16"), list[list[0,1,3,2,],list[2,3,0,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967300 / 4294967300 (100%)
Max absolute difference: 476.
Max relative difference: 0.695
 x: array([[[[-209.1],
         [-209.1],
         [-209.1],...
 y: array([[[[-685.],
         [-685.],
         [-685.],...
2025-04-22 16:30:27.398841 test begin: paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([171798692, 1, 1, 1],"float16"), list[list[1,0,3,2,],list[2,3,0,1,],], )

[accuracy error] backward  paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([171798692, 1, 1, 1],"float16"), list[list[1,0,3,2,],list[2,3,0,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967300 / 4294967300 (100%)
Max absolute difference: 476.
Max relative difference: 0.695
 x: array([[[[-209.1],
         [-209.1],
         [-209.1],...
 y: array([[[[-685.],
         [-685.],
         [-685.],...
2025-04-22 16:42:49.787899 test begin: paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([171798692, 1, 1, 1],"float16"), list[list[1,0,3,],list[2,3,0,],], )

[accuracy error] backward  paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([171798692, 1, 1, 1],"float16"), list[list[1,0,3,],list[2,3,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967300 / 4294967300 (100%)
Max absolute difference: 1431.
Max relative difference: 0.695
 x: array([[[[ 629. ],
         [ 599. ],
         [-609. ],...
 y: array([[[[ 2060. ],
         [ 1962. ],
         [-1995. ],...
2025-04-22 16:54:55.491631 test begin: paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([171798692, 1, 1, 1],"float16"), list[list[1,2,0,],list[1,2,3,],], )

[accuracy error] paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([171798692, 1, 1, 1],"float16"), list[list[1,2,0,],list[1,2,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 171797994 / 171798692 (100%)
Max absolute difference: 2376.
Max relative difference: 5.95
 x: array([[ 1563. , -2740. , -1792. , ..., -1314. ,  -113.6,  1448. ]],
      dtype=float16)
 y: array([[ 225.2 , -394.8 , -258.2 , ..., -189.4 ,  -16.38,  208.6 ]],
      dtype=float16)
2025-04-22 16:55:35.786241 test begin: paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([171798692, 1, 1, 1],"float16"), list[list[1,2,0,],list[1,3,2,],], )

[accuracy error] paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([171798692, 1, 1, 1],"float16"), list[list[1,2,0,],list[1,3,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 171797994 / 171798692 (100%)
Max absolute difference: 2376.
Max relative difference: 5.95
 x: array([[ 1563. , -2740. , -1792. , ..., -1314. ,  -113.6,  1448. ]],
      dtype=float16)
 y: array([[ 225.2 , -394.8 , -258.2 , ..., -189.4 ,  -16.38,  208.6 ]],
      dtype=float16)
2025-04-22 16:56:15.864655 test begin: paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([171798692, 1, 1, 1],"float16"), list[list[1,2,3,0,],list[1,3,0,2,],], )

[accuracy error] backward  paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([171798692, 1, 1, 1],"float16"), list[list[1,2,3,0,],list[1,3,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967300 / 4294967300 (100%)
Max absolute difference: 476.
Max relative difference: 0.695
 x: array([[[[-209.1],
         [-209.1],
         [-209.1],...
 y: array([[[[-685.],
         [-685.],
         [-685.],...
2025-04-22 17:08:56.747961 test begin: paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([171798692, 1, 1, 1],"float16"), list[list[1,3,0,],list[3,0,2,],], )

[accuracy error] backward  paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([171798692, 1, 1, 1],"float16"), list[list[1,3,0,],list[3,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967300 / 4294967300 (100%)
Max absolute difference: 1431.
Max relative difference: 0.695
 x: array([[[[ 629. ],
         [ 599. ],
         [-609. ],...
 y: array([[[[ 2060. ],
         [ 1962. ],
         [-1995. ],...
2025-04-22 17:21:21.837695 test begin: paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([171798692, 1, 1, 1],"float16"), list[list[1,3,2,0,],list[3,0,1,2,],], )

[accuracy error] backward  paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([171798692, 1, 1, 1],"float16"), list[list[1,3,2,0,],list[3,0,1,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967300 / 4294967300 (100%)
Max absolute difference: 476.
Max relative difference: 0.695
 x: array([[[[-209.1],
         [-209.1],
         [-209.1],...
 y: array([[[[-685.],
         [-685.],
         [-685.],...
2025-04-22 17:33:52.627092 test begin: paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([171798692, 1, 1, 1],"float16"), list[list[1,3,],list[1,0,],], )

[accuracy error] paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([171798692, 1, 1, 1],"float16"), list[list[1,3,],list[1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 858967319 / 858993460 (100%)
Max absolute difference: 8036.
Max relative difference: 0.6953
 x: array([[[[ -993.5 ]],

        [[  697.  ]],...
 y: array([[[[-3254. ]],

        [[ 2284. ]],...
2025-04-22 17:36:32.632728 test begin: paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([171798692, 1, 1, 1],"float16"), list[list[2,1,0,3,],list[3,1,2,0,],], )

[accuracy error] backward  paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([171798692, 1, 1, 1],"float16"), list[list[2,1,0,3,],list[3,1,2,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967300 / 4294967300 (100%)
Max absolute difference: 476.
Max relative difference: 0.695
 x: array([[[[-209.1],
         [-209.1],
         [-209.1],...
 y: array([[[[-685.],
         [-685.],
         [-685.],...
2025-04-22 17:49:18.140748 test begin: paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([171798692, 1, 1, 1],"float16"), list[list[2,3,1,],list[1,0,2,],], )

[accuracy error] paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([171798692, 1, 1, 1],"float16"), list[list[2,3,1,],list[1,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 171793320 / 171798692 (100%)
Max absolute difference: 25312.
Max relative difference: 0.6953
 x: array([[ -664.5],
       [-3264. ],
       [ 2504. ],...
 y: array([[ -2176.],
       [-10696.],
       [  8208.],...
2025-04-22 18:17:13.706831 test begin: paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([171798692, 1, 1, 1],"float16"), list[list[3,1,2,],list[0,3,2,],], )

[accuracy error] paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([171798692, 1, 1, 1],"float16"), list[list[3,1,2,],list[0,3,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 171793196 / 171798692 (100%)
Max absolute difference: 25312.
Max relative difference: 0.6953
 x: array([[ -664.5],
       [-3268. ],
       [ 2504. ],...
 y: array([[ -2178. ],
       [-10704. ],
       [  8208. ],...
2025-04-23 09:58:32.983671 test begin: paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([171798692, 1, 1, 1],"float16"), list[list[3,2,1,0,],list[0,1,3,2,],], )

/usr/local/lib/python3.9/dist-packages/torch/autograd/graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:135.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
W0423 10:00:14.106613 42329 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0423 10:00:14.108335 42329 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] backward  paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([171798692, 1, 1, 1],"float16"), list[list[3,2,1,0,],list[0,1,3,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967300 / 4294967300 (100%)
Max absolute difference: 511.
Max relative difference: 0.488
 x: array([[[[536.],
         [536.],
         [536.],...
 y: array([[[[1047.],
         [1047.],
         [1047.],...
2025-04-23 11:37:20.154399 test begin: paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([171798692, 5, 1, 5],"float16"), list[list[1,3,],list[1,0,],], )

[accuracy error] paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([171798692, 5, 1, 5],"float16"), list[list[1,3,],list[1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294952559 / 4294967300 (100%)
Max absolute difference: 9240.
Max relative difference: 1.075
 x: array([[[[ 96.5, 756. , 892. , 927.5, 828.5]],

        [[  0. ,   0. ,   0. ,   0. ,   0. ]],...
 y: array([[[[ 1269.  ,  1517.  ,  3078.  ,   447.  ,   898.  ]],

        [[ -776.  ,  -341.  ,  1039.  ,   582.  ,  -328.5 ]],...
2025-04-23 12:08:54.183596 test begin: paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([171798692, 5, 1, 5],"float16"), list[list[2,3,0,],list[1,2,0,],], )

[accuracy error] backward  paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([171798692, 5, 1, 5],"float16"), list[list[2,3,0,],list[1,2,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4083251160 / 4294967300 (95.1%)
Max absolute difference: 0.5967
Max relative difference: 1.
 x: array([[[[-0.00917, -0.08923,  0.1313 ,  0.00858,  0.     ]],

        [[ 0.     ,  0.     ,  0.     ,  0.     ,  0.     ]],...
 y: array([[[[-0.00917 , -0.08923 ,  0.1313  ,  0.00858 ,  0.1692  ]],

        [[ 0.08356 , -0.02423 ,  0.1068  , -0.1036  ,  0.05048 ]],...
2025-04-23 13:08:42.928391 test begin: paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([171798692, 5, 1, 5],"float16"), list[list[3,2,0,],list[2,1,0,],], )

[accuracy error] backward  paddle.tensordot(Tensor([171798692, 5, 5, 1],"float16"), Tensor([171798692, 5, 1, 5],"float16"), list[list[3,2,0,],list[2,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 4294967300 (1.16e-07%)
Max absolute difference: 0.3613
Max relative difference: 3.354
 x: array([[[[ 7.9346e-02],
         [-1.2573e-01],
         [-5.2704e-02],...
 y: array([[[[ 7.9346e-02],
         [-1.2573e-01],
         [-5.2704e-02],...
2025-04-21 14:35:24.942277 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[1,3,2,0,],list[2,1,0,3,],], )

/usr/local/lib/python3.9/dist-packages/torch/autograd/graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:135.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
W0421 14:36:54.582257 101556 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 14:36:54.583472 101556 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[1,3,2,0,],list[2,1,0,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 4720.
Max relative difference: 0.8877
 x: array(595.5, dtype=float16)
 y: array(5316., dtype=float16)
2025-04-21 14:36:55.917403 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[1,3,2,0,],list[3,0,1,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[1,3,2,0,],list[3,0,1,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 4720.
Max relative difference: 0.8877
 x: array(595.5, dtype=float16)
 y: array(5316., dtype=float16)
2025-04-21 14:37:05.855156 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[2,0,1,3,],list[3,1,0,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[2,0,1,3,],list[3,1,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 4720.
Max relative difference: 0.8877
 x: array(595., dtype=float16)
 y: array(5316., dtype=float16)
2025-04-21 14:37:21.942131 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[2,0,1,],list[0,1,3,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[2,0,1,],list[0,1,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 4720.
Max relative difference: 0.8877
 x: array([[595.]], dtype=float16)
 y: array([[5316.]], dtype=float16)
2025-04-21 14:37:31.875946 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[2,1,0,3,],list[2,0,1,3,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[2,1,0,3,],list[2,0,1,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 4720.
Max relative difference: 0.8877
 x: array(595., dtype=float16)
 y: array(5316., dtype=float16)
2025-04-21 14:37:44.417934 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[2,1,0,3,],list[2,0,3,1,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[2,1,0,3,],list[2,0,3,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 4720.
Max relative difference: 0.8877
 x: array(595., dtype=float16)
 y: array(5316., dtype=float16)
2025-04-21 14:37:55.404978 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[2,1,0,3,],list[3,1,2,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[2,1,0,3,],list[3,1,2,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 4720.
Max relative difference: 0.8877
 x: array(595., dtype=float16)
 y: array(5316., dtype=float16)
2025-04-21 14:38:07.531582 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[2,1,],list[0,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[2,1,],list[0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 1978.
Max relative difference: 1.103
 x: array([[[[ -123.7]]],

...
 y: array([[[[1213. ]]],

...
2025-04-21 14:38:18.881324 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[2,3,0,],list[1,2,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[2,3,0,],list[1,2,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 1835.
Max relative difference: 1.993
 x: array([[-822. ],
       [-600. ],
       [ 913. ],...
 y: array([[ 828. ],
       [-922.5],
       [2748. ],...
2025-04-21 14:38:36.643700 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[2,3,0,],list[3,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[2,3,0,],list[3,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 1835.
Max relative difference: 1.993
 x: array([[-822. ],
       [-600. ],
       [ 913. ],...
 y: array([[ 828. ],
       [-922.5],
       [2748. ],...
2025-04-21 14:38:55.005784 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[2,3,1,],list[1,0,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[2,3,1,],list[1,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 1978.
Max relative difference: 1.103
 x: array([[ -123.7],
       [  408. ],
       [ 1313. ],...
 y: array([[1213. ],
       [1214. ],
       [1338. ],...
2025-04-21 14:39:06.563297 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[2,3,],list[1,3,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[2,3,],list[1,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 25 / 25 (100%)
Max absolute difference: 1171.
Max relative difference: 28.17
 x: array([[[[-364.8 ]],

        [[ 271.5 ]],...
 y: array([[[[ 598.5 ]],

        [[-409.2 ]],...
2025-04-21 14:39:17.981106 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,0,2,1,],list[2,1,0,3,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,0,2,1,],list[2,1,0,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 4720.
Max relative difference: 0.8877
 x: array(594.5, dtype=float16)
 y: array(5316., dtype=float16)
2025-04-21 14:39:29.076496 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,1,0,2,],list[2,3,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,1,0,2,],list[2,3,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 4724.
Max relative difference: 0.888
 x: array(595.5, dtype=float16)
 y: array(5320., dtype=float16)
2025-04-21 14:39:39.532518 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,1,0,2,],list[3,1,0,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,1,0,2,],list[3,1,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 4724.
Max relative difference: 0.888
 x: array(595.5, dtype=float16)
 y: array(5320., dtype=float16)
2025-04-21 14:39:52.525981 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,1,2,0,],list[1,2,3,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,1,2,0,],list[1,2,3,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 4720.
Max relative difference: 0.8877
 x: array(595.5, dtype=float16)
 y: array(5316., dtype=float16)
2025-04-21 14:40:04.345676 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,1,2,0,],list[2,3,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,1,2,0,],list[2,3,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 4720.
Max relative difference: 0.8877
 x: array(595.5, dtype=float16)
 y: array(5316., dtype=float16)
2025-04-21 14:40:21.084986 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,1,2,],list[0,3,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,1,2,],list[0,3,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 1977.
Max relative difference: 1.103
 x: array([[ -123.6],
       [  408. ],
       [ 1313. ],...
 y: array([[1212. ],
       [1214. ],
       [1338. ],...
2025-04-21 14:40:32.125558 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,1,2,],list[1,2,3,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,1,2,],list[1,2,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 1977.
Max relative difference: 1.103
 x: array([[ -123.6],
       [  408. ],
       [ 1313. ],...
 y: array([[1212. ],
       [1214. ],
       [1338. ],...
2025-04-21 14:40:43.114100 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,1,2,],list[1,3,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,1,2,],list[1,3,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 1977.
Max relative difference: 1.103
 x: array([[ -123.6],
       [  408. ],
       [ 1313. ],...
 y: array([[1212. ],
       [1214. ],
       [1338. ],...
2025-04-21 14:40:54.222959 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,1,2,],list[2,3,1,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,1,2,],list[2,3,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 1977.
Max relative difference: 1.103
 x: array([[ -123.6],
       [  408. ],
       [ 1313. ],...
 y: array([[1212. ],
       [1214. ],
       [1338. ],...
2025-04-21 14:41:06.019618 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,1,2,],list[3,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,1,2,],list[3,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 1977.
Max relative difference: 1.103
 x: array([[ -123.6],
       [  408. ],
       [ 1313. ],...
 y: array([[1212. ],
       [1214. ],
       [1338. ],...
2025-04-21 14:41:16.834232 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,2,0,],list[2,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,2,0,],list[2,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 1835.
Max relative difference: 1.993
 x: array([[-822. ],
       [-600. ],
       [ 913. ],...
 y: array([[ 828. ],
       [-922.5],
       [2748. ],...
2025-04-21 14:41:35.622852 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,2,0,],list[3,2,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,2,0,],list[3,2,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 1835.
Max relative difference: 1.993
 x: array([[-822. ],
       [-600. ],
       [ 913. ],...
 y: array([[ 828. ],
       [-922.5],
       [2748. ],...
2025-04-21 14:41:54.325866 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,2,1,0,],list[0,1,3,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,2,1,0,],list[0,1,3,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 4720.
Max relative difference: 0.8877
 x: array(595., dtype=float16)
 y: array(5316., dtype=float16)
2025-04-21 14:42:05.847058 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,2,1,],list[2,0,1,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,2,1,],list[2,0,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 1978.
Max relative difference: 1.103
 x: array([[ -123.7],
       [  408. ],
       [ 1313. ],...
 y: array([[1213. ],
       [1214. ],
       [1338. ],...
2025-04-21 14:42:16.807973 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,2,],list[3,1,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[3,2,],list[3,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 25 / 25 (100%)
Max absolute difference: 1171.
Max relative difference: 28.17
 x: array([[[[-364.8 ]],

        [[ 271.5 ]],...
 y: array([[[[ 598.5 ]],

        [[-409.2 ]],...
2025-04-21 14:42:27.894726 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 171798692, 1],"float16"), list[list[0,2,3,],list[0,1,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 171798692, 1],"float16"), list[list[0,2,3,],list[0,1,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[ inf],
       [ inf],
       [-inf],...
 y: array([[ inf],
       [-inf],
       [ inf],...
2025-04-21 14:51:20.306347 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 171798692, 1],"float16"), list[list[0,3,1,2,],list[3,2,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 171798692, 1],"float16"), list[list[0,3,1,2,],list[3,2,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array(-inf, dtype=float16)
 y: array(inf, dtype=float16)
2025-04-21 14:51:34.890901 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 171798692, 1],"float16"), list[list[1,0,2,3,],list[3,0,1,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 171798692, 1],"float16"), list[list[1,0,2,3,],list[3,0,1,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array(-inf, dtype=float16)
 y: array(inf, dtype=float16)
2025-04-21 14:51:50.556740 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 171798692, 1],"float16"), list[list[2,0,1,3,],list[3,1,0,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 171798692, 1],"float16"), list[list[2,0,1,3,],list[3,1,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array(-inf, dtype=float16)
 y: array(inf, dtype=float16)
2025-04-21 14:52:03.899193 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 171798692, 1],"float16"), list[list[2,0,1,],list[0,1,3,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 171798692, 1],"float16"), list[list[2,0,1,],list[0,1,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 171798570 / 171798692 (100%)
Max absolute difference: 14112.
Max relative difference: 0.8887
 x: array([[  595. , -1735. , -1702. , ...,   831. ,   998.5,  1508. ]],
      dtype=float16)
 y: array([[  5316., -15504., -15216., ...,   7424.,   8920.,  13472.]],
      dtype=float16)
2025-04-21 14:52:43.655457 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 171798692, 1],"float16"), list[list[2,3,0,],list[1,2,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 171798692, 1],"float16"), list[list[2,3,0,],list[1,2,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[ inf],
       [ inf],
       [-inf],...
 y: array([[ inf],
       [-inf],
       [ inf],...
2025-04-21 14:53:05.244991 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 171798692, 1],"float16"), list[list[2,3,0,],list[3,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 171798692, 1],"float16"), list[list[2,3,0,],list[3,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 858988189 / 858993460 (100%)
Max absolute difference: 5480.
Max relative difference: 1.994
 x: array([[ -822. ,  2398. ,  2352. , ..., -1148. , -1379. , -2084. ],
       [ -600. ,  1749. ,  1716. , ...,  -837.5, -1006. , -1520. ],
       [  913. , -2662. , -2612. , ...,  1275. ,  1532. ,  2314. ],...
 y: array([[  828. , -2414. , -2368. , ...,  1156. ,  1389. ,  2098. ],
       [ -922.5,  2690. ,  2638. , ..., -1288. , -1548. , -2338. ],
       [ 2748. , -8012. , -7860. , ...,  3836. ,  4608. ,  6960. ],...
2025-04-21 14:55:49.553668 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 171798692, 1],"float16"), list[list[2,3,],list[1,3,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 171798692, 1],"float16"), list[list[2,3,],list[1,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294801427 / 4294967300 (100%)
Max absolute difference: 3498.
Max relative difference: 28.23
 x: array([[[[ -364.8 ,  1064.  ,  1044.  , ...,  -509.2 ,  -612.  ,
           -924.5 ]],
...
 y: array([[[[  598.5 , -1745.  , -1712.  , ...,   835.5 ,  1004.  ,
           1516.  ]],
...
2025-04-21 15:08:30.835914 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 171798692, 1],"float16"), list[list[3,0,2,1,],list[2,1,0,3,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 171798692, 1],"float16"), list[list[3,0,2,1,],list[2,1,0,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array(-inf, dtype=float16)
 y: array(inf, dtype=float16)
2025-04-21 15:08:45.528755 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 171798692, 1],"float16"), list[list[3,0,],list[2,1,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 171798692, 1],"float16"), list[list[3,0,],list[2,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 858967549 / 858993460 (100%)
Max absolute difference: 14192.
Max relative difference: 1.61
 x: array([[[[  222.8]],

        [[  265.8]],...
 y: array([[[[ -365.2]],

        [[ -436. ]],...
2025-04-21 15:11:17.000542 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 171798692, 1],"float16"), list[list[3,1,0,2,],list[2,3,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 171798692, 1],"float16"), list[list[3,1,0,2,],list[2,3,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array(-inf, dtype=float16)
 y: array(inf, dtype=float16)
2025-04-21 15:11:32.959311 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 171798692, 1],"float16"), list[list[3,1,2,0,],list[2,3,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 171798692, 1],"float16"), list[list[3,1,2,0,],list[2,3,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array(-inf, dtype=float16)
 y: array(inf, dtype=float16)
2025-04-21 15:11:44.813689 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 171798692, 1],"float16"), list[list[3,1,2,],list[2,3,1,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 171798692, 1],"float16"), list[list[3,1,2,],list[2,3,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[ inf],
       [-inf],
       [-inf],...
 y: array([[ inf],
       [ inf],
       [ inf],...
2025-04-21 15:11:55.767729 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 171798692, 1],"float16"), list[list[3,1,2,],list[3,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 171798692, 1],"float16"), list[list[3,1,2,],list[3,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 858940375 / 858993460 (100%)
Max absolute difference: 5908.
Max relative difference: 1.103
 x: array([[ -123.6,   360.5,   353.5, ...,  -172.5,  -207.4,  -313.2],
       [  408. , -1190. , -1167. , ...,   569.5,   684.5,  1034. ],
       [ 1313. , -3830. , -3758. , ...,  1834. ,  2204. ,  3328. ],...
 y: array([[ 1212. , -3536. , -3468. , ...,  1693. ,  2034. ,  3072. ],
       [ 1214. , -3540. , -3472. , ...,  1694. ,  2036. ,  3076. ],
       [ 1338. , -3902. , -3828. , ...,  1868. ,  2244. ,  3390. ],...
2025-04-21 15:14:38.635368 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 171798692, 1],"float16"), list[list[3,2,0,],list[2,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 171798692, 1],"float16"), list[list[3,2,0,],list[2,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[ inf],
       [ inf],
       [-inf],...
 y: array([[ inf],
       [-inf],
       [ inf],...
2025-04-21 15:23:58.828651 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 171798692, 1],"float16"), list[list[3,2,1,],list[2,0,1,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 171798692, 1],"float16"), list[list[3,2,1,],list[2,0,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[ inf],
       [-inf],
       [-inf],...
 y: array([[ inf],
       [ inf],
       [ inf],...
2025-04-21 15:24:12.923480 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 171798692, 1],"float16"), list[list[3,2,],list[3,1,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 171798692, 1],"float16"), list[list[3,2,],list[3,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294801427 / 4294967300 (100%)
Max absolute difference: 3498.
Max relative difference: 28.23
 x: array([[[[ -364.8 ,  1064.  ,  1044.  , ...,  -509.2 ,  -612.  ,
           -924.5 ]],
...
 y: array([[[[  598.5 , -1745.  , -1712.  , ...,   835.5 ,  1004.  ,
           1516.  ]],
...
2025-04-21 15:37:28.841791 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[0,1,3,2,],list[1,3,0,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[0,1,3,2,],list[1,3,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 1073.
Max relative difference: 0.671
 x: array(-2672., dtype=float16)
 y: array(-1599., dtype=float16)
2025-04-21 15:37:41.607175 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[0,2,3,],list[0,2,1,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[0,2,3,],list[0,2,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 25 / 25 (100%)
Max absolute difference: 7724.
Max relative difference: 47.66
 x: array([[-2.115e+00, -3.962e+03, -1.390e+03, -1.342e+03,  7.775e+02],
       [-6.150e+01, -5.536e+03, -4.788e+03, -2.888e+03,  3.348e+03],
       [ 3.938e+03, -1.574e+03, -1.426e+03, -2.594e+03, -3.762e+03],...
 y: array([[ -7728. ,  -6900. ,  -6532. ,   -941.5,   5772. ],
       [  1536. ,  -1793. ,    102.6,   -907.5,   -193.9],
       [   354.5,   -229.9,  -4844. ,  -4856. ,  -1015.5],...
2025-04-21 15:37:58.425419 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[1,0,3,2,],list[3,0,1,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[1,0,3,2,],list[3,0,1,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 6780.
Max relative difference: 0.4756
 x: array(-7476., dtype=float16)
 y: array(-14256., dtype=float16)
2025-04-21 15:48:08.481992 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[1,2,0,3,],list[0,2,1,3,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[1,2,0,3,],list[0,2,1,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 2956.
Max relative difference: 0.2903
 x: array(7228., dtype=float16)
 y: array(10184., dtype=float16)
2025-04-21 15:48:25.739826 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[1,2,0,],list[1,2,3,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[1,2,0,],list[1,2,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 7176.
Max relative difference: 1.303
 x: array([[-4864. , -2628. , -5584. , -2930. ,   755.5]], dtype=float16)
 y: array([[-10584.,  -6276., -12760.,  -7068.,  -2498.]], dtype=float16)
2025-04-21 15:48:44.176650 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[1,2,3,0,],list[3,2,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[1,2,3,0,],list[3,2,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 6780.
Max relative difference: 0.4756
 x: array(-7476., dtype=float16)
 y: array(-14256., dtype=float16)
2025-04-21 15:58:43.552381 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[1,3,2,0,],list[1,3,2,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[1,3,2,0,],list[1,3,2,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 5656.
Max relative difference: 0.7183
 x: array(-2218., dtype=float16)
 y: array(-7872., dtype=float16)
2025-04-21 15:59:04.812921 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[2,1,0,3,],list[2,0,1,3,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[2,1,0,3,],list[2,0,1,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 2956.
Max relative difference: 0.2903
 x: array(7228., dtype=float16)
 y: array(10184., dtype=float16)
2025-04-21 15:59:20.068328 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[2,1,0,3,],list[2,0,3,1,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[2,1,0,3,],list[2,0,3,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 2378.
Max relative difference: 0.4712
 x: array(2670., dtype=float16)
 y: array(5048., dtype=float16)
2025-04-21 15:59:35.625405 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[3,1,0,2,],list[3,1,0,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[3,1,0,2,],list[3,1,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 5656.
Max relative difference: 0.7183
 x: array(-2218., dtype=float16)
 y: array(-7872., dtype=float16)
2025-04-21 16:09:22.099307 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[3,1,2,],list[0,3,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[3,1,2,],list[0,3,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 25 / 25 (100%)
Max absolute difference: 11696.
Max relative difference: 3.102
 x: array([[-2338.  , -1718.  ,  4232.  , -1974.  ,  3924.  ],
       [-4332.  ,  2128.  ,  -346.5 , -3308.  ,   404.5 ],
       [   61.38,  3792.  , -4184.  , -5212.  ,  4976.  ],...
 y: array([[ -1263. ,   1284. ,  -7464. , -11688. ,   9504. ],
       [-11704. ,   5400. ,  -1412. , -12856. ,   4472. ],
       [  1719. ,   4828. ,  -4716. ,  -3080. ,   2770. ],...
2025-04-21 16:09:44.160322 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[3,1,2,],list[1,3,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[3,1,2,],list[1,3,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 25 / 25 (100%)
Max absolute difference: 8624.
Max relative difference: 13.23
 x: array([[ -492.5 ,   199.5 ,  -537.5 ,  1468.  ,  1487.  ],
       [  107.5 , -4716.  ,  1002.  , -1972.  ,   122.25],
       [-3936.  ,  -743.5 ,   -48.66,  2784.  ,  1377.  ],...
 y: array([[-5196. , -7012. ,  3462. , -1829. ,   948. ],
       [ 3828. , -7772. , -2484. , -6776. , -2898. ],
       [-2104. ,  3072. , -2606. ,  3272. ,  -112.6],...
2025-04-21 16:10:02.620108 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[3,2,0,],list[3,2,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[3,2,0,],list[3,2,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 25 / 25 (100%)
Max absolute difference: 8800.
Max relative difference: 3.29
 x: array([[  819. , -4964. ,  1398. , -4504. ,  1329. ],
       [ 1810. , -1468. ,  -155.5, -5572. , -4536. ],
       [ 2270. ,  9808. , -8168. , -2668. , -6652. ],...
 y: array([[  -541.5,   2170. ,  -2794. , -10192. ,  -4968. ],
       [   753. ,  -4984. ,   2208. ,  -1378. ,   2142. ],
       [ -3554. ,  18608. , -10264. ,  -5332. , -10032. ],...
2025-04-21 17:23:21.424285 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 1, 4294967297, 1],"float16"), list[list[0,2,3,],list[0,1,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 1, 4294967297, 1],"float16"), list[list[0,2,3,],list[0,1,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y -inf location mismatch:
 x: array([[ -8936.],
       [    inf],
       [-56064.],...
 y: array([[-14800.],
       [    inf],
       [   -inf],...
2025-04-21 17:23:37.188464 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 1, 4294967297, 1],"float16"), list[list[0,3,1,2,],list[3,2,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 1, 4294967297, 1],"float16"), list[list[0,3,1,2,],list[3,2,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 11104.
Max relative difference: 0.3962
 x: array(-16912., dtype=float16)
 y: array(-28016., dtype=float16)
2025-04-21 17:23:49.111719 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 1, 4294967297, 1],"float16"), list[list[1,0,2,3,],list[3,0,1,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 1, 4294967297, 1],"float16"), list[list[1,0,2,3,],list[3,0,1,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 11072.
Max relative difference: 0.3962
 x: array(-16864., dtype=float16)
 y: array(-27936., dtype=float16)
2025-04-21 17:24:03.686617 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 1, 4294967297, 1],"float16"), list[list[2,0,1,3,],list[3,1,0,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 1, 4294967297, 1],"float16"), list[list[2,0,1,3,],list[3,1,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 11072.
Max relative difference: 0.3962
 x: array(-16864., dtype=float16)
 y: array(-27936., dtype=float16)
2025-04-21 17:24:14.272636 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 1, 4294967297, 1],"float16"), list[list[2,3,0,],list[1,2,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 1, 4294967297, 1],"float16"), list[list[2,3,0,],list[1,2,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y -inf location mismatch:
 x: array([[ -8928.],
       [    inf],
       [-56064.],...
 y: array([[-14792.],
       [    inf],
       [   -inf],...
2025-04-21 17:24:24.901972 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 1, 4294967297, 1],"float16"), list[list[3,0,2,1,],list[2,1,0,3,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 1, 4294967297, 1],"float16"), list[list[3,0,2,1,],list[2,1,0,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 11072.
Max relative difference: 0.396
 x: array(-16880., dtype=float16)
 y: array(-27952., dtype=float16)
2025-04-21 17:24:39.583922 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 1, 4294967297, 1],"float16"), list[list[3,0,],list[2,1,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 1, 4294967297, 1],"float16"), list[list[3,0,],list[2,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 25 / 25 (100%)
Max absolute difference: 17712.
Max relative difference: 0.3965
 x: array([[[[ -4160. ]],

        [[-11304. ]],...
 y: array([[[[ -6892. ]],

        [[-18720. ]],...
2025-04-21 17:24:50.777024 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 1, 4294967297, 1],"float16"), list[list[3,1,0,2,],list[2,3,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 1, 4294967297, 1],"float16"), list[list[3,1,0,2,],list[2,3,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 11072.
Max relative difference: 0.3962
 x: array(-16864., dtype=float16)
 y: array(-27936., dtype=float16)
2025-04-21 17:25:04.621205 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 1, 4294967297, 1],"float16"), list[list[3,1,2,0,],list[2,3,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 1, 4294967297, 1],"float16"), list[list[3,1,2,0,],list[2,3,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 11072.
Max relative difference: 0.396
 x: array(-16880., dtype=float16)
 y: array(-27952., dtype=float16)
2025-04-21 17:25:16.586990 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 1, 4294967297, 1],"float16"), list[list[3,1,2,],list[2,3,1,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 1, 4294967297, 1],"float16"), list[list[3,1,2,],list[2,3,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 18912.
Max relative difference: 0.3965
 x: array([[-28832.],
       [-24144.],
       [  4360.],...
 y: array([[-47744.],
       [-40000.],
       [  7220.],...
2025-04-21 17:25:27.545441 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 1, 4294967297, 1],"float16"), list[list[3,2,0,],list[2,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 1, 4294967297, 1],"float16"), list[list[3,2,0,],list[2,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y -inf location mismatch:
 x: array([[ -8928.],
       [    inf],
       [-56064.],...
 y: array([[-14792.],
       [    inf],
       [   -inf],...
2025-04-21 17:25:39.044913 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 1, 4294967297, 1],"float16"), list[list[3,2,1,],list[2,0,1,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 1, 4294967297, 1],"float16"), list[list[3,2,1,],list[2,0,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 18896.
Max relative difference: 0.3965
 x: array([[-28816.],
       [-24160.],
       [  4364.],...
 y: array([[-47712.],
       [-40032.],
       [  7232.],...
2025-04-21 17:25:53.554184 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[0,1,3,2,],list[0,2,1,3,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[0,1,3,2,],list[0,2,1,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 11104.
Max relative difference: 0.3962
 x: array(-16912., dtype=float16)
 y: array(-28016., dtype=float16)
2025-04-21 17:26:05.398163 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[0,1,3,],list[0,3,1,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[0,1,3,],list[0,3,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 18112.
Max relative difference: 0.3962
 x: array([[-27616.],
       [ 10432.],
       [  2918.],...
 y: array([[-45728.],
       [ 17280.],
       [  4832.],...
2025-04-21 17:26:16.588307 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[0,2,3,],list[0,2,1,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[0,2,3,],list[0,2,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y -inf location mismatch:
 x: array([[ -8936.],
       [    inf],
       [-56064.],...
 y: array([[-14800.],
       [    inf],
       [   -inf],...
2025-04-21 17:26:27.513548 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[0,3,2,1,],list[2,1,3,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[0,3,2,1,],list[2,1,3,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 11072.
Max relative difference: 0.396
 x: array(-16880., dtype=float16)
 y: array(-27952., dtype=float16)
2025-04-21 17:26:40.830558 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[1,0,3,2,],list[3,0,1,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[1,0,3,2,],list[3,0,1,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 11072.
Max relative difference: 0.3962
 x: array(-16864., dtype=float16)
 y: array(-27936., dtype=float16)
2025-04-21 17:26:55.946694 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[1,2,3,0,],list[3,2,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[1,2,3,0,],list[3,2,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 11072.
Max relative difference: 0.396
 x: array(-16880., dtype=float16)
 y: array(-27952., dtype=float16)
2025-04-21 17:27:08.000989 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[1,3,2,0,],list[2,1,0,3,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[1,3,2,0,],list[2,1,0,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 11072.
Max relative difference: 0.396
 x: array(-16880., dtype=float16)
 y: array(-27952., dtype=float16)
2025-04-21 17:27:18.644896 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[2,1,0,3,],list[2,0,3,1,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[2,1,0,3,],list[2,0,3,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 11072.
Max relative difference: 0.396
 x: array(-16880., dtype=float16)
 y: array(-27952., dtype=float16)
2025-04-21 17:27:32.419426 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[2,3,0,],list[3,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[2,3,0,],list[3,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y -inf location mismatch:
 x: array([[ -8928.],
       [    inf],
       [-56064.],...
 y: array([[-14792.],
       [    inf],
       [   -inf],...
2025-04-21 17:27:43.574606 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[3,1,2,0,],list[1,2,3,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[3,1,2,0,],list[1,2,3,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 11072.
Max relative difference: 0.396
 x: array(-16880., dtype=float16)
 y: array(-27952., dtype=float16)
2025-04-21 17:27:54.569261 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[3,1,2,],list[1,2,3,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[3,1,2,],list[1,2,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 18912.
Max relative difference: 0.3965
 x: array([[-28832.],
       [-24144.],
       [  4360.],...
 y: array([[-47744.],
       [-40000.],
       [  7220.],...
2025-04-21 17:28:04.793362 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[3,1,2,],list[1,3,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([1, 4294967297, 1, 1],"float16"), list[list[3,1,2,],list[1,3,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 18912.
Max relative difference: 0.3965
 x: array([[-28832.],
       [-24144.],
       [  4360.],...
 y: array([[-47744.],
       [-40000.],
       [  7220.],...
2025-04-21 19:44:46.959403 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([171798692, 5, 1, 5],"float16"), list[list[3,1,2,],list[2,3,1,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([171798692, 5, 1, 5],"float16"), list[list[3,1,2,],list[2,3,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 20 / 858993460 (2.33e-06%)
Max absolute difference: 0.11694
Max relative difference: 5.67
 x: array([[-0.04327, -0.3118 ,  0.714  , ...,  0.658  , -0.3486 ,  0.04898],
       [-0.3118 ,  0.0851 ,  0.07837, ...,  0.3152 , -0.2695 ,  0.0505 ],
       [ 0.714  ,  0.07837,  0.6577 , ...,  0.0847 , -0.2455 , -0.345  ],...
 y: array([[-0.04327, -0.3118 ,  0.714  , ...,  0.696  , -0.449  ,  0.1465 ],
       [-0.3118 ,  0.0851 ,  0.07837, ...,  0.359  , -0.3865 ,  0.1642 ],
       [ 0.714  ,  0.07837,  0.6577 , ...,  0.10504, -0.2998 , -0.292  ],...
2025-04-21 20:11:22.408326 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([4294967297, 1, 1, 1],"float16"), list[list[0,1,3,2,],list[1,3,0,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([4294967297, 1, 1, 1],"float16"), list[list[0,1,3,2,],list[1,3,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 11104.
Max relative difference: 0.3962
 x: array(-16912., dtype=float16)
 y: array(-28016., dtype=float16)
2025-04-21 20:11:35.384227 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([4294967297, 1, 1, 1],"float16"), list[list[0,1,3,2,],list[2,3,0,1,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([4294967297, 1, 1, 1],"float16"), list[list[0,1,3,2,],list[2,3,0,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 11104.
Max relative difference: 0.3962
 x: array(-16912., dtype=float16)
 y: array(-28016., dtype=float16)
2025-04-21 20:11:45.431014 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([4294967297, 1, 1, 1],"float16"), list[list[1,0,3,2,],list[2,3,0,1,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([4294967297, 1, 1, 1],"float16"), list[list[1,0,3,2,],list[2,3,0,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 11072.
Max relative difference: 0.3962
 x: array(-16864., dtype=float16)
 y: array(-27936., dtype=float16)
2025-04-21 20:11:56.019096 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([4294967297, 1, 1, 1],"float16"), list[list[1,0,3,],list[2,3,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([4294967297, 1, 1, 1],"float16"), list[list[1,0,3,],list[2,3,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 18112.
Max relative difference: 0.3965
 x: array([[-27616.],
       [ 10456.],
       [  2922.],...
 y: array([[-45728.],
       [ 17312.],
       [  4840.],...
2025-04-21 20:12:06.545130 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([4294967297, 1, 1, 1],"float16"), list[list[1,2,3,0,],list[1,3,0,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([4294967297, 1, 1, 1],"float16"), list[list[1,2,3,0,],list[1,3,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 11072.
Max relative difference: 0.396
 x: array(-16880., dtype=float16)
 y: array(-27952., dtype=float16)
2025-04-21 20:12:18.821554 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([4294967297, 1, 1, 1],"float16"), list[list[1,3,0,],list[3,0,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([4294967297, 1, 1, 1],"float16"), list[list[1,3,0,],list[3,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 18112.
Max relative difference: 0.3965
 x: array([[-27616.],
       [ 10456.],
       [  2922.],...
 y: array([[-45728.],
       [ 17312.],
       [  4840.],...
2025-04-21 20:12:29.610302 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([4294967297, 1, 1, 1],"float16"), list[list[1,3,2,0,],list[3,0,1,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([4294967297, 1, 1, 1],"float16"), list[list[1,3,2,0,],list[3,0,1,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 11072.
Max relative difference: 0.396
 x: array(-16880., dtype=float16)
 y: array(-27952., dtype=float16)
2025-04-21 20:12:40.521328 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([4294967297, 1, 1, 1],"float16"), list[list[1,3,],list[1,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([4294967297, 1, 1, 1],"float16"), list[list[1,3,],list[1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 25 / 25 (100%)
Max absolute difference: 16832.
Max relative difference: 0.3965
 x: array([[[[ -7484. ]],

        [[ -8092. ]],...
 y: array([[[[-12400. ]],

        [[-13408. ]],...
2025-04-21 20:12:52.170415 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([4294967297, 1, 1, 1],"float16"), list[list[2,1,0,3,],list[3,1,2,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([4294967297, 1, 1, 1],"float16"), list[list[2,1,0,3,],list[3,1,2,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 11072.
Max relative difference: 0.396
 x: array(-16880., dtype=float16)
 y: array(-27952., dtype=float16)
2025-04-21 20:13:04.828109 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([4294967297, 1, 1, 1],"float16"), list[list[2,3,1,],list[1,0,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([4294967297, 1, 1, 1],"float16"), list[list[2,3,1,],list[1,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 18896.
Max relative difference: 0.3965
 x: array([[-28816.],
       [-24160.],
       [  4364.],...
 y: array([[-47712.],
       [-40032.],
       [  7232.],...
2025-04-21 20:13:15.372476 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([4294967297, 1, 1, 1],"float16"), list[list[3,1,2,],list[0,3,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([4294967297, 1, 1, 1],"float16"), list[list[3,1,2,],list[0,3,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 18912.
Max relative difference: 0.3965
 x: array([[-28832.],
       [-24144.],
       [  4360.],...
 y: array([[-47744.],
       [-40000.],
       [  7220.],...
2025-04-21 20:13:26.507635 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([4294967297, 1, 1, 1],"float16"), list[list[3,2,1,0,],list[0,1,3,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([4294967297, 1, 1, 1],"float16"), list[list[3,2,1,0,],list[0,1,3,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 11072.
Max relative difference: 0.396
 x: array(-16880., dtype=float16)
 y: array(-27952., dtype=float16)
2025-04-21 23:28:39.196267 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([5, 5, 1, 171798692],"float16"), list[list[3,2,0,],list[2,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([5, 5, 1, 171798692],"float16"), list[list[3,2,0,],list[2,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 16 / 858993460 (1.86e-06%)
Max absolute difference: 0.188
Max relative difference: 11.914
 x: array([[-0.2612  , -0.1277  , -0.09064 , ..., -0.1343  ,  0.3013  ,
         0.7812  ],
       [ 0.3699  , -0.5273  , -0.4053  , ...,  0.238   , -0.2089  ,...
 y: array([[-0.2612  , -0.1277  , -0.09064 , ..., -0.09454 ,  0.3438  ,
         0.628   ],
       [ 0.3699  , -0.5273  , -0.4053  , ...,  0.2355  , -0.2117  ,...
2025-04-21 23:48:02.552053 test begin: paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([5, 5, 34359739, 5],"float16"), list[list[0,1,3,],list[0,3,1,],], )

[accuracy error] backward  paddle.tensordot(Tensor([5, 5, 5, 1],"float16"), Tensor([5, 5, 34359739, 5],"float16"), list[list[0,1,3,],list[0,3,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 125 (0.8%)
Max absolute difference: 2.
Max relative difference: 0.02795
 x: array([[[[ -965.5 ],
         [  747.5 ],
         [-1810.  ],...
 y: array([[[[ -965.5  ],
         [  747.5  ],
         [-1810.   ],...
2025-04-22 07:03:23.230333 test begin: paddle.tensordot(Tensor([5, 5, 5, 34359739],"float16"), Tensor([5, 5, 1, 34359739],"float16"), list[list[0,2,3,],list[0,1,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 34359739],"float16"), Tensor([5, 5, 1, 34359739],"float16"), list[list[0,2,3,],list[0,1,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 27832 / 171798695 (0.0162%)
Max absolute difference: 8.
Max relative difference: 500.
 x: array([[-3284. ,  1535. ,  1616. , ..., -1789. ,  -230.2,  -519.5],
       [ 5904. , -4724. ,    77.6, ...,  -256.5,  -171.5,  2972. ],
       [-3218. , -2168. ,  3788. , ..., -5656. , -2176. ,  -637. ],...
 y: array([[-3284. ,  1535. ,  1616. , ..., -1789. ,  -230.2,  -519.5],
       [ 5904. , -4724. ,    77.6, ...,  -256.5,  -171.5,  2972. ],
       [-3218. , -2168. ,  3788. , ..., -5656. , -2176. ,  -637. ],...
2025-04-22 07:04:11.137043 test begin: paddle.tensordot(Tensor([5, 5, 5, 34359739],"float16"), Tensor([5, 5, 1, 34359739],"float16"), list[list[2,3,0,],list[1,2,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 34359739],"float16"), Tensor([5, 5, 1, 34359739],"float16"), list[list[2,3,0,],list[1,2,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 27828 / 171798695 (0.0162%)
Max absolute difference: 8.
Max relative difference: 399.8
 x: array([[-3284. ,  1535. ,  1616. , ..., -1789. ,  -230.2,  -519.5],
       [ 5904. , -4724. ,    77.6, ...,  -256.5,  -171.5,  2972. ],
       [-3218. , -2168. ,  3788. , ..., -5656. , -2176. ,  -637. ],...
 y: array([[-3284. ,  1535. ,  1616. , ..., -1789. ,  -230.2,  -519.5],
       [ 5904. , -4724. ,    77.6, ...,  -256.5,  -171.5,  2972. ],
       [-3218. , -2168. ,  3788. , ..., -5656. , -2176. ,  -637. ],...
2025-04-22 07:04:58.235319 test begin: paddle.tensordot(Tensor([5, 5, 5, 34359739],"float16"), Tensor([5, 5, 1, 34359739],"float16"), list[list[3,0,],list[2,1,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 34359739],"float16"), Tensor([5, 5, 1, 34359739],"float16"), list[list[3,0,],list[2,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 366959 / 4294967375 (0.00854%)
Max absolute difference: 2.
Max relative difference: 1249.
 x: array([[[[-1.2250e+03, -1.7100e+02, -2.0425e+02, ..., -3.2950e+02,
          -9.6850e+02,  7.4400e+02],
         [ 2.8350e+02, -6.2500e+02,  2.1250e+02, ..., -2.0740e+03,...
 y: array([[[[-1.2250e+03, -1.7100e+02, -2.0425e+02, ..., -3.2950e+02,
          -9.6850e+02,  7.4400e+02],
         [ 2.8350e+02, -6.2500e+02,  2.1250e+02, ..., -2.0740e+03,...
2025-04-22 07:17:44.827888 test begin: paddle.tensordot(Tensor([5, 5, 5, 34359739],"float16"), Tensor([5, 5, 1, 34359739],"float16"), list[list[3,2,0,],list[2,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 34359739],"float16"), Tensor([5, 5, 1, 34359739],"float16"), list[list[3,2,0,],list[2,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 27828 / 171798695 (0.0162%)
Max absolute difference: 8.
Max relative difference: 399.8
 x: array([[-3284. ,  1535. ,  1616. , ..., -1789. ,  -230.2,  -519.5],
       [ 5904. , -4724. ,    77.6, ...,  -256.5,  -171.5,  2972. ],
       [-3218. , -2168. ,  3788. , ..., -5656. , -2176. ,  -637. ],...
 y: array([[-3284. ,  1535. ,  1616. , ..., -1789. ,  -230.2,  -519.5],
       [ 5904. , -4724. ,    77.6, ...,  -256.5,  -171.5,  2972. ],
       [-3218. , -2168. ,  3788. , ..., -5656. , -2176. ,  -637. ],...
2025-04-22 07:29:58.480930 test begin: paddle.tensordot(Tensor([5, 5, 5, 34359739],"float16"), Tensor([5, 5, 1, 34359739],"float16"), list[list[3,2,1,],list[2,0,1,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 5, 34359739],"float16"), Tensor([5, 5, 1, 34359739],"float16"), list[list[3,2,1,],list[2,0,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 21199 / 171798695 (0.0123%)
Max absolute difference: 8.
Max relative difference: 459.5
 x: array([[ 4216. ,   116.5,  2972. , ...,  2362. ,  -268.2, -1157. ],
       [  527.5, -1978. ,  3436. , ..., -1278. ,  1768. , -1387. ],
       [  188.6,  1009. ,  3536. , ...,  4224. , -3048. ,  4592. ],...
 y: array([[ 4216. ,   116.5,  2972. , ...,  2362. ,  -268.2, -1157. ],
       [  527.5, -1978. ,  3436. , ..., -1278. ,  1768. , -1387. ],
       [  188.6,  1009. ,  3536. , ...,  4224. , -3048. ,  4592. ],...
2025-04-22 08:16:39.242018 test begin: paddle.tensordot(Tensor([5, 5, 5, 34359739],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[1,2,0,],list[1,3,2,],], )

[accuracy error] backward  paddle.tensordot(Tensor([5, 5, 5, 34359739],"float16"), Tensor([5, 5, 1, 5],"float16"), list[list[1,2,0,],list[1,3,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 125 (0.8%)
Max absolute difference: 1.
Max relative difference: 0.0447
 x: array([[[[ 1.4650e+03, -3.8225e+02,  1.2656e+02, -3.6525e+02,
          -8.8250e+02]],
...
 y: array([[[[ 1.466e+03, -3.820e+02,  1.265e+02, -3.652e+02, -8.825e+02]],

        [[-5.875e+02, -7.090e+02,  1.554e+02,  5.670e+02,  4.295e+02]],...
2025-04-22 09:55:24.576631 test begin: paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[0,2,3,],list[0,1,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[0,2,3,],list[0,1,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 1265.18393204
Max relative difference: 2.61021492
 x: array([[ -379.140286],
       [   74.409673],
       [ -699.462126],...
 y: array([[-1092.133648],
       [  -46.211019],
       [ -907.19327 ],...
2025-04-22 09:57:22.826124 test begin: paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[0,2,3,],list[0,2,1,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[0,2,3,],list[0,2,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 1265.18393204
Max relative difference: 2.61021492
 x: array([[ -379.140286],
       [   74.409673],
       [ -699.462126],...
 y: array([[-1092.133648],
       [  -46.211019],
       [ -907.19327 ],...
2025-04-22 09:57:46.315047 test begin: paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[0,3,2,1,],list[2,1,3,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[0,3,2,1,],list[2,1,3,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 2555.81393505
Max relative difference: 0.44868056
 x: array(-3140.47456)
 y: array(-5696.288495)
2025-04-22 09:58:10.581397 test begin: paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[1,2,0,3,],list[0,2,1,3,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[1,2,0,3,],list[0,2,1,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 2555.81393505
Max relative difference: 0.44868056
 x: array(-3140.47456)
 y: array(-5696.288495)
2025-04-22 09:58:29.835971 test begin: paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[1,2,0,],list[1,2,3,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[1,2,0,],list[1,2,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 2555.81393505
Max relative difference: 0.44868056
 x: array([[-3140.47456]])
 y: array([[-5696.288495]])
2025-04-22 09:58:55.032475 test begin: paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[1,2,0,],list[1,3,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[1,2,0,],list[1,3,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 2555.81393505
Max relative difference: 0.44868056
 x: array([[-3140.47456]])
 y: array([[-5696.288495]])
2025-04-22 09:59:16.808263 test begin: paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[1,2,3,0,],list[1,3,0,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[1,2,3,0,],list[1,3,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 2555.81393505
Max relative difference: 0.44868056
 x: array(-3140.47456)
 y: array(-5696.288495)
2025-04-22 09:59:40.215064 test begin: paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[1,2,3,0,],list[3,2,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[1,2,3,0,],list[3,2,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 2555.81393505
Max relative difference: 0.44868056
 x: array(-3140.47456)
 y: array(-5696.288495)
2025-04-22 10:00:04.278418 test begin: paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[1,3,2,0,],list[1,3,2,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[1,3,2,0,],list[1,3,2,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 2555.81393505
Max relative difference: 0.44868056
 x: array(-3140.47456)
 y: array(-5696.288495)
2025-04-22 10:00:25.988564 test begin: paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[1,3,2,0,],list[2,1,0,3,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[1,3,2,0,],list[2,1,0,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 2555.81393505
Max relative difference: 0.44868056
 x: array(-3140.47456)
 y: array(-5696.288495)
2025-04-22 10:00:42.469576 test begin: paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[1,3,2,0,],list[3,0,1,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[1,3,2,0,],list[3,0,1,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 2555.81393505
Max relative difference: 0.44868056
 x: array(-3140.47456)
 y: array(-5696.288495)
2025-04-22 10:01:05.505888 test begin: paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[3,0,2,1,],list[2,1,0,3,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[3,0,2,1,],list[2,1,0,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 2555.81393505
Max relative difference: 0.44868056
 x: array(-3140.47456)
 y: array(-5696.288495)
2025-04-22 10:01:28.880584 test begin: paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[3,1,2,0,],list[1,2,3,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[3,1,2,0,],list[1,2,3,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 2555.81393505
Max relative difference: 0.44868056
 x: array(-3140.47456)
 y: array(-5696.288495)
2025-04-22 10:01:52.581085 test begin: paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[3,1,2,0,],list[2,3,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[3,1,2,0,],list[2,3,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 2555.81393505
Max relative difference: 0.44868056
 x: array(-3140.47456)
 y: array(-5696.288495)
2025-04-22 10:02:16.681300 test begin: paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[3,1,2,],list[0,3,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[3,1,2,],list[0,3,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 986.70572693
Max relative difference: 2.65732613
 x: array([[ -552.623122],
       [ -191.570929],
       [ -729.086905],...
 y: array([[-1320.0688  ],
       [ -882.341974],
       [-1715.792632],...
2025-04-22 10:02:39.041595 test begin: paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[3,1,2,],list[1,2,3,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[3,1,2,],list[1,2,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 986.70572693
Max relative difference: 2.65732613
 x: array([[ -552.623122],
       [ -191.570929],
       [ -729.086905],...
 y: array([[-1320.0688  ],
       [ -882.341974],
       [-1715.792632],...
2025-04-22 10:03:13.946359 test begin: paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[3,1,2,],list[1,3,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[3,1,2,],list[1,3,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 986.70572693
Max relative difference: 2.65732613
 x: array([[ -552.623122],
       [ -191.570929],
       [ -729.086905],...
 y: array([[-1320.0688  ],
       [ -882.341974],
       [-1715.792632],...
2025-04-22 10:03:35.156941 test begin: paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[3,1,2,],list[2,3,1,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[3,1,2,],list[2,3,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 986.70572693
Max relative difference: 2.65732613
 x: array([[ -552.623122],
       [ -191.570929],
       [ -729.086905],...
 y: array([[-1320.0688  ],
       [ -882.341974],
       [-1715.792632],...
2025-04-22 10:03:57.697830 test begin: paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[3,1,2,],list[3,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[3,1,2,],list[3,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 986.70572693
Max relative difference: 2.65732613
 x: array([[ -552.623122],
       [ -191.570929],
       [ -729.086905],...
 y: array([[-1320.0688  ],
       [ -882.341974],
       [-1715.792632],...
2025-04-22 10:04:23.751204 test begin: paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 85899346, 1],"float64"), list[list[0,2,3,],list[0,1,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 85899346, 1],"float64"), list[list[0,2,3,],list[0,1,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 24721825.46873351
Max relative difference: 2.61021492
 x: array([[ -7408440.576904],
       [  1453972.737595],
       [-13667562.599381],...
 y: array([[-21340404.934203],
       [  -902968.117828],
       [-17726650.750257],...
2025-04-22 10:04:44.051083 test begin: paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 85899346, 1],"float64"), list[list[3,0,2,1,],list[2,1,0,3,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 85899346, 1],"float64"), list[list[3,0,2,1,],list[2,1,0,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 49940869.80771827
Max relative difference: 0.44868056
 x: array(-61365199.160282)
 y: array(-1.113061e+08)
2025-04-22 10:05:05.947598 test begin: paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 85899346, 1],"float64"), list[list[3,1,2,0,],list[2,3,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 85899346, 1],"float64"), list[list[3,1,2,0,],list[2,3,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 49940869.80771802
Max relative difference: 0.44868056
 x: array(-61365199.160282)
 y: array(-1.113061e+08)
2025-04-22 10:05:29.912085 test begin: paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 85899346, 1],"float64"), list[list[3,1,2,],list[2,3,1,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 85899346, 1],"float64"), Tensor([1, 1, 85899346, 1],"float64"), list[list[3,1,2,],list[2,3,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 19280332.41047183
Max relative difference: 2.65732613
 x: array([[-10798313.215169],
       [ -3743315.84937 ],
       [-14246433.872146],...
 y: array([[-25794281.495626],
       [-17241053.839875],
       [-33526766.282618],...
2025-04-22 10:05:53.475174 test begin: paddle.tensordot(Tensor([5, 85899346, 5, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[2,1,0,3,],list[2,0,1,3,],], )

[accuracy error] paddle.tensordot(Tensor([5, 85899346, 5, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[2,1,0,3,],list[2,0,1,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 4524.07186628
Max relative difference: 0.79421396
 x: array(-1172.216629)
 y: array(-5696.288495)
2025-04-22 10:06:19.866974 test begin: paddle.tensordot(Tensor([5, 85899346, 5, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[2,1,0,3,],list[2,0,3,1,],], )

[accuracy error] paddle.tensordot(Tensor([5, 85899346, 5, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[2,1,0,3,],list[2,0,3,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 4524.07186628
Max relative difference: 0.79421396
 x: array(-1172.216629)
 y: array(-5696.288495)
2025-04-22 10:06:41.433518 test begin: paddle.tensordot(Tensor([5, 85899346, 5, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[2,1,0,3,],list[3,1,2,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 85899346, 5, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[2,1,0,3,],list[3,1,2,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 4524.07186628
Max relative difference: 0.79421396
 x: array(-1172.216629)
 y: array(-5696.288495)
2025-04-22 10:07:03.238878 test begin: paddle.tensordot(Tensor([5, 85899346, 5, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[2,1,],list[0,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 85899346, 5, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[2,1,],list[0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 1979.20536608
Max relative difference: 1.49931986
 x: array([[[[ 6.591366e+02]]],

...
 y: array([[[[-1320.0688  ]]],

...
2025-04-22 10:07:26.215316 test begin: paddle.tensordot(Tensor([5, 85899346, 5, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[2,3,1,],list[1,0,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 85899346, 5, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[2,3,1,],list[1,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 1979.20536608
Max relative difference: 1.49931986
 x: array([[ 6.591366e+02],
       [ 9.526101e-01],
       [ 1.607899e+02],...
 y: array([[-1320.0688  ],
       [ -882.341974],
       [-1715.792632],...
2025-04-22 10:07:46.668472 test begin: paddle.tensordot(Tensor([5, 85899346, 5, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[3,2,1,0,],list[0,1,3,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 85899346, 5, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[3,2,1,0,],list[0,1,3,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 4524.07186628
Max relative difference: 0.79421396
 x: array(-1172.216629)
 y: array(-5696.288495)
2025-04-22 10:08:09.828506 test begin: paddle.tensordot(Tensor([5, 85899346, 5, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[3,2,1,],list[2,0,1,],], )

[accuracy error] paddle.tensordot(Tensor([5, 85899346, 5, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), list[list[3,2,1,],list[2,0,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 1979.20536608
Max relative difference: 1.49931986
 x: array([[ 6.591366e+02],
       [ 9.526101e-01],
       [ 1.607899e+02],...
 y: array([[-1320.0688  ],
       [ -882.341974],
       [-1715.792632],...
2025-04-22 10:08:32.144607 test begin: paddle.tensordot(Tensor([5, 85899346, 5, 1],"float64"), Tensor([1, 85899346, 1, 1],"float64"), list[list[2,1,0,3,],list[2,0,3,1,],], )

[accuracy error] paddle.tensordot(Tensor([5, 85899346, 5, 1],"float64"), Tensor([1, 85899346, 1, 1],"float64"), list[list[2,1,0,3,],list[2,0,3,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 88400834.26103035
Max relative difference: 0.79421396
 x: array(-22905234.70697)
 y: array(-1.113061e+08)
2025-04-22 10:08:52.960142 test begin: paddle.tensordot(Tensor([5, 85899346, 5, 1],"float64"), Tensor([1, 85899346, 1, 1],"float64"), list[list[2,1,],list[0,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 85899346, 5, 1],"float64"), Tensor([1, 85899346, 1, 1],"float64"), list[list[2,1,],list[0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 429492494 / 429496730 (100%)
Max absolute difference: 5606.46333081
Max relative difference: 1.49931986
 x: array([[[[ 6.591366e+02],
         [ 6.196949e+02],
         [ 1.404651e+03],...
 y: array([[[[-1320.0688  ],
         [-1241.078093],
         [-2813.129263],...
2025-04-22 12:10:01.265192 test begin: paddle.tensordot(Tensor([858993460, 1, 1, 5],"float16"), Tensor([1, 5, 1, 1],"float16"), list[list[1,2,3,0,],list[1,3,0,2,],], )

[accuracy error] paddle.tensordot(Tensor([858993460, 1, 1, 5],"float16"), Tensor([1, 5, 1, 1],"float16"), list[list[1,2,3,0,],list[1,3,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 3100.
Max relative difference: 0.5903
 x: array(-2152., dtype=float16)
 y: array(-5252., dtype=float16)
2025-04-22 12:18:13.418252 test begin: paddle.tensordot(Tensor([858993460, 1, 1, 5],"float16"), Tensor([1, 5, 1, 1],"float16"), list[list[1,3,0,],list[3,0,2,],], )

[accuracy error] paddle.tensordot(Tensor([858993460, 1, 1, 5],"float16"), Tensor([1, 5, 1, 1],"float16"), list[list[1,3,0,],list[3,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 9152.
Max relative difference: 0.5903
 x: array([[ 2180., -6356., -6236.,  5960.,  2300.]], dtype=float16)
 y: array([[  5316., -15504., -15216.,  14536.,   5612.]], dtype=float16)
2025-04-22 12:18:27.912682 test begin: paddle.tensordot(Tensor([858993460, 1, 1, 5],"float16"), Tensor([1, 5, 1, 1],"float16"), list[list[1,3,2,0,],list[1,3,2,0,],], )

[accuracy error] paddle.tensordot(Tensor([858993460, 1, 1, 5],"float16"), Tensor([1, 5, 1, 1],"float16"), list[list[1,3,2,0,],list[1,3,2,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 3100.
Max relative difference: 0.5903
 x: array(-2152., dtype=float16)
 y: array(-5252., dtype=float16)
2025-04-22 12:26:40.286774 test begin: paddle.tensordot(Tensor([858993460, 1, 1, 5],"float16"), Tensor([1, 5, 1, 1],"float16"), list[list[1,3,2,0,],list[3,0,1,2,],], )

[accuracy error] paddle.tensordot(Tensor([858993460, 1, 1, 5],"float16"), Tensor([1, 5, 1, 1],"float16"), list[list[1,3,2,0,],list[3,0,1,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 3100.
Max relative difference: 0.5903
 x: array(-2152., dtype=float16)
 y: array(-5252., dtype=float16)
2025-04-22 12:42:57.837812 test begin: paddle.tensordot(Tensor([858993460, 1, 1, 5],"float16"), Tensor([1, 5, 1, 1],"float16"), list[list[2,3,0,],list[1,2,0,],], )

[accuracy error] paddle.tensordot(Tensor([858993460, 1, 1, 5],"float16"), Tensor([1, 5, 1, 1],"float16"), list[list[2,3,0,],list[1,2,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 3100.
Max relative difference: 0.5903
 x: array([[-2152.]], dtype=float16)
 y: array([[-5252.]], dtype=float16)
2025-04-22 12:51:03.464916 test begin: paddle.tensordot(Tensor([858993460, 1, 1, 5],"float16"), Tensor([1, 5, 1, 1],"float16"), list[list[3,1,0,2,],list[3,1,0,2,],], )

[accuracy error] paddle.tensordot(Tensor([858993460, 1, 1, 5],"float16"), Tensor([1, 5, 1, 1],"float16"), list[list[3,1,0,2,],list[3,1,0,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 3100.
Max relative difference: 0.5903
 x: array(-2152., dtype=float16)
 y: array(-5252., dtype=float16)
2025-04-22 12:59:33.696462 test begin: paddle.tensordot(Tensor([858993460, 1, 1, 5],"float16"), Tensor([1, 5, 1, 1],"float16"), list[list[3,1,2,0,],list[2,3,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([858993460, 1, 1, 5],"float16"), Tensor([1, 5, 1, 1],"float16"), list[list[3,1,2,0,],list[2,3,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 3100.
Max relative difference: 0.5903
 x: array(-2152., dtype=float16)
 y: array(-5252., dtype=float16)
2025-04-22 12:59:48.914177 test begin: paddle.tensordot(Tensor([858993460, 1, 1, 5],"float16"), Tensor([1, 5, 1, 1],"float16"), list[list[3,2,0,],list[2,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([858993460, 1, 1, 5],"float16"), Tensor([1, 5, 1, 1],"float16"), list[list[3,2,0,],list[2,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 3100.
Max relative difference: 0.5903
 x: array([[-2152.]], dtype=float16)
 y: array([[-5252.]], dtype=float16)
2025-04-22 12:59:59.039552 test begin: paddle.tensordot(Tensor([858993460, 1, 1, 5],"float16"), Tensor([1, 5, 1, 1],"float16"), list[list[3,2,0,],list[3,2,0,],], )

[accuracy error] paddle.tensordot(Tensor([858993460, 1, 1, 5],"float16"), Tensor([1, 5, 1, 1],"float16"), list[list[3,2,0,],list[3,2,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 9152.
Max relative difference: 0.5903
 x: array([[ 2180., -6356., -6236.,  5960.,  2300.]], dtype=float16)
 y: array([[  5316., -15504., -15216.,  14536.,   5612.]], dtype=float16)
2025-04-22 13:00:11.654463 test begin: paddle.tensordot(Tensor([858993460, 1, 1, 5],"float16"), Tensor([1, 5, 1, 1],"float16"), list[list[3,2,1,0,],list[0,1,3,2,],], )

[accuracy error] paddle.tensordot(Tensor([858993460, 1, 1, 5],"float16"), Tensor([1, 5, 1, 1],"float16"), list[list[3,2,1,0,],list[0,1,3,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 3100.
Max relative difference: 0.5903
 x: array(-2152., dtype=float16)
 y: array(-5252., dtype=float16)
2025-04-22 13:16:11.799156 test begin: paddle.tensordot(Tensor([858993460, 1, 1, 5],"float16"), Tensor([858993460, 5, 1, 1],"float16"), list[list[0,3,1,2,],list[3,2,1,0,],], )

[accuracy error] backward  paddle.tensordot(Tensor([858993460, 1, 1, 5],"float16"), Tensor([858993460, 5, 1, 1],"float16"), list[list[0,3,1,2,],list[3,2,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294967300 / 4294967300 (100%)
Max absolute difference: 5096.
Max relative difference: 0.59
 x: array([[[[-3542., -3542., -3542., -3542., -3542.]]],

...
 y: array([[[[-8640., -8640., -8640., -8640., -8640.]]],

...
2025-04-22 15:06:07.325632 test begin: paddle.tensordot(Tensor([858993460, 1, 1, 5],"float16"), Tensor([858993460, 5, 1, 1],"float16"), list[list[3,1,2,],list[3,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([858993460, 1, 1, 5],"float16"), Tensor([858993460, 5, 1, 1],"float16"), list[list[3,1,2,],list[3,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[-2152. ],
       [15048. ],
       [-7064. ],...
 y: array([[ -5252.],
       [ 36704.],
       [-17232.],...
2025-04-22 15:06:55.460286 test begin: paddle.tensordot(Tensor([858993460, 1, 1, 5],"float16"), Tensor([858993460, 5, 1, 1],"float16"), list[list[3,2,0,],list[3,2,0,],], )

[accuracy error] backward  paddle.tensordot(Tensor([858993460, 1, 1, 5],"float16"), Tensor([858993460, 5, 1, 1],"float16"), list[list[3,2,0,],list[3,2,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4084227101 / 4294967300 (95.1%)
Max absolute difference: 1.102
Max relative difference: 1.
 x: array([[[[ 0.0737 ]],

        [[ 0.05392]],...
 y: array([[[[ 0.0737  ]],

        [[ 0.05392 ]],...
2025-04-22 17:21:01.727634 test begin: paddle.trapezoid(Tensor([2281701379],"float32"), dx=2.0, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745313676 (unix time) try "date -d @1745313676" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x18c25) received by PID 101413 (TID 0x7f205dfff700) from PID 101413 ***]

2025-04-22 17:22:01.761943 test begin: paddle.trapezoid(y=Tensor([1073741825, 4],"float16"), x=Tensor([1073741825, 4],"float16"), )

W0422 17:23:44.271209 36095 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 17:23:44.272392 36095 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /host_home/wanghuan29/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745313946 (unix time) try "date -d @1745313946" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x8c8e) received by PID 35982 (TID 0x7f967c949700) from PID 35982 ***]

2025-04-22 17:26:33.589343 test begin: paddle.tril(Tensor([1, 1, 2281701379, 1],"float32"), )

W0422 17:28:05.940403 46730 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0422 17:28:05.941601 46730 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.tril(Tensor([1, 1, 2281701379, 1],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 131530562 / 2281701379 (5.76%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[-0.363139],
         [-0.303217],
         [-0.045127],...
 y: array([[[[-0.363139],
         [-0.303217],
         [-0.045127],...
2025-04-22 17:29:47.768125 test begin: paddle.tril(Tensor([1, 2281701379, 1, 1],"float32"), )

[accuracy error] paddle.tril(Tensor([1, 2281701379, 1, 1],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 131530562 / 2281701379 (5.76%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[-0.363139]],

        [[-0.303217]],...
 y: array([[[[-0.363139]],

        [[-0.303217]],...
2025-04-22 17:32:02.213980 test begin: paddle.tril(Tensor([10, 228170138, 1],"float32"), 0, )

[accuracy error] paddle.tril(Tensor([10, 228170138, 1],"float32"), 0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 131530563 / 2281701380 (5.76%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[-0.363139],
        [-0.303217],
        [-0.045127],...
 y: array([[[-0.363139],
        [-0.303217],
        [-0.045127],...
2025-04-22 17:34:32.644528 test begin: paddle.tril(Tensor([114085069, 20, 1],"float32"), 0, )

[accuracy error] paddle.tril(Tensor([114085069, 20, 1],"float32"), 0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 131530563 / 2281701380 (5.76%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[-0.363139],
        [-0.303217],
        [-0.045127],...
 y: array([[[-0.363139],
        [-0.303217],
        [-0.045127],...
2025-04-22 17:36:57.871515 test begin: paddle.tril(Tensor([2281701379, 1, 1, 1],"float32"), )

[accuracy error] paddle.tril(Tensor([2281701379, 1, 1, 1],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 131530562 / 2281701379 (5.76%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[-0.363139]]],

...
 y: array([[[[-0.363139]]],

...
2025-04-22 17:39:36.281045 test begin: paddle.tril(Tensor([2281701379, 1],"float32"), diagonal=0, )

[accuracy error] paddle.tril(Tensor([2281701379, 1],"float32"), diagonal=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 131530562 / 2281701379 (5.76%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[-0.363139],
       [-0.303217],
       [-0.045127],...
 y: array([[-0.363139],
       [-0.303217],
       [-0.045127],...
2025-04-22 17:41:50.194533 test begin: paddle.tril(Tensor([40139882, 107],"float16"), )

[accuracy error] paddle.tril(Tensor([40139882, 107],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 38971321 / 4294967374 (0.907%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[-0.4253 ,  0.     ,  0.     , ...,  0.     ,  0.     ,  0.     ],
       [-0.03314,  0.0325 ,  0.     , ...,  0.     ,  0.     ,  0.     ],
       [-0.2878 , -0.328  , -0.4556 , ...,  0.     ,  0.     ,  0.     ],...
 y: array([[-0.4253 ,  0.     ,  0.     , ...,  0.     ,  0.     ,  0.     ],
       [-0.03314,  0.0325 ,  0.     , ...,  0.     ,  0.     ,  0.     ],
       [-0.2878 , -0.328  , -0.4556 , ...,  0.     ,  0.     ,  0.     ],...
2025-04-22 17:56:23.718192 test begin: paddle.tril(x=Tensor([1073741825, 2, 2],"float16"), diagonal=-1, )

[accuracy error] paddle.tril(x=Tensor([1073741825, 2, 2],"float16"), diagonal=-1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 526123137 / 4294967300 (12.2%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[ 0.     ,  0.     ],
        [-0.4812 ,  0.     ]],
...
 y: array([[[ 0.     ,  0.     ],
        [-0.4812 ,  0.     ]],
...
2025-04-22 18:08:23.191647 test begin: paddle.tril(x=Tensor([1073741825, 2, 2],"float16"), diagonal=0, )

[accuracy error] paddle.tril(x=Tensor([1073741825, 2, 2],"float16"), diagonal=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1578379782 / 4294967300 (36.7%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[-0.4253 ,  0.     ],
        [-0.4812 ,  0.4077 ]],
...
 y: array([[[-0.4253 ,  0.     ],
        [-0.4812 ,  0.4077 ]],
...
2025-04-23 09:58:47.282344 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[0,1,3,2,],list[2,3,0,1,],], )

/usr/local/lib/python3.9/dist-packages/torch/autograd/graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:135.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
W0423 10:00:34.963909 43876 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0423 10:00:34.965427 43876 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[0,1,3,2,],list[2,3,0,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 3780.
Max relative difference: 0.559
 x: array(2984., dtype=float16)
 y: array(6764., dtype=float16)
2025-04-23 10:00:36.826527 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[0,2,3,],list[0,1,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[0,2,3,],list[0,1,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 1441.
Max relative difference: 1.754
 x: array([[1394. ],
       [1249. ],
       [-130. ],...
 y: array([[2646. ],
       [1824. ],
       [1311. ],...
2025-04-23 10:00:47.068505 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[0,2,3,],list[0,2,1,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[0,2,3,],list[0,2,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 1441.
Max relative difference: 1.754
 x: array([[1394. ],
       [1249. ],
       [-130. ],...
 y: array([[2646. ],
       [1824. ],
       [1311. ],...
2025-04-23 10:01:00.058724 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[0,3,1,2,],list[3,2,1,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[0,3,1,2,],list[3,2,1,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 3780.
Max relative difference: 0.559
 x: array(2984., dtype=float16)
 y: array(6764., dtype=float16)
2025-04-23 10:01:12.722362 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[0,3,2,1,],list[2,1,3,0,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[0,3,2,1,],list[2,1,3,0,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 3780.
Max relative difference: 0.559
 x: array(2984., dtype=float16)
 y: array(6764., dtype=float16)
2025-04-23 10:01:23.551262 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[1,0,2,3,],list[3,0,1,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[1,0,2,3,],list[3,0,1,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 3778.
Max relative difference: 0.5586
 x: array(2986., dtype=float16)
 y: array(6764., dtype=float16)
2025-04-23 10:01:34.759603 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[1,0,3,2,],list[2,3,0,1,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[1,0,3,2,],list[2,3,0,1,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 3778.
Max relative difference: 0.5586
 x: array(2986., dtype=float16)
 y: array(6764., dtype=float16)
2025-04-23 10:01:46.458403 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[1,0,3,2,],list[3,0,1,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[1,0,3,2,],list[3,0,1,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 3778.
Max relative difference: 0.5586
 x: array(2986., dtype=float16)
 y: array(6764., dtype=float16)
2025-04-23 10:01:57.257217 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[1,2,0,3,],list[0,2,1,3,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[1,2,0,3,],list[0,2,1,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 3782.
Max relative difference: 0.5586
 x: array(2986., dtype=float16)
 y: array(6768., dtype=float16)
2025-04-23 10:02:08.597524 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[1,2,0,],list[1,2,3,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[1,2,0,],list[1,2,3,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 3782.
Max relative difference: 0.5586
 x: array([[2986.]], dtype=float16)
 y: array([[6768.]], dtype=float16)
2025-04-23 10:02:19.250157 test begin: paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[1,2,0,],list[1,3,2,],], )

[accuracy error] paddle.tensordot(Tensor([5, 5, 171798692, 1],"float16"), Tensor([1, 1, 1, 1],"float16"), list[list[1,2,0,],list[1,3,2,],], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 3782.
Max relative difference: 0.5586
 x: array([[2986.]], dtype=float16)
 y: array([[6768.]], dtype=float16)
2025-04-23 10:02:31.876250 test begin: paddle.tril(x=Tensor([1073741825, 2, 2],"float16"), diagonal=1, )

[accuracy error] paddle.tril(x=Tensor([1073741825, 2, 2],"float16"), diagonal=1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2104508735 / 4294967300 (49%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[ 0.2563 , -0.195  ],
        [-0.0532 , -0.164  ]],
...
 y: array([[[ 0.2563 , -0.195  ],
        [-0.0532 , -0.164  ]],
...
2025-04-23 10:14:27.133418 test begin: paddle.tril(x=Tensor([1073741825, 2, 2],"float16"), diagonal=5, )

[accuracy error] paddle.tril(x=Tensor([1073741825, 2, 2],"float16"), diagonal=5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2104508735 / 4294967300 (49%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[ 0.2563 , -0.195  ],
        [-0.0532 , -0.164  ]],
...
 y: array([[[ 0.2563 , -0.195  ],
        [-0.0532 , -0.164  ]],
...
2025-04-23 10:26:18.768862 test begin: paddle.tril(x=Tensor([3, 715827883, 2],"float16"), diagonal=-1, )

[accuracy error] paddle.tril(x=Tensor([3, 715827883, 2],"float16"), diagonal=-1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2104508730 / 4294967298 (49%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[ 0.      ,  0.      ],
        [-0.0532  ,  0.      ],
        [-0.377   , -0.2405  ],...
 y: array([[[ 0.      ,  0.      ],
        [-0.0532  ,  0.      ],
        [-0.377   , -0.2405  ],...
2025-04-23 10:38:19.497054 test begin: paddle.tril(x=Tensor([3, 715827883, 2],"float16"), diagonal=-5, )

[accuracy error] paddle.tril(x=Tensor([3, 715827883, 2],"float16"), diagonal=-5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2104508723 / 4294967298 (49%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[ 0.      ,  0.      ],
        [ 0.      ,  0.      ],
        [ 0.      ,  0.      ],...
 y: array([[[ 0.      ,  0.      ],
        [ 0.      ,  0.      ],
        [ 0.      ,  0.      ],...
2025-04-23 10:50:28.333171 test begin: paddle.tril(x=Tensor([3, 715827883, 2],"float16"), diagonal=0, )

[accuracy error] paddle.tril(x=Tensor([3, 715827883, 2],"float16"), diagonal=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2104508732 / 4294967298 (49%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[ 0.2563  ,  0.      ],
        [-0.0532  , -0.164   ],
        [-0.377   , -0.2405  ],...
 y: array([[[ 0.2563  ,  0.      ],
        [-0.0532  , -0.164   ],
        [-0.377   , -0.2405  ],...
2025-04-23 11:02:53.375216 test begin: paddle.tril(x=Tensor([3, 715827883, 2],"float16"), diagonal=1, )

[accuracy error] paddle.tril(x=Tensor([3, 715827883, 2],"float16"), diagonal=1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2104508733 / 4294967298 (49%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[ 0.2563  , -0.195   ],
        [-0.0532  , -0.164   ],
        [-0.377   , -0.2405  ],...
 y: array([[[ 0.2563  , -0.195   ],
        [-0.0532  , -0.164   ],
        [-0.377   , -0.2405  ],...
2025-04-23 11:15:19.561977 test begin: paddle.tril(x=Tensor([3, 715827883, 2],"float16"), diagonal=5, )

[accuracy error] paddle.tril(x=Tensor([3, 715827883, 2],"float16"), diagonal=5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2104508733 / 4294967298 (49%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[ 0.2563  , -0.195   ],
        [-0.0532  , -0.164   ],
        [-0.377   , -0.2405  ],...
 y: array([[[ 0.2563  , -0.195   ],
        [-0.0532  , -0.164   ],
        [-0.377   , -0.2405  ],...
2025-04-23 11:27:42.609611 test begin: paddle.roll(Tensor([3567, 16, 14, 14, 384],"float16"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), )

W0423 11:29:30.965332 113767 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0423 11:29:30.966557 113767 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.roll(Tensor([3567, 16, 14, 14, 384],"float16"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 505918 / 4295467008 (0.0118%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[ 2.6367e-01,  2.3010e-01,  4.3262e-01, ...,  1.0138e-01,
           -2.4109e-01, -2.8955e-01],
          [ 1.4514e-01, -3.0273e-01, -6.7444e-02, ...,  1.3477e-01,...
 y: array([[[[[ 2.6367e-01,  2.3010e-01,  4.3262e-01, ...,  1.0138e-01,
           -2.4109e-01, -2.8955e-01],
          [ 1.4514e-01, -3.0273e-01, -6.7444e-02, ...,  1.3477e-01,...
2025-04-23 11:42:00.851040 test begin: paddle.roll(Tensor([3567, 16, 14, 7, 768],"float16"), shifts=tuple(-4,-3,0,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([3567, 16, 14, 7, 768],"float16"), shifts=tuple(-4,-3,0,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 505508 / 4295467008 (0.0118%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[ 6.3354e-02, -2.8491e-01, -1.7847e-01, ...,  1.4612e-01,
           -3.2739e-01, -3.3661e-02],
          [ 1.9971e-01, -6.2012e-02,  1.4734e-01, ..., -1.6626e-01,...
 y: array([[[[[ 6.3354e-02, -2.8491e-01, -1.7847e-01, ...,  1.4612e-01,
           -3.2739e-01, -3.3661e-02],
          [ 1.9971e-01, -6.2012e-02,  1.4734e-01, ..., -1.6626e-01,...
2025-04-23 11:54:55.151344 test begin: paddle.roll(Tensor([3567, 16, 14, 7, 768],"float16"), shifts=tuple(4,3,0,), axis=tuple(1,2,3,), )

[accuracy error] paddle.roll(Tensor([3567, 16, 14, 7, 768],"float16"), shifts=tuple(4,3,0,), axis=tuple(1,2,3,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 505664 / 4295467008 (0.0118%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[[ 1.3550e-01, -3.3887e-01,  4.8798e-02, ..., -3.4375e-01,
           -2.1545e-01,  1.9409e-01],
          [ 2.6709e-01,  2.7008e-02, -3.9258e-01, ...,  4.6997e-01,...
 y: array([[[[[ 1.3550e-01, -3.3887e-01,  4.8798e-02, ..., -3.4375e-01,
           -2.1545e-01,  1.9409e-01],
          [ 2.6709e-01,  2.7008e-02, -3.9258e-01, ...,  4.6997e-01,...
