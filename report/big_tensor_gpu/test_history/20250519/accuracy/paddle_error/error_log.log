[Worker 3] Processing Task 3: paddle.argsort(Tensor([26, 87757746],"int64"), axis=-1, )
element 0 of tensors does not require grad and does not have a grad_fn
W0519 16:54:35.599203  4016 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 16:54:35.600221  4016 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.argsort(Tensor([26, 87757746],"int64"), axis=-1, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_argsort(_object*, _object*, _object*)
1   argsort_ad_func(paddle::Tensor const&, int, bool, bool)
2   paddle::experimental::argsort(paddle::Tensor const&, int, bool, bool)
3   void phi::ArgsortKernel<long, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, phi::DenseTensor*, phi::DenseTensor*)
4   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
5   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
6   paddle::memory::allocation::Allocator::Allocate(unsigned long)
7   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 69.610352GB memory has been allocated and available memory is only 9.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 3] Completed Task 3

[Worker 3] Processing Task 5: paddle.argsort(Tensor([3, 380283564, 2],"int64"), axis=2, stable=True, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.argsort(Tensor([3, 380283564, 2],"int64"), axis=2, stable=True, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_argsort(_object*, _object*, _object*)
1   argsort_ad_func(paddle::Tensor const&, int, bool, bool)
2   paddle::experimental::argsort(paddle::Tensor const&, int, bool, bool)
3   void phi::ArgsortKernel<long, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, phi::DenseTensor*, phi::DenseTensor*)
4   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
5   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
6   paddle::memory::allocation::Allocator::Allocate(unsigned long)
7   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 69.610352GB memory has been allocated and available memory is only 9.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 3] Completed Task 5

[Worker 3] Processing Task 19: paddle.chunk(Tensor([1, 1, 1, 4294967297],"float16"), 2, axis=-1, )
[paddle error] paddle.chunk(Tensor([1, 1, 1, 4294967297],"float16"), 2, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 2, input(X)'s shape = [1, 1, 1, 4294967297], Attr(dim) = 3.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 3] Completed Task 19

[Worker 3] Processing Task 25: paddle.chunk(Tensor([13, 4, 7, 6268411],"float32"), 3, axis=-1, )
[paddle error] paddle.chunk(Tensor([13, 4, 7, 6268411],"float32"), 3, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 3, input(X)'s shape = [13, 4, 7, 6268411], Attr(dim) = 3.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 3] Completed Task 25

[Worker 3] Processing Task 29: paddle.chunk(Tensor([2048, 1114113],"float32"), 2, axis=-1, )
[paddle error] paddle.chunk(Tensor([2048, 1114113],"float32"), 2, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 2, input(X)'s shape = [2048, 1114113], Attr(dim) = 1.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 3] Completed Task 29

[Worker 3] Processing Task 33: paddle.chunk(Tensor([52, 5484860, 8],"float32"), 3, axis=1, )
[paddle error] paddle.chunk(Tensor([52, 5484860, 8],"float32"), 3, axis=1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 3, input(X)'s shape = [52, 5484860, 8], Attr(dim) = 1.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:2 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 3] Completed Task 33

[Worker 3] Processing Task 36: paddle.chunk(x=Tensor([2281701379],"bool"), chunks=3, axis=0, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.chunk(x=Tensor([2281701379],"bool"), chunks=3, axis=0, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 3, input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 3] Completed Task 36

[Worker 3] Processing Task 39: paddle.chunk(x=Tensor([3, 760567127],"int32"), chunks=3, axis=-1, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.chunk(x=Tensor([3, 760567127],"int32"), chunks=3, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 3, input(X)'s shape = [3, 760567127], Attr(dim) = 1.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:2 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 3] Completed Task 39

[Worker 3] Processing Task 41: paddle.chunk(x=Tensor([760567127, 3],"float32"), chunks=3, axis=0, )
[paddle error] paddle.chunk(x=Tensor([760567127, 3],"float32"), chunks=3, axis=0, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 3, input(X)'s shape = [760567127, 3], Attr(dim) = 0.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:2 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 3] Completed Task 41

[Worker 3] Processing Task 63: paddle.flatten(Tensor([1, 16, 44739243, 6],"float32"), 1, )
[paddle error] paddle.flatten(Tensor([1, 16, 44739243, 6],"float32"), 1, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 16, 44739243, 6], X's size = 4294967328, 'shape' is [1, 32], the capacity of 'shape' is 32.
  [Hint: Expected capacity == in_size, but received capacity:32 != in_size:4294967328.] (at /paddle/paddle/phi/infermeta/unary.cc:2245)

[Worker 3] Completed Task 63

[Worker 3] Processing Task 67: paddle.flatten(Tensor([1, 171798692, 5, 5],"float32"), 1, )
[paddle error] paddle.flatten(Tensor([1, 171798692, 5, 5],"float32"), 1, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 171798692, 5, 5], X's size = 4294967300, 'shape' is [1, 4], the capacity of 'shape' is 4.
  [Hint: Expected capacity == in_size, but received capacity:4 != in_size:4294967300.] (at /paddle/paddle/phi/infermeta/unary.cc:2245)

[Worker 3] Completed Task 67

[Worker 3] Processing Task 71: paddle.flatten(Tensor([1, 512, 8388608, 1],"float32"), 1, )
[paddle error] paddle.flatten(Tensor([1, 512, 8388608, 1],"float32"), 1, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 512, 8388608, 1], X's size = 4294967296, 'shape' is [1, 0], the capacity of 'shape' is 512.
  [Hint: Expected capacity == in_size, but received capacity:512 != in_size:4294967296.] (at /paddle/paddle/phi/infermeta/unary.cc:2245)

[Worker 3] Completed Task 71

[Worker 3] Processing Task 75: paddle.flatten(Tensor([1, 87652394, 7, 7],"float32"), start_axis=1, stop_axis=-1, )
[paddle error] paddle.flatten(Tensor([1, 87652394, 7, 7],"float32"), start_axis=1, stop_axis=-1, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 87652394, 7, 7], X's size = 4294967306, 'shape' is [1, 10], the capacity of 'shape' is 10.
  [Hint: Expected capacity == in_size, but received capacity:10 != in_size:4294967306.] (at /paddle/paddle/phi/infermeta/unary.cc:2245)

[Worker 3] Completed Task 75

[Worker 3] Processing Task 79: paddle.flatten(Tensor([107374183, 4, 5],"int64"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.flatten(Tensor([107374183, 4, 5],"int64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2147483636], shape[0] = -2147483636.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483636 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 3] Completed Task 79

[Worker 3] Processing Task 80: paddle.flatten(Tensor([2, 1024, 1, 2097152],"float32"), 1, )
[paddle error] paddle.flatten(Tensor([2, 1024, 1, 2097152],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483648], shape[1] = -2147483648.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483648 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 3] Completed Task 80

[Worker 3] Processing Task 84: paddle.flatten(Tensor([2, 16, 26843546, 5],"float32"), 1, )
[paddle error] paddle.flatten(Tensor([2, 16, 26843546, 5],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483616], shape[1] = -2147483616.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483616 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 3] Completed Task 84

[Worker 3] Processing Task 89: paddle.flatten(Tensor([2, 2048, 1048576, 1],"float32"), start_axis=1, stop_axis=-1, )
[paddle error] paddle.flatten(Tensor([2, 2048, 1048576, 1],"float32"), start_axis=1, stop_axis=-1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483648], shape[1] = -2147483648.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483648 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 3] Completed Task 89

[Worker 3] Processing Task 93: paddle.flatten(Tensor([2, 2147483648],"float32"), )
[paddle error] paddle.flatten(Tensor([2, 2147483648],"float32"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [2, 2147483648], X's size = 4294967296, 'shape' is [0], the capacity of 'shape' is 2.
  [Hint: Expected capacity == in_size, but received capacity:2 != in_size:4294967296.] (at /paddle/paddle/phi/infermeta/unary.cc:2245)

[Worker 3] Completed Task 93

[Worker 3] Processing Task 99: paddle.flatten(Tensor([2, 4, 268435457],"float64"), )
[paddle error] paddle.flatten(Tensor([2, 4, 268435457],"float64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2147483640], shape[0] = -2147483640.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483640 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 3] Completed Task 99

[Worker 3] Processing Task 101: paddle.flatten(Tensor([2, 4, 536870912],"float32"), )
[paddle error] paddle.flatten(Tensor([2, 4, 536870912],"float32"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [2, 4, 536870912], X's size = 4294967296, 'shape' is [0], the capacity of 'shape' is 2.
  [Hint: Expected capacity == in_size, but received capacity:2 != in_size:4294967296.] (at /paddle/paddle/phi/infermeta/unary.cc:2245)

[Worker 3] Completed Task 101

[Worker 3] Processing Task 107: paddle.flatten(Tensor([2, 512, 1, 4194304],"float32"), start_axis=1, stop_axis=-1, )
[paddle error] paddle.flatten(Tensor([2, 512, 1, 4194304],"float32"), start_axis=1, stop_axis=-1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483648], shape[1] = -2147483648.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483648 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 3] Completed Task 107

[Worker 3] Processing Task 111: paddle.flatten(Tensor([2, 6, 10, 35791395],"float32"), 1, )
[paddle error] paddle.flatten(Tensor([2, 6, 10, 35791395],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483596], shape[1] = -2147483596.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483596 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 3] Completed Task 111

[Worker 3] Processing Task 115: paddle.flatten(Tensor([214748365, 4, 5],"int32"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.flatten(Tensor([214748365, 4, 5],"int32"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [214748365, 4, 5], X's size = 4294967300, 'shape' is [4], the capacity of 'shape' is 4.
  [Hint: Expected capacity == in_size, but received capacity:4 != in_size:4294967300.] (at /paddle/paddle/phi/infermeta/unary.cc:2245)

[Worker 3] Completed Task 115

[Worker 3] Processing Task 117: paddle.flatten(Tensor([268435457, 4, 2],"int64"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.flatten(Tensor([268435457, 4, 2],"int64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2147483640], shape[0] = -2147483640.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483640 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 3] Completed Task 117

[Worker 3] Processing Task 121: paddle.flatten(Tensor([3, 4, 178956971],"int64"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.flatten(Tensor([3, 4, 178956971],"int64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2147483644], shape[0] = -2147483644.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483644 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 3] Completed Task 121

[Worker 3] Processing Task 124: paddle.flatten(Tensor([3, 715827883, 2],"float32"), )
[paddle error] paddle.flatten(Tensor([3, 715827883, 2],"float32"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [3, 715827883, 2], X's size = 4294967298, 'shape' is [2], the capacity of 'shape' is 2.
  [Hint: Expected capacity == in_size, but received capacity:2 != in_size:4294967298.] (at /paddle/paddle/phi/infermeta/unary.cc:2245)

[Worker 3] Completed Task 124

[Worker 3] Processing Task 129: paddle.flatten(Tensor([5, 429496730],"float64"), )
[paddle error] paddle.flatten(Tensor([5, 429496730],"float64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2147483646], shape[0] = -2147483646.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483646 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 3] Completed Task 129

[Worker 3] Processing Task 133: paddle.flatten(Tensor([536870913, 4],"float64"), )
[paddle error] paddle.flatten(Tensor([536870913, 4],"float64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2147483644], shape[0] = -2147483644.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483644 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 3] Completed Task 133

[Worker 3] Processing Task 136: paddle.fmin(Tensor([2281701379],"int64"), Tensor([1],"int64"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.fmin(Tensor([2281701379],"int64"), Tensor([1],"int64"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_fmin(_object*, _object*, _object*)
1   fmin_ad_func(paddle::Tensor const&, paddle::Tensor const&)
2   paddle::experimental::fmin(paddle::Tensor const&, paddle::Tensor const&)
3   void phi::FMinKernel<long, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*)
4   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
5   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
6   paddle::memory::allocation::Allocator::Allocate(unsigned long)
7   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 69.614258GB memory has been allocated and available memory is only 9.570618GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 3] Completed Task 136

[Worker 3] Processing Task 140: paddle.gcd(Tensor([2281701379],"int64"), Tensor([1],"int64"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.gcd(Tensor([2281701379],"int64"), Tensor([1],"int64"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_expand(_object*, _object*, _object*)
1   expand_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>)
2   paddle::experimental::expand(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&)
3   void phi::ExpandKernel<long, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
4   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
5   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
6   paddle::memory::allocation::Allocator::Allocate(unsigned long)
7   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 69.614258GB memory has been allocated and available memory is only 9.570618GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 3] Completed Task 140

[Worker 3] Processing Task 149: paddle.linalg.cond(Tensor([2, 126761188, 3, 3],"float32"), "fro", )
one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [2, 126761188, 3, 3]], which is output 0 of LinalgInvExBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
[paddle error] paddle.linalg.cond(Tensor([2, 126761188, 3, 3],"float32"), "fro", ) 
 (PreconditionNotMet) For batch [177123906]: U(3, 3) is zero, singular U. Please check the matrix value and change it to a non-singular matrix
  [Hint: Expected info[i] == 0, but received info[i]:3 != 0:0.] (at /paddle/paddle/phi/kernels/funcs/matrix_inverse.cu:117)

[Worker 3] Completed Task 149

[Worker 3] Processing Task 150: paddle.linalg.cond(Tensor([2, 126761188, 3, 3],"float32"), "nuc", )
[paddle error] paddle.linalg.cond(Tensor([2, 126761188, 3, 3],"float32"), "nuc", ) 
 (PreconditionNotMet) For batch [17033329]: U(3, 3) is zero, singular U. Please check the matrix value and change it to a non-singular matrix
  [Hint: Expected info[i] == 0, but received info[i]:3 != 0:0.] (at /paddle/paddle/phi/kernels/funcs/matrix_inverse.cu:117)

[Worker 3] Completed Task 150

[Worker 3] Processing Task 159: paddle.linalg.cond(Tensor([63380594, 4, 3, 3],"float32"), -math.inf, )
one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [63380594, 4, 3, 3]], which is output 0 of LinalgInvExBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
[paddle error] paddle.linalg.cond(Tensor([63380594, 4, 3, 3],"float32"), -math.inf, ) 
 (PreconditionNotMet) For batch [127644469]: U(3, 3) is zero, singular U. Please check the matrix value and change it to a non-singular matrix
  [Hint: Expected info[i] == 0, but received info[i]:3 != 0:0.] (at /paddle/paddle/phi/kernels/funcs/matrix_inverse.cu:117)

[Worker 3] Completed Task 159

[Worker 3] Processing Task 184: paddle.nanmean(Tensor([2, 2147483648],"float32"), None, True, )
[cuda error] paddle.nanmean(Tensor([2, 2147483648],"float32"), None, True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747649587 (unix time) try "date -d @1747649587" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xfb0) received by PID 4016 (TID 0x7f98c6a93740) from PID 4016 ***]

[Worker 3] Started on GPU 3

[Worker 3] Processing Task 189: paddle.nanmean(Tensor([2, 3, 143165577, 5],"float32"), list[0,2,], False, )
W0519 18:14:45.880928  5609 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:14:45.881940  5609 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([2, 3, 143165577, 5],"float32"), list[0,2,], False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747649687 (unix time) try "date -d @1747649687" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x15e9) received by PID 5609 (TID 0x7f98c6a93740) from PID 5609 ***]

[Worker 3] Started on GPU 3

[Worker 3] Processing Task 192: paddle.nanmean(Tensor([2, 3, 143165577, 5],"float32"), tuple(0,2,), False, )
W0519 18:16:03.329078  5912 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:16:03.330734  5912 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([2, 3, 143165577, 5],"float32"), tuple(0,2,), False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747649765 (unix time) try "date -d @1747649765" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1718) received by PID 5912 (TID 0x7f98c6a93740) from PID 5912 ***]

[Worker 3] Started on GPU 3

[Worker 3] Processing Task 197: paddle.nanmean(Tensor([2, 3, 4, 178956971],"float32"), None, True, )
W0519 18:17:32.571606  6216 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:17:32.572618  6216 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([2, 3, 4, 178956971],"float32"), None, True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747649854 (unix time) try "date -d @1747649854" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1848) received by PID 6216 (TID 0x7f98c6a93740) from PID 6216 ***]

[Worker 3] Started on GPU 3

[Worker 3] Processing Task 200: paddle.nanmean(Tensor([4294967295],"float32"), axis=0, )
W0519 18:19:03.993671  6518 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:19:03.995038  6518 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([4294967295],"float32"), axis=0, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747649946 (unix time) try "date -d @1747649946" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1976) received by PID 6518 (TID 0x7f98c6a93740) from PID 6518 ***]

[Worker 3] Started on GPU 3

[Worker 3] Processing Task 205: paddle.nanmean(Tensor([71582789, 3, 4, 5],"float32"), None, True, )
W0519 18:20:41.772786  6898 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:20:41.774130  6898 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([71582789, 3, 4, 5],"float32"), None, True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747650043 (unix time) try "date -d @1747650043" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1af2) received by PID 6898 (TID 0x7f98c6a93740) from PID 6898 ***]

[Worker 3] Started on GPU 3

[Worker 3] Processing Task 209: paddle.nn.functional.normalize(Tensor([1, 2281701379],"float32"), axis=1, )
W0519 18:21:59.079486  7128 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:21:59.080551  7128 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.nn.functional.normalize(Tensor([1, 2281701379],"float32"), axis=1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 5.e+11
Max relative difference: 1.3789265e+16
 x: array([[ 4.738529e+11,  3.603156e+11, -3.769296e+11, ..., -2.172946e+10,
        -3.375084e+10, -4.227412e+10]], dtype=float32)
 y: array([[ 3.436390e-05,  2.613015e-05, -2.733501e-05, ..., -1.575824e-06,
        -2.447618e-06, -3.065727e-06]], dtype=float32)
[Worker 3] Completed Task 209

[Worker 3] Processing Task 214: paddle.nn.functional.normalize(Tensor([2281701379],"float32"), axis=0, epsilon=1e-12, )
[accuracy error] paddle.nn.functional.normalize(Tensor([2281701379],"float32"), axis=0, epsilon=1e-12, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 5.e+11
Max relative difference: 1.3789059e+16
 x: array([ 3.429502e+10,  4.907905e+10,  1.325573e+10, ..., -1.482540e+11,
        2.944603e+11,  4.087553e+11], dtype=float32)
 y: array([ 2.487118e-06,  3.559275e-06,  9.613220e-07, ..., -1.075157e-05,
        2.135463e-05,  2.964346e-05], dtype=float32)
[Worker 3] Completed Task 214

[Worker 3] Processing Task 221: paddle.nn.functional.normalize(x=Tensor([4294967297],"float16"), axis=0, )
W0519 18:30:05.630510  7734 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:30:05.632189  7734 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.nn.functional.normalize(x=Tensor([4294967297],"float16"), axis=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4277173276 / 4294967297 (99.6%)
Max absolute difference: 2.414
Max relative difference: inf
 x: array([-1.     , -1.512  ,  0.10803, ..., -0.309  ,  1.051  , -1.176  ],
      dtype=float16)
 y: array([-1.097e-05, -1.657e-05,  1.192e-06, ..., -3.397e-06,  1.150e-05,
       -1.287e-05], dtype=float16)
[Worker 3] Completed Task 221

[Worker 3] Processing Task 255: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=True, reduction="mean", )
[accuracy error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=True, reduction="mean", ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 0.3
Max relative difference: inf
 x: array(0.3, dtype=float16)
 y: array(0., dtype=float16)
[Worker 3] Completed Task 255

[Worker 3] Processing Task 260: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=True, reduction="mean", name=None, )
[accuracy error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=True, reduction="mean", name=None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 0.3
Max relative difference: inf
 x: array(0.3, dtype=float16)
 y: array(0., dtype=float16)
[Worker 3] Completed Task 260

[Worker 3] Processing Task 326: paddle.sum(Tensor([2, 3, 143165577, 5],"bool"), axis=list[-1,], keepdim=False, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.sum(Tensor([2, 3, 143165577, 5],"bool"), axis=list[-1,], keepdim=False, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_sum(_object*, _object*, _object*)
1   sum_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, phi::DataType, bool)
2   paddle::experimental::sum(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, phi::DataType, bool)
3   void phi::SumKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
4   void phi::SumRawKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
5   void phi::CastCUDAKernelImpl<bool, long>(phi::GPUContext const&, phi::DenseTensor const&, phi::DataType, phi::DenseTensor*)
6   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  paddle::memory::allocation::Allocator::Allocate(unsigned long)
12  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
13  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 32.000000GB memory on GPU 0, 48.008789GB memory has been allocated and available memory is only 31.176086GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 3] Completed Task 326

[Worker 3] Processing Task 335: paddle.sum(Tensor([2, 3, 4, 178956971],"bool"), axis=list[0,2,], keepdim=False, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.sum(Tensor([2, 3, 4, 178956971],"bool"), axis=list[0,2,], keepdim=False, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_sum(_object*, _object*, _object*)
1   sum_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, phi::DataType, bool)
2   paddle::experimental::sum(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, phi::DataType, bool)
3   void phi::SumKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
4   void phi::SumRawKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
5   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 4.000000GB memory on GPU 0, 77.612305GB memory has been allocated and available memory is only 1.572571GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 3] Completed Task 335

[Worker 3] Processing Task 349: paddle.sum(Tensor([2, 3, 715827883],"int32"), axis=tuple(0,1,), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.sum(Tensor([2, 3, 715827883],"int32"), axis=tuple(0,1,), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_sum(_object*, _object*, _object*)
1   sum_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, phi::DataType, bool)
2   paddle::experimental::sum(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, phi::DataType, bool)
3   void phi::SumKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
4   void phi::SumRawKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
5   void phi::CastCUDAKernelImpl<int, long>(phi::GPUContext const&, phi::DenseTensor const&, phi::DataType, phi::DenseTensor*)
6   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  paddle::memory::allocation::Allocator::Allocate(unsigned long)
12  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
13  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 32.000000GB memory on GPU 0, 70.942383GB memory has been allocated and available memory is only 8.242493GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 3] Completed Task 349

[Worker 3] Processing Task 359: paddle.sum(Tensor([3, 4, 357913942],"int32"), axis=1, keepdim=True, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.sum(Tensor([3, 4, 357913942],"int32"), axis=1, keepdim=True, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_sum(_object*, _object*, _object*)
1   sum_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, phi::DataType, bool)
2   paddle::experimental::sum(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, phi::DataType, bool)
3   void phi::SumKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
4   void phi::SumRawKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
5   void phi::CastCUDAKernelImpl<int, long>(phi::GPUContext const&, phi::DenseTensor const&, phi::DataType, phi::DenseTensor*)
6   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  paddle::memory::allocation::Allocator::Allocate(unsigned long)
12  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
13  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 32.000000GB memory on GPU 0, 73.610352GB memory has been allocated and available memory is only 5.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 3] Completed Task 359

[Worker 3] Processing Task 374: paddle.sum(Tensor([71582789, 3, 4, 5],"bool"), axis=2, keepdim=True, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.sum(Tensor([71582789, 3, 4, 5],"bool"), axis=2, keepdim=True, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_sum(_object*, _object*, _object*)
1   sum_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, phi::DataType, bool)
2   paddle::experimental::sum(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, phi::DataType, bool)
3   void phi::SumKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
4   void phi::SumRawKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
5   void phi::CastCUDAKernelImpl<bool, long>(phi::GPUContext const&, phi::DenseTensor const&, phi::DataType, phi::DenseTensor*)
6   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  paddle::memory::allocation::Allocator::Allocate(unsigned long)
12  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
13  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 32.000000GB memory on GPU 0, 49.610352GB memory has been allocated and available memory is only 29.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 3] Completed Task 374

[Worker 3] Processing Task 379: paddle.sum(Tensor([71582789, 3, 4, 5],"bool"), axis=list[2,], keepdim=True, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.sum(Tensor([71582789, 3, 4, 5],"bool"), axis=list[2,], keepdim=True, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_sum(_object*, _object*, _object*)
1   sum_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, phi::DataType, bool)
2   paddle::experimental::sum(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, phi::DataType, bool)
3   void phi::SumKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
4   void phi::SumRawKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
5   void phi::CastCUDAKernelImpl<bool, long>(phi::GPUContext const&, phi::DenseTensor const&, phi::DataType, phi::DenseTensor*)
6   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  paddle::memory::allocation::Allocator::Allocate(unsigned long)
12  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
13  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 32.000000GB memory on GPU 0, 49.610352GB memory has been allocated and available memory is only 29.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 3] Completed Task 379

[Worker 3] Processing Task 396: paddle.Tensor.chunk(Tensor([1, 1, 10164, 224489],"float32"), 2, axis=-1, )
[paddle error] paddle.Tensor.chunk(Tensor([1, 1, 10164, 224489],"float32"), 2, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 2, input(X)'s shape = [1, 1, 10164, 224489], Attr(dim) = 3.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 3] Completed Task 396

[Worker 3] Processing Task 400: paddle.Tensor.chunk(Tensor([1, 10, 1, 228170138],"float32"), 4, axis=-1, )
[paddle error] paddle.Tensor.chunk(Tensor([1, 10, 1, 228170138],"float32"), 4, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 4, input(X)'s shape = [1, 10, 1, 228170138], Attr(dim) = 3.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:2 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 3] Completed Task 400

[Worker 3] Processing Task 404: paddle.Tensor.chunk(Tensor([1, 103, 1, 22152441],"float32"), 4, axis=-1, )
[paddle error] paddle.Tensor.chunk(Tensor([1, 103, 1, 22152441],"float32"), 4, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 4, input(X)'s shape = [1, 103, 1, 22152441], Attr(dim) = 3.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 3] Completed Task 404

[Worker 3] Processing Task 408: paddle.Tensor.chunk(Tensor([475355, 300, 16],"float32"), 2, )
[paddle error] paddle.Tensor.chunk(Tensor([475355, 300, 16],"float32"), 2, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 2, input(X)'s shape = [475355, 300, 16], Attr(dim) = 0.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 3] Completed Task 408

[Worker 3] Processing Task 412: paddle.Tensor.expand_as(Tensor([1, 1],"int32"), Tensor([2281701379, 1],"int64"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.expand_as(Tensor([1, 1],"int32"), Tensor([2281701379, 1],"int64"), ) 
 (InvalidArgument) When the value in shape is negative for expand_as_v2 op, only -1 is supported, but the value received is -2013265917.
  [Hint: Expected target_shape[i] == -1, but received target_shape[i]:-2013265917 != -1:-1.] (at /paddle/paddle/phi/kernels/gpu/expand_as_kernel.cu:70)

[Worker 3] Completed Task 412

[Worker 3] Processing Task 415: paddle.Tensor.flatten(Tensor([1, 1, 16, 142606337],"float32"), start_axis=2, )
[paddle error] paddle.Tensor.flatten(Tensor([1, 1, 16, 142606337],"float32"), start_axis=2, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, 1, -2013265904], shape[2] = -2013265904.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265904 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 3] Completed Task 415

[Worker 3] Processing Task 419: paddle.Tensor.flatten(Tensor([1, 1, 2281701379],"int64"), stop_axis=-2, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.flatten(Tensor([1, 1, 2281701379],"int64"), stop_axis=-2, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265917], shape[1] = -2013265917.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265917 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 3] Completed Task 419

[Worker 3] Processing Task 423: paddle.Tensor.flatten(Tensor([1, 10, 228170138],"float32"), 1, )
[paddle error] paddle.Tensor.flatten(Tensor([1, 10, 228170138],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265916], shape[1] = -2013265916.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265916 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 3] Completed Task 423

[Worker 3] Processing Task 428: paddle.Tensor.flatten(Tensor([1, 11408507, 200],"float32"), 1, )
[paddle error] paddle.Tensor.flatten(Tensor([1, 11408507, 200],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265896], shape[1] = -2013265896.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265896 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 3] Completed Task 428

[Worker 3] Processing Task 432: paddle.Tensor.flatten(Tensor([1, 15845149, 144],"float32"), 1, )
[paddle error] paddle.Tensor.flatten(Tensor([1, 15845149, 144],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265840], shape[1] = -2013265840.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265840 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 3] Completed Task 432

[Worker 3] Processing Task 436: paddle.Tensor.flatten(Tensor([1, 2281701379],"float32"), 1, )
[paddle error] paddle.Tensor.flatten(Tensor([1, 2281701379],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265917], shape[1] = -2013265917.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265917 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 3] Completed Task 436

[Worker 3] Processing Task 441: paddle.Tensor.flatten(Tensor([1, 285212673, 8],"float32"), 1, )
[paddle error] paddle.Tensor.flatten(Tensor([1, 285212673, 8],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265912], shape[1] = -2013265912.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265912 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 3] Completed Task 441

[Worker 3] Processing Task 444: paddle.Tensor.flatten(Tensor([1, 6, 4, 95070891],"float32"), start_axis=1, stop_axis=3, )
[paddle error] paddle.Tensor.flatten(Tensor([1, 6, 4, 95070891],"float32"), start_axis=1, stop_axis=3, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265912], shape[1] = -2013265912.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265912 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 3] Completed Task 444

[Worker 3] Processing Task 446: paddle.Tensor.flatten(Tensor([1, 91268056, 25],"float32"), 1, )
[paddle error] paddle.Tensor.flatten(Tensor([1, 91268056, 25],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265896], shape[1] = -2013265896.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265896 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 3] Completed Task 446

[Worker 3] Processing Task 450: paddle.Tensor.flatten(Tensor([12, 19, 10007463, 1],"float32"), )
[paddle error] paddle.Tensor.flatten(Tensor([12, 19, 10007463, 1],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265732], shape[0] = -2013265732.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265732 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 3] Completed Task 450

[Worker 3] Processing Task 454: paddle.Tensor.flatten(Tensor([12, 38, 5003732, 1],"float32"), )
[paddle error] paddle.Tensor.flatten(Tensor([12, 38, 5003732, 1],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265504], shape[0] = -2013265504.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265504 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 3] Completed Task 454

[Worker 3] Processing Task 458: paddle.Tensor.flatten(Tensor([12, 4, 349526, 136],"float32"), )
[paddle error] paddle.Tensor.flatten(Tensor([12, 4, 349526, 136],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013261568], shape[0] = -2013261568.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013261568 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 3] Completed Task 458

[Worker 3] Processing Task 462: paddle.Tensor.flatten(Tensor([12, 5592406, 34, 1],"float32"), )
[paddle error] paddle.Tensor.flatten(Tensor([12, 5592406, 34, 1],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265648], shape[0] = -2013265648.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265648 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 3] Completed Task 462

[Worker 3] Processing Task 466: paddle.Tensor.flatten(Tensor([2, 3, 6, 6, 10563433],"float32"), )
[paddle error] paddle.Tensor.flatten(Tensor([2, 3, 6, 6, 10563433],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265768], shape[0] = -2013265768.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265768 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 3] Completed Task 466

[Worker 3] Processing Task 470: paddle.Tensor.flatten(Tensor([22369622, 102, 1],"int32"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.flatten(Tensor([22369622, 102, 1],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265852], shape[0] = -2013265852.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265852 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 3] Completed Task 470

[Worker 3] Processing Task 474: paddle.Tensor.flatten(Tensor([2279422, 1001],"int32"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.flatten(Tensor([2279422, 1001],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265874], shape[0] = -2013265874.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265874 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 3] Completed Task 474

[Worker 3] Processing Task 477: paddle.Tensor.flatten(Tensor([2281701379],"float32"), )
[paddle error] paddle.Tensor.flatten(Tensor([2281701379],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265917], shape[0] = -2013265917.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265917 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 3] Completed Task 477

[Worker 3] Processing Task 483: paddle.Tensor.flatten(Tensor([4294967297],"bfloat16"), )
[paddle error] paddle.Tensor.flatten(Tensor([4294967297],"bfloat16"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [4294967297], X's size = 4294967297, 'shape' is [1], the capacity of 'shape' is 1.
  [Hint: Expected capacity == in_size, but received capacity:1 != in_size:4294967297.] (at /paddle/paddle/phi/infermeta/unary.cc:2245)

[Worker 3] Completed Task 483

[Worker 0] Processing Task 0: paddle.argsort(Tensor([2, 1140850690],"int64"), axis=1, stable=True, )
element 0 of tensors does not require grad and does not have a grad_fn
W0519 16:54:34.412034  4013 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 16:54:34.413091  4013 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.argsort(Tensor([2, 1140850690],"int64"), axis=1, stable=True, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_argsort(_object*, _object*, _object*)
1   argsort_ad_func(paddle::Tensor const&, int, bool, bool)
2   paddle::experimental::argsort(paddle::Tensor const&, int, bool, bool)
3   void phi::ArgsortKernel<long, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, phi::DenseTensor*, phi::DenseTensor*)
4   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
5   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
6   paddle::memory::allocation::Allocator::Allocate(unsigned long)
7   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 69.610352GB memory has been allocated and available memory is only 9.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 0] Completed Task 0

[Worker 0] Processing Task 4: paddle.argsort(Tensor([285212673, 4, 2],"int64"), axis=2, stable=True, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.argsort(Tensor([285212673, 4, 2],"int64"), axis=2, stable=True, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_argsort(_object*, _object*, _object*)
1   argsort_ad_func(paddle::Tensor const&, int, bool, bool)
2   paddle::experimental::argsort(paddle::Tensor const&, int, bool, bool)
3   void phi::ArgsortKernel<long, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, phi::DenseTensor*, phi::DenseTensor*)
4   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
5   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
6   paddle::memory::allocation::Allocator::Allocate(unsigned long)
7   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 69.610352GB memory has been allocated and available memory is only 9.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 0] Completed Task 4

[Worker 0] Processing Task 9: paddle.argsort(Tensor([5, 456340276],"int64"), axis=1, stable=True, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.argsort(Tensor([5, 456340276],"int64"), axis=1, stable=True, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_argsort(_object*, _object*, _object*)
1   argsort_ad_func(paddle::Tensor const&, int, bool, bool)
2   paddle::experimental::argsort(paddle::Tensor const&, int, bool, bool)
3   void phi::ArgsortKernel<long, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, phi::DenseTensor*, phi::DenseTensor*)
4   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
5   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
6   paddle::memory::allocation::Allocator::Allocate(unsigned long)
7   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 69.610352GB memory has been allocated and available memory is only 9.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 0] Completed Task 9

[Worker 0] Processing Task 22: paddle.chunk(Tensor([1, 4, 20, 28521268],"float32"), 3, axis=-1, )
[paddle error] paddle.chunk(Tensor([1, 4, 20, 28521268],"float32"), 3, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 3, input(X)'s shape = [1, 4, 20, 28521268], Attr(dim) = 3.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 0] Completed Task 22

[Worker 0] Processing Task 24: paddle.chunk(Tensor([13, 175515491, 1],"float32"), 4, axis=1, )
[paddle error] paddle.chunk(Tensor([13, 175515491, 1],"float32"), 4, axis=1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 4, input(X)'s shape = [13, 175515491, 1], Attr(dim) = 1.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:3 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 0] Completed Task 24

[Worker 0] Processing Task 28: paddle.chunk(Tensor([16, 5593, 25500],"float32"), 2, axis=1, )
[paddle error] paddle.chunk(Tensor([16, 5593, 25500],"float32"), 2, axis=1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 2, input(X)'s shape = [16, 5593, 25500], Attr(dim) = 1.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 0] Completed Task 28

[Worker 0] Processing Task 32: paddle.chunk(Tensor([52, 4, 7, 1567103],"float32"), 3, axis=-1, )
[paddle error] paddle.chunk(Tensor([52, 4, 7, 1567103],"float32"), 3, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 3, input(X)'s shape = [52, 4, 7, 1567103], Attr(dim) = 3.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:2 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 0] Completed Task 32

[Worker 0] Processing Task 35: paddle.chunk(x=Tensor([1431655766, 3],"float16"), chunks=3, axis=0, )
[paddle error] paddle.chunk(x=Tensor([1431655766, 3],"float16"), chunks=3, axis=0, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 3, input(X)'s shape = [1431655766, 3], Attr(dim) = 0.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:2 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 0] Completed Task 35

[Worker 0] Processing Task 42: paddle.chunk(x=Tensor([760567127, 3],"int32"), chunks=3, axis=0, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.chunk(x=Tensor([760567127, 3],"int32"), chunks=3, axis=0, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 3, input(X)'s shape = [760567127, 3], Attr(dim) = 0.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:2 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 0] Completed Task 42

[Worker 0] Processing Task 43: paddle.chunk(x=Tensor([760567127, 3],"int64"), chunks=3, axis=0, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.chunk(x=Tensor([760567127, 3],"int64"), chunks=3, axis=0, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 3, input(X)'s shape = [760567127, 3], Attr(dim) = 0.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:2 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 0] Completed Task 43

[Worker 0] Processing Task 62: paddle.flatten(Tensor([1, 119304648, 6, 6],"float32"), 1, )
[paddle error] paddle.flatten(Tensor([1, 119304648, 6, 6],"float32"), 1, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 119304648, 6, 6], X's size = 4294967328, 'shape' is [1, 32], the capacity of 'shape' is 32.
  [Hint: Expected capacity == in_size, but received capacity:32 != in_size:4294967328.] (at /paddle/paddle/phi/infermeta/unary.cc:2245)

[Worker 0] Completed Task 62

[Worker 0] Processing Task 66: paddle.flatten(Tensor([1, 16, 6, 44739243],"float32"), 1, )
[paddle error] paddle.flatten(Tensor([1, 16, 6, 44739243],"float32"), 1, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 16, 6, 44739243], X's size = 4294967328, 'shape' is [1, 32], the capacity of 'shape' is 32.
  [Hint: Expected capacity == in_size, but received capacity:32 != in_size:4294967328.] (at /paddle/paddle/phi/infermeta/unary.cc:2245)

[Worker 0] Completed Task 66

[Worker 0] Processing Task 70: paddle.flatten(Tensor([1, 512, 1, 8388608],"float32"), 1, )
[paddle error] paddle.flatten(Tensor([1, 512, 1, 8388608],"float32"), 1, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 512, 1, 8388608], X's size = 4294967296, 'shape' is [1, 0], the capacity of 'shape' is 512.
  [Hint: Expected capacity == in_size, but received capacity:512 != in_size:4294967296.] (at /paddle/paddle/phi/infermeta/unary.cc:2245)

[Worker 0] Completed Task 70

[Worker 0] Processing Task 74: paddle.flatten(Tensor([1, 87652394, 7, 7],"float32"), 1, )
[paddle error] paddle.flatten(Tensor([1, 87652394, 7, 7],"float32"), 1, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 87652394, 7, 7], X's size = 4294967306, 'shape' is [1, 10], the capacity of 'shape' is 10.
  [Hint: Expected capacity == in_size, but received capacity:10 != in_size:4294967306.] (at /paddle/paddle/phi/infermeta/unary.cc:2245)

[Worker 0] Completed Task 74

[Worker 0] Processing Task 78: paddle.flatten(Tensor([107374183, 4, 5],"float64"), )
[paddle error] paddle.flatten(Tensor([107374183, 4, 5],"float64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2147483636], shape[0] = -2147483636.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483636 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 0] Completed Task 78

[Worker 0] Processing Task 82: paddle.flatten(Tensor([2, 1024, 2097152, 1],"float32"), 1, )
[paddle error] paddle.flatten(Tensor([2, 1024, 2097152, 1],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483648], shape[1] = -2147483648.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483648 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 0] Completed Task 82

[Worker 0] Processing Task 86: paddle.flatten(Tensor([2, 2048, 1, 1048576],"float32"), 1, )
[paddle error] paddle.flatten(Tensor([2, 2048, 1, 1048576],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483648], shape[1] = -2147483648.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483648 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 0] Completed Task 86

[Worker 0] Processing Task 90: paddle.flatten(Tensor([2, 2147483648, 1, 1],"float32"), 1, )
[paddle error] paddle.flatten(Tensor([2, 2147483648, 1, 1],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483648], shape[1] = -2147483648.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483648 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 0] Completed Task 90

[Worker 0] Processing Task 94: paddle.flatten(Tensor([2, 2147483648],"int32"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.flatten(Tensor([2, 2147483648],"int32"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [2, 2147483648], X's size = 4294967296, 'shape' is [0], the capacity of 'shape' is 2.
  [Hint: Expected capacity == in_size, but received capacity:2 != in_size:4294967296.] (at /paddle/paddle/phi/infermeta/unary.cc:2245)

[Worker 0] Completed Task 94

[Worker 0] Processing Task 96: paddle.flatten(Tensor([2, 214748365, 5],"int64"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.flatten(Tensor([2, 214748365, 5],"int64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2147483646], shape[0] = -2147483646.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483646 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 0] Completed Task 96

[Worker 0] Processing Task 100: paddle.flatten(Tensor([2, 4, 268435457],"int64"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.flatten(Tensor([2, 4, 268435457],"int64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2147483640], shape[0] = -2147483640.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483640 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 0] Completed Task 100

[Worker 0] Processing Task 102: paddle.flatten(Tensor([2, 4, 536870912],"int32"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.flatten(Tensor([2, 4, 536870912],"int32"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [2, 4, 536870912], X's size = 4294967296, 'shape' is [0], the capacity of 'shape' is 2.
  [Hint: Expected capacity == in_size, but received capacity:2 != in_size:4294967296.] (at /paddle/paddle/phi/infermeta/unary.cc:2245)

[Worker 0] Completed Task 102

[Worker 0] Processing Task 105: paddle.flatten(Tensor([2, 43826197, 7, 7],"float32"), 1, )
[paddle error] paddle.flatten(Tensor([2, 43826197, 7, 7],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483643], shape[1] = -2147483643.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483643 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 0] Completed Task 105

[Worker 0] Processing Task 109: paddle.flatten(Tensor([2, 512, 4194304, 1],"float32"), start_axis=1, stop_axis=-1, )
[paddle error] paddle.flatten(Tensor([2, 512, 4194304, 1],"float32"), start_axis=1, stop_axis=-1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483648], shape[1] = -2147483648.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483648 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 0] Completed Task 109

[Worker 0] Processing Task 113: paddle.flatten(Tensor([2, 85899346, 5, 5],"float32"), 1, )
[paddle error] paddle.flatten(Tensor([2, 85899346, 5, 5],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483646], shape[1] = -2147483646.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483646 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 0] Completed Task 113

[Worker 0] Processing Task 118: paddle.flatten(Tensor([3, 357913942, 2],"float64"), )
[paddle error] paddle.flatten(Tensor([3, 357913942, 2],"float64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2147483644], shape[0] = -2147483644.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483644 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 0] Completed Task 118

[Worker 0] Processing Task 123: paddle.flatten(Tensor([3, 4, 357913942],"int32"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.flatten(Tensor([3, 4, 357913942],"int32"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [3, 4, 357913942], X's size = 4294967304, 'shape' is [8], the capacity of 'shape' is 8.
  [Hint: Expected capacity == in_size, but received capacity:8 != in_size:4294967304.] (at /paddle/paddle/phi/infermeta/unary.cc:2245)

[Worker 0] Completed Task 123

[Worker 0] Processing Task 126: paddle.flatten(Tensor([3, 715827883],"float64"), )
[paddle error] paddle.flatten(Tensor([3, 715827883],"float64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2147483647], shape[0] = -2147483647.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483647 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 0] Completed Task 126

[Worker 0] Processing Task 130: paddle.flatten(Tensor([51130564, 7, 6],"float64"), )
[paddle error] paddle.flatten(Tensor([51130564, 7, 6],"float64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2147483608], shape[0] = -2147483608.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483608 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 0] Completed Task 130

[Worker 0] Processing Task 134: paddle.fmin(Tensor([1],"int64"), Tensor([2281701379],"int64"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.fmin(Tensor([1],"int64"), Tensor([2281701379],"int64"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_fmin(_object*, _object*, _object*)
1   fmin_ad_func(paddle::Tensor const&, paddle::Tensor const&)
2   paddle::experimental::fmin(paddle::Tensor const&, paddle::Tensor const&)
3   void phi::FMinKernel<long, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*)
4   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
5   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
6   paddle::memory::allocation::Allocator::Allocate(unsigned long)
7   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 69.614258GB memory has been allocated and available memory is only 9.570618GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 0] Completed Task 134

[Worker 0] Processing Task 138: paddle.frac(Tensor([2, 2147483649],"float16"), )
[paddle error] paddle.frac(Tensor([2, 2147483649],"float16"), ) 
 The data type of input must be one of ['int32', 'int64', 'float32', 'float64'], but got paddle.float16
[Worker 0] Completed Task 138

[Worker 0] Processing Task 144: paddle.lerp(Tensor([1, 3, 3],"float32"), Tensor([1, 3, 3],"float32"), Tensor([253522376, 3, 3],"float32"), )
W0519 17:55:31.343137  4013 backward.cc:437] While running Node (LerpGradNode) raises an EnforceNotMet exception
[paddle error] paddle.lerp(Tensor([1, 3, 3],"float32"), Tensor([1, 3, 3],"float32"), Tensor([253522376, 3, 3],"float32"), ) 
 (InvalidArgument) numel is expected to be greater than or equal 0, but received -2013265912.
  [Hint: Expected numel >= 0, but received numel:-2013265912 < 0:0.] (at /paddle/paddle/phi/backends/gpu/gpu_launch_config.h:110)

[Worker 0] Completed Task 144

[Worker 0] Processing Task 146: paddle.lerp(Tensor([1],"float32"), Tensor([1],"float32"), Tensor([2281701379],"float32"), )
W0519 17:57:36.123531  4013 backward.cc:437] While running Node (LerpGradNode) raises an EnforceNotMet exception
[paddle error] paddle.lerp(Tensor([1],"float32"), Tensor([1],"float32"), Tensor([2281701379],"float32"), ) 
 (InvalidArgument) numel is expected to be greater than or equal 0, but received -2013265917.
  [Hint: Expected numel >= 0, but received numel:-2013265917 < 0:0.] (at /paddle/paddle/phi/backends/gpu/gpu_launch_config.h:110)

[Worker 0] Completed Task 146

[Worker 0] Processing Task 151: paddle.linalg.cond(Tensor([2, 126761188, 3, 3],"float32"), 1, )
one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [2, 126761188, 3, 3]], which is output 0 of LinalgInvExBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
[paddle error] paddle.linalg.cond(Tensor([2, 126761188, 3, 3],"float32"), 1, ) 
 (PreconditionNotMet) For batch [84483657]: U(3, 3) is zero, singular U. Please check the matrix value and change it to a non-singular matrix
  [Hint: Expected info[i] == 0, but received info[i]:3 != 0:0.] (at /paddle/paddle/phi/kernels/funcs/matrix_inverse.cu:117)

[Worker 0] Completed Task 151

[Worker 0] Processing Task 153: paddle.linalg.cond(Tensor([2, 126761188, 3, 3],"float32"), math.inf, )
one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [2, 126761188, 3, 3]], which is output 0 of LinalgInvExBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
[paddle error] paddle.linalg.cond(Tensor([2, 126761188, 3, 3],"float32"), math.inf, ) 
 (PreconditionNotMet) For batch [64666729]: U(3, 3) is zero, singular U. Please check the matrix value and change it to a non-singular matrix
  [Hint: Expected info[i] == 0, but received info[i]:3 != 0:0.] (at /paddle/paddle/phi/kernels/funcs/matrix_inverse.cu:117)

[Worker 0] Completed Task 153

[Worker 0] Processing Task 156: paddle.linalg.cond(Tensor([63380594, 4, 3, 3],"float32"), 1, )
one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [63380594, 4, 3, 3]], which is output 0 of LinalgInvExBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
[paddle error] paddle.linalg.cond(Tensor([63380594, 4, 3, 3],"float32"), 1, ) 
 (PreconditionNotMet) For batch [34153973]: U(3, 3) is zero, singular U. Please check the matrix value and change it to a non-singular matrix
  [Hint: Expected info[i] == 0, but received info[i]:3 != 0:0.] (at /paddle/paddle/phi/kernels/funcs/matrix_inverse.cu:117)

[Worker 0] Completed Task 156

[Worker 0] Processing Task 160: paddle.linalg.det(Tensor([253522376, 3, 3],"float32"), )
[accuracy error] backward  paddle.linalg.det(Tensor([253522376, 3, 3],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 95 / 2281701384 (4.16e-06%)
Max absolute difference: 0.078996
Max relative difference: 72.014885
 x: array([[[-2.529472e-03, -1.167172e-03,  1.270644e-03],
        [-7.322327e-04,  3.681862e-03, -2.508971e-03],
        [ 2.244774e-03,  4.620200e-04,  3.363507e-03]],...
 y: array([[[-2.529472e-03, -1.167172e-03,  1.270644e-03],
        [-7.322328e-04,  3.681862e-03, -2.508971e-03],
        [ 2.244774e-03,  4.620200e-04,  3.363506e-03]],...
[Worker 0] Completed Task 160

[Worker 0] Processing Task 175: paddle.nanmean(Tensor([1431655765, 3],"float32"), None, True, )
W0519 18:09:02.016671  4696 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:09:02.017659  4696 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([1431655765, 3],"float32"), None, True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747649344 (unix time) try "date -d @1747649344" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1258) received by PID 4696 (TID 0x7f98c6a93740) from PID 4696 ***]

[Worker 0] Started on GPU 0

[Worker 0] Processing Task 178: paddle.nanmean(Tensor([2, 107374183, 4, 5],"float32"), list[0,1,2,3,], False, )
W0519 18:10:30.660331  4926 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:10:30.661353  4926 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([2, 107374183, 4, 5],"float32"), list[0,1,2,3,], False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747649432 (unix time) try "date -d @1747649432" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x133e) received by PID 4926 (TID 0x7f98c6a93740) from PID 4926 ***]

[Worker 0] Started on GPU 0

[Worker 0] Processing Task 182: paddle.nanmean(Tensor([2, 2147483648],"float32"), -1, False, )
W0519 18:11:58.543182  5228 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:11:58.544075  5228 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([2, 2147483648],"float32"), -1, False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747649520 (unix time) try "date -d @1747649520" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x146c) received by PID 5228 (TID 0x7f98c6a93740) from PID 5228 ***]

[Worker 0] Started on GPU 0

[Worker 0] Processing Task 185: paddle.nanmean(Tensor([2, 2147483648],"float32"), tuple(0,1,), False, )
W0519 18:13:22.231616  5380 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:13:22.233124  5380 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([2, 2147483648],"float32"), tuple(0,1,), False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747649604 (unix time) try "date -d @1747649604" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1504) received by PID 5380 (TID 0x7f98c6a93740) from PID 5380 ***]

[Worker 0] Started on GPU 0

[Worker 0] Processing Task 188: paddle.nanmean(Tensor([2, 3, 143165577, 5],"float32"), list[0,1,2,3,], False, )
W0519 18:14:38.442348  5608 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:14:38.443403  5608 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([2, 3, 143165577, 5],"float32"), list[0,1,2,3,], False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747649680 (unix time) try "date -d @1747649680" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x15e8) received by PID 5608 (TID 0x7f98c6a93740) from PID 5608 ***]

[Worker 0] Started on GPU 0

[Worker 0] Processing Task 193: paddle.nanmean(Tensor([2, 3, 4, 178956971],"float32"), -1, False, )
W0519 18:16:15.182451  5986 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:16:15.183550  5986 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([2, 3, 4, 178956971],"float32"), -1, False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747649777 (unix time) try "date -d @1747649777" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1762) received by PID 5986 (TID 0x7f98c6a93740) from PID 5986 ***]

[Worker 0] Started on GPU 0

[Worker 0] Processing Task 196: paddle.nanmean(Tensor([2, 3, 4, 178956971],"float32"), None, False, )
W0519 18:17:32.504279  6215 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:17:32.505333  6215 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([2, 3, 4, 178956971],"float32"), None, False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747649854 (unix time) try "date -d @1747649854" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1847) received by PID 6215 (TID 0x7f98c6a93740) from PID 6215 ***]

[Worker 0] Started on GPU 0

[Worker 0] Processing Task 201: paddle.nanmean(Tensor([71582789, 3, 4, 5],"float32"), list[], False, )
W0519 18:19:11.543359  6593 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:19:11.544425  6593 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([71582789, 3, 4, 5],"float32"), list[], False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747649953 (unix time) try "date -d @1747649953" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x19c1) received by PID 6593 (TID 0x7f98c6a93740) from PID 6593 ***]

[Worker 0] Started on GPU 0

[Worker 0] Processing Task 204: paddle.nanmean(Tensor([71582789, 3, 4, 5],"float32"), None, False, )
W0519 18:20:28.153420  6823 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:20:28.154425  6823 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([71582789, 3, 4, 5],"float32"), None, False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747650030 (unix time) try "date -d @1747650030" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1aa7) received by PID 6823 (TID 0x7f98c6a93740) from PID 6823 ***]

[Worker 0] Started on GPU 0

[Worker 0] Processing Task 208: paddle.nanmean(Tensor([858993459, 5],"float32"), keepdim=True, )
W0519 18:21:59.183233  7127 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:21:59.184242  7127 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([858993459, 5],"float32"), keepdim=True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747650121 (unix time) try "date -d @1747650121" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1bd7) received by PID 7127 (TID 0x7f98c6a93740) from PID 7127 ***]

[Worker 0] Started on GPU 0

[Worker 0] Processing Task 212: paddle.nn.functional.normalize(Tensor([2281701379],"float32"), axis=0, )
W0519 18:23:32.669149  7431 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:23:32.670203  7431 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.nn.functional.normalize(Tensor([2281701379],"float32"), axis=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 5.e+11
Max relative difference: 1.3788965e+16
 x: array([-3.972803e+11,  4.571604e+11, -3.899427e+10, ..., -9.850412e+10,
       -1.712309e+11,  2.386652e+11], dtype=float32)
 y: array([-2.881147e-05,  3.315408e-05, -2.827933e-06, ..., -7.143693e-06,
       -1.241797e-05,  1.730842e-05], dtype=float32)
[Worker 0] Completed Task 212

[Worker 0] Processing Task 220: paddle.nn.functional.normalize(x=Tensor([2147483649, 2],"float16"), p=1.2, )
W0519 18:29:27.939512  7660 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:29:27.940510  7660 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.nn.functional.normalize(x=Tensor([2147483649, 2],"float16"), p=1.2, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /paddle/paddle/phi/core/dense_tensor.cc:113)

[Worker 0] Completed Task 220

[Worker 0] Processing Task 223: paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 2147483649],"float16"), 1, 1e-06, False, None, )
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 2147483649],"float16"), 1, 1e-06, False, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([0., 0.], dtype=float16)
 y: array([inf, inf], dtype=float16)
[Worker 0] Completed Task 223

[Worker 0] Processing Task 226: paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 2147483649],"float16"), -math.inf, 1e-06, False, None, )
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 2147483649],"float16"), -math.inf, 1e-06, False, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([inf, inf], dtype=float16)
 y: array([6.e-08, 6.e-08], dtype=float16)
[Worker 0] Completed Task 226

[Worker 0] Processing Task 231: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), -1, 1e-06, False, None, )
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), -1, 1e-06, False, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array(inf, dtype=float32)
 y: array(1.143778e-11, dtype=float32)
[Worker 0] Completed Task 231

[Worker 0] Processing Task 235: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), 2, 1e-06, True, None, )
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), 2, 1e-06, True, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 19500.498
Max relative difference: 1.
 x: array([0.], dtype=float32)
 y: array([19500.498], dtype=float32)
[Worker 0] Completed Task 235

[Worker 0] Processing Task 238: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), math.inf, 1e-06, True, None, )
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), math.inf, 1e-06, True, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y -inf location mismatch:
 x: array([-inf], dtype=float32)
 y: array([0.999996], dtype=float32)
[Worker 0] Completed Task 238

[Worker 0] Processing Task 241: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), 0, 1e-06, True, None, )
One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), 0, 1e-06, True, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([1.], dtype=float16)
 y: array([inf], dtype=float16)
[Worker 0] Completed Task 241

[Worker 0] Processing Task 244: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), 1, 1e-06, True, None, )
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), 1, 1e-06, True, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([0.03735], dtype=float16)
 y: array([inf], dtype=float16)
[Worker 0] Completed Task 244

[Worker 0] Processing Task 247: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), 2, 1e-06, True, None, )
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), 2, 1e-06, True, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 26752.
Max relative difference: 1.
 x: array([0.7676], dtype=float16)
 y: array([26752.], dtype=float16)
[Worker 0] Completed Task 247

[Worker 0] Processing Task 250: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), math.inf, 1e-06, True, None, )
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), math.inf, 1e-06, True, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 0.664
Max relative difference: 0.664
 x: array([0.336], dtype=float16)
 y: array([1.], dtype=float16)
[Worker 0] Completed Task 250

[Worker 0] Processing Task 253: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=False, reduction="none", )
[accuracy error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=False, reduction="none", ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 0.3
Max relative difference: inf
 x: array([0.3, 0.3, 0.3, 0.3, 0.3], dtype=float16)
 y: array([0., 0., 0., 0., 0.], dtype=float16)
[Worker 0] Completed Task 253

[Worker 0] Processing Task 257: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=False, reduction="mean", name=None, )
[accuracy error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=False, reduction="mean", name=None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 0.3
Max relative difference: inf
 x: array(0.3, dtype=float16)
 y: array(0., dtype=float16)
[Worker 0] Completed Task 257

[Worker 0] Processing Task 261: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([858993460, 5],"float16"), Tensor([858993460, 5],"float16"), Tensor([858993460, 5],"float16"), margin=0.3, swap=False, reduction="sum", name=None, )
W0519 19:21:02.365468  7660 backward.cc:441] While running Node (SubtractGradNode) raises a std::exception: paddle::memory::allocation::BadAlloc
[paddle error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([858993460, 5],"float16"), Tensor([858993460, 5],"float16"), Tensor([858993460, 5],"float16"), margin=0.3, swap=False, reduction="sum", name=None, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   SubtractGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::subtract_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::SubtractGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   phi::dtype::float16* phi::DeviceContext::Alloc<phi::dtype::float16>(phi::TensorBase*, unsigned long, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 8.000000GB memory on GPU 0, 73.622009GB memory has been allocated and available memory is only 5.562866GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747653663 (unix time) try "date -d @1747653663" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1dec) received by PID 7660 (TID 0x7f98c6a93740) from PID 7660 ***]

[Worker 0] Started on GPU 0

[Worker 0] Processing Task 278: paddle.sum(Tensor([1431655765, 3],"bool"), axis=-1, keepdim=False, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.sum(Tensor([1431655765, 3],"bool"), axis=-1, keepdim=False, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_sum(_object*, _object*, _object*)
1   sum_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, phi::DataType, bool)
2   paddle::experimental::sum(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, phi::DataType, bool)
3   void phi::SumKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
4   void phi::SumRawKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
5   void phi::CastCUDAKernelImpl<bool, long>(phi::GPUContext const&, phi::DenseTensor const&, phi::DataType, phi::DenseTensor*)
6   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  paddle::memory::allocation::Allocator::Allocate(unsigned long)
12  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
13  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 32.000000GB memory on GPU 0, 52.270508GB memory has been allocated and available memory is only 26.914368GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 0] Completed Task 278

[Worker 0] Processing Task 280: paddle.sum(Tensor([1431655765, 3],"bool"), axis=list[1,], keepdim=False, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.sum(Tensor([1431655765, 3],"bool"), axis=list[1,], keepdim=False, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_sum(_object*, _object*, _object*)
1   sum_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, phi::DataType, bool)
2   paddle::experimental::sum(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, phi::DataType, bool)
3   void phi::SumKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
4   void phi::SumRawKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
5   void phi::CastCUDAKernelImpl<bool, long>(phi::GPUContext const&, phi::DenseTensor const&, phi::DataType, phi::DenseTensor*)
6   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  paddle::memory::allocation::Allocator::Allocate(unsigned long)
12  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
13  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 32.000000GB memory on GPU 0, 52.270508GB memory has been allocated and available memory is only 26.914368GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 0] Completed Task 280

[Worker 0] Processing Task 290: paddle.sum(Tensor([2, 107374183, 4, 5],"bool"), axis=-1, keepdim=False, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.sum(Tensor([2, 107374183, 4, 5],"bool"), axis=-1, keepdim=False, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_sum(_object*, _object*, _object*)
1   sum_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, phi::DataType, bool)
2   paddle::experimental::sum(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, phi::DataType, bool)
3   void phi::SumKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
4   void phi::SumRawKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
5   void phi::CastCUDAKernelImpl<bool, long>(phi::GPUContext const&, phi::DenseTensor const&, phi::DataType, phi::DenseTensor*)
6   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  paddle::memory::allocation::Allocator::Allocate(unsigned long)
12  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
13  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 32.000000GB memory on GPU 0, 48.008789GB memory has been allocated and available memory is only 31.176086GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 0] Completed Task 290

[Worker 0] Processing Task 296: paddle.sum(Tensor([2, 107374183, 4, 5],"bool"), axis=list[2,], keepdim=True, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.sum(Tensor([2, 107374183, 4, 5],"bool"), axis=list[2,], keepdim=True, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_sum(_object*, _object*, _object*)
1   sum_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, phi::DataType, bool)
2   paddle::experimental::sum(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, phi::DataType, bool)
3   void phi::SumKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
4   void phi::SumRawKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
5   void phi::CastCUDAKernelImpl<bool, long>(phi::GPUContext const&, phi::DenseTensor const&, phi::DataType, phi::DenseTensor*)
6   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  paddle::memory::allocation::Allocator::Allocate(unsigned long)
12  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
13  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 32.000000GB memory on GPU 0, 49.610352GB memory has been allocated and available memory is only 29.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 0] Completed Task 296

[Worker 0] Processing Task 299: paddle.sum(Tensor([2, 107374183, 4, 5],"bool"), axis=tuple(0,2,), keepdim=False, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.sum(Tensor([2, 107374183, 4, 5],"bool"), axis=tuple(0,2,), keepdim=False, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_sum(_object*, _object*, _object*)
1   sum_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, phi::DataType, bool)
2   paddle::experimental::sum(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, phi::DataType, bool)
3   void phi::SumKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
4   void phi::SumRawKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
5   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 4.000000GB memory on GPU 0, 77.612305GB memory has been allocated and available memory is only 1.572571GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 0] Completed Task 299

[Worker 0] Processing Task 321: paddle.sum(Tensor([2, 3, 143165577, 5],"bool"), axis=-1, keepdim=False, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.sum(Tensor([2, 3, 143165577, 5],"bool"), axis=-1, keepdim=False, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_sum(_object*, _object*, _object*)
1   sum_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, phi::DataType, bool)
2   paddle::experimental::sum(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, phi::DataType, bool)
3   void phi::SumKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
4   void phi::SumRawKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
5   void phi::CastCUDAKernelImpl<bool, long>(phi::GPUContext const&, phi::DenseTensor const&, phi::DataType, phi::DenseTensor*)
6   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  paddle::memory::allocation::Allocator::Allocate(unsigned long)
12  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
13  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 32.000000GB memory on GPU 0, 48.008789GB memory has been allocated and available memory is only 31.176086GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 0] Completed Task 321

[Worker 0] Processing Task 337: paddle.sum(Tensor([2, 3, 4, 178956971],"bool"), axis=list[2,], keepdim=True, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.sum(Tensor([2, 3, 4, 178956971],"bool"), axis=list[2,], keepdim=True, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_sum(_object*, _object*, _object*)
1   sum_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, phi::DataType, bool)
2   paddle::experimental::sum(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, phi::DataType, bool)
3   void phi::SumKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
4   void phi::SumRawKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
5   void phi::CastCUDAKernelImpl<bool, long>(phi::GPUContext const&, phi::DenseTensor const&, phi::DataType, phi::DenseTensor*)
6   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  paddle::memory::allocation::Allocator::Allocate(unsigned long)
12  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
13  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 32.000000GB memory on GPU 0, 49.610352GB memory has been allocated and available memory is only 29.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 0] Completed Task 337

[Worker 0] Processing Task 361: paddle.sum(Tensor([3, 715827883, 2],"int32"), axis=0, keepdim=True, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.sum(Tensor([3, 715827883, 2],"int32"), axis=0, keepdim=True, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_sum(_object*, _object*, _object*)
1   sum_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, phi::DataType, bool)
2   paddle::experimental::sum(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, phi::DataType, bool)
3   void phi::SumKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
4   void phi::SumRawKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
5   void phi::CastCUDAKernelImpl<int, long>(phi::GPUContext const&, phi::DenseTensor const&, phi::DataType, phi::DenseTensor*)
6   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  paddle::memory::allocation::Allocator::Allocate(unsigned long)
12  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
13  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 32.000000GB memory on GPU 0, 76.276367GB memory has been allocated and available memory is only 2.908508GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 0] Completed Task 361

[Worker 0] Processing Task 372: paddle.sum(Tensor([536870912, 4, 2],"int32"), axis=1, keepdim=True, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.sum(Tensor([536870912, 4, 2],"int32"), axis=1, keepdim=True, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_sum(_object*, _object*, _object*)
1   sum_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, phi::DataType, bool)
2   paddle::experimental::sum(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, phi::DataType, bool)
3   void phi::SumKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
4   void phi::SumRawKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
5   void phi::CastCUDAKernelImpl<int, long>(phi::GPUContext const&, phi::DenseTensor const&, phi::DataType, phi::DenseTensor*)
6   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  paddle::memory::allocation::Allocator::Allocate(unsigned long)
12  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
13  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 32.000000GB memory on GPU 0, 73.602539GB memory has been allocated and available memory is only 5.582336GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 0] Completed Task 372

[Worker 0] Processing Task 397: paddle.Tensor.chunk(Tensor([1, 1, 11109, 205393],"float32"), 2, axis=-1, )
[paddle error] paddle.Tensor.chunk(Tensor([1, 1, 11109, 205393],"float32"), 2, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 2, input(X)'s shape = [1, 1, 11109, 205393], Attr(dim) = 3.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 0] Completed Task 397

[Worker 0] Processing Task 401: paddle.Tensor.chunk(Tensor([1, 101, 1, 22591103],"float32"), 4, axis=-1, )
[paddle error] paddle.Tensor.chunk(Tensor([1, 101, 1, 22591103],"float32"), 4, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 4, input(X)'s shape = [1, 101, 1, 22591103], Attr(dim) = 3.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:3 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 0] Completed Task 401

[Worker 0] Processing Task 405: paddle.Tensor.chunk(Tensor([1, 2281701379],"float32"), 2, axis=1, )
[paddle error] paddle.Tensor.chunk(Tensor([1, 2281701379],"float32"), 2, axis=1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 2, input(X)'s shape = [1, 2281701379], Attr(dim) = 1.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 0] Completed Task 405

[Worker 0] Processing Task 409: paddle.Tensor.chunk(Tensor([5704254, 100, 4],"float32"), 4, )
[paddle error] paddle.Tensor.chunk(Tensor([5704254, 100, 4],"float32"), 4, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 4, input(X)'s shape = [5704254, 100, 4], Attr(dim) = 0.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:2 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 0] Completed Task 409

[Worker 0] Processing Task 413: paddle.Tensor.expand_as(Tensor([2281701379, 1],"int32"), Tensor([2281701379, 1],"int64"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.expand_as(Tensor([2281701379, 1],"int32"), Tensor([2281701379, 1],"int64"), ) 
 (InvalidArgument) When the value in shape is negative for expand_as_v2 op, only -1 is supported, but the value received is -2013265917.
  [Hint: Expected target_shape[i] == -1, but received target_shape[i]:-2013265917 != -1:-1.] (at /paddle/paddle/phi/kernels/gpu/expand_as_kernel.cu:70)

[Worker 0] Completed Task 413

[Worker 0] Processing Task 417: paddle.Tensor.flatten(Tensor([1, 1, 2281701379],"float32"), stop_axis=-2, )
[paddle error] paddle.Tensor.flatten(Tensor([1, 1, 2281701379],"float32"), stop_axis=-2, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265917], shape[1] = -2013265917.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265917 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 0] Completed Task 417

[Worker 0] Processing Task 422: paddle.Tensor.flatten(Tensor([1, 1, 8, 285212673],"float32"), start_axis=2, )
[paddle error] paddle.Tensor.flatten(Tensor([1, 1, 8, 285212673],"float32"), start_axis=2, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, 1, -2013265912], shape[2] = -2013265912.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265912 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 0] Completed Task 422

[Worker 0] Processing Task 426: paddle.Tensor.flatten(Tensor([1, 102, 22369622],"int32"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.flatten(Tensor([1, 102, 22369622],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265852], shape[0] = -2013265852.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265852 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 0] Completed Task 426

[Worker 0] Processing Task 430: paddle.Tensor.flatten(Tensor([1, 142606337, 4, 4],"float32"), start_axis=1, stop_axis=3, )
[paddle error] paddle.Tensor.flatten(Tensor([1, 142606337, 4, 4],"float32"), start_axis=1, stop_axis=3, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265904], shape[1] = -2013265904.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265904 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 0] Completed Task 430

[Worker 0] Processing Task 434: paddle.Tensor.flatten(Tensor([1, 192, 11883862],"float32"), 1, )
[paddle error] paddle.Tensor.flatten(Tensor([1, 192, 11883862],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265792], shape[1] = -2013265792.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265792 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 0] Completed Task 434

[Worker 0] Processing Task 439: paddle.Tensor.flatten(Tensor([1, 253522376, 9],"float32"), 1, )
[paddle error] paddle.Tensor.flatten(Tensor([1, 253522376, 9],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265912], shape[1] = -2013265912.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265912 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 0] Completed Task 439

[Worker 0] Processing Task 443: paddle.Tensor.flatten(Tensor([1, 6, 4, 178956971],"float16"), start_axis=1, stop_axis=3, )
[paddle error] paddle.Tensor.flatten(Tensor([1, 6, 4, 178956971],"float16"), start_axis=1, stop_axis=3, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 6, 4, 178956971], X's size = 4294967304, 'shape' is [1, 8], the capacity of 'shape' is 8.
  [Hint: Expected capacity == in_size, but received capacity:8 != in_size:4294967304.] (at /paddle/paddle/phi/infermeta/unary.cc:2245)

[Worker 0] Completed Task 443

[Worker 0] Processing Task 448: paddle.Tensor.flatten(Tensor([100, 22817014],"float32"), 0, 1, )
[paddle error] paddle.Tensor.flatten(Tensor([100, 22817014],"float32"), 0, 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265896], shape[0] = -2013265896.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265896 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 0] Completed Task 448

[Worker 0] Processing Task 452: paddle.Tensor.flatten(Tensor([12, 2796203, 68, 1],"float32"), )
[paddle error] paddle.Tensor.flatten(Tensor([12, 2796203, 68, 1],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265648], shape[0] = -2013265648.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265648 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 0] Completed Task 452

[Worker 0] Processing Task 456: paddle.Tensor.flatten(Tensor([12, 4, 1398102, 34],"float32"), )
[paddle error] paddle.Tensor.flatten(Tensor([12, 4, 1398102, 34],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013264832], shape[0] = -2013264832.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013264832 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 0] Completed Task 456

[Worker 0] Processing Task 460: paddle.Tensor.flatten(Tensor([12, 4, 699051, 68],"float32"), )
[paddle error] paddle.Tensor.flatten(Tensor([12, 4, 699051, 68],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013264832], shape[0] = -2013264832.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013264832 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 0] Completed Task 460

[Worker 0] Processing Task 464: paddle.Tensor.flatten(Tensor([2, 3, 15845149, 6, 4],"float32"), )
[paddle error] paddle.Tensor.flatten(Tensor([2, 3, 15845149, 6, 4],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265840], shape[0] = -2013265840.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265840 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 0] Completed Task 464

[Worker 0] Processing Task 468: paddle.Tensor.flatten(Tensor([220753, 4, 38, 68],"float32"), )
[paddle error] paddle.Tensor.flatten(Tensor([220753, 4, 38, 68],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013264288], shape[0] = -2013264288.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013264288 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 0] Completed Task 468

[Worker 0] Processing Task 472: paddle.Tensor.flatten(Tensor([2277148, 1002],"int32"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.flatten(Tensor([2277148, 1002],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265000], shape[0] = -2013265000.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265000 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 0] Completed Task 472

[Worker 0] Processing Task 476: paddle.Tensor.flatten(Tensor([2281701379, 1, 1],"int32"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.flatten(Tensor([2281701379, 1, 1],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265917], shape[0] = -2013265917.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265917 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 0] Completed Task 476

[Worker 0] Processing Task 479: paddle.Tensor.flatten(Tensor([228170138, 10, 1],"int32"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.flatten(Tensor([228170138, 10, 1],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265916], shape[0] = -2013265916.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265916 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 0] Completed Task 479

[Worker 0] Processing Task 482: paddle.Tensor.flatten(Tensor([3532046, 19, 34, 1],"float32"), )
[paddle error] paddle.Tensor.flatten(Tensor([3532046, 19, 34, 1],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265580], shape[0] = -2013265580.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265580 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 0] Completed Task 482

[Worker 0] Processing Task 486: paddle.Tensor.flatten(Tensor([883012, 38, 68, 1],"float32"), )
[paddle error] paddle.Tensor.flatten(Tensor([883012, 38, 68, 1],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013264288], shape[0] = -2013264288.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013264288 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 0] Completed Task 486

[Worker 1] Processing Task 1: paddle.argsort(Tensor([228170138, 10],"int64"), axis=1, stable=True, )
element 0 of tensors does not require grad and does not have a grad_fn
W0519 16:54:44.409768  4014 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 16:54:44.410727  4014 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.argsort(Tensor([228170138, 10],"int64"), axis=1, stable=True, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_argsort(_object*, _object*, _object*)
1   argsort_ad_func(paddle::Tensor const&, int, bool, bool)
2   paddle::experimental::argsort(paddle::Tensor const&, int, bool, bool)
3   void phi::ArgsortKernel<long, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, phi::DenseTensor*, phi::DenseTensor*)
4   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
5   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
6   paddle::memory::allocation::Allocator::Allocate(unsigned long)
7   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 69.610352GB memory has been allocated and available memory is only 9.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 1] Completed Task 1

[Worker 1] Processing Task 7: paddle.argsort(Tensor([3, 760567127],"int64"), axis=1, stable=True, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.argsort(Tensor([3, 760567127],"int64"), axis=1, stable=True, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_argsort(_object*, _object*, _object*)
1   argsort_ad_func(paddle::Tensor const&, int, bool, bool)
2   paddle::experimental::argsort(paddle::Tensor const&, int, bool, bool)
3   void phi::ArgsortKernel<long, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, phi::DenseTensor*, phi::DenseTensor*)
4   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
5   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
6   paddle::memory::allocation::Allocator::Allocate(unsigned long)
7   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 69.610352GB memory has been allocated and available memory is only 9.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 1] Completed Task 7

[Worker 1] Processing Task 10: paddle.argsort(Tensor([570425345, 4],"int64"), axis=1, stable=True, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.argsort(Tensor([570425345, 4],"int64"), axis=1, stable=True, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_argsort(_object*, _object*, _object*)
1   argsort_ad_func(paddle::Tensor const&, int, bool, bool)
2   paddle::experimental::argsort(paddle::Tensor const&, int, bool, bool)
3   void phi::ArgsortKernel<long, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, phi::DenseTensor*, phi::DenseTensor*)
4   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
5   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
6   paddle::memory::allocation::Allocator::Allocate(unsigned long)
7   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 69.610352GB memory has been allocated and available memory is only 9.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 1] Completed Task 10

[Worker 1] Processing Task 21: paddle.chunk(Tensor([1, 11, 207427399],"float32"), chunks=2, axis=-1, )
[paddle error] paddle.chunk(Tensor([1, 11, 207427399],"float32"), chunks=2, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 2, input(X)'s shape = [1, 11, 207427399], Attr(dim) = 2.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 1] Completed Task 21

[Worker 1] Processing Task 23: paddle.chunk(Tensor([13, 16, 10969719],"float32"), chunks=2, axis=-1, )
[paddle error] paddle.chunk(Tensor([13, 16, 10969719],"float32"), chunks=2, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 2, input(X)'s shape = [13, 16, 10969719], Attr(dim) = 2.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 1] Completed Task 23

[Worker 1] Processing Task 27: paddle.chunk(Tensor([13, 56, 3134206],"float32"), 3, axis=-1, )
[paddle error] paddle.chunk(Tensor([13, 56, 3134206],"float32"), 3, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 3, input(X)'s shape = [13, 56, 3134206], Attr(dim) = 2.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 1] Completed Task 27

[Worker 1] Processing Task 31: paddle.chunk(Tensor([4, 262145, 64, 64],"float16"), 3, axis=1, )
[paddle error] paddle.chunk(Tensor([4, 262145, 64, 64],"float16"), 3, axis=1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 3, input(X)'s shape = [4, 262145, 64, 64], Attr(dim) = 1.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:2 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 1] Completed Task 31

[Worker 1] Processing Task 38: paddle.chunk(x=Tensor([3, 760567127],"float32"), chunks=3, axis=-1, )
[paddle error] paddle.chunk(x=Tensor([3, 760567127],"float32"), chunks=3, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 3, input(X)'s shape = [3, 760567127], Attr(dim) = 1.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:2 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 1] Completed Task 38

[Worker 1] Processing Task 40: paddle.chunk(x=Tensor([3, 760567127],"int64"), chunks=3, axis=-1, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.chunk(x=Tensor([3, 760567127],"int64"), chunks=3, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 3, input(X)'s shape = [3, 760567127], Attr(dim) = 1.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:2 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 1] Completed Task 40

[Worker 1] Processing Task 61: paddle.flatten(Tensor([1, 1024, 4194304, 1],"float32"), 1, )
[paddle error] paddle.flatten(Tensor([1, 1024, 4194304, 1],"float32"), 1, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 1024, 4194304, 1], X's size = 4294967296, 'shape' is [1, 0], the capacity of 'shape' is 1024.
  [Hint: Expected capacity == in_size, but received capacity:1024 != in_size:4294967296.] (at /paddle/paddle/phi/infermeta/unary.cc:2245)

[Worker 1] Completed Task 61

[Worker 1] Processing Task 65: paddle.flatten(Tensor([1, 16, 53687092, 5],"float32"), 1, )
[paddle error] paddle.flatten(Tensor([1, 16, 53687092, 5],"float32"), 1, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 16, 53687092, 5], X's size = 4294967360, 'shape' is [1, 64], the capacity of 'shape' is 64.
  [Hint: Expected capacity == in_size, but received capacity:64 != in_size:4294967360.] (at /paddle/paddle/phi/infermeta/unary.cc:2245)

[Worker 1] Completed Task 65

[Worker 1] Processing Task 69: paddle.flatten(Tensor([1, 2048, 2097152, 1],"float32"), 1, )
[paddle error] paddle.flatten(Tensor([1, 2048, 2097152, 1],"float32"), 1, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 2048, 2097152, 1], X's size = 4294967296, 'shape' is [1, 0], the capacity of 'shape' is 2048.
  [Hint: Expected capacity == in_size, but received capacity:2048 != in_size:4294967296.] (at /paddle/paddle/phi/infermeta/unary.cc:2245)

[Worker 1] Completed Task 69

[Worker 1] Processing Task 73: paddle.flatten(Tensor([1, 8, 76695845, 7],"float32"), start_axis=1, stop_axis=-1, )
[paddle error] paddle.flatten(Tensor([1, 8, 76695845, 7],"float32"), start_axis=1, stop_axis=-1, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 8, 76695845, 7], X's size = 4294967320, 'shape' is [1, 24], the capacity of 'shape' is 24.
  [Hint: Expected capacity == in_size, but received capacity:24 != in_size:4294967320.] (at /paddle/paddle/phi/infermeta/unary.cc:2245)

[Worker 1] Completed Task 73

[Worker 1] Processing Task 77: paddle.flatten(Tensor([1073741824, 4],"float32"), )
[paddle error] paddle.flatten(Tensor([1073741824, 4],"float32"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1073741824, 4], X's size = 4294967296, 'shape' is [0], the capacity of 'shape' is 1073741824.
  [Hint: Expected capacity == in_size, but received capacity:1073741824 != in_size:4294967296.] (at /paddle/paddle/phi/infermeta/unary.cc:2245)

[Worker 1] Completed Task 77

[Worker 1] Processing Task 83: paddle.flatten(Tensor([2, 1024, 2097152, 1],"float32"), start_axis=1, stop_axis=-1, )
[paddle error] paddle.flatten(Tensor([2, 1024, 2097152, 1],"float32"), start_axis=1, stop_axis=-1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483648], shape[1] = -2147483648.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483648 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 1] Completed Task 83

[Worker 1] Processing Task 87: paddle.flatten(Tensor([2, 2048, 1, 1048576],"float32"), start_axis=1, stop_axis=-1, )
[paddle error] paddle.flatten(Tensor([2, 2048, 1, 1048576],"float32"), start_axis=1, stop_axis=-1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483648], shape[1] = -2147483648.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483648 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 1] Completed Task 87

[Worker 1] Processing Task 91: paddle.flatten(Tensor([2, 2147483648, 1, 1],"float32"), start_axis=1, stop_axis=-1, )
[paddle error] paddle.flatten(Tensor([2, 2147483648, 1, 1],"float32"), start_axis=1, stop_axis=-1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483648], shape[1] = -2147483648.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483648 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 1] Completed Task 91

[Worker 1] Processing Task 95: paddle.flatten(Tensor([2, 214748365, 5],"float64"), )
[paddle error] paddle.flatten(Tensor([2, 214748365, 5],"float64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2147483646], shape[0] = -2147483646.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483646 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 1] Completed Task 95

[Worker 1] Processing Task 98: paddle.flatten(Tensor([2, 238609295, 3, 3],"float32"), start_axis=1, stop_axis=-1, )
[paddle error] paddle.flatten(Tensor([2, 238609295, 3, 3],"float32"), start_axis=1, stop_axis=-1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483641], shape[1] = -2147483641.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483641 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 1] Completed Task 98

[Worker 1] Processing Task 104: paddle.flatten(Tensor([2, 429496730, 5],"int32"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.flatten(Tensor([2, 429496730, 5],"int32"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [2, 429496730, 5], X's size = 4294967300, 'shape' is [4], the capacity of 'shape' is 4.
  [Hint: Expected capacity == in_size, but received capacity:4 != in_size:4294967300.] (at /paddle/paddle/phi/infermeta/unary.cc:2245)

[Worker 1] Completed Task 104

[Worker 1] Processing Task 106: paddle.flatten(Tensor([2, 512, 1, 4194304],"float32"), 1, )
[paddle error] paddle.flatten(Tensor([2, 512, 1, 4194304],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483648], shape[1] = -2147483648.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483648 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 1] Completed Task 106

[Worker 1] Processing Task 110: paddle.flatten(Tensor([2, 59652324, 6, 6],"float32"), start_axis=1, stop_axis=-1, )
[paddle error] paddle.flatten(Tensor([2, 59652324, 6, 6],"float32"), start_axis=1, stop_axis=-1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483632], shape[1] = -2147483632.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483632 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 1] Completed Task 110

[Worker 1] Processing Task 114: paddle.flatten(Tensor([214748365, 4, 5],"float32"), )
[paddle error] paddle.flatten(Tensor([214748365, 4, 5],"float32"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [214748365, 4, 5], X's size = 4294967300, 'shape' is [4], the capacity of 'shape' is 4.
  [Hint: Expected capacity == in_size, but received capacity:4 != in_size:4294967300.] (at /paddle/paddle/phi/infermeta/unary.cc:2245)

[Worker 1] Completed Task 114

[Worker 1] Processing Task 119: paddle.flatten(Tensor([3, 357913942, 2],"int64"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.flatten(Tensor([3, 357913942, 2],"int64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2147483644], shape[0] = -2147483644.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483644 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 1] Completed Task 119

[Worker 1] Processing Task 122: paddle.flatten(Tensor([3, 4, 357913942],"float32"), )
[paddle error] paddle.flatten(Tensor([3, 4, 357913942],"float32"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [3, 4, 357913942], X's size = 4294967304, 'shape' is [8], the capacity of 'shape' is 8.
  [Hint: Expected capacity == in_size, but received capacity:8 != in_size:4294967304.] (at /paddle/paddle/phi/infermeta/unary.cc:2245)

[Worker 1] Completed Task 122

[Worker 1] Processing Task 128: paddle.flatten(Tensor([4, 89478486, 6],"float64"), )
[paddle error] paddle.flatten(Tensor([4, 89478486, 6],"float64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2147483632], shape[0] = -2147483632.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483632 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 1] Completed Task 128

[Worker 1] Processing Task 132: paddle.flatten(Tensor([536870912, 4, 2],"int32"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.flatten(Tensor([536870912, 4, 2],"int32"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [536870912, 4, 2], X's size = 4294967296, 'shape' is [0], the capacity of 'shape' is 536870912.
  [Hint: Expected capacity == in_size, but received capacity:536870912 != in_size:4294967296.] (at /paddle/paddle/phi/infermeta/unary.cc:2245)

[Worker 1] Completed Task 132

[Worker 1] Processing Task 135: paddle.fmin(Tensor([2147483649],"int64"), Tensor([1],"int64"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.fmin(Tensor([2147483649],"int64"), Tensor([1],"int64"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_fmin(_object*, _object*, _object*)
1   fmin_ad_func(paddle::Tensor const&, paddle::Tensor const&)
2   paddle::experimental::fmin(paddle::Tensor const&, paddle::Tensor const&)
3   void phi::FMinKernel<long, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*)
4   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
5   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
6   paddle::memory::allocation::Allocator::Allocate(unsigned long)
7   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 16.000000GB memory on GPU 0, 65.614258GB memory has been allocated and available memory is only 13.570618GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 1] Completed Task 135

[Worker 1] Processing Task 139: paddle.gcd(Tensor([2281701379],"int32"), Tensor([1],"int32"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.gcd(Tensor([2281701379],"int32"), Tensor([1],"int32"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   not_equal_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::not_equal(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::NotEqualRawKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
3   bool* phi::DeviceContext::Alloc<bool>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 2.125000GB memory on GPU 0, 78.124023GB memory has been allocated and available memory is only 1.060852GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 1] Completed Task 139

[Worker 1] Processing Task 141: paddle.gcd(x=Tensor([2281701379],"int32"), y=Tensor([1],"int32"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.gcd(x=Tensor([2281701379],"int32"), y=Tensor([1],"int32"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   not_equal_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::not_equal(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::NotEqualRawKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
3   bool* phi::DeviceContext::Alloc<bool>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 2.125000GB memory on GPU 0, 78.124023GB memory has been allocated and available memory is only 1.060852GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 1] Completed Task 141

[Worker 1] Processing Task 145: paddle.lerp(Tensor([1, 3, 3],"float32"), Tensor([253522376, 3, 3],"float32"), Tensor([1, 3, 3],"float32"), )
W0519 17:55:35.601768  4014 backward.cc:437] While running Node (LerpGradNode) raises an EnforceNotMet exception
[paddle error] paddle.lerp(Tensor([1, 3, 3],"float32"), Tensor([253522376, 3, 3],"float32"), Tensor([1, 3, 3],"float32"), ) 
 (InvalidArgument) numel is expected to be greater than or equal 0, but received -2013265912.
  [Hint: Expected numel >= 0, but received numel:-2013265912 < 0:0.] (at /paddle/paddle/phi/backends/gpu/gpu_launch_config.h:110)

[Worker 1] Completed Task 145

[Worker 1] Processing Task 147: paddle.lerp(Tensor([2, 1],"float32"), Tensor([2, 1],"float32"), Tensor([2, 1140850690],"float32"), )
W0519 17:57:39.595423  4014 backward.cc:437] While running Node (LerpGradNode) raises an EnforceNotMet exception
[paddle error] paddle.lerp(Tensor([2, 1],"float32"), Tensor([2, 1],"float32"), Tensor([2, 1140850690],"float32"), ) 
 (InvalidArgument) numel is expected to be greater than or equal 0, but received -2013265916.
  [Hint: Expected numel >= 0, but received numel:-2013265916 < 0:0.] (at /paddle/paddle/phi/backends/gpu/gpu_launch_config.h:110)

[Worker 1] Completed Task 147

[Worker 1] Processing Task 152: paddle.linalg.cond(Tensor([2, 126761188, 3, 3],"float32"), -1, )
one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [2, 126761188, 3, 3]], which is output 0 of LinalgInvExBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
[paddle error] paddle.linalg.cond(Tensor([2, 126761188, 3, 3],"float32"), -1, ) 
 (PreconditionNotMet) For batch [20457990]: U(3, 3) is zero, singular U. Please check the matrix value and change it to a non-singular matrix
  [Hint: Expected info[i] == 0, but received info[i]:3 != 0:0.] (at /paddle/paddle/phi/kernels/funcs/matrix_inverse.cu:117)

[Worker 1] Completed Task 152

[Worker 1] Processing Task 155: paddle.linalg.cond(Tensor([63380594, 4, 3, 3],"float32"), "fro", )
one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [63380594, 4, 3, 3]], which is output 0 of LinalgInvExBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
[accuracy error] paddle.linalg.cond(Tensor([63380594, 4, 3, 3],"float32"), "fro", ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 507 / 253522376 (0.0002%)
Max absolute difference: 5.5291565e+08
Max relative difference: 1.130433
 x: array([[  3.915946,  10.104055,  12.537356,  12.106485],
       [ 10.140597,   4.589926,   5.869419,   5.309055],
       [  5.40309 ,   8.726755,   6.963555,   4.985357],...
 y: array([[  3.915946,  10.104057,  12.537356,  12.106488],
       [ 10.140597,   4.589926,   5.869418,   5.309055],
       [  5.40309 ,   8.726754,   6.963556,   4.985357],...
[Worker 1] Completed Task 155

[Worker 1] Processing Task 158: paddle.linalg.cond(Tensor([63380594, 4, 3, 3],"float32"), math.inf, )
one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [63380594, 4, 3, 3]], which is output 0 of LinalgInvExBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
[paddle error] paddle.linalg.cond(Tensor([63380594, 4, 3, 3],"float32"), math.inf, ) 
 (PreconditionNotMet) For batch [102865670]: U(3, 3) is zero, singular U. Please check the matrix value and change it to a non-singular matrix
  [Hint: Expected info[i] == 0, but received info[i]:3 != 0:0.] (at /paddle/paddle/phi/kernels/funcs/matrix_inverse.cu:117)

[Worker 1] Completed Task 158

[Worker 1] Processing Task 162: paddle.linalg.det(Tensor([30422686, 3, 5, 5],"float32"), )
W0519 18:01:15.058089  4014 backward.cc:437] While running Node (DetGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.det(Tensor([30422686, 3, 5, 5],"float32"), ) 
 (PreconditionNotMet) For batch [77754224]: U(5, 5) is zero, singular U. Please check the matrix value and change it to a non-singular matrix
  [Hint: Expected info[i] == 0, but received info[i]:5 != 0:0.] (at /paddle/paddle/phi/kernels/funcs/matrix_inverse.cu:117)

[Worker 1] Completed Task 162

[Worker 1] Processing Task 165: paddle.logsumexp(Tensor([4294967295],"float32"), axis=0, )
[accuracy error] backward  paddle.logsumexp(Tensor([4294967295],"float32"), axis=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4252010056 / 4294967295 (99%)
Max absolute difference: 1.
Max relative difference: 2.4350562e+10
 x: array([-0.55719 , -0.885476, -0.146411, ..., -0.108754, -0.40947 ,
       -0.735466], dtype=float32)
 y: array([-6.394417e-11, -4.604988e-11, -9.642739e-11, ..., -1.001278e-10,
       -7.412336e-11, -5.350282e-11], dtype=float32)
[Worker 1] Completed Task 165

[Worker 1] Processing Task 174: paddle.nanmean(Tensor([1431655765, 3],"float32"), None, False, )
W0519 18:08:02.872452  4619 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:08:02.873463  4619 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([1431655765, 3],"float32"), None, False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747649284 (unix time) try "date -d @1747649284" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x120b) received by PID 4619 (TID 0x7f98c6a93740) from PID 4619 ***]

[Worker 1] Started on GPU 1

[Worker 1] Processing Task 176: paddle.nanmean(Tensor([1431655765, 3],"float32"), tuple(0,1,), False, )
W0519 18:09:27.346451  4772 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:09:27.347437  4772 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([1431655765, 3],"float32"), tuple(0,1,), False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747649369 (unix time) try "date -d @1747649369" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x12a4) received by PID 4772 (TID 0x7f98c6a93740) from PID 4772 ***]

[Worker 1] Started on GPU 1

[Worker 1] Processing Task 180: paddle.nanmean(Tensor([2, 107374183, 4, 5],"float32"), None, True, )
W0519 18:10:53.713147  5074 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:10:53.714195  5074 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([2, 107374183, 4, 5],"float32"), None, True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747649455 (unix time) try "date -d @1747649455" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x13d2) received by PID 5074 (TID 0x7f98c6a93740) from PID 5074 ***]

[Worker 1] Started on GPU 1

[Worker 1] Processing Task 183: paddle.nanmean(Tensor([2, 2147483648],"float32"), None, False, )
W0519 18:12:28.483893  5302 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:12:28.484907  5302 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([2, 2147483648],"float32"), None, False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747649550 (unix time) try "date -d @1747649550" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x14b6) received by PID 5302 (TID 0x7f98c6a93740) from PID 5302 ***]

[Worker 1] Started on GPU 1

[Worker 1] Processing Task 187: paddle.nanmean(Tensor([2, 3, 143165577, 5],"float32"), list[], False, )
W0519 18:13:56.091732  5530 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:13:56.092739  5530 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([2, 3, 143165577, 5],"float32"), list[], False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747649638 (unix time) try "date -d @1747649638" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x159a) received by PID 5530 (TID 0x7f98c6a93740) from PID 5530 ***]

[Worker 1] Started on GPU 1

[Worker 1] Processing Task 191: paddle.nanmean(Tensor([2, 3, 143165577, 5],"float32"), None, True, )
W0519 18:15:35.553979  5832 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:15:35.555086  5832 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([2, 3, 143165577, 5],"float32"), None, True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747649737 (unix time) try "date -d @1747649737" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x16c8) received by PID 5832 (TID 0x7f98c6a93740) from PID 5832 ***]

[Worker 1] Started on GPU 1

[Worker 1] Processing Task 195: paddle.nanmean(Tensor([2, 3, 4, 178956971],"float32"), list[0,1,2,3,], False, )
W0519 18:17:00.881419  6137 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:17:00.882499  6137 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([2, 3, 4, 178956971],"float32"), list[0,1,2,3,], False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747649822 (unix time) try "date -d @1747649822" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x17f9) received by PID 6137 (TID 0x7f98c6a93740) from PID 6137 ***]

[Worker 1] Started on GPU 1

[Worker 1] Processing Task 199: paddle.nanmean(Tensor([3, 1431655765],"float32"), keepdim=True, )
W0519 18:18:28.482601  6442 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:18:28.483621  6442 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([3, 1431655765],"float32"), keepdim=True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747649910 (unix time) try "date -d @1747649910" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x192a) received by PID 6442 (TID 0x7f98c6a93740) from PID 6442 ***]

[Worker 1] Started on GPU 1

[Worker 1] Processing Task 203: paddle.nanmean(Tensor([71582789, 3, 4, 5],"float32"), list[0,2,], False, )
W0519 18:19:56.744020  6745 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:19:56.745038  6745 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([71582789, 3, 4, 5],"float32"), list[0,2,], False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747649998 (unix time) try "date -d @1747649998" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1a59) received by PID 6745 (TID 0x7f98c6a93740) from PID 6745 ***]

[Worker 1] Started on GPU 1

[Worker 1] Processing Task 207: paddle.nanmean(Tensor([858993459, 5],"float32"), axis=None, )
W0519 18:21:30.526067  7048 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:21:30.527137  7048 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([858993459, 5],"float32"), axis=None, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747650092 (unix time) try "date -d @1747650092" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1b88) received by PID 7048 (TID 0x7f98c6a93740) from PID 7048 ***]

[Worker 1] Started on GPU 1

[Worker 1] Processing Task 211: paddle.nn.functional.normalize(Tensor([2, 2147483649],"float16"), p=2, axis=-1, )
W0519 18:24:20.382674  7354 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:24:20.383646  7354 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.nn.functional.normalize(Tensor([2, 2147483649],"float16"), p=2, axis=-1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[ inf,  inf,  inf, ..., -inf,  inf,  inf],
       [ inf, -inf, -inf, ..., -inf,  inf, -inf]], dtype=float16)
 y: array([[ 2.563e-05,  2.980e-06,  3.600e-05, ..., -2.503e-05,  1.746e-05,
         2.682e-05],
       [ 2.718e-05, -1.019e-05, -1.967e-05, ..., -2.986e-05,  2.563e-05,
        -2.289e-05]], dtype=float16)
[Worker 1] Completed Task 211

[Worker 1] Processing Task 219: paddle.nn.functional.normalize(x=Tensor([2, 2147483649],"float16"), )
W0519 18:29:11.101652  7586 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:29:11.102641  7586 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.nn.functional.normalize(x=Tensor([2, 2147483649],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[-inf,  inf, -inf, ..., -inf, -inf,  inf],
       [ inf, -inf, -inf, ...,  inf,  inf, -inf]], dtype=float16)
 y: array([[-2.325e-05,  2.795e-05, -2.444e-05, ..., -4.828e-06, -3.564e-05,
         2.962e-05],
       [ 3.755e-06, -2.146e-05, -2.903e-05, ...,  2.879e-05,  3.695e-06,
        -5.364e-06]], dtype=float16)
[Worker 1] Completed Task 219

[Worker 1] Processing Task 224: paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 2147483649],"float16"), 2, 1e-06, False, None, )
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 2147483649],"float16"), 2, 1e-06, False, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2 / 2 (100%)
Max absolute difference: 18912.
Max relative difference: 1.
 x: array([0., 0.], dtype=float16)
 y: array([18912., 18912.], dtype=float16)
[Worker 1] Completed Task 224

[Worker 1] Processing Task 227: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), )
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 19500.87
Max relative difference: 1.
 x: array(0., dtype=float32)
 y: array(19500.87, dtype=float32)
[Worker 1] Completed Task 227

[Worker 1] Processing Task 229: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), 0, 1e-06, True, None, )
One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), 0, 1e-06, True, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 2.2817014e+09
Max relative difference: 1.
 x: array([0.], dtype=float32)
 y: array([2.281701e+09], dtype=float32)
[Worker 1] Completed Task 229

[Worker 1] Processing Task 232: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), 1, 1e-06, True, None, )
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), 1, 1e-06, True, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 7.605501e+08
Max relative difference: 1.
 x: array([0.], dtype=float32)
 y: array([7.605501e+08], dtype=float32)
[Worker 1] Completed Task 232

[Worker 1] Processing Task 234: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), 2, 1e-06, False, None, )
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), 2, 1e-06, False, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 19501.17
Max relative difference: 1.
 x: array(0., dtype=float32)
 y: array(19501.17, dtype=float32)
[Worker 1] Completed Task 234

[Worker 1] Processing Task 237: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), -math.inf, 1e-06, False, None, )
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), -math.inf, 1e-06, False, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array(inf, dtype=float32)
 y: array(1.622197e-09, dtype=float32)
[Worker 1] Completed Task 237

[Worker 1] Processing Task 240: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), 0, 1e-06, False, None, )
One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), 0, 1e-06, False, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array(1., dtype=float16)
 y: array(inf, dtype=float16)
[Worker 1] Completed Task 240

[Worker 1] Processing Task 243: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), -1, 1e-06, False, None, )
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), -1, 1e-06, False, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 0.6113
Max relative difference: inf
 x: array(0.6113, dtype=float16)
 y: array(0., dtype=float16)
[Worker 1] Completed Task 243

[Worker 1] Processing Task 246: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), 2, 1e-06, False, None, )
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), 2, 1e-06, False, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 26752.
Max relative difference: 1.
 x: array(0.6045, dtype=float16)
 y: array(26752., dtype=float16)
[Worker 1] Completed Task 246

[Worker 1] Processing Task 249: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), -math.inf, 1e-06, False, None, )
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), -math.inf, 1e-06, False, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 0.2642
Max relative difference: inf
 x: array(0.2642, dtype=float16)
 y: array(6.e-08, dtype=float16)
[Worker 1] Completed Task 249

[Worker 1] Processing Task 252: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=False, reduction="mean", )
[accuracy error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=False, reduction="mean", ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 0.3
Max relative difference: inf
 x: array(0.3, dtype=float16)
 y: array(0., dtype=float16)
[Worker 1] Completed Task 252

[Worker 1] Processing Task 256: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([858993460, 5],"float16"), positive=Tensor([858993460, 5],"float16"), negative=Tensor([858993460, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", )
W0519 19:04:53.223716  7586 backward.cc:441] While running Node (SubtractGradNode) raises a std::exception: paddle::memory::allocation::BadAlloc
[paddle error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([858993460, 5],"float16"), positive=Tensor([858993460, 5],"float16"), negative=Tensor([858993460, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   SubtractGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::subtract_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::SubtractGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   phi::dtype::float16* phi::DeviceContext::Alloc<phi::dtype::float16>(phi::TensorBase*, unsigned long, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 8.000000GB memory on GPU 0, 73.622070GB memory has been allocated and available memory is only 5.562805GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747652694 (unix time) try "date -d @1747652694" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1da2) received by PID 7586 (TID 0x7f98c6a93740) from PID 7586 ***]

[Worker 1] Started on GPU 1

[Worker 1] Processing Task 259: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=False, reduction="sum", name=None, )
W0519 19:08:52.296671  7814 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 19:08:52.297631  7814 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=False, reduction="sum", name=None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 1.5
Max relative difference: inf
 x: array(1.5, dtype=float16)
 y: array(0., dtype=float16)
[Worker 1] Completed Task 259

[Worker 1] Processing Task 268: paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([3, 2, 4],"float32"),Tensor([3],"float32"),], )
[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([3, 2, 4],"float32"),Tensor([3],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [24, 3], input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:27 != input_axis_dim:2281701379.] (at /paddle/paddle/phi/infermeta/unary.cc:4451)

[Worker 1] Completed Task 268

[Worker 1] Processing Task 270: paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )
[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [512, 1024, 64, 64, 1024, 1024, 64, 64], input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:3840 != input_axis_dim:2281701379.] (at /paddle/paddle/phi/infermeta/unary.cc:4451)

[Worker 1] Completed Task 270

[Worker 1] Processing Task 277: paddle.sum(Tensor([1431655765, 3],"bool"), axis=1, keepdim=False, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.sum(Tensor([1431655765, 3],"bool"), axis=1, keepdim=False, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_sum(_object*, _object*, _object*)
1   sum_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, phi::DataType, bool)
2   paddle::experimental::sum(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, phi::DataType, bool)
3   void phi::SumKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
4   void phi::SumRawKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
5   void phi::CastCUDAKernelImpl<bool, long>(phi::GPUContext const&, phi::DenseTensor const&, phi::DataType, phi::DenseTensor*)
6   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  paddle::memory::allocation::Allocator::Allocate(unsigned long)
12  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
13  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 32.000000GB memory on GPU 0, 52.270508GB memory has been allocated and available memory is only 26.914368GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 1] Completed Task 277

[Worker 1] Processing Task 281: paddle.sum(Tensor([1431655765, 3],"bool"), axis=list[-1,], keepdim=False, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.sum(Tensor([1431655765, 3],"bool"), axis=list[-1,], keepdim=False, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_sum(_object*, _object*, _object*)
1   sum_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, phi::DataType, bool)
2   paddle::experimental::sum(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, phi::DataType, bool)
3   void phi::SumKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
4   void phi::SumRawKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
5   void phi::CastCUDAKernelImpl<bool, long>(phi::GPUContext const&, phi::DenseTensor const&, phi::DataType, phi::DenseTensor*)
6   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  paddle::memory::allocation::Allocator::Allocate(unsigned long)
12  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
13  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 32.000000GB memory on GPU 0, 52.270508GB memory has been allocated and available memory is only 26.914368GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 1] Completed Task 281

[Worker 1] Processing Task 291: paddle.sum(Tensor([2, 107374183, 4, 5],"bool"), axis=2, keepdim=True, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.sum(Tensor([2, 107374183, 4, 5],"bool"), axis=2, keepdim=True, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_sum(_object*, _object*, _object*)
1   sum_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, phi::DataType, bool)
2   paddle::experimental::sum(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, phi::DataType, bool)
3   void phi::SumKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
4   void phi::SumRawKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
5   void phi::CastCUDAKernelImpl<bool, long>(phi::GPUContext const&, phi::DenseTensor const&, phi::DataType, phi::DenseTensor*)
6   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  paddle::memory::allocation::Allocator::Allocate(unsigned long)
12  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
13  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 32.000000GB memory on GPU 0, 49.610352GB memory has been allocated and available memory is only 29.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 1] Completed Task 291

[Worker 1] Processing Task 294: paddle.sum(Tensor([2, 107374183, 4, 5],"bool"), axis=list[0,2,], keepdim=False, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.sum(Tensor([2, 107374183, 4, 5],"bool"), axis=list[0,2,], keepdim=False, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_sum(_object*, _object*, _object*)
1   sum_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, phi::DataType, bool)
2   paddle::experimental::sum(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, phi::DataType, bool)
3   void phi::SumKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
4   void phi::SumRawKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
5   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 4.000000GB memory on GPU 0, 77.612305GB memory has been allocated and available memory is only 1.572571GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 1] Completed Task 294

[Worker 1] Processing Task 309: paddle.sum(Tensor([2, 2147483648],"bool"), axis=0, keepdim=True, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.sum(Tensor([2, 2147483648],"bool"), axis=0, keepdim=True, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_sum(_object*, _object*, _object*)
1   sum_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, phi::DataType, bool)
2   paddle::experimental::sum(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, phi::DataType, bool)
3   void phi::SumKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
4   void phi::SumRawKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
5   void phi::CastCUDAKernelImpl<bool, long>(phi::GPUContext const&, phi::DenseTensor const&, phi::DataType, phi::DenseTensor*)
6   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  paddle::memory::allocation::Allocator::Allocate(unsigned long)
12  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
13  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 32.000000GB memory on GPU 0, 57.602539GB memory has been allocated and available memory is only 21.582336GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 1] Completed Task 309

[Worker 1] Processing Task 332: paddle.sum(Tensor([2, 3, 4, 178956971],"bool"), axis=2, keepdim=True, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.sum(Tensor([2, 3, 4, 178956971],"bool"), axis=2, keepdim=True, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_sum(_object*, _object*, _object*)
1   sum_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, phi::DataType, bool)
2   paddle::experimental::sum(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, phi::DataType, bool)
3   void phi::SumKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
4   void phi::SumRawKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
5   void phi::CastCUDAKernelImpl<bool, long>(phi::GPUContext const&, phi::DenseTensor const&, phi::DataType, phi::DenseTensor*)
6   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  paddle::memory::allocation::Allocator::Allocate(unsigned long)
12  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
13  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 32.000000GB memory on GPU 0, 49.610352GB memory has been allocated and available memory is only 29.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 1] Completed Task 332

[Worker 1] Processing Task 373: paddle.sum(Tensor([71582789, 3, 4, 5],"bool"), axis=-1, keepdim=False, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.sum(Tensor([71582789, 3, 4, 5],"bool"), axis=-1, keepdim=False, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_sum(_object*, _object*, _object*)
1   sum_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, phi::DataType, bool)
2   paddle::experimental::sum(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, phi::DataType, bool)
3   void phi::SumKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
4   void phi::SumRawKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
5   void phi::CastCUDAKernelImpl<bool, long>(phi::GPUContext const&, phi::DenseTensor const&, phi::DataType, phi::DenseTensor*)
6   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  paddle::memory::allocation::Allocator::Allocate(unsigned long)
12  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
13  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 32.000000GB memory on GPU 0, 48.008789GB memory has been allocated and available memory is only 31.176086GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 1] Completed Task 373

[Worker 1] Processing Task 378: paddle.sum(Tensor([71582789, 3, 4, 5],"bool"), axis=list[-1,], keepdim=False, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.sum(Tensor([71582789, 3, 4, 5],"bool"), axis=list[-1,], keepdim=False, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_sum(_object*, _object*, _object*)
1   sum_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, phi::DataType, bool)
2   paddle::experimental::sum(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, phi::DataType, bool)
3   void phi::SumKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
4   void phi::SumRawKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
5   void phi::CastCUDAKernelImpl<bool, long>(phi::GPUContext const&, phi::DenseTensor const&, phi::DataType, phi::DenseTensor*)
6   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  paddle::memory::allocation::Allocator::Allocate(unsigned long)
12  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
13  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 32.000000GB memory on GPU 0, 48.008789GB memory has been allocated and available memory is only 31.176086GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 1] Completed Task 378

[Worker 1] Processing Task 398: paddle.Tensor.chunk(Tensor([1, 1, 12096, 188633],"float32"), 2, axis=-1, )
[paddle error] paddle.Tensor.chunk(Tensor([1, 1, 12096, 188633],"float32"), 2, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 2, input(X)'s shape = [1, 1, 12096, 188633], Attr(dim) = 3.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 1] Completed Task 398

[Worker 1] Processing Task 402: paddle.Tensor.chunk(Tensor([1, 101, 22591103],"float32"), 2, axis=-1, )
[paddle error] paddle.Tensor.chunk(Tensor([1, 101, 22591103],"float32"), 2, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 2, input(X)'s shape = [1, 101, 22591103], Attr(dim) = 2.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 1] Completed Task 402

[Worker 1] Processing Task 406: paddle.Tensor.chunk(Tensor([13, 5484860, 32],"float32"), 8, axis=1, )
[paddle error] paddle.Tensor.chunk(Tensor([13, 5484860, 32],"float32"), 8, axis=1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 8, input(X)'s shape = [13, 5484860, 32], Attr(dim) = 1.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:4 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 1] Completed Task 406

[Worker 1] Processing Task 410: paddle.Tensor.chunk(Tensor([75304, 300, 101],"float32"), 16, )
[paddle error] paddle.Tensor.chunk(Tensor([75304, 300, 101],"float32"), 16, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 16, input(X)'s shape = [75304, 300, 101], Attr(dim) = 0.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:8 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 1] Completed Task 410

[Worker 1] Processing Task 414: paddle.Tensor.flatten(Tensor([1, 1, 12, 190141782],"float32"), start_axis=2, )
[paddle error] paddle.Tensor.flatten(Tensor([1, 1, 12, 190141782],"float32"), start_axis=2, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, 1, -2013265912], shape[2] = -2013265912.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265912 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 1] Completed Task 414

[Worker 1] Processing Task 418: paddle.Tensor.flatten(Tensor([1, 1, 2281701379],"int32"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.flatten(Tensor([1, 1, 2281701379],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265917], shape[0] = -2013265917.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265917 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 1] Completed Task 418

[Worker 1] Processing Task 421: paddle.Tensor.flatten(Tensor([1, 1, 6, 380283564],"float32"), start_axis=2, )
[paddle error] paddle.Tensor.flatten(Tensor([1, 1, 6, 380283564],"float32"), start_axis=2, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, 1, -2013265912], shape[2] = -2013265912.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265912 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 1] Completed Task 421

[Worker 1] Processing Task 425: paddle.Tensor.flatten(Tensor([1, 101, 22591103],"int32"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.flatten(Tensor([1, 101, 22591103],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265893], shape[0] = -2013265893.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265893 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 1] Completed Task 425

[Worker 1] Processing Task 429: paddle.Tensor.flatten(Tensor([1, 12, 190141782],"float32"), 1, )
[paddle error] paddle.Tensor.flatten(Tensor([1, 12, 190141782],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265912], shape[1] = -2013265912.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265912 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 1] Completed Task 429

[Worker 1] Processing Task 433: paddle.Tensor.flatten(Tensor([1, 18, 126761188],"float32"), 1, )
[paddle error] paddle.Tensor.flatten(Tensor([1, 18, 126761188],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265912], shape[1] = -2013265912.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265912 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 1] Completed Task 433

[Worker 1] Processing Task 437: paddle.Tensor.flatten(Tensor([1, 2281701379],"int32"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.flatten(Tensor([1, 2281701379],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265917], shape[0] = -2013265917.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265917 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 1] Completed Task 437

[Worker 1] Processing Task 440: paddle.Tensor.flatten(Tensor([1, 268435457, 4, 4],"float16"), start_axis=1, stop_axis=3, )
[paddle error] paddle.Tensor.flatten(Tensor([1, 268435457, 4, 4],"float16"), start_axis=1, stop_axis=3, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 268435457, 4, 4], X's size = 4294967312, 'shape' is [1, 16], the capacity of 'shape' is 16.
  [Hint: Expected capacity == in_size, but received capacity:16 != in_size:4294967312.] (at /paddle/paddle/phi/infermeta/unary.cc:2245)

[Worker 1] Completed Task 440

[Worker 1] Processing Task 445: paddle.Tensor.flatten(Tensor([1, 6, 95070891, 4],"float32"), start_axis=1, stop_axis=3, )
[paddle error] paddle.Tensor.flatten(Tensor([1, 6, 95070891, 4],"float32"), start_axis=1, stop_axis=3, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265912], shape[1] = -2013265912.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265912 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 1] Completed Task 445

[Worker 1] Processing Task 449: paddle.Tensor.flatten(Tensor([12, 18397, 76, 136],"float32"), )
[paddle error] paddle.Tensor.flatten(Tensor([12, 18397, 76, 136],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013150592], shape[0] = -2013150592.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013150592 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 1] Completed Task 449

[Worker 1] Processing Task 453: paddle.Tensor.flatten(Tensor([12, 294338, 19, 34],"float32"), )
[paddle error] paddle.Tensor.flatten(Tensor([12, 294338, 19, 34],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013259120], shape[0] = -2013259120.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013259120 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 1] Completed Task 453

[Worker 1] Processing Task 457: paddle.Tensor.flatten(Tensor([12, 4, 19, 2501866],"float32"), )
[paddle error] paddle.Tensor.flatten(Tensor([12, 4, 19, 2501866],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265504], shape[0] = -2013265504.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265504 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 1] Completed Task 457

[Worker 1] Processing Task 461: paddle.Tensor.flatten(Tensor([12, 4, 76, 625467],"float32"), )
[paddle error] paddle.Tensor.flatten(Tensor([12, 4, 76, 625467],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013263680], shape[0] = -2013263680.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013263680 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 1] Completed Task 461

[Worker 1] Processing Task 465: paddle.Tensor.flatten(Tensor([2, 3, 6, 15845149, 4],"float32"), )
[paddle error] paddle.Tensor.flatten(Tensor([2, 3, 6, 15845149, 4],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265840], shape[0] = -2013265840.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265840 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 1] Completed Task 465

[Worker 1] Processing Task 469: paddle.Tensor.flatten(Tensor([22152441, 103, 1],"int32"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.flatten(Tensor([22152441, 103, 1],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265873], shape[0] = -2013265873.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265873 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 1] Completed Task 469

[Worker 1] Processing Task 473: paddle.Tensor.flatten(Tensor([2277148, 1002],"int64"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.flatten(Tensor([2277148, 1002],"int64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265000], shape[0] = -2013265000.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265000 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 1] Completed Task 473

[Worker 1] Processing Task 478: paddle.Tensor.flatten(Tensor([2281701379],"int32"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.flatten(Tensor([2281701379],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265917], shape[0] = -2013265917.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265917 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 1] Completed Task 478

[Worker 1] Processing Task 481: paddle.Tensor.flatten(Tensor([22817014, 100],"float32"), 0, 1, )
[paddle error] paddle.Tensor.flatten(Tensor([22817014, 100],"float32"), 0, 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265896], shape[0] = -2013265896.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265896 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 1] Completed Task 481

[Worker 1] Processing Task 485: paddle.Tensor.flatten(Tensor([55189, 4, 76, 136],"float32"), )
[paddle error] paddle.Tensor.flatten(Tensor([55189, 4, 76, 136],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013233280], shape[0] = -2013233280.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013233280 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 1] Completed Task 485

[Worker 1] Processing Task 488: paddle.Tensor.gcd(x=Tensor([2281701379],"int32"), y=Tensor([1],"int32"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.gcd(x=Tensor([2281701379],"int32"), y=Tensor([1],"int32"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   not_equal_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::not_equal(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::NotEqualRawKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
3   bool* phi::DeviceContext::Alloc<bool>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 2.125000GB memory on GPU 0, 78.124023GB memory has been allocated and available memory is only 1.060852GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 1] Completed Task 488
[Worker 1] Received stop signal.

[Worker 2] Processing Task 2: paddle.argsort(Tensor([22817014, 100],"int64"), axis=1, stable=True, )
element 0 of tensors does not require grad and does not have a grad_fn
W0519 16:54:36.079674  4015 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 16:54:36.080646  4015 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.argsort(Tensor([22817014, 100],"int64"), axis=1, stable=True, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_argsort(_object*, _object*, _object*)
1   argsort_ad_func(paddle::Tensor const&, int, bool, bool)
2   paddle::experimental::argsort(paddle::Tensor const&, int, bool, bool)
3   void phi::ArgsortKernel<long, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, phi::DenseTensor*, phi::DenseTensor*)
4   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
5   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
6   paddle::memory::allocation::Allocator::Allocate(unsigned long)
7   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 69.610352GB memory has been allocated and available memory is only 9.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 2] Completed Task 2

[Worker 2] Processing Task 6: paddle.argsort(Tensor([3, 4, 190141782],"int64"), axis=2, stable=True, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.argsort(Tensor([3, 4, 190141782],"int64"), axis=2, stable=True, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_argsort(_object*, _object*, _object*)
1   argsort_ad_func(paddle::Tensor const&, int, bool, bool)
2   paddle::experimental::argsort(paddle::Tensor const&, int, bool, bool)
3   void phi::ArgsortKernel<long, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, phi::DenseTensor*, phi::DenseTensor*)
4   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
5   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
6   paddle::memory::allocation::Allocator::Allocate(unsigned long)
7   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 69.610352GB memory has been allocated and available memory is only 9.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 2] Completed Task 6

[Worker 2] Processing Task 8: paddle.argsort(Tensor([35651585, 64],"int64"), axis=-1, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.argsort(Tensor([35651585, 64],"int64"), axis=-1, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_argsort(_object*, _object*, _object*)
1   argsort_ad_func(paddle::Tensor const&, int, bool, bool)
2   paddle::experimental::argsort(paddle::Tensor const&, int, bool, bool)
3   void phi::ArgsortKernel<long, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, phi::DenseTensor*, phi::DenseTensor*)
4   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
5   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
6   paddle::memory::allocation::Allocator::Allocate(unsigned long)
7   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 69.610352GB memory has been allocated and available memory is only 9.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 2] Completed Task 8

[Worker 2] Processing Task 20: paddle.chunk(Tensor([1, 1, 64, 67108865],"float16"), 2, axis=-1, )
[paddle error] paddle.chunk(Tensor([1, 1, 64, 67108865],"float16"), 2, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 2, input(X)'s shape = [1, 1, 64, 67108865], Attr(dim) = 3.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 2] Completed Task 20

[Worker 2] Processing Task 26: paddle.chunk(Tensor([13, 5484860, 32],"float32"), 3, axis=1, )
[paddle error] paddle.chunk(Tensor([13, 5484860, 32],"float32"), 3, axis=1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 3, input(X)'s shape = [13, 5484860, 32], Attr(dim) = 1.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:2 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 2] Completed Task 26

[Worker 2] Processing Task 30: paddle.chunk(Tensor([4, 139265, 64, 64],"float32"), 3, axis=1, )
[paddle error] paddle.chunk(Tensor([4, 139265, 64, 64],"float32"), 3, axis=1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 3, input(X)'s shape = [4, 139265, 64, 64], Attr(dim) = 1.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:2 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 2] Completed Task 30

[Worker 2] Processing Task 34: paddle.chunk(Tensor([8192, 278529],"float32"), 2, axis=-1, )
[paddle error] paddle.chunk(Tensor([8192, 278529],"float32"), 2, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 2, input(X)'s shape = [8192, 278529], Attr(dim) = 1.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 2] Completed Task 34

[Worker 2] Processing Task 37: paddle.chunk(x=Tensor([3, 1431655766],"float16"), chunks=3, axis=-1, )
[paddle error] paddle.chunk(x=Tensor([3, 1431655766],"float16"), chunks=3, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 3, input(X)'s shape = [3, 1431655766], Attr(dim) = 1.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:2 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 2] Completed Task 37

[Worker 2] Processing Task 60: paddle.flatten(Tensor([1, 1024, 1, 4194304],"float32"), 1, )
[paddle error] paddle.flatten(Tensor([1, 1024, 1, 4194304],"float32"), 1, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 1024, 1, 4194304], X's size = 4294967296, 'shape' is [1, 0], the capacity of 'shape' is 1024.
  [Hint: Expected capacity == in_size, but received capacity:1024 != in_size:4294967296.] (at /paddle/paddle/phi/infermeta/unary.cc:2245)

[Worker 2] Completed Task 60

[Worker 2] Processing Task 64: paddle.flatten(Tensor([1, 16, 5, 53687092],"float32"), 1, )
[paddle error] paddle.flatten(Tensor([1, 16, 5, 53687092],"float32"), 1, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 16, 5, 53687092], X's size = 4294967360, 'shape' is [1, 64], the capacity of 'shape' is 64.
  [Hint: Expected capacity == in_size, but received capacity:64 != in_size:4294967360.] (at /paddle/paddle/phi/infermeta/unary.cc:2245)

[Worker 2] Completed Task 64

[Worker 2] Processing Task 68: paddle.flatten(Tensor([1, 2048, 1, 2097152],"float32"), 1, )
[paddle error] paddle.flatten(Tensor([1, 2048, 1, 2097152],"float32"), 1, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 2048, 1, 2097152], X's size = 4294967296, 'shape' is [1, 0], the capacity of 'shape' is 2048.
  [Hint: Expected capacity == in_size, but received capacity:2048 != in_size:4294967296.] (at /paddle/paddle/phi/infermeta/unary.cc:2245)

[Worker 2] Completed Task 68

[Worker 2] Processing Task 72: paddle.flatten(Tensor([1, 8, 7, 76695845],"float32"), start_axis=1, stop_axis=-1, )
[paddle error] paddle.flatten(Tensor([1, 8, 7, 76695845],"float32"), start_axis=1, stop_axis=-1, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 8, 7, 76695845], X's size = 4294967320, 'shape' is [1, 24], the capacity of 'shape' is 24.
  [Hint: Expected capacity == in_size, but received capacity:24 != in_size:4294967320.] (at /paddle/paddle/phi/infermeta/unary.cc:2245)

[Worker 2] Completed Task 72

[Worker 2] Processing Task 76: paddle.flatten(Tensor([1073741824, 4],"float16"), )
[paddle error] paddle.flatten(Tensor([1073741824, 4],"float16"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1073741824, 4], X's size = 4294967296, 'shape' is [0], the capacity of 'shape' is 1073741824.
  [Hint: Expected capacity == in_size, but received capacity:1073741824 != in_size:4294967296.] (at /paddle/paddle/phi/infermeta/unary.cc:2245)

[Worker 2] Completed Task 76

[Worker 2] Processing Task 81: paddle.flatten(Tensor([2, 1024, 1, 2097152],"float32"), start_axis=1, stop_axis=-1, )
[paddle error] paddle.flatten(Tensor([2, 1024, 1, 2097152],"float32"), start_axis=1, stop_axis=-1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483648], shape[1] = -2147483648.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483648 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 2] Completed Task 81

[Worker 2] Processing Task 85: paddle.flatten(Tensor([2, 16, 5, 26843546],"float32"), 1, )
[paddle error] paddle.flatten(Tensor([2, 16, 5, 26843546],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483616], shape[1] = -2147483616.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483616 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 2] Completed Task 85

[Worker 2] Processing Task 88: paddle.flatten(Tensor([2, 2048, 1048576, 1],"float32"), 1, )
[paddle error] paddle.flatten(Tensor([2, 2048, 1048576, 1],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483648], shape[1] = -2147483648.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483648 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 2] Completed Task 88

[Worker 2] Processing Task 92: paddle.flatten(Tensor([2, 2147483648],"bfloat16"), )
[paddle error] paddle.flatten(Tensor([2, 2147483648],"bfloat16"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [2, 2147483648], X's size = 4294967296, 'shape' is [0], the capacity of 'shape' is 2.
  [Hint: Expected capacity == in_size, but received capacity:2 != in_size:4294967296.] (at /paddle/paddle/phi/infermeta/unary.cc:2245)

[Worker 2] Completed Task 92

[Worker 2] Processing Task 97: paddle.flatten(Tensor([2, 21474837, 10, 10],"float32"), 1, )
[paddle error] paddle.flatten(Tensor([2, 21474837, 10, 10],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483596], shape[1] = -2147483596.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483596 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 2] Completed Task 97

[Worker 2] Processing Task 103: paddle.flatten(Tensor([2, 429496730, 5],"float32"), )
[paddle error] paddle.flatten(Tensor([2, 429496730, 5],"float32"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [2, 429496730, 5], X's size = 4294967300, 'shape' is [4], the capacity of 'shape' is 4.
  [Hint: Expected capacity == in_size, but received capacity:4 != in_size:4294967300.] (at /paddle/paddle/phi/infermeta/unary.cc:2245)

[Worker 2] Completed Task 103

[Worker 2] Processing Task 108: paddle.flatten(Tensor([2, 512, 4194304, 1],"float32"), 1, )
[paddle error] paddle.flatten(Tensor([2, 512, 4194304, 1],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483648], shape[1] = -2147483648.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483648 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 2] Completed Task 108

[Worker 2] Processing Task 112: paddle.flatten(Tensor([2, 6, 35791395, 10],"float32"), 1, )
[paddle error] paddle.flatten(Tensor([2, 6, 35791395, 10],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [2, -2147483596], shape[1] = -2147483596.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483596 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 2] Completed Task 112

[Worker 2] Processing Task 116: paddle.flatten(Tensor([268435457, 4, 2],"float64"), )
[paddle error] paddle.flatten(Tensor([268435457, 4, 2],"float64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2147483640], shape[0] = -2147483640.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483640 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 2] Completed Task 116

[Worker 2] Processing Task 120: paddle.flatten(Tensor([3, 4, 178956971],"float64"), )
[paddle error] paddle.flatten(Tensor([3, 4, 178956971],"float64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2147483644], shape[0] = -2147483644.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483644 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 2] Completed Task 120

[Worker 2] Processing Task 125: paddle.flatten(Tensor([3, 715827883, 2],"int32"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.flatten(Tensor([3, 715827883, 2],"int32"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [3, 715827883, 2], X's size = 4294967298, 'shape' is [2], the capacity of 'shape' is 2.
  [Hint: Expected capacity == in_size, but received capacity:2 != in_size:4294967298.] (at /paddle/paddle/phi/infermeta/unary.cc:2245)

[Worker 2] Completed Task 125

[Worker 2] Processing Task 127: paddle.flatten(Tensor([4, 7, 76695845],"float64"), )
[paddle error] paddle.flatten(Tensor([4, 7, 76695845],"float64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2147483636], shape[0] = -2147483636.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2147483636 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 2] Completed Task 127

[Worker 2] Processing Task 131: paddle.flatten(Tensor([536870912, 4, 2],"float32"), )
[paddle error] paddle.flatten(Tensor([536870912, 4, 2],"float32"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [536870912, 4, 2], X's size = 4294967296, 'shape' is [0], the capacity of 'shape' is 536870912.
  [Hint: Expected capacity == in_size, but received capacity:536870912 != in_size:4294967296.] (at /paddle/paddle/phi/infermeta/unary.cc:2245)

[Worker 2] Completed Task 131

[Worker 2] Processing Task 137: paddle.frac(Tensor([1431655766, 3],"float16"), )
[paddle error] paddle.frac(Tensor([1431655766, 3],"float16"), ) 
 The data type of input must be one of ['int32', 'int64', 'float32', 'float64'], but got paddle.float16
[Worker 2] Completed Task 137

[Worker 2] Processing Task 148: paddle.lerp(Tensor([2, 1],"float32"), Tensor([2, 1140850690],"float32"), Tensor([2, 1],"float32"), )
W0519 17:58:24.212899  4015 backward.cc:437] While running Node (LerpGradNode) raises an EnforceNotMet exception
[paddle error] paddle.lerp(Tensor([2, 1],"float32"), Tensor([2, 1140850690],"float32"), Tensor([2, 1],"float32"), ) 
 (InvalidArgument) numel is expected to be greater than or equal 0, but received -2013265916.
  [Hint: Expected numel >= 0, but received numel:-2013265916 < 0:0.] (at /paddle/paddle/phi/backends/gpu/gpu_launch_config.h:110)

[Worker 2] Completed Task 148

[Worker 2] Processing Task 154: paddle.linalg.cond(Tensor([2, 126761188, 3, 3],"float32"), -math.inf, )
one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [2, 126761188, 3, 3]], which is output 0 of LinalgInvExBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
[paddle error] paddle.linalg.cond(Tensor([2, 126761188, 3, 3],"float32"), -math.inf, ) 
 (PreconditionNotMet) For batch [15255433]: U(3, 3) is zero, singular U. Please check the matrix value and change it to a non-singular matrix
  [Hint: Expected info[i] == 0, but received info[i]:3 != 0:0.] (at /paddle/paddle/phi/kernels/funcs/matrix_inverse.cu:117)

[Worker 2] Completed Task 154

[Worker 2] Processing Task 157: paddle.linalg.cond(Tensor([63380594, 4, 3, 3],"float32"), -1, )
one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [63380594, 4, 3, 3]], which is output 0 of LinalgInvExBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
[paddle error] paddle.linalg.cond(Tensor([63380594, 4, 3, 3],"float32"), -1, ) 
 (PreconditionNotMet) For batch [50688639]: U(3, 3) is zero, singular U. Please check the matrix value and change it to a non-singular matrix
  [Hint: Expected info[i] == 0, but received info[i]:3 != 0:0.] (at /paddle/paddle/phi/kernels/funcs/matrix_inverse.cu:117)

[Worker 2] Completed Task 157

[Worker 2] Processing Task 161: paddle.linalg.det(Tensor([3, 30422686, 5, 5],"float32"), )
W0519 18:01:06.771013  4015 backward.cc:437] While running Node (DetGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.det(Tensor([3, 30422686, 5, 5],"float32"), ) 
 (PreconditionNotMet) For batch [15462833]: U(5, 5) is zero, singular U. Please check the matrix value and change it to a non-singular matrix
  [Hint: Expected info[i] == 0, but received info[i]:5 != 0:0.] (at /paddle/paddle/phi/kernels/funcs/matrix_inverse.cu:117)

[Worker 2] Completed Task 161

[Worker 2] Processing Task 173: paddle.nanmean(Tensor([1431655765, 3],"float32"), 0, True, )
W0519 18:07:49.818536  4544 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:07:49.819782  4544 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([1431655765, 3],"float32"), 0, True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747649272 (unix time) try "date -d @1747649272" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x11c0) received by PID 4544 (TID 0x7f98c6a93740) from PID 4544 ***]

[Worker 2] Started on GPU 2

[Worker 2] Processing Task 177: paddle.nanmean(Tensor([2, 107374183, 4, 5],"float32"), list[], False, )
W0519 18:09:20.045222  4773 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:09:20.046227  4773 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([2, 107374183, 4, 5],"float32"), list[], False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747649362 (unix time) try "date -d @1747649362" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x12a5) received by PID 4773 (TID 0x7f98c6a93740) from PID 4773 ***]

[Worker 2] Started on GPU 2

[Worker 2] Processing Task 179: paddle.nanmean(Tensor([2, 107374183, 4, 5],"float32"), None, False, )
W0519 18:10:34.317523  5000 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:10:34.318584  5000 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([2, 107374183, 4, 5],"float32"), None, False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747649436 (unix time) try "date -d @1747649436" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1388) received by PID 5000 (TID 0x7f98c6a93740) from PID 5000 ***]

[Worker 2] Started on GPU 2

[Worker 2] Processing Task 181: paddle.nanmean(Tensor([2, 2147483648],"float32"), 1, False, )
W0519 18:11:58.541247  5153 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:11:58.542309  5153 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([2, 2147483648],"float32"), 1, False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747649520 (unix time) try "date -d @1747649520" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1421) received by PID 5153 (TID 0x7f98c6a93740) from PID 5153 ***]

[Worker 2] Started on GPU 2

[Worker 2] Processing Task 186: paddle.nanmean(Tensor([2, 3, 143165577, 5],"float32"), 2, True, )
W0519 18:13:26.533596  5456 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:13:26.534610  5456 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([2, 3, 143165577, 5],"float32"), 2, True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747649608 (unix time) try "date -d @1747649608" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1550) received by PID 5456 (TID 0x7f98c6a93740) from PID 5456 ***]

[Worker 2] Started on GPU 2

[Worker 2] Processing Task 190: paddle.nanmean(Tensor([2, 3, 143165577, 5],"float32"), None, False, )
W0519 18:14:58.278030  5758 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:14:58.279115  5758 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([2, 3, 143165577, 5],"float32"), None, False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747649700 (unix time) try "date -d @1747649700" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x167e) received by PID 5758 (TID 0x7f98c6a93740) from PID 5758 ***]

[Worker 2] Started on GPU 2

[Worker 2] Processing Task 194: paddle.nanmean(Tensor([2, 3, 4, 178956971],"float32"), list[], False, )
W0519 18:16:35.632395  6060 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:16:35.633421  6060 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([2, 3, 4, 178956971],"float32"), list[], False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747649797 (unix time) try "date -d @1747649797" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x17ac) received by PID 6060 (TID 0x7f98c6a93740) from PID 6060 ***]

[Worker 2] Started on GPU 2

[Worker 2] Processing Task 198: paddle.nanmean(Tensor([3, 1431655765],"float32"), axis=None, )
W0519 18:18:03.245416  6366 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:18:03.246483  6366 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([3, 1431655765],"float32"), axis=None, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747649885 (unix time) try "date -d @1747649885" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x18de) received by PID 6366 (TID 0x7f98c6a93740) from PID 6366 ***]

[Worker 2] Started on GPU 2

[Worker 2] Processing Task 202: paddle.nanmean(Tensor([71582789, 3, 4, 5],"float32"), list[0,1,2,3,], False, )
W0519 18:19:31.667016  6670 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:19:31.668000  6670 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([71582789, 3, 4, 5],"float32"), list[0,1,2,3,], False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747649973 (unix time) try "date -d @1747649973" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1a0e) received by PID 6670 (TID 0x7f98c6a93740) from PID 6670 ***]

[Worker 2] Started on GPU 2

[Worker 2] Processing Task 206: paddle.nanmean(Tensor([71582789, 3, 4, 5],"float32"), tuple(0,2,), False, )
W0519 18:20:58.194672  6974 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:20:58.195662  6974 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nanmean(Tensor([71582789, 3, 4, 5],"float32"), tuple(0,2,), False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747650060 (unix time) try "date -d @1747650060" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1b3e) received by PID 6974 (TID 0x7f98c6a93740) from PID 6974 ***]

[Worker 2] Started on GPU 2

[Worker 2] Processing Task 213: paddle.nn.functional.normalize(Tensor([2281701379],"float32"), axis=0, epsilon=1e-10, )
W0519 18:24:31.114333  7508 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0519 18:24:31.115311  7508 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.nn.functional.normalize(Tensor([2281701379],"float32"), axis=0, epsilon=1e-10, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 5.e+09
Max relative difference: 1.378911e+14
 x: array([ 1.442904e+09,  1.589111e+09,  4.722245e+09, ..., -1.912361e+09,
       -4.534847e+09,  2.990911e+09], dtype=float32)
 y: array([ 1.046408e-05,  1.152439e-05,  3.424620e-05, ..., -1.386864e-05,
       -3.288716e-05,  2.169039e-05], dtype=float32)
[Worker 2] Completed Task 213

[Worker 2] Processing Task 218: paddle.nn.functional.normalize(x=Tensor([1, 2281701379],"float32"), axis=-1, )
[accuracy error] paddle.nn.functional.normalize(x=Tensor([1, 2281701379],"float32"), axis=-1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 5.e+11
Max relative difference: 1.3789216e+16
 x: array([[ 1.504387e+11,  5.790017e+10,  3.437902e+11, ..., -8.426852e+10,
        -2.133166e+11, -4.355962e+10]], dtype=float32)
 y: array([[ 1.090988e-05,  4.198946e-06,  2.493182e-05, ..., -6.111190e-06,
        -1.546982e-05, -3.158963e-06]], dtype=float32)
[Worker 2] Completed Task 218

[Worker 2] Processing Task 222: paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 2147483649],"float16"), 0, 1e-06, False, None, )
One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 2147483649],"float16"), 0, 1e-06, False, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([0., 0.], dtype=float16)
 y: array([inf, inf], dtype=float16)
[Worker 2] Completed Task 222

[Worker 2] Processing Task 225: paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 2147483649],"float16"), math.inf, 1e-06, False, None, )
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 2147483649],"float16"), math.inf, 1e-06, False, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y -inf location mismatch:
 x: array([-inf, -inf], dtype=float16)
 y: array([1., 1.], dtype=float16)
[Worker 2] Completed Task 225

[Worker 2] Processing Task 228: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), 0, 1e-06, False, None, )
One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), 0, 1e-06, False, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 2.2817014e+09
Max relative difference: 1.
 x: array(0., dtype=float32)
 y: array(2.281701e+09, dtype=float32)
[Worker 2] Completed Task 228

[Worker 2] Processing Task 230: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), 1, 1e-06, False, None, )
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), 1, 1e-06, False, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 7.605567e+08
Max relative difference: 1.
 x: array(0., dtype=float32)
 y: array(7.605567e+08, dtype=float32)
[Worker 2] Completed Task 230

[Worker 2] Processing Task 233: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), -1, 1e-06, True, None, )
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), -1, 1e-06, True, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([inf], dtype=float32)
 y: array([1.076916e-11], dtype=float32)
[Worker 2] Completed Task 233

[Worker 2] Processing Task 236: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), math.inf, 1e-06, False, None, )
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), math.inf, 1e-06, False, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y -inf location mismatch:
 x: array(-inf, dtype=float32)
 y: array(0.999987, dtype=float32)
[Worker 2] Completed Task 236

[Worker 2] Processing Task 239: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), -math.inf, 1e-06, True, None, )
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), -math.inf, 1e-06, True, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([inf], dtype=float32)
 y: array([6.908749e-10], dtype=float32)
[Worker 2] Completed Task 239

[Worker 2] Processing Task 242: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), 1, 1e-06, False, None, )
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), 1, 1e-06, False, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array(0.495, dtype=float16)
 y: array(inf, dtype=float16)
[Worker 2] Completed Task 242

[Worker 2] Processing Task 245: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), -1, 1e-06, True, None, )
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), -1, 1e-06, True, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 0.2695
Max relative difference: inf
 x: array([0.2695], dtype=float16)
 y: array([0.], dtype=float16)
[Worker 2] Completed Task 245

[Worker 2] Processing Task 248: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), math.inf, 1e-06, False, None, )
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), math.inf, 1e-06, False, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 0.4272
Max relative difference: 0.4272
 x: array(0.5728, dtype=float16)
 y: array(1., dtype=float16)
[Worker 2] Completed Task 248

[Worker 2] Processing Task 251: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), -math.inf, 1e-06, True, None, )
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), -math.inf, 1e-06, True, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 0.02466
Max relative difference: inf
 x: array([0.02466], dtype=float16)
 y: array([6.e-08], dtype=float16)
[Worker 2] Completed Task 251

[Worker 2] Processing Task 254: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", )
[accuracy error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 1.5
Max relative difference: inf
 x: array(1.5, dtype=float16)
 y: array(0., dtype=float16)
[Worker 2] Completed Task 254

[Worker 2] Processing Task 258: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=False, reduction="none", name=None, )
[accuracy error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=False, reduction="none", name=None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 0.3
Max relative difference: inf
 x: array([0.3, 0.3, 0.3, 0.3, 0.3], dtype=float16)
 y: array([0., 0., 0., 0., 0.], dtype=float16)
[Worker 2] Completed Task 258

[Worker 2] Processing Task 262: paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([10, 15],"float32"),Tensor([15],"float32"),], )
[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([10, 15],"float32"),Tensor([15],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [150, 15], input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:165 != input_axis_dim:2281701379.] (at /paddle/paddle/phi/infermeta/unary.cc:4451)

[Worker 2] Completed Task 262

[Worker 2] Processing Task 263: paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([10, 3],"float32"),], )
[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([10, 3],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [30, 0], input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:30 != input_axis_dim:2281701379.] (at /paddle/paddle/phi/infermeta/unary.cc:4451)

[Worker 2] Completed Task 263

[Worker 2] Processing Task 264: paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )
[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [128, 256, 16, 16, 256, 256, 16, 16], input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:960 != input_axis_dim:2281701379.] (at /paddle/paddle/phi/infermeta/unary.cc:4451)

[Worker 2] Completed Task 264

[Worker 2] Processing Task 265: paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([2, 2],"float32"),Tensor([2],"float32"),], )
[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([2, 2],"float32"),Tensor([2],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [4, 2], input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:6 != input_axis_dim:2281701379.] (at /paddle/paddle/phi/infermeta/unary.cc:4451)

[Worker 2] Completed Task 265

[Worker 2] Processing Task 266: paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([3, 2, 4, 4, 4],"float32"),Tensor([3],"float32"),], )
[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([3, 2, 4, 4, 4],"float32"),Tensor([3],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [384, 3], input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:387 != input_axis_dim:2281701379.] (at /paddle/paddle/phi/infermeta/unary.cc:4451)

[Worker 2] Completed Task 266

[Worker 2] Processing Task 267: paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([3, 2, 4, 4],"float32"),Tensor([3],"float32"),], )
[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([3, 2, 4, 4],"float32"),Tensor([3],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [96, 3], input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:99 != input_axis_dim:2281701379.] (at /paddle/paddle/phi/infermeta/unary.cc:4451)

[Worker 2] Completed Task 267

[Worker 2] Processing Task 269: paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )
[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [384, 768, 48, 48, 768, 768, 48, 48], input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:2880 != input_axis_dim:2281701379.] (at /paddle/paddle/phi/infermeta/unary.cc:4451)

[Worker 2] Completed Task 269

[Worker 2] Processing Task 271: paddle.outer(Tensor([2281701379],"float32"), Tensor([1],"float32"), )
[paddle error] paddle.outer(Tensor([2281701379],"float32"), Tensor([1],"float32"), ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /paddle/paddle/phi/core/dense_tensor.cc:113)

[Worker 2] Completed Task 271

[Worker 2] Processing Task 295: paddle.sum(Tensor([2, 107374183, 4, 5],"bool"), axis=list[-1,], keepdim=False, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.sum(Tensor([2, 107374183, 4, 5],"bool"), axis=list[-1,], keepdim=False, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_sum(_object*, _object*, _object*)
1   sum_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, phi::DataType, bool)
2   paddle::experimental::sum(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, phi::DataType, bool)
3   void phi::SumKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
4   void phi::SumRawKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
5   void phi::CastCUDAKernelImpl<bool, long>(phi::GPUContext const&, phi::DenseTensor const&, phi::DataType, phi::DenseTensor*)
6   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  paddle::memory::allocation::Allocator::Allocate(unsigned long)
12  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
13  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 32.000000GB memory on GPU 0, 48.008789GB memory has been allocated and available memory is only 31.176086GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 2] Completed Task 295

[Worker 2] Processing Task 312: paddle.sum(Tensor([2, 2147483648],"bool"), axis=list[0,], keepdim=True, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.sum(Tensor([2, 2147483648],"bool"), axis=list[0,], keepdim=True, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_sum(_object*, _object*, _object*)
1   sum_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, phi::DataType, bool)
2   paddle::experimental::sum(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, phi::DataType, bool)
3   void phi::SumKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
4   void phi::SumRawKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
5   void phi::CastCUDAKernelImpl<bool, long>(phi::GPUContext const&, phi::DenseTensor const&, phi::DataType, phi::DenseTensor*)
6   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  paddle::memory::allocation::Allocator::Allocate(unsigned long)
12  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
13  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 32.000000GB memory on GPU 0, 57.602539GB memory has been allocated and available memory is only 21.582336GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 2] Completed Task 312

[Worker 2] Processing Task 340: paddle.sum(Tensor([2, 3, 4, 178956971],"bool"), axis=tuple(0,2,), keepdim=False, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.sum(Tensor([2, 3, 4, 178956971],"bool"), axis=tuple(0,2,), keepdim=False, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_sum(_object*, _object*, _object*)
1   sum_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, phi::DataType, bool)
2   paddle::experimental::sum(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, phi::DataType, bool)
3   void phi::SumKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
4   void phi::SumRawKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
5   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 4.000000GB memory on GPU 0, 77.612305GB memory has been allocated and available memory is only 1.572571GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 2] Completed Task 340

[Worker 2] Processing Task 358: paddle.sum(Tensor([3, 4, 357913942],"int32"), axis=0, keepdim=True, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.sum(Tensor([3, 4, 357913942],"int32"), axis=0, keepdim=True, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_sum(_object*, _object*, _object*)
1   sum_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, phi::DataType, bool)
2   paddle::experimental::sum(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, phi::DataType, bool)
3   void phi::SumKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, bool, phi::DenseTensor*)
4   void phi::SumRawKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, bool, bool, phi::DataType, phi::DenseTensor*)
5   void phi::CastCUDAKernelImpl<int, long>(phi::GPUContext const&, phi::DenseTensor const&, phi::DataType, phi::DenseTensor*)
6   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  paddle::memory::allocation::Allocator::Allocate(unsigned long)
12  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
13  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 32.000000GB memory on GPU 0, 76.276367GB memory has been allocated and available memory is only 2.908508GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 2] Completed Task 358

[Worker 2] Processing Task 395: paddle.Tensor.chunk(Tensor([1, 1, 1, 2281701379],"float32"), 4, axis=-1, )
[paddle error] paddle.Tensor.chunk(Tensor([1, 1, 1, 2281701379],"float32"), 4, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 4, input(X)'s shape = [1, 1, 1, 2281701379], Attr(dim) = 3.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:3 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 2] Completed Task 395

[Worker 2] Processing Task 399: paddle.Tensor.chunk(Tensor([1, 1, 2281701379],"float32"), 2, axis=-1, )
[paddle error] paddle.Tensor.chunk(Tensor([1, 1, 2281701379],"float32"), 2, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 2, input(X)'s shape = [1, 1, 2281701379], Attr(dim) = 2.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 2] Completed Task 399

[Worker 2] Processing Task 403: paddle.Tensor.chunk(Tensor([1, 102, 1, 22369622],"float32"), 4, axis=-1, )
[paddle error] paddle.Tensor.chunk(Tensor([1, 102, 1, 22369622],"float32"), 4, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 4, input(X)'s shape = [1, 102, 1, 22369622], Attr(dim) = 3.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:2 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 2] Completed Task 403

[Worker 2] Processing Task 407: paddle.Tensor.chunk(Tensor([1901418, 300, 4],"float32"), 4, )
[paddle error] paddle.Tensor.chunk(Tensor([1901418, 300, 4],"float32"), 4, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 4, input(X)'s shape = [1901418, 300, 4], Attr(dim) = 0.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:2 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 2] Completed Task 407

[Worker 2] Processing Task 411: paddle.Tensor.chunk(Tensor([950709, 300, 8],"float32"), 8, )
[paddle error] paddle.Tensor.chunk(Tensor([950709, 300, 8],"float32"), 8, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 8, input(X)'s shape = [950709, 300, 8], Attr(dim) = 0.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:5 != 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:4504)

[Worker 2] Completed Task 411

[Worker 2] Processing Task 416: paddle.Tensor.flatten(Tensor([1, 1, 20, 114085069],"float32"), start_axis=2, )
[paddle error] paddle.Tensor.flatten(Tensor([1, 1, 20, 114085069],"float32"), start_axis=2, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, 1, -2013265916], shape[2] = -2013265916.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265916 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 2] Completed Task 416

[Worker 2] Processing Task 420: paddle.Tensor.flatten(Tensor([1, 1, 35651585, 64],"float32"), start_axis=2, )
[paddle error] paddle.Tensor.flatten(Tensor([1, 1, 35651585, 64],"float32"), start_axis=2, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, 1, -2013265856], shape[2] = -2013265856.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265856 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 2] Completed Task 420

[Worker 2] Processing Task 424: paddle.Tensor.flatten(Tensor([1, 10, 228170138],"int32"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.flatten(Tensor([1, 10, 228170138],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265916], shape[0] = -2013265916.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265916 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 2] Completed Task 424

[Worker 2] Processing Task 427: paddle.Tensor.flatten(Tensor([1, 103, 22152441],"int32"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.flatten(Tensor([1, 103, 22152441],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265873], shape[0] = -2013265873.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265873 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 2] Completed Task 427

[Worker 2] Processing Task 431: paddle.Tensor.flatten(Tensor([1, 144, 15845149],"float32"), 1, )
[paddle error] paddle.Tensor.flatten(Tensor([1, 144, 15845149],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265840], shape[1] = -2013265840.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265840 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 2] Completed Task 431

[Worker 2] Processing Task 435: paddle.Tensor.flatten(Tensor([1, 2281701379, 1],"int32"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.flatten(Tensor([1, 2281701379, 1],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265917], shape[0] = -2013265917.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265917 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 2] Completed Task 435

[Worker 2] Processing Task 438: paddle.Tensor.flatten(Tensor([1, 2281701379],"int64"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.flatten(Tensor([1, 2281701379],"int64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265917], shape[0] = -2013265917.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265917 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 2] Completed Task 438

[Worker 2] Processing Task 442: paddle.Tensor.flatten(Tensor([1, 6, 178956971, 4],"float16"), start_axis=1, stop_axis=3, )
[paddle error] paddle.Tensor.flatten(Tensor([1, 6, 178956971, 4],"float16"), start_axis=1, stop_axis=3, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 6, 178956971, 4], X's size = 4294967304, 'shape' is [1, 8], the capacity of 'shape' is 8.
  [Hint: Expected capacity == in_size, but received capacity:8 != in_size:4294967304.] (at /paddle/paddle/phi/infermeta/unary.cc:2245)

[Worker 2] Completed Task 442

[Worker 2] Processing Task 447: paddle.Tensor.flatten(Tensor([10, 228170138],"float32"), 0, 1, )
[paddle error] paddle.Tensor.flatten(Tensor([10, 228170138],"float32"), 0, 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265916], shape[0] = -2013265916.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265916 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 2] Completed Task 447

[Worker 2] Processing Task 451: paddle.Tensor.flatten(Tensor([12, 19, 34, 294338],"float32"), )
[paddle error] paddle.Tensor.flatten(Tensor([12, 19, 34, 294338],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013259120], shape[0] = -2013259120.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013259120 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 2] Completed Task 451

[Worker 2] Processing Task 455: paddle.Tensor.flatten(Tensor([12, 38, 68, 73585],"float32"), )
[paddle error] paddle.Tensor.flatten(Tensor([12, 38, 68, 73585],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013243616], shape[0] = -2013243616.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013243616 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 2] Completed Task 455

[Worker 2] Processing Task 459: paddle.Tensor.flatten(Tensor([12, 4, 38, 1250933],"float32"), )
[paddle error] paddle.Tensor.flatten(Tensor([12, 4, 38, 1250933],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265504], shape[0] = -2013265504.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265504 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 2] Completed Task 459

[Worker 2] Processing Task 463: paddle.Tensor.flatten(Tensor([12, 73585, 38, 68],"float32"), )
[paddle error] paddle.Tensor.flatten(Tensor([12, 73585, 38, 68],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013243616], shape[0] = -2013243616.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013243616 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 2] Completed Task 463

[Worker 2] Processing Task 467: paddle.Tensor.flatten(Tensor([2, 7922575, 6, 6, 4],"float32"), )
[paddle error] paddle.Tensor.flatten(Tensor([2, 7922575, 6, 6, 4],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265696], shape[0] = -2013265696.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265696 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 2] Completed Task 467

[Worker 2] Processing Task 471: paddle.Tensor.flatten(Tensor([22591103, 101, 1],"int32"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.flatten(Tensor([22591103, 101, 1],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265893], shape[0] = -2013265893.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265893 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 2] Completed Task 471

[Worker 2] Processing Task 475: paddle.Tensor.flatten(Tensor([2279422, 1001],"int64"), )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.flatten(Tensor([2279422, 1001],"int64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265874], shape[0] = -2013265874.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265874 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 2] Completed Task 475

[Worker 2] Processing Task 480: paddle.Tensor.flatten(Tensor([228170138, 10],"float32"), 0, 1, )
[paddle error] paddle.Tensor.flatten(Tensor([228170138, 10],"float32"), 0, 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265916], shape[0] = -2013265916.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265916 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 2] Completed Task 480

[Worker 2] Processing Task 484: paddle.Tensor.flatten(Tensor([5281717, 3, 6, 6, 4],"float32"), )
[paddle error] paddle.Tensor.flatten(Tensor([5281717, 3, 6, 6, 4],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265552], shape[0] = -2013265552.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265552 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 2] Completed Task 484

[Worker 2] Processing Task 487: paddle.Tensor.flatten(Tensor([883012, 4, 19, 34],"float32"), )
[paddle error] paddle.Tensor.flatten(Tensor([883012, 4, 19, 34],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013264288], shape[0] = -2013264288.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013264288 <= 0:0.] (at /paddle/paddle/phi/infermeta/unary.cc:2189)

[Worker 2] Completed Task 487
[Worker 2] Received stop signal.

