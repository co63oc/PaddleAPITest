paddle.nn.functional.adaptive_log_softmax_with_loss(Tensor([128, 16],"float32"), Tensor([128],"int64"), Tensor([16, 8],"float32"), list[list[Tensor([16, 8],"float32"),Tensor([8, 5],"float32"),],list[Tensor([16, 4],"float32"),Tensor([4, 5],"float32"),],list[Tensor([16, 2],"float32"),Tensor([2, 5],"float32"),],], list[5,10,15,20,], None, )
paddle.nn.functional.adaptive_log_softmax_with_loss(Tensor([4, 8],"float32"), Tensor([4],"int64"), Tensor([8, 3],"float32"), list[list[Tensor([8, 4],"float32"),Tensor([4, 2],"float32"),],], list[2,4,], None, )
paddle.nn.functional.adaptive_log_softmax_with_loss(Tensor([8, 8],"float32"), Tensor([8],"int64"), Tensor([8, 3],"float32"), list[list[Tensor([8, 4],"float32"),Tensor([4, 2],"float32"),],], list[2,4,], None, )


打印了源码中的label_mask 和 row_indices 变量 row_indices为paddle时paddle出错 需要unsqueeze
grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
2025-05-26 05:57:14.293442 test begin: paddle.nn.functional.adaptive_log_softmax_with_loss(Tensor([128, 16],"float32"), Tensor([128],"int64"), Tensor([16, 8],"float32"), list[list[Tensor([16, 8],"float32"),Tensor([8, 5],"float32"),],list[Tensor([16, 4],"float32"),Tensor([4, 5],"float32"),],list[Tensor([16, 2],"float32"),Tensor([2, 5],"float32"),],], list[5,10,15,20,], None, )
One of the differentiated Tensors does not require grad
W0526 05:57:20.140054 70558 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0526 05:57:20.141441 70558 gpu_resources.cc:164] device: 0, cuDNN Version: 8.8.
W0526 05:57:20.157261 70558 gpu_resources.cc:306] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.8, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
!!!!!!!
Tensor(shape=[128], dtype=bool, place=Place(gpu:0), stop_gradient=True,
       [False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, True , True , False,
        False, False, False, False, True , False, True , True , False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, True , False, False, True , False,
        False, False, True , False, False, True , False, False, False, False,
        False, False, True , False, False, False, True , True , True , True ,
        False, False, False, False, False, False, True , False, False, False,
        False, False, False, False, False, True , False, True , True , False,
        True , False, True , True , True , False, True , False, False, False,
        False, False, False, False, True , False, False, False, False, False,
        True , False, False, False, False, False, True , False, False, False,
        False, False, True , True , False, True , False, False])
Tensor(shape=[29], dtype=int64, place=Place(gpu:0), stop_gradient=True,
       [17 , 18 , 24 , 26 , 27 , 45 , 48 , 52 , 55 , 62 , 66 , 67 , 68 , 69 ,
        76 , 85 , 87 , 88 , 90 , 92 , 93 , 94 , 96 , 104, 110, 116, 122, 123,
        125])
!!!!!!!
Tensor(shape=[128], dtype=bool, place=Place(gpu:0), stop_gradient=True,
       [True , False, False, False, False, False, False, True , False, True ,
        False, False, False, True , False, True , False, False, False, True ,
        True , False, True , True , False, False, False, False, False, False,
        False, True , False, True , False, False, False, True , False, False,
        True , True , False, False, False, False, True , False, False, False,
        False, False, False, False, False, False, False, False, True , True ,
        False, False, False, False, True , False, False, False, False, False,
        False, True , False, True , True , False, False, False, False, False,
        True , False, False, False, True , False, True , False, False, True ,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, True , False, False,
        False, False, False, False, False, False, False, False, False, True ,
        False, True , False, False, False, False, True , False])
Tensor(shape=[29], dtype=int64, place=Place(gpu:0), stop_gradient=True,
       [0  , 7  , 9  , 13 , 15 , 19 , 20 , 22 , 23 , 31 , 33 , 37 , 40 , 41 ,
        46 , 58 , 59 , 64 , 71 , 73 , 74 , 80 , 84 , 86 , 89 , 107, 119, 121,
        126])
!!!!!!!
Tensor(shape=[128], dtype=bool, place=Place(gpu:0), stop_gradient=True,
       [False, True , False, True , True , True , False, False, False, False,
        True , False, True , False, False, False, False, False, False, False,
        False, True , False, False, False, True , False, False, False, True ,
        False, False, False, False, False, False, True , False, True , True ,
        False, False, False, False, True , False, False, False, False, False,
        True , False, False, False, False, False, True , False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, True , False, False, False, False, True , True , False,
        False, False, False, True , False, False, False, False, False, False,
        False, True , False, False, False, False, False, True , False, True ,
        True , False, False, False, False, False, True , False, False, False,
        False, True , True , True , True , False, False, True , False, False,
        False, False, False, False, False, False, False, False])
Tensor(shape=[29], dtype=int64, place=Place(gpu:0), stop_gradient=True,
       [1  , 3  , 4  , 5  , 10 , 12 , 21 , 25 , 29 , 36 , 38 , 39 , 44 , 50 ,
        56 , 72 , 77 , 78 , 83 , 91 , 97 , 99 , 100, 106, 111, 112, 113, 114,
        117])
!!!!!!!
Tensor(shape=[128], dtype=bool, place=Place(gpu:0), stop_gradient=True,
       [False, False, True , False, False, False, True , False, True , False,
        False, True , False, False, True , False, True , False, False, False,
        False, False, False, False, False, False, False, False, True , False,
        True , False, True , False, True , True , False, False, False, False,
        False, False, True , True , False, False, False, True , False, True ,
        False, True , False, True , True , False, False, True , False, False,
        True , True , False, True , False, True , False, False, False, False,
        True , False, False, False, False, True , False, False, False, True ,
        False, True , True , False, False, False, False, False, False, False,
        False, False, False, False, False, True , False, False, True , False,
        False, True , True , True , False, True , False, False, True , True ,
        False, False, False, False, False, True , False, False, True , False,
        True , False, False, False, True , False, False, True ])
Tensor(shape=[41], dtype=int64, place=Place(gpu:0), stop_gradient=True,
       [2  , 6  , 8  , 11 , 14 , 16 , 28 , 30 , 32 , 34 , 35 , 42 , 43 , 47 ,
        49 , 51 , 53 , 54 , 57 , 60 , 61 , 63 , 65 , 70 , 75 , 79 , 81 , 82 ,
        95 , 98 , 101, 102, 103, 105, 108, 109, 115, 118, 120, 124, 127])
[Pass] paddle.nn.functional.adaptive_log_softmax_with_loss(Tensor([128, 16],"float32"), Tensor([128],"int64"), Tensor([16, 8],"float32"), list[list[Tensor([16, 8],"float32"),Tensor([8, 5],"float32"),],list[Tensor([16, 4],"float32"),Tensor([4, 5],"float32"),],list[Tensor([16, 2],"float32"),Tensor([2, 5],"float32"),],], list[5,10,15,20,], None, )
2025-05-26 05:57:22.307543 test begin: paddle.nn.functional.adaptive_log_softmax_with_loss(Tensor([4, 8],"float32"), Tensor([4],"int64"), Tensor([8, 3],"float32"), list[list[Tensor([8, 4],"float32"),Tensor([4, 2],"float32"),],], list[2,4,], None, )
One of the differentiated Tensors does not require grad
!!!!!!!
Tensor(shape=[4], dtype=bool, place=Place(gpu:0), stop_gradient=True,
       [False, False, False, True ])
Tensor(shape=[], dtype=int64, place=Place(gpu:0), stop_gradient=True,
       3)
[paddle error] paddle.nn.functional.adaptive_log_softmax_with_loss(Tensor([4, 8],"float32"), Tensor([4],"int64"), Tensor([8, 3],"float32"), list[list[Tensor([8, 4],"float32"),Tensor([4, 2],"float32"),],], list[2,4,], None, ) 
 (InvalidArgument) The insert dimension value should not be larger than the dimension size of input tensor
  [Hint: Expected cur <= cur_output_rank, but received cur:1 > cur_output_rank:0.] (at ../paddle/phi/kernels/funcs/unsqueeze.h:128)

2025-05-26 05:57:22.429554 test begin: paddle.nn.functional.adaptive_log_softmax_with_loss(Tensor([8, 8],"float32"), Tensor([8],"int64"), Tensor([8, 3],"float32"), list[list[Tensor([8, 4],"float32"),Tensor([4, 2],"float32"),],], list[2,4,], None, )
One of the differentiated Tensors does not require grad
!!!!!!!
Tensor(shape=[8], dtype=bool, place=Place(gpu:0), stop_gradient=True,
       [True , False, False, True , False, False, False, False])
Tensor(shape=[2], dtype=int64, place=Place(gpu:0), stop_gradient=True,
       [0, 3])
!!!!!!!
Tensor(shape=[8], dtype=bool, place=Place(gpu:0), stop_gradient=True,
       [False, True , True , False, True , True , True , True ])
Tensor(shape=[6], dtype=int64, place=Place(gpu:0), stop_gradient=True,
       [1, 2, 4, 5, 6, 7])
[Pass] paddle.nn.functional.adaptive_log_softmax_with_loss(Tensor([8, 8],"float32"), Tensor([8],"int64"), Tensor([8, 3],"float32"), list[list[Tensor([8, 4],"float32"),Tensor([4, 2],"float32"),],], list[2,4,], None, )
