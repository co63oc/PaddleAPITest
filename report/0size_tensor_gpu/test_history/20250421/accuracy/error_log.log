2025-04-21 10:05:35.809530 test begin: paddle.addmm(input=Tensor([1, 1],"float64"), x=Tensor([0, 4],"float64"), y=Tensor([4, 5],"float64"), beta=-3.3, alpha=3.3, )

[paddle error] paddle.addmm(input=Tensor([1, 1],"float64"), x=Tensor([0, 4],"float64"), y=Tensor([4, 5],"float64"), beta=-3.3, alpha=3.3, ) 
 (PreconditionNotMet) The Input variable 'x' has not been initialized. You may need to confirm if you put exe.run(startup_program) after optimizer.minimize function.
  [Hint: Expected product(x_dims) != 0, but received product(x_dims):0 == 0:0.] (at ../paddle/phi/infermeta/ternary.cc:109)

2025-04-21 10:05:36.040922 test begin: paddle.addmm(input=Tensor([1, 1],"float64"), x=Tensor([5, 4],"float64"), y=Tensor([4, 0],"float64"), beta=-3.3, alpha=3.3, )

[paddle error] paddle.addmm(input=Tensor([1, 1],"float64"), x=Tensor([5, 4],"float64"), y=Tensor([4, 0],"float64"), beta=-3.3, alpha=3.3, ) 
 (PreconditionNotMet) The Input variable 'y' has not been initialized. You may need to confirm if you put exe.run(startup_program) after optimizer.minimize function.
  [Hint: Expected product(y_dims) != 0, but received product(y_dims):0 == 0:0.] (at ../paddle/phi/infermeta/ternary.cc:117)

2025-04-21 10:05:36.264876 test begin: paddle.addmm(input=Tensor([5, 1],"float64"), x=Tensor([5, 4],"float64"), y=Tensor([4, 0],"float64"), beta=-3.3, alpha=3.3, )

[paddle error] paddle.addmm(input=Tensor([5, 1],"float64"), x=Tensor([5, 4],"float64"), y=Tensor([4, 0],"float64"), beta=-3.3, alpha=3.3, ) 
 (PreconditionNotMet) The Input variable 'y' has not been initialized. You may need to confirm if you put exe.run(startup_program) after optimizer.minimize function.
  [Hint: Expected product(y_dims) != 0, but received product(y_dims):0 == 0:0.] (at ../paddle/phi/infermeta/ternary.cc:117)

2025-04-21 10:05:44.959423 test begin: paddle.amax(Tensor([0, 10, 10],"float32"), axis=list[-1,-2,], keepdim=False, )

[paddle error] paddle.amax(Tensor([0, 10, 10],"float32"), axis=list[-1,-2,], keepdim=False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:05:45.141316 test begin: paddle.amax(Tensor([0, 2, 4, 5],"float32"), axis=-1, keepdim=True, )

[paddle error] paddle.amax(Tensor([0, 2, 4, 5],"float32"), axis=-1, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:05:45.322314 test begin: paddle.amax(Tensor([0, 2, 5, 4],"float32"), axis=2, keepdim=True, )

[paddle error] paddle.amax(Tensor([0, 2, 5, 4],"float32"), axis=2, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:05:45.498213 test begin: paddle.amax(Tensor([0, 2, 5, 4],"float32"), axis=tuple(1,2,), keepdim=True, )

[paddle error] paddle.amax(Tensor([0, 2, 5, 4],"float32"), axis=tuple(1,2,), keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:05:45.668175 test begin: paddle.amax(Tensor([0, 4],"float64"), 1, True, )

[paddle error] paddle.amax(Tensor([0, 4],"float64"), 1, True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:05:45.841655 test begin: paddle.amax(Tensor([10, 0, 10],"float32"), axis=list[-1,0,], keepdim=False, )

[paddle error] paddle.amax(Tensor([10, 0, 10],"float32"), axis=list[-1,0,], keepdim=False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:05:45.999869 test begin: paddle.amax(Tensor([10, 10, 0],"float32"), axis=list[0,1,], keepdim=False, )

[paddle error] paddle.amax(Tensor([10, 10, 0],"float32"), axis=list[0,1,], keepdim=False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:05:46.178846 test begin: paddle.amax(Tensor([2, 0],"float64"), 0, False, )

[paddle error] paddle.amax(Tensor([2, 0],"float64"), 0, False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:05:46.364799 test begin: paddle.amax(Tensor([2, 2, 0],"int32"), tuple(0,1,), False, )

[paddle error] paddle.amax(Tensor([2, 2, 0],"int32"), tuple(0,1,), False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:05:46.514262 test begin: paddle.amax(Tensor([3, 0, 4, 5],"float32"), axis=-1, keepdim=True, )

[paddle error] paddle.amax(Tensor([3, 0, 4, 5],"float32"), axis=-1, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:05:46.636878 test begin: paddle.amax(Tensor([3, 0, 5, 4],"float32"), axis=2, keepdim=True, )

[paddle error] paddle.amax(Tensor([3, 0, 5, 4],"float32"), axis=2, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:05:46.778277 test begin: paddle.amax(Tensor([3, 2, 0, 5],"float32"), axis=-1, keepdim=True, )

[paddle error] paddle.amax(Tensor([3, 2, 0, 5],"float32"), axis=-1, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:05:46.951830 test begin: paddle.amax(Tensor([3, 2, 5, 0],"float32"), axis=2, keepdim=True, )

[paddle error] paddle.amax(Tensor([3, 2, 5, 0],"float32"), axis=2, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:05:47.123261 test begin: paddle.amax(Tensor([3, 2, 5, 0],"float32"), axis=tuple(1,2,), keepdim=True, )

[paddle error] paddle.amax(Tensor([3, 2, 5, 0],"float32"), axis=tuple(1,2,), keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:05:47.308076 test begin: paddle.amin(Tensor([0, 10, 10],"float32"), axis=list[-1,-2,], keepdim=False, )

[paddle error] paddle.amin(Tensor([0, 10, 10],"float32"), axis=list[-1,-2,], keepdim=False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:05:47.493767 test begin: paddle.amin(Tensor([0, 2, 4, 5],"float32"), axis=-1, keepdim=True, )

[paddle error] paddle.amin(Tensor([0, 2, 4, 5],"float32"), axis=-1, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:05:47.672330 test begin: paddle.amin(Tensor([0, 2, 5, 4],"float32"), axis=2, keepdim=True, )

[paddle error] paddle.amin(Tensor([0, 2, 5, 4],"float32"), axis=2, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:05:47.855509 test begin: paddle.amin(Tensor([0, 2, 5, 4],"float32"), axis=tuple(1,2,), keepdim=True, )

[paddle error] paddle.amin(Tensor([0, 2, 5, 4],"float32"), axis=tuple(1,2,), keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:05:48.078871 test begin: paddle.amin(Tensor([0, 4],"float64"), 1, True, )

[paddle error] paddle.amin(Tensor([0, 4],"float64"), 1, True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:05:48.298861 test begin: paddle.amin(Tensor([10, 0, 10],"float32"), axis=list[-1,0,], keepdim=False, )

[paddle error] paddle.amin(Tensor([10, 0, 10],"float32"), axis=list[-1,0,], keepdim=False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:05:48.514396 test begin: paddle.amin(Tensor([10, 10, 0],"float32"), axis=list[0,1,], keepdim=False, )

[paddle error] paddle.amin(Tensor([10, 10, 0],"float32"), axis=list[0,1,], keepdim=False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:05:48.741673 test begin: paddle.amin(Tensor([2, 0],"float64"), 0, False, )

[paddle error] paddle.amin(Tensor([2, 0],"float64"), 0, False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:05:48.999815 test begin: paddle.amin(Tensor([2, 2, 0],"int32"), tuple(0,1,), False, )

[paddle error] paddle.amin(Tensor([2, 2, 0],"int32"), tuple(0,1,), False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:05:49.190294 test begin: paddle.amin(Tensor([3, 0, 4, 5],"float32"), axis=-1, keepdim=True, )

[paddle error] paddle.amin(Tensor([3, 0, 4, 5],"float32"), axis=-1, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:05:49.406621 test begin: paddle.amin(Tensor([3, 0, 5, 4],"float32"), axis=2, keepdim=True, )

[paddle error] paddle.amin(Tensor([3, 0, 5, 4],"float32"), axis=2, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:05:49.609354 test begin: paddle.amin(Tensor([3, 2, 0, 5],"float32"), axis=-1, keepdim=True, )

[paddle error] paddle.amin(Tensor([3, 2, 0, 5],"float32"), axis=-1, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:05:49.860522 test begin: paddle.amin(Tensor([3, 2, 5, 0],"float32"), axis=2, keepdim=True, )

[paddle error] paddle.amin(Tensor([3, 2, 5, 0],"float32"), axis=2, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:05:50.123839 test begin: paddle.amin(Tensor([3, 2, 5, 0],"float32"), axis=tuple(1,2,), keepdim=True, )

[paddle error] paddle.amin(Tensor([3, 2, 5, 0],"float32"), axis=tuple(1,2,), keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:05:50.379574 test begin: paddle.angle(Tensor([0, 3],"complex128"), )

[cuda error] paddle.angle(Tensor([0, 3],"complex128"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:05:50.648061 test begin: paddle.angle(Tensor([2, 0],"complex128"), )

[cuda error] paddle.angle(Tensor([2, 0],"complex128"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:06:03.828821 test begin: paddle.argmax(Tensor([0, 1000],"float32"), axis=1, )

[paddle error] paddle.argmax(Tensor([0, 1000],"float32"), axis=1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:04.014940 test begin: paddle.argmax(Tensor([0, 100],"float32"), axis=-1, )

[paddle error] paddle.argmax(Tensor([0, 100],"float32"), axis=-1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:04.194787 test begin: paddle.argmax(Tensor([0, 1024, 50304],"float16"), -1, )

[paddle error] paddle.argmax(Tensor([0, 1024, 50304],"float16"), -1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:04.471171 test begin: paddle.argmax(Tensor([0, 10],"float32"), axis=-1, keepdim=True, )

[paddle error] paddle.argmax(Tensor([0, 10],"float32"), axis=-1, keepdim=True, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:04.696270 test begin: paddle.argmax(Tensor([0, 10],"float32"), axis=1, )

[paddle error] paddle.argmax(Tensor([0, 10],"float32"), axis=1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:05.079977 test begin: paddle.argmax(Tensor([0, 2, 4, 16, 2],"float32"), axis=-1, )

[paddle error] paddle.argmax(Tensor([0, 2, 4, 16, 2],"float32"), axis=-1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:05.370747 test begin: paddle.argmax(Tensor([0, 256],"float32"), axis=-1, )

[paddle error] paddle.argmax(Tensor([0, 256],"float32"), axis=-1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:05.650732 test begin: paddle.argmax(Tensor([0, 3, 4],"float64"), axis=-1, keepdim=True, )

[paddle error] paddle.argmax(Tensor([0, 3, 4],"float64"), axis=-1, keepdim=True, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:05.933062 test begin: paddle.argmax(Tensor([0, 32, 64],"float16"), axis=1, )

[paddle error] paddle.argmax(Tensor([0, 32, 64],"float16"), axis=1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:06.233214 test begin: paddle.argmax(Tensor([0, 32, 64],"float32"), axis=1, )

[paddle error] paddle.argmax(Tensor([0, 32, 64],"float32"), axis=1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:06.514446 test begin: paddle.argmax(Tensor([0, 7, 99],"float32"), axis=-1, )

[paddle error] paddle.argmax(Tensor([0, 7, 99],"float32"), axis=-1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:06.788463 test begin: paddle.argmax(Tensor([0, 8, 14, 12],"float32"), axis=1, keepdim=True, )

[paddle error] paddle.argmax(Tensor([0, 8, 14, 12],"float32"), axis=1, keepdim=True, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:07.052782 test begin: paddle.argmax(Tensor([1, 8, 0, 12],"float32"), axis=1, keepdim=True, )

[paddle error] paddle.argmax(Tensor([1, 8, 0, 12],"float32"), axis=1, keepdim=True, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:07.284908 test begin: paddle.argmax(Tensor([1, 8, 14, 0],"float32"), axis=1, keepdim=True, )

[paddle error] paddle.argmax(Tensor([1, 8, 14, 0],"float32"), axis=1, keepdim=True, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:07.536992 test begin: paddle.argmax(Tensor([12988, 32, 0],"float16"), axis=1, )

[paddle error] paddle.argmax(Tensor([12988, 32, 0],"float16"), axis=1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:07.795527 test begin: paddle.argmax(Tensor([12988, 32, 0],"float32"), axis=1, )

[paddle error] paddle.argmax(Tensor([12988, 32, 0],"float32"), axis=1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:08.010936 test begin: paddle.argmax(Tensor([13, 0, 4, 16, 2],"float32"), axis=-1, )

[paddle error] paddle.argmax(Tensor([13, 0, 4, 16, 2],"float32"), axis=-1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:08.280111 test begin: paddle.argmax(Tensor([13, 0, 99],"float32"), axis=-1, )

[paddle error] paddle.argmax(Tensor([13, 0, 99],"float32"), axis=-1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:08.484172 test begin: paddle.argmax(Tensor([13, 2, 0, 16, 2],"float32"), axis=-1, )

[paddle error] paddle.argmax(Tensor([13, 2, 0, 16, 2],"float32"), axis=-1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:08.636881 test begin: paddle.argmax(Tensor([13, 2, 4, 0, 2],"float32"), axis=-1, )

[paddle error] paddle.argmax(Tensor([13, 2, 4, 0, 2],"float32"), axis=-1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:08.825481 test begin: paddle.argmax(Tensor([16, 0, 50304],"float16"), -1, )

[paddle error] paddle.argmax(Tensor([16, 0, 50304],"float16"), -1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:09.039046 test begin: paddle.argmax(Tensor([2, 0, 4],"float64"), axis=-1, keepdim=True, )

[paddle error] paddle.argmax(Tensor([2, 0, 4],"float64"), axis=-1, keepdim=True, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:09.245417 test begin: paddle.argmax(Tensor([3, 0, 3, 3, 3, 3],"float64"), axis=0, )

[paddle error] paddle.argmax(Tensor([3, 0, 3, 3, 3, 3],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:09.449827 test begin: paddle.argmax(Tensor([3, 3, 0, 3, 3, 3],"float64"), axis=0, )

[paddle error] paddle.argmax(Tensor([3, 3, 0, 3, 3, 3],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:09.629569 test begin: paddle.argmax(Tensor([3, 3, 3, 0, 3, 3],"float64"), axis=0, )

[paddle error] paddle.argmax(Tensor([3, 3, 3, 0, 3, 3],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:09.787338 test begin: paddle.argmax(Tensor([3, 3, 3, 3, 0, 3],"float64"), axis=0, )

[paddle error] paddle.argmax(Tensor([3, 3, 3, 3, 0, 3],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:09.917703 test begin: paddle.argmax(Tensor([3, 3, 3, 3, 3, 0],"float64"), axis=0, )

[paddle error] paddle.argmax(Tensor([3, 3, 3, 3, 3, 0],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:10.084609 test begin: paddle.argmax(Tensor([4, 0, 4, 4, 4],"float64"), axis=0, )

[paddle error] paddle.argmax(Tensor([4, 0, 4, 4, 4],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:10.260087 test begin: paddle.argmax(Tensor([4, 4, 0, 4, 4],"float64"), axis=0, )

[paddle error] paddle.argmax(Tensor([4, 4, 0, 4, 4],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:10.436603 test begin: paddle.argmax(Tensor([4, 4, 4, 0, 4],"float64"), axis=0, )

[paddle error] paddle.argmax(Tensor([4, 4, 4, 0, 4],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:10.630232 test begin: paddle.argmax(Tensor([4, 4, 4, 4, 0],"float64"), axis=0, )

[paddle error] paddle.argmax(Tensor([4, 4, 4, 4, 0],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:10.833150 test begin: paddle.argmax(Tensor([5, 0, 5, 5],"float64"), axis=0, )

[paddle error] paddle.argmax(Tensor([5, 0, 5, 5],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:11.096471 test begin: paddle.argmax(Tensor([5, 5, 0, 5],"float64"), axis=0, )

[paddle error] paddle.argmax(Tensor([5, 5, 0, 5],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:11.285776 test begin: paddle.argmax(Tensor([5, 5, 5, 0],"float64"), axis=0, )

[paddle error] paddle.argmax(Tensor([5, 5, 5, 0],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:11.497028 test begin: paddle.argmax(x=Tensor([0, 3, 4],"float64"), axis=1, keepdim=False, )

[paddle error] paddle.argmax(x=Tensor([0, 3, 4],"float64"), axis=1, keepdim=False, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:11.694700 test begin: paddle.argmax(x=Tensor([0, 3],"int64"), axis=-1, )

[paddle error] paddle.argmax(x=Tensor([0, 3],"int64"), axis=-1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:11.879426 test begin: paddle.argmax(x=Tensor([3, 0],"int64"), axis=-2, )

[paddle error] paddle.argmax(x=Tensor([3, 0],"int64"), axis=-2, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:12.070360 test begin: paddle.argmax(x=Tensor([3, 3, 0],"float64"), axis=1, keepdim=False, )

[paddle error] paddle.argmax(x=Tensor([3, 3, 0],"float64"), axis=1, keepdim=False, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:12.277168 test begin: paddle.argmin(Tensor([0, 10],"float32"), axis=-1, )

[paddle error] paddle.argmin(Tensor([0, 10],"float32"), axis=-1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:12.454501 test begin: paddle.argmin(Tensor([0, 10],"float32"), axis=-1, keepdim=True, )

[paddle error] paddle.argmin(Tensor([0, 10],"float32"), axis=-1, keepdim=True, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:12.629571 test begin: paddle.argmin(Tensor([0, 10],"float32"), axis=1, )

[paddle error] paddle.argmin(Tensor([0, 10],"float32"), axis=1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:12.808850 test begin: paddle.argmin(Tensor([3, 0, 3, 3, 3, 3],"float64"), axis=0, )

[paddle error] paddle.argmin(Tensor([3, 0, 3, 3, 3, 3],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:13.001509 test begin: paddle.argmin(Tensor([3, 3, 0, 3, 3, 3],"float64"), axis=0, )

[paddle error] paddle.argmin(Tensor([3, 3, 0, 3, 3, 3],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:13.175733 test begin: paddle.argmin(Tensor([3, 3, 3, 0, 3, 3],"float64"), axis=0, )

[paddle error] paddle.argmin(Tensor([3, 3, 3, 0, 3, 3],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:13.366162 test begin: paddle.argmin(Tensor([3, 3, 3, 3, 0, 3],"float64"), axis=0, )

[paddle error] paddle.argmin(Tensor([3, 3, 3, 3, 0, 3],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:13.503523 test begin: paddle.argmin(Tensor([3, 3, 3, 3, 3, 0],"float64"), axis=0, )

[paddle error] paddle.argmin(Tensor([3, 3, 3, 3, 3, 0],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:13.687293 test begin: paddle.argmin(Tensor([4, 0, 4, 4, 4],"float64"), axis=0, )

[paddle error] paddle.argmin(Tensor([4, 0, 4, 4, 4],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:13.921942 test begin: paddle.argmin(Tensor([4, 4, 0, 4, 4],"float64"), axis=0, )

[paddle error] paddle.argmin(Tensor([4, 4, 0, 4, 4],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:14.081521 test begin: paddle.argmin(Tensor([4, 4, 4, 0, 4],"float64"), axis=0, )

[paddle error] paddle.argmin(Tensor([4, 4, 4, 0, 4],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:14.243087 test begin: paddle.argmin(Tensor([4, 4, 4, 4, 0],"float64"), axis=0, )

[paddle error] paddle.argmin(Tensor([4, 4, 4, 4, 0],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:14.435457 test begin: paddle.argmin(Tensor([5, 0, 5, 5],"float64"), axis=0, )

[paddle error] paddle.argmin(Tensor([5, 0, 5, 5],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:14.557410 test begin: paddle.argmin(Tensor([5, 5, 0, 5],"float64"), axis=0, )

[paddle error] paddle.argmin(Tensor([5, 5, 0, 5],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:14.734322 test begin: paddle.argmin(Tensor([5, 5, 5, 0],"float64"), axis=0, )

[paddle error] paddle.argmin(Tensor([5, 5, 5, 0],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:14.929050 test begin: paddle.argmin(x=Tensor([0, 3, 4],"float64"), axis=1, keepdim=False, )

[paddle error] paddle.argmin(x=Tensor([0, 3, 4],"float64"), axis=1, keepdim=False, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:15.114049 test begin: paddle.argmin(x=Tensor([0, 3],"int64"), axis=-1, )

[paddle error] paddle.argmin(x=Tensor([0, 3],"int64"), axis=-1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:15.300478 test begin: paddle.argmin(x=Tensor([3, 0],"int64"), axis=-2, )

[paddle error] paddle.argmin(x=Tensor([3, 0],"int64"), axis=-2, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:15.489958 test begin: paddle.argmin(x=Tensor([3, 3, 0],"float64"), axis=1, keepdim=False, )

[paddle error] paddle.argmin(x=Tensor([3, 3, 0],"float64"), axis=1, keepdim=False, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:06:38.456270 test begin: paddle.atan2(Tensor([0, 17],"float64"), Tensor([0, 17],"float64"), )

[cuda error] paddle.atan2(Tensor([0, 17],"float64"), Tensor([0, 17],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:06:38.653065 test begin: paddle.atan2(Tensor([0, 222, 333],"float64"), Tensor([222, 333],"float64"), )

[cuda error] paddle.atan2(Tensor([0, 222, 333],"float64"), Tensor([222, 333],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:06:38.833477 test begin: paddle.atan2(Tensor([0, 3, 2],"float16"), Tensor([0, 3, 2],"float32"), )

W0421 10:06:38.961074 138647 dygraph_functions.cc:6470] got different data type, run type promotion automatically, this may cause data type been changed.
[cuda error] paddle.atan2(Tensor([0, 3, 2],"float16"), Tensor([0, 3, 2],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:06:38.963168 test begin: paddle.atan2(Tensor([0, 3, 2],"float16"), Tensor([0, 3, 2],"float64"), )

[cuda error] paddle.atan2(Tensor([0, 3, 2],"float16"), Tensor([0, 3, 2],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:06:39.090291 test begin: paddle.atan2(Tensor([100],"float64"), Tensor([0, 100],"float64"), )

[cuda error] paddle.atan2(Tensor([100],"float64"), Tensor([0, 100],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:06:39.260847 test begin: paddle.atan2(Tensor([11, 0],"float64"), Tensor([11, 0],"float64"), )

[cuda error] paddle.atan2(Tensor([11, 0],"float64"), Tensor([11, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:06:39.768235 test begin: paddle.atan2(Tensor([4, 0, 2],"float16"), Tensor([4, 0, 2],"float32"), )

[cuda error] paddle.atan2(Tensor([4, 0, 2],"float16"), Tensor([4, 0, 2],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:06:39.910918 test begin: paddle.atan2(Tensor([4, 0, 2],"float16"), Tensor([4, 0, 2],"float64"), )

[cuda error] paddle.atan2(Tensor([4, 0, 2],"float16"), Tensor([4, 0, 2],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:06:40.050808 test begin: paddle.atan2(Tensor([4, 3, 0],"float16"), Tensor([4, 3, 0],"float32"), )

[cuda error] paddle.atan2(Tensor([4, 3, 0],"float16"), Tensor([4, 3, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:06:40.233057 test begin: paddle.atan2(Tensor([4, 3, 0],"float16"), Tensor([4, 3, 0],"float64"), )

[cuda error] paddle.atan2(Tensor([4, 3, 0],"float16"), Tensor([4, 3, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:06:40.432736 test begin: paddle.atan2(x=Tensor([0, 6, 6, 6, 6],"float64"), y=Tensor([0, 6, 6, 6, 6],"float64"), )

[cuda error] paddle.atan2(x=Tensor([0, 6, 6, 6, 6],"float64"), y=Tensor([0, 6, 6, 6, 6],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:06:40.638623 test begin: paddle.atan2(x=Tensor([0, 6, 6, 6],"float64"), y=Tensor([0, 6, 6, 6],"float64"), )

[cuda error] paddle.atan2(x=Tensor([0, 6, 6, 6],"float64"), y=Tensor([0, 6, 6, 6],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:06:40.855948 test begin: paddle.atan2(x=Tensor([0, 6, 6],"float64"), y=Tensor([0, 6, 6],"float64"), )

[cuda error] paddle.atan2(x=Tensor([0, 6, 6],"float64"), y=Tensor([0, 6, 6],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:06:41.056769 test begin: paddle.atan2(x=Tensor([0, 6],"float16"), y=Tensor([0, 6],"float16"), )

[cuda error] paddle.atan2(x=Tensor([0, 6],"float16"), y=Tensor([0, 6],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:06:41.267572 test begin: paddle.atan2(x=Tensor([0, 6],"float32"), y=Tensor([0, 6],"float32"), )

[cuda error] paddle.atan2(x=Tensor([0, 6],"float32"), y=Tensor([0, 6],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:06:41.454025 test begin: paddle.atan2(x=Tensor([3, 0, 6, 6, 6],"float64"), y=Tensor([3, 0, 6, 6, 6],"float64"), )

[cuda error] paddle.atan2(x=Tensor([3, 0, 6, 6, 6],"float64"), y=Tensor([3, 0, 6, 6, 6],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:06:41.667013 test begin: paddle.atan2(x=Tensor([3, 6, 0, 6, 6],"float64"), y=Tensor([3, 6, 0, 6, 6],"float64"), )

[cuda error] paddle.atan2(x=Tensor([3, 6, 0, 6, 6],"float64"), y=Tensor([3, 6, 0, 6, 6],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:06:41.861555 test begin: paddle.atan2(x=Tensor([3, 6, 6, 0, 6],"float64"), y=Tensor([3, 6, 6, 0, 6],"float64"), )

[cuda error] paddle.atan2(x=Tensor([3, 6, 6, 0, 6],"float64"), y=Tensor([3, 6, 6, 0, 6],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:06:42.045135 test begin: paddle.atan2(x=Tensor([3, 6, 6, 6, 0],"float64"), y=Tensor([3, 6, 6, 6, 0],"float64"), )

[cuda error] paddle.atan2(x=Tensor([3, 6, 6, 6, 0],"float64"), y=Tensor([3, 6, 6, 6, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:06:42.261788 test begin: paddle.atan2(x=Tensor([6, 0, 6, 6],"float64"), y=Tensor([6, 0, 6, 6],"float64"), )

[cuda error] paddle.atan2(x=Tensor([6, 0, 6, 6],"float64"), y=Tensor([6, 0, 6, 6],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:06:42.394427 test begin: paddle.atan2(x=Tensor([6, 0, 6],"float64"), y=Tensor([6, 0, 6],"float64"), )

[cuda error] paddle.atan2(x=Tensor([6, 0, 6],"float64"), y=Tensor([6, 0, 6],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:06:42.584673 test begin: paddle.atan2(x=Tensor([6, 0],"float16"), y=Tensor([6, 0],"float16"), )

[cuda error] paddle.atan2(x=Tensor([6, 0],"float16"), y=Tensor([6, 0],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:06:42.776496 test begin: paddle.atan2(x=Tensor([6, 0],"float32"), y=Tensor([6, 0],"float32"), )

[cuda error] paddle.atan2(x=Tensor([6, 0],"float32"), y=Tensor([6, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:06:42.971453 test begin: paddle.atan2(x=Tensor([6, 6, 0, 6],"float64"), y=Tensor([6, 6, 0, 6],"float64"), )

[cuda error] paddle.atan2(x=Tensor([6, 6, 0, 6],"float64"), y=Tensor([6, 6, 0, 6],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:06:43.155299 test begin: paddle.atan2(x=Tensor([6, 6, 0],"float64"), y=Tensor([6, 6, 0],"float64"), )

[cuda error] paddle.atan2(x=Tensor([6, 6, 0],"float64"), y=Tensor([6, 6, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:06:43.334473 test begin: paddle.atan2(x=Tensor([6, 6, 6, 0],"float64"), y=Tensor([6, 6, 6, 0],"float64"), )

[cuda error] paddle.atan2(x=Tensor([6, 6, 6, 0],"float64"), y=Tensor([6, 6, 6, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:14.804441 test begin: paddle.bmm(Tensor([0, 300, 128],"float32"), Tensor([0, 128, 30976],"float32"), )

W0421 10:07:14.996913 160542 backward.cc:437] While running Node (BmmGradNode) raises an EnforceNotMet exception
[paddle error] paddle.bmm(Tensor([0, 300, 128],"float32"), Tensor([0, 128, 30976],"float32"), ) 
 (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 15859712, memory's size is 0.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):15859712 > memory_size():0.] (at ../paddle/phi/core/dense_tensor_impl.cc:48)

2025-04-21 10:07:14.997325 test begin: paddle.bmm(Tensor([0, 300, 128],"float32"), Tensor([0, 128, 33856],"float32"), )

W0421 10:07:15.241526 160563 backward.cc:437] While running Node (BmmGradNode) raises an EnforceNotMet exception
[paddle error] paddle.bmm(Tensor([0, 300, 128],"float32"), Tensor([0, 128, 33856],"float32"), ) 
 (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 17334272, memory's size is 0.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):17334272 > memory_size():0.] (at ../paddle/phi/core/dense_tensor_impl.cc:48)

2025-04-21 10:07:17.588210 test begin: paddle.bmm(Tensor([1, 300, 128],"float32"), Tensor([1, 128, 0],"float32"), )

W0421 10:07:17.808451 161432 backward.cc:437] While running Node (BmmGradNode) raises an EnforceNotMet exception
[paddle error] paddle.bmm(Tensor([1, 300, 128],"float32"), Tensor([1, 128, 0],"float32"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:76)

2025-04-21 10:07:17.808923 test begin: paddle.bmm(x=Tensor([0, 2, 3],"float32"), y=Tensor([0, 3, 2],"float32"), )

W0421 10:07:17.932406 161448 backward.cc:437] While running Node (BmmGradNode) raises an EnforceNotMet exception
[paddle error] paddle.bmm(x=Tensor([0, 2, 3],"float32"), y=Tensor([0, 3, 2],"float32"), ) 
 (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 24, memory's size is 0.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):24 > memory_size():0.] (at ../paddle/phi/core/dense_tensor_impl.cc:48)

2025-04-21 10:07:17.936227 test begin: paddle.bmm(x=Tensor([0, 2, 3],"float64"), y=Tensor([0, 3, 2],"float64"), )

W0421 10:07:18.116430 161472 backward.cc:437] While running Node (BmmGradNode) raises an EnforceNotMet exception
[paddle error] paddle.bmm(x=Tensor([0, 2, 3],"float64"), y=Tensor([0, 3, 2],"float64"), ) 
 (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 48, memory's size is 0.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):48 > memory_size():0.] (at ../paddle/phi/core/dense_tensor_impl.cc:48)

2025-04-21 10:07:18.523398 test begin: paddle.bmm(x=Tensor([2, 2, 3],"float32"), y=Tensor([2, 3, 0],"float32"), )

W0421 10:07:18.681833 161667 backward.cc:437] While running Node (BmmGradNode) raises an EnforceNotMet exception
[paddle error] paddle.bmm(x=Tensor([2, 2, 3],"float32"), y=Tensor([2, 3, 0],"float32"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:76)

2025-04-21 10:07:18.685168 test begin: paddle.bmm(x=Tensor([2, 2, 3],"float64"), y=Tensor([2, 3, 0],"float64"), )

W0421 10:07:18.831545 161809 backward.cc:437] While running Node (BmmGradNode) raises an EnforceNotMet exception
[paddle error] paddle.bmm(x=Tensor([2, 2, 3],"float64"), y=Tensor([2, 3, 0],"float64"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:206)

2025-04-21 10:06:53.943178 test begin: paddle.bucketize(Tensor([0, 4],"float64"), Tensor([4],"float64"), )

[cuda error] paddle.bucketize(Tensor([0, 4],"float64"), Tensor([4],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:06:54.064508 test begin: paddle.bucketize(Tensor([0, 4],"float64"), Tensor([4],"float64"), out_int32=True, )

[cuda error] paddle.bucketize(Tensor([0, 4],"float64"), Tensor([4],"float64"), out_int32=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:06:54.246629 test begin: paddle.bucketize(Tensor([0, 4],"float64"), Tensor([4],"float64"), right=True, )

[cuda error] paddle.bucketize(Tensor([0, 4],"float64"), Tensor([4],"float64"), right=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:06:54.452533 test begin: paddle.bucketize(Tensor([2, 0],"float64"), Tensor([4],"float64"), )

[cuda error] paddle.bucketize(Tensor([2, 0],"float64"), Tensor([4],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:06:54.615411 test begin: paddle.bucketize(Tensor([2, 0],"float64"), Tensor([4],"float64"), out_int32=True, )

[cuda error] paddle.bucketize(Tensor([2, 0],"float64"), Tensor([4],"float64"), out_int32=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:06:54.807585 test begin: paddle.bucketize(Tensor([2, 0],"float64"), Tensor([4],"float64"), right=True, )

[cuda error] paddle.bucketize(Tensor([2, 0],"float64"), Tensor([4],"float64"), right=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:10.309787 test begin: paddle.column_stack(list[Tensor([0],"float64"),Tensor([0],"float64"),Tensor([0],"float64"),], )

W0421 10:08:10.548934 18544 backward.cc:437] While running Node (ReshapeGradNode) raises an EnforceNotMet exception
[paddle error] paddle.column_stack(list[Tensor([0],"float64"),Tensor([0],"float64"),Tensor([0],"float64"),], ) 
 (PermissionDenied) Null pointer error, the impl_ of Tensor should not be Null when calling Tensor::place().
  [Hint: impl_ should not be null.] (at ../paddle/phi/api/lib/tensor.cc:180)

2025-04-21 10:08:10.556704 test begin: paddle.column_stack(list[Tensor([0],"float64"),], )

W0421 10:08:10.800231 18569 backward.cc:437] While running Node (ReshapeGradNode) raises an EnforceNotMet exception
[paddle error] paddle.column_stack(list[Tensor([0],"float64"),], ) 
 (PermissionDenied) Null pointer error, the impl_ of Tensor should not be Null when calling Tensor::place().
  [Hint: impl_ should not be null.] (at ../paddle/phi/api/lib/tensor.cc:180)

2025-04-21 10:08:21.404431 test begin: paddle.complex(Tensor([0, 10],"float64"), Tensor([0, 10],"float64"), )

[cuda error] paddle.complex(Tensor([0, 10],"float64"), Tensor([0, 10],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:21.612543 test begin: paddle.complex(Tensor([0, 3, 4],"float32"), Tensor([0, 3, 4],"float32"), )

[cuda error] paddle.complex(Tensor([0, 3, 4],"float32"), Tensor([0, 3, 4],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:21.852658 test begin: paddle.complex(Tensor([0, 3],"float32"), Tensor([0, 3],"float32"), )

[cuda error] paddle.complex(Tensor([0, 3],"float32"), Tensor([0, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:22.198639 test begin: paddle.complex(Tensor([0],"float32"), Tensor([0],"float32"), )

[cuda error] paddle.complex(Tensor([0],"float32"), Tensor([0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:22.398178 test begin: paddle.complex(Tensor([0],"float64"), Tensor([0],"float64"), )

[cuda error] paddle.complex(Tensor([0],"float64"), Tensor([0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:22.629893 test begin: paddle.complex(Tensor([10, 0],"float64"), Tensor([10, 0],"float64"), )

[cuda error] paddle.complex(Tensor([10, 0],"float64"), Tensor([10, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:22.864138 test begin: paddle.complex(Tensor([2, 0, 4],"float32"), Tensor([2, 0, 4],"float32"), )

[cuda error] paddle.complex(Tensor([2, 0, 4],"float32"), Tensor([2, 0, 4],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:23.119654 test begin: paddle.complex(Tensor([2, 0],"float32"), Tensor([2, 0],"float32"), )

[cuda error] paddle.complex(Tensor([2, 0],"float32"), Tensor([2, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:23.257265 test begin: paddle.complex(Tensor([2, 3, 0],"float32"), Tensor([2, 3, 0],"float32"), )

[cuda error] paddle.complex(Tensor([2, 3, 0],"float32"), Tensor([2, 3, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:23.391919 test begin: paddle.complex(real=Tensor([0, 2, 3],"float32"), imag=Tensor([0, 2, 3],"float32"), )

[cuda error] paddle.complex(real=Tensor([0, 2, 3],"float32"), imag=Tensor([0, 2, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:23.602254 test begin: paddle.complex(real=Tensor([0, 2, 3],"float64"), imag=Tensor([0, 2, 3],"float64"), )

[cuda error] paddle.complex(real=Tensor([0, 2, 3],"float64"), imag=Tensor([0, 2, 3],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:23.827378 test begin: paddle.complex(real=Tensor([0, 2],"float32"), imag=Tensor([0, 2],"float32"), )

[cuda error] paddle.complex(real=Tensor([0, 2],"float32"), imag=Tensor([0, 2],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:24.094722 test begin: paddle.complex(real=Tensor([0, 2],"float64"), imag=Tensor([0, 2],"float64"), )

[cuda error] paddle.complex(real=Tensor([0, 2],"float64"), imag=Tensor([0, 2],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:24.323820 test begin: paddle.complex(real=Tensor([0],"float32"), imag=Tensor([0],"float32"), )

[cuda error] paddle.complex(real=Tensor([0],"float32"), imag=Tensor([0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:24.474611 test begin: paddle.complex(real=Tensor([0],"float64"), imag=Tensor([0],"float64"), )

[cuda error] paddle.complex(real=Tensor([0],"float64"), imag=Tensor([0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:24.603027 test begin: paddle.complex(real=Tensor([3, 0],"float32"), imag=Tensor([3, 0],"float32"), )

[cuda error] paddle.complex(real=Tensor([3, 0],"float32"), imag=Tensor([3, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:24.797770 test begin: paddle.complex(real=Tensor([3, 0],"float64"), imag=Tensor([3, 0],"float64"), )

[cuda error] paddle.complex(real=Tensor([3, 0],"float64"), imag=Tensor([3, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:25.005579 test begin: paddle.complex(real=Tensor([9, 0, 3],"float32"), imag=Tensor([9, 0, 3],"float32"), )

[cuda error] paddle.complex(real=Tensor([9, 0, 3],"float32"), imag=Tensor([9, 0, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:25.243492 test begin: paddle.complex(real=Tensor([9, 0, 3],"float64"), imag=Tensor([9, 0, 3],"float64"), )

[cuda error] paddle.complex(real=Tensor([9, 0, 3],"float64"), imag=Tensor([9, 0, 3],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:25.477445 test begin: paddle.complex(real=Tensor([9, 2, 0],"float32"), imag=Tensor([9, 2, 0],"float32"), )

[cuda error] paddle.complex(real=Tensor([9, 2, 0],"float32"), imag=Tensor([9, 2, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:25.684059 test begin: paddle.complex(real=Tensor([9, 2, 0],"float64"), imag=Tensor([9, 2, 0],"float64"), )

[cuda error] paddle.complex(real=Tensor([9, 2, 0],"float64"), imag=Tensor([9, 2, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:26:50.641867 test begin: paddle.cross(x=Tensor([0, 3, 3],"float64"), y=Tensor([0, 3, 3],"float64"), axis=1, )

[cuda error] paddle.cross(x=Tensor([0, 3, 3],"float64"), y=Tensor([0, 3, 3],"float64"), axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:26:50.847767 test begin: paddle.cross(x=Tensor([0, 3, 3],"float64"), y=Tensor([0, 3, 3],"float64"), axis=2, )

[cuda error] paddle.cross(x=Tensor([0, 3, 3],"float64"), y=Tensor([0, 3, 3],"float64"), axis=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:26:50.994933 test begin: paddle.cross(x=Tensor([0, 3],"float32"), y=Tensor([0, 3],"float32"), )

/usr/local/lib/python3.9/dist-packages/torch/utils/_device.py:106: UserWarning: Using torch.cross without specifying the dim arg is deprecated.
Please either pass the dim explicitly or simply use torch.linalg.cross.
The default value of dim will change to agree with that of linalg.cross in a future release. (Triggered internally at ../aten/src/ATen/native/Cross.cpp:62.)
  return func(*args, **kwargs)
[cuda error] paddle.cross(x=Tensor([0, 3],"float32"), y=Tensor([0, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:26:51.106408 test begin: paddle.cross(x=Tensor([0, 3],"float64"), y=Tensor([0, 3],"float64"), axis=-1, )

[cuda error] paddle.cross(x=Tensor([0, 3],"float64"), y=Tensor([0, 3],"float64"), axis=-1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:26:51.360257 test begin: paddle.cross(x=Tensor([0, 3],"float64"), y=Tensor([0, 3],"float64"), axis=1, )

[cuda error] paddle.cross(x=Tensor([0, 3],"float64"), y=Tensor([0, 3],"float64"), axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:26:51.601415 test begin: paddle.cross(x=Tensor([3, 0, 3],"float64"), y=Tensor([3, 0, 3],"float64"), axis=0, )

[cuda error] paddle.cross(x=Tensor([3, 0, 3],"float64"), y=Tensor([3, 0, 3],"float64"), axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:26:51.824439 test begin: paddle.cross(x=Tensor([3, 0, 3],"float64"), y=Tensor([3, 0, 3],"float64"), axis=2, )

[cuda error] paddle.cross(x=Tensor([3, 0, 3],"float64"), y=Tensor([3, 0, 3],"float64"), axis=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:26:52.096990 test begin: paddle.cross(x=Tensor([3, 0],"float32"), y=Tensor([3, 0],"float32"), )

[cuda error] paddle.cross(x=Tensor([3, 0],"float32"), y=Tensor([3, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:26:52.345116 test begin: paddle.cross(x=Tensor([3, 0],"float64"), y=Tensor([3, 0],"float64"), axis=0, )

[cuda error] paddle.cross(x=Tensor([3, 0],"float64"), y=Tensor([3, 0],"float64"), axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:26:52.559465 test begin: paddle.cross(x=Tensor([3, 3, 0],"float64"), y=Tensor([3, 3, 0],"float64"), axis=0, )

[cuda error] paddle.cross(x=Tensor([3, 3, 0],"float64"), y=Tensor([3, 3, 0],"float64"), axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:26:52.801995 test begin: paddle.cross(x=Tensor([3, 3, 0],"float64"), y=Tensor([3, 3, 0],"float64"), axis=1, )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   CrossGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::cross_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::CrossGradKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   phi::funcs::IndexCalculator::IndexCalculator(int, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202413 (unix time) try "date -d @1745202413" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f7ab751c521) received by PID 149483 (TID 0x7f78755fe700) from PID 18446744072490173729 ***]

2025-04-21 10:27:16.400360 test begin: paddle.cumsum(Tensor([0, 10],"int64"), 1, )

[cuda error] paddle.cumsum(Tensor([0, 10],"int64"), 1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:27:16.600006 test begin: paddle.cumsum(Tensor([0, 1],"int64"), axis=-1, )

[cuda error] paddle.cumsum(Tensor([0, 1],"int64"), axis=-1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:27:16.877061 test begin: paddle.cumsum(Tensor([0, 2, 4],"float32"), axis=1, )

[cuda error] paddle.cumsum(Tensor([0, 2, 4],"float32"), axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:27:17.272283 test begin: paddle.cumsum(Tensor([0, 20],"int64"), axis=1, )

[cuda error] paddle.cumsum(Tensor([0, 20],"int64"), axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:27:18.321935 test begin: paddle.cumsum(Tensor([1, 0],"float32"), axis=0, )

[cuda error] paddle.cumsum(Tensor([1, 0],"float32"), axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:27:19.444486 test begin: paddle.cumsum(Tensor([3, 0],"int64"), axis=-2, )

[cuda error] paddle.cumsum(Tensor([3, 0],"int64"), axis=-2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:27:19.581687 test begin: paddle.cumsum(Tensor([3, 2, 0],"float32"), axis=1, )

[cuda error] paddle.cumsum(Tensor([3, 2, 0],"float32"), axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:27:19.764106 test begin: paddle.cumsum(x=Tensor([0, 16, 96, 32],"float64"), axis=2, )

[cuda error] paddle.cumsum(x=Tensor([0, 16, 96, 32],"float64"), axis=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:27:20.047015 test begin: paddle.cumsum(x=Tensor([0, 2, 1, 3],"float64"), axis=3, )

[cuda error] paddle.cumsum(x=Tensor([0, 2, 1, 3],"float64"), axis=3, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:27:20.242999 test begin: paddle.cumsum(x=Tensor([1, 0, 1, 3],"float64"), axis=-4, )

[cuda error] paddle.cumsum(x=Tensor([1, 0, 1, 3],"float64"), axis=-4, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:27:20.451797 test begin: paddle.cumsum(x=Tensor([1, 0, 1, 3],"float64"), axis=3, )

[cuda error] paddle.cumsum(x=Tensor([1, 0, 1, 3],"float64"), axis=3, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:27:20.659057 test begin: paddle.cumsum(x=Tensor([1, 0, 96, 32],"float64"), axis=2, )

[cuda error] paddle.cumsum(x=Tensor([1, 0, 96, 32],"float64"), axis=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:27:21.108998 test begin: paddle.cumsum(x=Tensor([1, 16, 96, 0],"float64"), axis=2, )

[cuda error] paddle.cumsum(x=Tensor([1, 16, 96, 0],"float64"), axis=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:27:21.317544 test begin: paddle.cumsum(x=Tensor([1, 2, 0, 3],"float64"), axis=-4, )

[cuda error] paddle.cumsum(x=Tensor([1, 2, 0, 3],"float64"), axis=-4, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:27:21.588134 test begin: paddle.cumsum(x=Tensor([1, 2, 0, 3],"float64"), axis=3, )

[cuda error] paddle.cumsum(x=Tensor([1, 2, 0, 3],"float64"), axis=3, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:27:21.902298 test begin: paddle.cumsum(x=Tensor([1, 2, 1, 0],"float64"), axis=-4, )

[cuda error] paddle.cumsum(x=Tensor([1, 2, 1, 0],"float64"), axis=-4, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:27:22.275173 test begin: paddle.cumulative_trapezoid(y=Tensor([0, 4],"float16"), x=Tensor([0, 4],"float16"), )

[paddle error] paddle.cumulative_trapezoid(y=Tensor([0, 4],"float16"), x=Tensor([0, 4],"float16"), ) 
 (PreconditionNotMet) Tensor not initialized yet when DenseTensor::place() is called.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:62)

2025-04-21 10:27:22.427387 test begin: paddle.cumulative_trapezoid(y=Tensor([4, 0],"float16"), x=Tensor([4, 0],"float16"), )

[paddle error] paddle.cumulative_trapezoid(y=Tensor([4, 0],"float16"), x=Tensor([4, 0],"float16"), ) 
 (PreconditionNotMet) Tensor not initialized yet when DenseTensor::place() is called.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:62)

2025-04-21 10:27:23.751102 test begin: paddle.diag(Tensor([0, 10],"float32"), offset=-1, )

[paddle error] paddle.diag(Tensor([0, 10],"float32"), offset=-1, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at ../paddle/phi/core/dense_tensor.cc:113)

2025-04-21 10:27:24.852459 test begin: paddle.diag(Tensor([10, 0],"float32"), offset=1, )

[paddle error] paddle.diag(Tensor([10, 0],"float32"), offset=1, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at ../paddle/phi/core/dense_tensor.cc:113)

2025-04-21 10:27:25.218454 test begin: paddle.diag(x=Tensor([0, 3],"float64"), offset=-1, )

[paddle error] paddle.diag(x=Tensor([0, 3],"float64"), offset=-1, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at ../paddle/phi/core/dense_tensor.cc:113)

2025-04-21 10:27:26.700780 test begin: paddle.diag(x=Tensor([2, 0],"float64"), offset=2, )

[paddle error] paddle.diag(x=Tensor([2, 0],"float64"), offset=2, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at ../paddle/phi/core/dense_tensor.cc:113)

2025-04-21 10:27:27.150062 test begin: paddle.diag(x=Tensor([3, 0],"float64"), offset=1, )

[paddle error] paddle.diag(x=Tensor([3, 0],"float64"), offset=1, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at ../paddle/phi/core/dense_tensor.cc:113)

2025-04-21 10:27:27.341245 test begin: paddle.diag_embed(Tensor([0, 12],"float64"), )

[cuda error] paddle.diag_embed(Tensor([0, 12],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:27:27.490992 test begin: paddle.diag_embed(Tensor([0, 3, 12],"float64"), )

[cuda error] paddle.diag_embed(Tensor([0, 3, 12],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:27:27.705131 test begin: paddle.diag_embed(Tensor([0],"float64"), )

[cuda error] paddle.diag_embed(Tensor([0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:27:27.903991 test begin: paddle.diag_embed(Tensor([1, 0],"float64"), )

[cuda error] paddle.diag_embed(Tensor([1, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:27:28.063165 test begin: paddle.diag_embed(Tensor([2, 0, 12],"float64"), )

[cuda error] paddle.diag_embed(Tensor([2, 0, 12],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:27:28.283646 test begin: paddle.diag_embed(Tensor([2, 3, 0],"float64"), )

[cuda error] paddle.diag_embed(Tensor([2, 3, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:27:33.561737 test begin: paddle.diagonal(Tensor([0, 2, 2],"float32"), offset=0, axis1=-1, axis2=-2, )

W0421 10:27:33.759766 123671 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(Tensor([0, 2, 2],"float32"), offset=0, axis1=-1, axis2=-2, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:33.760199 test begin: paddle.diagonal(Tensor([0, 3, 4],"float32"), )

W0421 10:27:33.895479 123685 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(Tensor([0, 3, 4],"float32"), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:33.895788 test begin: paddle.diagonal(Tensor([0, 3, 4],"float32"), offset=0, axis1=1, axis2=2, )

W0421 10:27:34.107728 123686 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(Tensor([0, 3, 4],"float32"), offset=0, axis1=1, axis2=2, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:34.108477 test begin: paddle.diagonal(Tensor([0, 3, 4],"float32"), offset=0, axis1=2, axis2=1, )

W0421 10:27:34.250095 123760 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(Tensor([0, 3, 4],"float32"), offset=0, axis1=2, axis2=1, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:34.250517 test begin: paddle.diagonal(Tensor([0, 3, 4],"float32"), offset=1, axis1=0, axis2=1, )

W0421 10:27:34.464696 123907 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(Tensor([0, 3, 4],"float32"), offset=1, axis1=0, axis2=1, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:34.465056 test begin: paddle.diagonal(Tensor([1, 0, 2],"float32"), offset=0, axis1=-1, axis2=-2, )

W0421 10:27:34.660215 123915 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(Tensor([1, 0, 2],"float32"), offset=0, axis1=-1, axis2=-2, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:34.660579 test begin: paddle.diagonal(Tensor([1, 2, 0],"float32"), offset=0, axis1=-1, axis2=-2, )

W0421 10:27:34.871699 123922 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(Tensor([1, 2, 0],"float32"), offset=0, axis1=-1, axis2=-2, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:34.872054 test begin: paddle.diagonal(Tensor([10, 0, 4],"float32"), )

W0421 10:27:35.094146 124065 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(Tensor([10, 0, 4],"float32"), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:35.094583 test begin: paddle.diagonal(Tensor([10, 0, 4],"float32"), offset=0, axis1=1, axis2=2, )

W0421 10:27:35.342875 124073 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(Tensor([10, 0, 4],"float32"), offset=0, axis1=1, axis2=2, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:35.343249 test begin: paddle.diagonal(Tensor([10, 0, 4],"float32"), offset=0, axis1=2, axis2=1, )

W0421 10:27:35.488600 124224 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(Tensor([10, 0, 4],"float32"), offset=0, axis1=2, axis2=1, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:35.489026 test begin: paddle.diagonal(Tensor([10, 0, 4],"float32"), offset=1, axis1=0, axis2=1, )

W0421 10:27:35.676450 124233 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(Tensor([10, 0, 4],"float32"), offset=1, axis1=0, axis2=1, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:35.676877 test begin: paddle.diagonal(Tensor([10, 3, 0],"float32"), )

W0421 10:27:35.931738 124309 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(Tensor([10, 3, 0],"float32"), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:35.932063 test begin: paddle.diagonal(Tensor([10, 3, 0],"float32"), offset=0, axis1=1, axis2=2, )

W0421 10:27:36.055649 124516 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(Tensor([10, 3, 0],"float32"), offset=0, axis1=1, axis2=2, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:36.055933 test begin: paddle.diagonal(Tensor([10, 3, 0],"float32"), offset=0, axis1=2, axis2=1, )

W0421 10:27:36.280308 124528 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(Tensor([10, 3, 0],"float32"), offset=0, axis1=2, axis2=1, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:36.280659 test begin: paddle.diagonal(Tensor([10, 3, 0],"float32"), offset=1, axis1=0, axis2=1, )

[cuda error] paddle.diagonal(Tensor([10, 3, 0],"float32"), offset=1, axis1=0, axis2=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:27:36.423624 test begin: paddle.diagonal(x=Tensor([0, 6, 6, 2, 2],"float64"), )

W0421 10:27:36.573495 124619 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([0, 6, 6, 2, 2],"float64"), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:36.573899 test begin: paddle.diagonal(x=Tensor([0, 6, 6, 2, 2],"float64"), axis1=-1, axis2=2, )

W0421 10:27:36.787794 124696 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([0, 6, 6, 2, 2],"float64"), axis1=-1, axis2=2, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:36.788164 test begin: paddle.diagonal(x=Tensor([0, 6, 6, 2, 2],"float64"), axis1=0, axis2=3, )

W0421 10:27:37.037681 124769 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([0, 6, 6, 2, 2],"float64"), axis1=0, axis2=3, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:37.038638 test begin: paddle.diagonal(x=Tensor([0, 6, 6, 2, 2],"float64"), axis1=2, axis2=3, )

W0421 10:27:37.309464 124984 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([0, 6, 6, 2, 2],"float64"), axis1=2, axis2=3, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:37.310026 test begin: paddle.diagonal(x=Tensor([0, 6, 6, 2, 2],"float64"), axis1=3, axis2=4, )

W0421 10:27:37.525127 125126 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([0, 6, 6, 2, 2],"float64"), axis1=3, axis2=4, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:37.525593 test begin: paddle.diagonal(x=Tensor([0, 6, 6, 2, 2],"float64"), axis1=4, axis2=2, )

W0421 10:27:37.746367 125142 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([0, 6, 6, 2, 2],"float64"), axis1=4, axis2=2, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:37.746728 test begin: paddle.diagonal(x=Tensor([0, 6, 6, 6],"float64"), )

W0421 10:27:37.976461 125210 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([0, 6, 6, 6],"float64"), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:37.976816 test begin: paddle.diagonal(x=Tensor([0, 6, 6],"float64"), )

W0421 10:27:38.205432 125245 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([0, 6, 6],"float64"), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:38.205726 test begin: paddle.diagonal(x=Tensor([0, 6],"float32"), )

W0421 10:27:38.406881 125256 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([0, 6],"float32"), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:38.407290 test begin: paddle.diagonal(x=Tensor([0, 6],"float64"), offset=-1, )

W0421 10:27:38.614566 125257 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([0, 6],"float64"), offset=-1, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:38.614976 test begin: paddle.diagonal(x=Tensor([0, 6],"float64"), offset=1, )

W0421 10:27:38.863533 125399 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([0, 6],"float64"), offset=1, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:38.864885 test begin: paddle.diagonal(x=Tensor([6, 0, 6, 2, 2],"float64"), )

W0421 10:27:39.089978 125477 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([6, 0, 6, 2, 2],"float64"), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:39.090385 test begin: paddle.diagonal(x=Tensor([6, 0, 6, 2, 2],"float64"), axis1=-1, axis2=2, )

W0421 10:27:39.302333 125483 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([6, 0, 6, 2, 2],"float64"), axis1=-1, axis2=2, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:39.303999 test begin: paddle.diagonal(x=Tensor([6, 0, 6, 2, 2],"float64"), axis1=0, axis2=3, )

W0421 10:27:39.610940 125560 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([6, 0, 6, 2, 2],"float64"), axis1=0, axis2=3, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:39.611354 test begin: paddle.diagonal(x=Tensor([6, 0, 6, 2, 2],"float64"), axis1=2, axis2=3, )

W0421 10:27:39.817634 125779 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([6, 0, 6, 2, 2],"float64"), axis1=2, axis2=3, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:39.817981 test begin: paddle.diagonal(x=Tensor([6, 0, 6, 2, 2],"float64"), axis1=3, axis2=4, )

W0421 10:27:40.121824 125794 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([6, 0, 6, 2, 2],"float64"), axis1=3, axis2=4, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:40.122226 test begin: paddle.diagonal(x=Tensor([6, 0, 6, 2, 2],"float64"), axis1=4, axis2=2, )

W0421 10:27:40.418478 125941 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([6, 0, 6, 2, 2],"float64"), axis1=4, axis2=2, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:40.418890 test begin: paddle.diagonal(x=Tensor([6, 0, 6, 6],"float64"), )

W0421 10:27:40.707948 126100 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([6, 0, 6, 6],"float64"), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:40.714094 test begin: paddle.diagonal(x=Tensor([6, 0, 6],"float64"), )

W0421 10:27:41.091521 126314 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([6, 0, 6],"float64"), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:41.091913 test begin: paddle.diagonal(x=Tensor([6, 0],"float32"), )

W0421 10:27:41.295841 126474 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([6, 0],"float32"), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:41.296177 test begin: paddle.diagonal(x=Tensor([6, 0],"float64"), offset=-1, )

W0421 10:27:41.498154 126500 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([6, 0],"float64"), offset=-1, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:41.498545 test begin: paddle.diagonal(x=Tensor([6, 0],"float64"), offset=1, )

W0421 10:27:41.745998 126578 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([6, 0],"float64"), offset=1, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:41.747397 test begin: paddle.diagonal(x=Tensor([6, 6, 0, 2, 2],"float64"), )

W0421 10:27:42.148258 126653 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([6, 6, 0, 2, 2],"float64"), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:42.148662 test begin: paddle.diagonal(x=Tensor([6, 6, 0, 2, 2],"float64"), axis1=-1, axis2=2, )

W0421 10:27:42.370898 126865 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([6, 6, 0, 2, 2],"float64"), axis1=-1, axis2=2, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:42.371218 test begin: paddle.diagonal(x=Tensor([6, 6, 0, 2, 2],"float64"), axis1=0, axis2=3, )

W0421 10:27:42.576884 126876 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([6, 6, 0, 2, 2],"float64"), axis1=0, axis2=3, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:42.577248 test begin: paddle.diagonal(x=Tensor([6, 6, 0, 2, 2],"float64"), axis1=2, axis2=3, )

W0421 10:27:42.799526 126885 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([6, 6, 0, 2, 2],"float64"), axis1=2, axis2=3, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:42.800836 test begin: paddle.diagonal(x=Tensor([6, 6, 0, 2, 2],"float64"), axis1=3, axis2=4, )

W0421 10:27:43.073908 126986 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([6, 6, 0, 2, 2],"float64"), axis1=3, axis2=4, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:43.074296 test begin: paddle.diagonal(x=Tensor([6, 6, 0, 2, 2],"float64"), axis1=4, axis2=2, )

W0421 10:27:43.269869 126997 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([6, 6, 0, 2, 2],"float64"), axis1=4, axis2=2, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:43.270245 test begin: paddle.diagonal(x=Tensor([6, 6, 0, 6],"float64"), )

W0421 10:27:43.473104 127002 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([6, 6, 0, 6],"float64"), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:43.474163 test begin: paddle.diagonal(x=Tensor([6, 6, 0],"float64"), )

W0421 10:27:43.738605 127213 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([6, 6, 0],"float64"), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:43.738948 test begin: paddle.diagonal(x=Tensor([6, 6, 6, 0, 2],"float64"), )

W0421 10:27:43.934453 127235 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([6, 6, 6, 0, 2],"float64"), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:43.934793 test begin: paddle.diagonal(x=Tensor([6, 6, 6, 0, 2],"float64"), axis1=-1, axis2=2, )

W0421 10:27:44.225909 127313 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([6, 6, 6, 0, 2],"float64"), axis1=-1, axis2=2, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:44.231072 test begin: paddle.diagonal(x=Tensor([6, 6, 6, 0, 2],"float64"), axis1=0, axis2=3, )

W0421 10:27:44.533746 127597 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([6, 6, 6, 0, 2],"float64"), axis1=0, axis2=3, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:44.534760 test begin: paddle.diagonal(x=Tensor([6, 6, 6, 0, 2],"float64"), axis1=2, axis2=3, )

W0421 10:27:44.867695 127815 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([6, 6, 6, 0, 2],"float64"), axis1=2, axis2=3, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:44.868041 test begin: paddle.diagonal(x=Tensor([6, 6, 6, 0, 2],"float64"), axis1=3, axis2=4, )

W0421 10:27:45.129500 127830 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([6, 6, 6, 0, 2],"float64"), axis1=3, axis2=4, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:45.131381 test begin: paddle.diagonal(x=Tensor([6, 6, 6, 0, 2],"float64"), axis1=4, axis2=2, )

W0421 10:27:45.419852 128035 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([6, 6, 6, 0, 2],"float64"), axis1=4, axis2=2, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:45.420206 test begin: paddle.diagonal(x=Tensor([6, 6, 6, 0],"float64"), )

W0421 10:27:45.638123 128053 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([6, 6, 6, 0],"float64"), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:45.638506 test begin: paddle.diagonal(x=Tensor([6, 6, 6, 2, 0],"float64"), )

W0421 10:27:45.945974 128069 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([6, 6, 6, 2, 0],"float64"), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:45.946368 test begin: paddle.diagonal(x=Tensor([6, 6, 6, 2, 0],"float64"), axis1=-1, axis2=2, )

W0421 10:27:46.203691 128144 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([6, 6, 6, 2, 0],"float64"), axis1=-1, axis2=2, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:46.205176 test begin: paddle.diagonal(x=Tensor([6, 6, 6, 2, 0],"float64"), axis1=0, axis2=3, )

W0421 10:27:46.558218 128223 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([6, 6, 6, 2, 0],"float64"), axis1=0, axis2=3, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:46.558665 test begin: paddle.diagonal(x=Tensor([6, 6, 6, 2, 0],"float64"), axis1=2, axis2=3, )

W0421 10:27:46.842574 128361 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([6, 6, 6, 2, 0],"float64"), axis1=2, axis2=3, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:46.843028 test begin: paddle.diagonal(x=Tensor([6, 6, 6, 2, 0],"float64"), axis1=3, axis2=4, )

W0421 10:27:47.167392 128441 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([6, 6, 6, 2, 0],"float64"), axis1=3, axis2=4, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:27:47.167795 test begin: paddle.diagonal(x=Tensor([6, 6, 6, 2, 0],"float64"), axis1=4, axis2=2, )

W0421 10:27:47.365002 128516 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.diagonal(x=Tensor([6, 6, 6, 2, 0],"float64"), axis1=4, axis2=2, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:28:08.250606 test begin: paddle.digamma(Tensor([0, 10, 10, 2],"float64"), )

[cuda error] paddle.digamma(Tensor([0, 10, 10, 2],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:08.524600 test begin: paddle.digamma(Tensor([0, 2, 2],"float32"), )

[cuda error] paddle.digamma(Tensor([0, 2, 2],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:08.733513 test begin: paddle.digamma(Tensor([0, 2],"float32"), )

[cuda error] paddle.digamma(Tensor([0, 2],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:08.955774 test begin: paddle.digamma(Tensor([0],"float32"), )

[cuda error] paddle.digamma(Tensor([0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:09.169585 test begin: paddle.digamma(Tensor([1, 0, 2],"float32"), )

[cuda error] paddle.digamma(Tensor([1, 0, 2],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:09.337860 test begin: paddle.digamma(Tensor([1, 0],"float32"), )

[cuda error] paddle.digamma(Tensor([1, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:09.459437 test begin: paddle.digamma(Tensor([1, 2, 0],"float32"), )

[cuda error] paddle.digamma(Tensor([1, 2, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:09.576856 test begin: paddle.digamma(Tensor([10, 0, 10, 2],"float64"), )

[cuda error] paddle.digamma(Tensor([10, 0, 10, 2],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:09.693972 test begin: paddle.digamma(Tensor([10, 10, 0, 2],"float64"), )

[cuda error] paddle.digamma(Tensor([10, 10, 0, 2],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:09.815815 test begin: paddle.digamma(Tensor([10, 10, 10, 0],"float64"), )

[cuda error] paddle.digamma(Tensor([10, 10, 10, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:09.940486 test begin: paddle.digamma(x=Tensor([0, 3],"float32"), )

[cuda error] paddle.digamma(x=Tensor([0, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:10.057009 test begin: paddle.digamma(x=Tensor([0, 6, 6, 6, 6],"float64"), )

[cuda error] paddle.digamma(x=Tensor([0, 6, 6, 6, 6],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:10.177818 test begin: paddle.digamma(x=Tensor([0, 6, 6, 6],"float64"), )

[cuda error] paddle.digamma(x=Tensor([0, 6, 6, 6],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:10.295206 test begin: paddle.digamma(x=Tensor([0, 6, 6],"float64"), )

[cuda error] paddle.digamma(x=Tensor([0, 6, 6],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:10.426130 test begin: paddle.digamma(x=Tensor([3, 0, 6, 6, 6],"float64"), )

[cuda error] paddle.digamma(x=Tensor([3, 0, 6, 6, 6],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:10.558137 test begin: paddle.digamma(x=Tensor([3, 0],"float32"), )

[cuda error] paddle.digamma(x=Tensor([3, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:10.723158 test begin: paddle.digamma(x=Tensor([3, 6, 0, 6, 6],"float64"), )

[cuda error] paddle.digamma(x=Tensor([3, 6, 0, 6, 6],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:10.839718 test begin: paddle.digamma(x=Tensor([3, 6, 6, 0, 6],"float64"), )

[cuda error] paddle.digamma(x=Tensor([3, 6, 6, 0, 6],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:11.063248 test begin: paddle.digamma(x=Tensor([3, 6, 6, 6, 0],"float64"), )

[cuda error] paddle.digamma(x=Tensor([3, 6, 6, 6, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:11.244758 test begin: paddle.digamma(x=Tensor([6, 0, 6, 6],"float64"), )

[cuda error] paddle.digamma(x=Tensor([6, 0, 6, 6],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:11.391345 test begin: paddle.digamma(x=Tensor([6, 0, 6],"float64"), )

[cuda error] paddle.digamma(x=Tensor([6, 0, 6],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:11.509596 test begin: paddle.digamma(x=Tensor([6, 6, 0, 6],"float64"), )

[cuda error] paddle.digamma(x=Tensor([6, 6, 0, 6],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:11.633404 test begin: paddle.digamma(x=Tensor([6, 6, 0],"float64"), )

[cuda error] paddle.digamma(x=Tensor([6, 6, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:11.768689 test begin: paddle.digamma(x=Tensor([6, 6, 6, 0],"float64"), )

[cuda error] paddle.digamma(x=Tensor([6, 6, 6, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:11.976737 test begin: paddle.dist(Tensor([0, 2, 3, 2],"float32"), Tensor([0, 1, 3, 1],"float32"), 2, )

[paddle error] paddle.dist(Tensor([0, 2, 3, 2],"float32"), Tensor([0, 1, 3, 1],"float32"), 2, ) 
 (InvalidArgument) The Input(X) has not been initialized properly. The shape of Input(X) = [0, 2, 3, 2].
  [Hint: Expected common::product(x_dims) != 0, but received common::product(x_dims):0 == 0:0.] (at ../paddle/phi/infermeta/binary.cc:1371)

2025-04-21 10:28:12.140900 test begin: paddle.dist(Tensor([0, 2, 3, 2],"float32"), Tensor([1, 1, 3, 1],"float32"), 2, )

[paddle error] paddle.dist(Tensor([0, 2, 3, 2],"float32"), Tensor([1, 1, 3, 1],"float32"), 2, ) 
 (InvalidArgument) The Input(X) has not been initialized properly. The shape of Input(X) = [0, 2, 3, 2].
  [Hint: Expected common::product(x_dims) != 0, but received common::product(x_dims):0 == 0:0.] (at ../paddle/phi/infermeta/binary.cc:1371)

2025-04-21 10:28:12.286085 test begin: paddle.dist(Tensor([0, 2],"float32"), Tensor([0, 2],"float32"), 0, )

Expected a proper Tensor but got None (or an undefined Tensor in C++) for argument #0 'self'
[paddle error] paddle.dist(Tensor([0, 2],"float32"), Tensor([0, 2],"float32"), 0, ) 
 (InvalidArgument) The Input(X) has not been initialized properly. The shape of Input(X) = [0, 2].
  [Hint: Expected common::product(x_dims) != 0, but received common::product(x_dims):0 == 0:0.] (at ../paddle/phi/infermeta/binary.cc:1371)

2025-04-21 10:28:12.415573 test begin: paddle.dist(Tensor([2, 0, 3, 2],"float32"), Tensor([1, 0, 3, 1],"float32"), 2, )

[paddle error] paddle.dist(Tensor([2, 0, 3, 2],"float32"), Tensor([1, 0, 3, 1],"float32"), 2, ) 
 (InvalidArgument) The Input(X) has not been initialized properly. The shape of Input(X) = [2, 0, 3, 2].
  [Hint: Expected common::product(x_dims) != 0, but received common::product(x_dims):0 == 0:0.] (at ../paddle/phi/infermeta/binary.cc:1371)

2025-04-21 10:28:12.596475 test begin: paddle.dist(Tensor([2, 0, 3, 2],"float32"), Tensor([1, 1, 3, 1],"float32"), 2, )

[paddle error] paddle.dist(Tensor([2, 0, 3, 2],"float32"), Tensor([1, 1, 3, 1],"float32"), 2, ) 
 (InvalidArgument) The Input(X) has not been initialized properly. The shape of Input(X) = [2, 0, 3, 2].
  [Hint: Expected common::product(x_dims) != 0, but received common::product(x_dims):0 == 0:0.] (at ../paddle/phi/infermeta/binary.cc:1371)

2025-04-21 10:28:12.731789 test begin: paddle.dist(Tensor([2, 0],"float32"), Tensor([2, 0],"float32"), 0, )

Expected a proper Tensor but got None (or an undefined Tensor in C++) for argument #0 'self'
[paddle error] paddle.dist(Tensor([2, 0],"float32"), Tensor([2, 0],"float32"), 0, ) 
 (InvalidArgument) The Input(X) has not been initialized properly. The shape of Input(X) = [2, 0].
  [Hint: Expected common::product(x_dims) != 0, but received common::product(x_dims):0 == 0:0.] (at ../paddle/phi/infermeta/binary.cc:1371)

2025-04-21 10:28:12.893168 test begin: paddle.dist(Tensor([2, 2, 0, 2],"float32"), Tensor([1, 1, 0, 1],"float32"), 2, )

[paddle error] paddle.dist(Tensor([2, 2, 0, 2],"float32"), Tensor([1, 1, 0, 1],"float32"), 2, ) 
 (InvalidArgument) The Input(X) has not been initialized properly. The shape of Input(X) = [2, 2, 0, 2].
  [Hint: Expected common::product(x_dims) != 0, but received common::product(x_dims):0 == 0:0.] (at ../paddle/phi/infermeta/binary.cc:1371)

2025-04-21 10:28:13.014532 test begin: paddle.dist(Tensor([2, 2, 3, 0],"float32"), Tensor([1, 1, 3, 0],"float32"), 2, )

[paddle error] paddle.dist(Tensor([2, 2, 3, 0],"float32"), Tensor([1, 1, 3, 0],"float32"), 2, ) 
 (InvalidArgument) The Input(X) has not been initialized properly. The shape of Input(X) = [2, 2, 3, 0].
  [Hint: Expected common::product(x_dims) != 0, but received common::product(x_dims):0 == 0:0.] (at ../paddle/phi/infermeta/binary.cc:1371)

2025-04-21 10:28:13.194820 test begin: paddle.dist(Tensor([2, 2, 3, 0],"float32"), Tensor([1, 1, 3, 1],"float32"), 2, )

[paddle error] paddle.dist(Tensor([2, 2, 3, 0],"float32"), Tensor([1, 1, 3, 1],"float32"), 2, ) 
 (InvalidArgument) The Input(X) has not been initialized properly. The shape of Input(X) = [2, 2, 3, 0].
  [Hint: Expected common::product(x_dims) != 0, but received common::product(x_dims):0 == 0:0.] (at ../paddle/phi/infermeta/binary.cc:1371)

2025-04-21 10:28:13.360347 test begin: paddle.dist(x=Tensor([0, 1, 1, 4, 4],"float64"), y=Tensor([0, 8, 7, 1, 4],"float64"), )

[paddle error] paddle.dist(x=Tensor([0, 1, 1, 4, 4],"float64"), y=Tensor([0, 8, 7, 1, 4],"float64"), ) 
 (InvalidArgument) The Input(X) has not been initialized properly. The shape of Input(X) = [0, 1, 1, 4, 4].
  [Hint: Expected common::product(x_dims) != 0, but received common::product(x_dims):0 == 0:0.] (at ../paddle/phi/infermeta/binary.cc:1371)

2025-04-21 10:28:13.576664 test begin: paddle.dist(x=Tensor([0, 1, 4, 4],"float64"), y=Tensor([7, 1, 4],"float64"), )

[paddle error] paddle.dist(x=Tensor([0, 1, 4, 4],"float64"), y=Tensor([7, 1, 4],"float64"), ) 
 (InvalidArgument) The Input(X) has not been initialized properly. The shape of Input(X) = [0, 1, 4, 4].
  [Hint: Expected common::product(x_dims) != 0, but received common::product(x_dims):0 == 0:0.] (at ../paddle/phi/infermeta/binary.cc:1371)

2025-04-21 10:28:13.816037 test begin: paddle.dist(x=Tensor([0, 2],"float64"), y=Tensor([0, 2],"float64"), p=0, )

Expected a proper Tensor but got None (or an undefined Tensor in C++) for argument #0 'self'
[paddle error] paddle.dist(x=Tensor([0, 2],"float64"), y=Tensor([0, 2],"float64"), p=0, ) 
 (InvalidArgument) The Input(X) has not been initialized properly. The shape of Input(X) = [0, 2].
  [Hint: Expected common::product(x_dims) != 0, but received common::product(x_dims):0 == 0:0.] (at ../paddle/phi/infermeta/binary.cc:1371)

2025-04-21 10:28:13.952225 test begin: paddle.dist(x=Tensor([0, 4, 1, 3],"float64"), y=Tensor([4, 3, 1],"float64"), p=7, )

[paddle error] paddle.dist(x=Tensor([0, 4, 1, 3],"float64"), y=Tensor([4, 3, 1],"float64"), p=7, ) 
 (InvalidArgument) The Input(X) has not been initialized properly. The shape of Input(X) = [0, 4, 1, 3].
  [Hint: Expected common::product(x_dims) != 0, but received common::product(x_dims):0 == 0:0.] (at ../paddle/phi/infermeta/binary.cc:1371)

2025-04-21 10:28:14.145867 test begin: paddle.dist(x=Tensor([0, 4],"float32"), y=Tensor([0, 4],"float32"), )

[paddle error] paddle.dist(x=Tensor([0, 4],"float32"), y=Tensor([0, 4],"float32"), ) 
 (InvalidArgument) The Input(X) has not been initialized properly. The shape of Input(X) = [0, 4].
  [Hint: Expected common::product(x_dims) != 0, but received common::product(x_dims):0 == 0:0.] (at ../paddle/phi/infermeta/binary.cc:1371)

2025-04-21 10:28:14.329542 test begin: paddle.dist(x=Tensor([0, 4],"float64"), y=Tensor([0, 4],"float64"), p=1, )

[paddle error] paddle.dist(x=Tensor([0, 4],"float64"), y=Tensor([0, 4],"float64"), p=1, ) 
 (InvalidArgument) The Input(X) has not been initialized properly. The shape of Input(X) = [0, 4].
  [Hint: Expected common::product(x_dims) != 0, but received common::product(x_dims):0 == 0:0.] (at ../paddle/phi/infermeta/binary.cc:1371)

2025-04-21 10:28:14.499007 test begin: paddle.dist(x=Tensor([0],"float64"), y=Tensor([0],"float64"), )

[paddle error] paddle.dist(x=Tensor([0],"float64"), y=Tensor([0],"float64"), ) 
 (InvalidArgument) The Input(X) has not been initialized properly. The shape of Input(X) = [0].
  [Hint: Expected common::product(x_dims) != 0, but received common::product(x_dims):0 == 0:0.] (at ../paddle/phi/infermeta/binary.cc:1371)

2025-04-21 10:28:14.629272 test begin: paddle.dist(x=Tensor([10],"float64"), y=Tensor([0, 10],"float64"), )

[paddle error] paddle.dist(x=Tensor([10],"float64"), y=Tensor([0, 10],"float64"), ) 
 (InvalidArgument) The Input(Y) has not been initialized properly. The shape of Input(Y) = [0, 10].
  [Hint: Expected common::product(y_dims) != 0, but received common::product(y_dims):0 == 0:0.] (at ../paddle/phi/infermeta/binary.cc:1377)

2025-04-21 10:28:14.753513 test begin: paddle.dist(x=Tensor([10],"float64"), y=Tensor([0, 10],"float64"), p=4, )

[paddle error] paddle.dist(x=Tensor([10],"float64"), y=Tensor([0, 10],"float64"), p=4, ) 
 (InvalidArgument) The Input(Y) has not been initialized properly. The shape of Input(Y) = [0, 10].
  [Hint: Expected common::product(y_dims) != 0, but received common::product(y_dims):0 == 0:0.] (at ../paddle/phi/infermeta/binary.cc:1377)

2025-04-21 10:28:14.941975 test begin: paddle.dist(x=Tensor([2, 0, 1, 4, 4],"float64"), y=Tensor([2, 0, 7, 1, 4],"float64"), )

[paddle error] paddle.dist(x=Tensor([2, 0, 1, 4, 4],"float64"), y=Tensor([2, 0, 7, 1, 4],"float64"), ) 
 (InvalidArgument) The Input(X) has not been initialized properly. The shape of Input(X) = [2, 0, 1, 4, 4].
  [Hint: Expected common::product(x_dims) != 0, but received common::product(x_dims):0 == 0:0.] (at ../paddle/phi/infermeta/binary.cc:1371)

2025-04-21 10:28:15.123097 test begin: paddle.dist(x=Tensor([2, 0],"float64"), y=Tensor([2, 0],"float64"), p=0, )

Expected a proper Tensor but got None (or an undefined Tensor in C++) for argument #0 'self'
[paddle error] paddle.dist(x=Tensor([2, 0],"float64"), y=Tensor([2, 0],"float64"), p=0, ) 
 (InvalidArgument) The Input(X) has not been initialized properly. The shape of Input(X) = [2, 0].
  [Hint: Expected common::product(x_dims) != 0, but received common::product(x_dims):0 == 0:0.] (at ../paddle/phi/infermeta/binary.cc:1371)

2025-04-21 10:28:15.275110 test begin: paddle.dist(x=Tensor([2, 1, 0, 4, 4],"float64"), y=Tensor([2, 8, 0, 1, 4],"float64"), )

[paddle error] paddle.dist(x=Tensor([2, 1, 0, 4, 4],"float64"), y=Tensor([2, 8, 0, 1, 4],"float64"), ) 
 (InvalidArgument) The Input(X) has not been initialized properly. The shape of Input(X) = [2, 1, 0, 4, 4].
  [Hint: Expected common::product(x_dims) != 0, but received common::product(x_dims):0 == 0:0.] (at ../paddle/phi/infermeta/binary.cc:1371)

2025-04-21 10:28:15.438171 test begin: paddle.dist(x=Tensor([2, 1, 0, 4],"float64"), y=Tensor([7, 1, 4],"float64"), )

[paddle error] paddle.dist(x=Tensor([2, 1, 0, 4],"float64"), y=Tensor([7, 1, 4],"float64"), ) 
 (InvalidArgument) The Input(X) has not been initialized properly. The shape of Input(X) = [2, 1, 0, 4].
  [Hint: Expected common::product(x_dims) != 0, but received common::product(x_dims):0 == 0:0.] (at ../paddle/phi/infermeta/binary.cc:1371)

2025-04-21 10:28:15.594629 test begin: paddle.dist(x=Tensor([2, 1, 1, 0, 4],"float64"), y=Tensor([2, 8, 7, 0, 4],"float64"), )

[paddle error] paddle.dist(x=Tensor([2, 1, 1, 0, 4],"float64"), y=Tensor([2, 8, 7, 0, 4],"float64"), ) 
 (InvalidArgument) The Input(X) has not been initialized properly. The shape of Input(X) = [2, 1, 1, 0, 4].
  [Hint: Expected common::product(x_dims) != 0, but received common::product(x_dims):0 == 0:0.] (at ../paddle/phi/infermeta/binary.cc:1371)

2025-04-21 10:28:15.753067 test begin: paddle.dist(x=Tensor([2, 1, 1, 0, 4],"float64"), y=Tensor([2, 8, 7, 1, 4],"float64"), )

[paddle error] paddle.dist(x=Tensor([2, 1, 1, 0, 4],"float64"), y=Tensor([2, 8, 7, 1, 4],"float64"), ) 
 (InvalidArgument) The Input(X) has not been initialized properly. The shape of Input(X) = [2, 1, 1, 0, 4].
  [Hint: Expected common::product(x_dims) != 0, but received common::product(x_dims):0 == 0:0.] (at ../paddle/phi/infermeta/binary.cc:1371)

2025-04-21 10:28:15.938210 test begin: paddle.dist(x=Tensor([2, 1, 1, 4, 0],"float64"), y=Tensor([2, 8, 7, 1, 0],"float64"), )

[paddle error] paddle.dist(x=Tensor([2, 1, 1, 4, 0],"float64"), y=Tensor([2, 8, 7, 1, 0],"float64"), ) 
 (InvalidArgument) The Input(X) has not been initialized properly. The shape of Input(X) = [2, 1, 1, 4, 0].
  [Hint: Expected common::product(x_dims) != 0, but received common::product(x_dims):0 == 0:0.] (at ../paddle/phi/infermeta/binary.cc:1371)

2025-04-21 10:28:16.131618 test begin: paddle.dist(x=Tensor([2, 1, 1, 4, 4],"float64"), y=Tensor([2, 0, 7, 1, 4],"float64"), )

[paddle error] paddle.dist(x=Tensor([2, 1, 1, 4, 4],"float64"), y=Tensor([2, 0, 7, 1, 4],"float64"), ) 
 (InvalidArgument) The Input(Y) has not been initialized properly. The shape of Input(Y) = [2, 0, 7, 1, 4].
  [Hint: Expected common::product(y_dims) != 0, but received common::product(y_dims):0 == 0:0.] (at ../paddle/phi/infermeta/binary.cc:1377)

2025-04-21 10:28:16.370032 test begin: paddle.dist(x=Tensor([2, 1, 1, 4, 4],"float64"), y=Tensor([2, 8, 0, 1, 4],"float64"), )

[paddle error] paddle.dist(x=Tensor([2, 1, 1, 4, 4],"float64"), y=Tensor([2, 8, 0, 1, 4],"float64"), ) 
 (InvalidArgument) The Input(Y) has not been initialized properly. The shape of Input(Y) = [2, 8, 0, 1, 4].
  [Hint: Expected common::product(y_dims) != 0, but received common::product(y_dims):0 == 0:0.] (at ../paddle/phi/infermeta/binary.cc:1377)

2025-04-21 10:28:16.591768 test begin: paddle.dist(x=Tensor([2, 1, 4, 4],"float64"), y=Tensor([0, 1, 4],"float64"), )

[paddle error] paddle.dist(x=Tensor([2, 1, 4, 4],"float64"), y=Tensor([0, 1, 4],"float64"), ) 
 (InvalidArgument) The Input(Y) has not been initialized properly. The shape of Input(Y) = [0, 1, 4].
  [Hint: Expected common::product(y_dims) != 0, but received common::product(y_dims):0 == 0:0.] (at ../paddle/phi/infermeta/binary.cc:1377)

2025-04-21 10:28:16.754539 test begin: paddle.dist(x=Tensor([2, 4, 1, 0],"float64"), y=Tensor([4, 3, 1],"float64"), p=7, )

[paddle error] paddle.dist(x=Tensor([2, 4, 1, 0],"float64"), y=Tensor([4, 3, 1],"float64"), p=7, ) 
 (InvalidArgument) The Input(X) has not been initialized properly. The shape of Input(X) = [2, 4, 1, 0].
  [Hint: Expected common::product(x_dims) != 0, but received common::product(x_dims):0 == 0:0.] (at ../paddle/phi/infermeta/binary.cc:1371)

2025-04-21 10:28:17.009391 test begin: paddle.dist(x=Tensor([2, 4, 1, 3],"float64"), y=Tensor([4, 0, 1],"float64"), p=7, )

[paddle error] paddle.dist(x=Tensor([2, 4, 1, 3],"float64"), y=Tensor([4, 0, 1],"float64"), p=7, ) 
 (InvalidArgument) The Input(Y) has not been initialized properly. The shape of Input(Y) = [4, 0, 1].
  [Hint: Expected common::product(y_dims) != 0, but received common::product(y_dims):0 == 0:0.] (at ../paddle/phi/infermeta/binary.cc:1377)

2025-04-21 10:28:17.245712 test begin: paddle.dist(x=Tensor([4, 0],"float32"), y=Tensor([4, 0],"float32"), )

[paddle error] paddle.dist(x=Tensor([4, 0],"float32"), y=Tensor([4, 0],"float32"), ) 
 (InvalidArgument) The Input(X) has not been initialized properly. The shape of Input(X) = [4, 0].
  [Hint: Expected common::product(x_dims) != 0, but received common::product(x_dims):0 == 0:0.] (at ../paddle/phi/infermeta/binary.cc:1371)

2025-04-21 10:28:17.473495 test begin: paddle.dist(x=Tensor([4, 0],"float64"), y=Tensor([4, 0],"float64"), p=1, )

[paddle error] paddle.dist(x=Tensor([4, 0],"float64"), y=Tensor([4, 0],"float64"), p=1, ) 
 (InvalidArgument) The Input(X) has not been initialized properly. The shape of Input(X) = [4, 0].
  [Hint: Expected common::product(x_dims) != 0, but received common::product(x_dims):0 == 0:0.] (at ../paddle/phi/infermeta/binary.cc:1371)

2025-04-21 10:28:22.425862 test begin: paddle.dstack(list[Tensor([0, 1],"float64"),Tensor([0, 1],"float64"),Tensor([0, 1],"float64"),], )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   UnsqueezeGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::unsqueeze_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   paddle::Tensor::type() const

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202502 (unix time) try "date -d @1745202502" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x0) received by PID 113022 (TID 0x7f22b51d3700) from PID 0 ***]

2025-04-21 10:28:49.762298 test begin: paddle.dstack(list[Tensor([0, 1],"float64"),], )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   UnsqueezeGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::unsqueeze_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   paddle::Tensor::type() const

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202530 (unix time) try "date -d @1745202530" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x0) received by PID 147848 (TID 0x7f328f2b7700) from PID 0 ***]

2025-04-21 10:28:54.388595 test begin: paddle.dstack(list[Tensor([0],"float64"),Tensor([0],"float64"),Tensor([0],"float64"),], )

W0421 10:29:00.404956 153328 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:29:00.406051 153328 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   UnsqueezeGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::unsqueeze_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   paddle::Tensor::type() const

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202540 (unix time) try "date -d @1745202540" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x0) received by PID 152051 (TID 0x7f7fe1d0b700) from PID 0 ***]

2025-04-21 10:29:09.783544 test begin: paddle.dstack(list[Tensor([0],"float64"),], )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   UnsqueezeGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::unsqueeze_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   paddle::Tensor::type() const

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202550 (unix time) try "date -d @1745202550" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x0) received by PID 155655 (TID 0x7f90d8b15700) from PID 0 ***]

2025-04-21 10:29:36.015816 test begin: paddle.dstack(list[Tensor([1, 0],"float64"),Tensor([1, 0],"float64"),Tensor([1, 0],"float64"),], )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   UnsqueezeGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::unsqueeze_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   paddle::Tensor::type() const

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202576 (unix time) try "date -d @1745202576" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x0) received by PID 1902 (TID 0x7fc44f5fe700) from PID 0 ***]

2025-04-21 10:29:59.209939 test begin: paddle.dstack(list[Tensor([1, 0],"float64"),], )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   UnsqueezeGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::unsqueeze_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   paddle::Tensor::type() const

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202599 (unix time) try "date -d @1745202599" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x0) received by PID 10919 (TID 0x7f5a7e949700) from PID 0 ***]

2025-04-21 10:30:57.633690 test begin: paddle.fft.fft2(x=Tensor([3, 3, 2, 0],"complex128"), s=tuple(1,2,), )

W0421 10:30:57.871443 37478 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.fft2(x=Tensor([3, 3, 2, 0],"complex128"), s=tuple(1,2,), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:30:58.142748 test begin: paddle.fft.fftn(Tensor([3, 3, 2, 0],"complex128"), tuple(1,2,), tuple(-2,-1,), "backward", None, )

W0421 10:30:58.277981 38089 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.fftn(Tensor([3, 3, 2, 0],"complex128"), tuple(1,2,), tuple(-2,-1,), "backward", None, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:30:58.441327 test begin: paddle.fft.fftn(x=Tensor([4, 0, 6, 2],"float64"), s=list[2,4,], axes=tuple(0,1,), )

W0421 10:30:58.657020 38182 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.fftn(x=Tensor([4, 0, 6, 2],"float64"), s=list[2,4,], axes=tuple(0,1,), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:30:58.663586 test begin: paddle.fft.fftn(x=Tensor([4, 0, 6],"float64"), s=list[2,4,], )

W0421 10:30:58.895325 38205 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.fftn(x=Tensor([4, 0, 6],"float64"), s=list[2,4,], ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:30:59.129964 test begin: paddle.fft.fftn(x=Tensor([4, 4, 0],"float64"), s=list[2,4,], )

W0421 10:30:59.295876 38219 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.fftn(x=Tensor([4, 4, 0],"float64"), s=list[2,4,], ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:30:59.297864 test begin: paddle.fft.fftshift(x=Tensor([0, 4, 2],"float64"), )

[cuda error] paddle.fft.fftshift(x=Tensor([0, 4, 2],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:02.060826 test begin: paddle.fft.fftshift(x=Tensor([0, 5, 4, 4],"complex128"), axes=3, )

W0421 10:07:07.636829 155240 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:07:07.637957 155240 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.fft.fftshift(x=Tensor([0, 5, 4, 4],"complex128"), axes=3, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:07.648309 test begin: paddle.fft.fftshift(x=Tensor([0, 5, 4, 4],"complex128"), axes=tuple(1,3,), )

[cuda error] paddle.fft.fftshift(x=Tensor([0, 5, 4, 4],"complex128"), axes=tuple(1,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:07.859943 test begin: paddle.fft.fftshift(x=Tensor([0],"float32"), )

[cuda error] paddle.fft.fftshift(x=Tensor([0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:08.050936 test begin: paddle.fft.fftshift(x=Tensor([2, 0, 2],"float64"), )

[cuda error] paddle.fft.fftshift(x=Tensor([2, 0, 2],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:08.241465 test begin: paddle.fft.fftshift(x=Tensor([2, 4, 0],"float64"), )

[cuda error] paddle.fft.fftshift(x=Tensor([2, 4, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:08.382123 test begin: paddle.fft.fftshift(x=Tensor([4, 0, 4, 4],"complex128"), )

[cuda error] paddle.fft.fftshift(x=Tensor([4, 0, 4, 4],"complex128"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:08.564276 test begin: paddle.fft.fftshift(x=Tensor([4, 0, 4, 4],"complex128"), axes=3, )

[cuda error] paddle.fft.fftshift(x=Tensor([4, 0, 4, 4],"complex128"), axes=3, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:08.788942 test begin: paddle.fft.fftshift(x=Tensor([4, 0, 4, 4],"complex128"), axes=tuple(1,3,), )

[cuda error] paddle.fft.fftshift(x=Tensor([4, 0, 4, 4],"complex128"), axes=tuple(1,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:08.978055 test begin: paddle.fft.fftshift(x=Tensor([4, 5, 0, 4],"complex128"), )

[cuda error] paddle.fft.fftshift(x=Tensor([4, 5, 0, 4],"complex128"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:09.183476 test begin: paddle.fft.fftshift(x=Tensor([4, 5, 0, 4],"complex128"), axes=3, )

[cuda error] paddle.fft.fftshift(x=Tensor([4, 5, 0, 4],"complex128"), axes=3, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:09.378386 test begin: paddle.fft.fftshift(x=Tensor([4, 5, 0, 4],"complex128"), axes=tuple(1,3,), )

[cuda error] paddle.fft.fftshift(x=Tensor([4, 5, 0, 4],"complex128"), axes=tuple(1,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:09.565464 test begin: paddle.fft.fftshift(x=Tensor([4, 5, 4, 0],"complex128"), )

[cuda error] paddle.fft.fftshift(x=Tensor([4, 5, 4, 0],"complex128"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:09.776635 test begin: paddle.fft.fftshift(x=Tensor([4, 5, 4, 0],"complex128"), axes=3, )

[cuda error] paddle.fft.fftshift(x=Tensor([4, 5, 4, 0],"complex128"), axes=3, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:09.993449 test begin: paddle.fft.fftshift(x=Tensor([4, 5, 4, 0],"complex128"), axes=tuple(1,3,), )

[cuda error] paddle.fft.fftshift(x=Tensor([4, 5, 4, 0],"complex128"), axes=tuple(1,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:15.873162 test begin: paddle.fft.ifft2(x=Tensor([3, 3, 2, 0],"complex128"), s=tuple(1,2,), )

W0421 10:07:16.072616 160941 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.ifft2(x=Tensor([3, 3, 2, 0],"complex128"), s=tuple(1,2,), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:07:16.231500 test begin: paddle.fft.ifftn(Tensor([3, 3, 2, 0],"complex128"), tuple(1,2,), tuple(-2,-1,), "backward", None, )

W0421 10:07:16.398911 160975 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.ifftn(Tensor([3, 3, 2, 0],"complex128"), tuple(1,2,), tuple(-2,-1,), "backward", None, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:07:16.606331 test begin: paddle.fft.ifftn(x=Tensor([4, 0, 6, 2],"float64"), s=list[2,4,], axes=tuple(0,1,), )

W0421 10:07:16.798939 161079 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.ifftn(x=Tensor([4, 0, 6, 2],"float64"), s=list[2,4,], axes=tuple(0,1,), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:07:16.800798 test begin: paddle.fft.ifftn(x=Tensor([4, 0, 6],"float64"), s=list[2,4,], )

W0421 10:07:16.997946 161084 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.ifftn(x=Tensor([4, 0, 6],"float64"), s=list[2,4,], ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:07:17.196989 test begin: paddle.fft.ifftn(x=Tensor([4, 4, 0],"float64"), s=list[2,4,], )

W0421 10:07:17.384858 161332 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.ifftn(x=Tensor([4, 4, 0],"float64"), s=list[2,4,], ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:07:17.386593 test begin: paddle.fft.ifftshift(x=Tensor([0, 4, 2],"float64"), )

[cuda error] paddle.fft.ifftshift(x=Tensor([0, 4, 2],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:17.642544 test begin: paddle.fft.ifftshift(x=Tensor([0, 5, 4, 4],"complex128"), )

[cuda error] paddle.fft.ifftshift(x=Tensor([0, 5, 4, 4],"complex128"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:17.838206 test begin: paddle.fft.ifftshift(x=Tensor([0, 5, 4, 4],"complex128"), axes=3, )

[cuda error] paddle.fft.ifftshift(x=Tensor([0, 5, 4, 4],"complex128"), axes=3, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:18.029752 test begin: paddle.fft.ifftshift(x=Tensor([0, 5, 4, 4],"complex128"), axes=tuple(0,3,), )

[cuda error] paddle.fft.ifftshift(x=Tensor([0, 5, 4, 4],"complex128"), axes=tuple(0,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:18.243019 test begin: paddle.fft.ifftshift(x=Tensor([0],"float32"), )

[cuda error] paddle.fft.ifftshift(x=Tensor([0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:18.388598 test begin: paddle.fft.ifftshift(x=Tensor([2, 0, 2],"float64"), )

[cuda error] paddle.fft.ifftshift(x=Tensor([2, 0, 2],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:18.557935 test begin: paddle.fft.ifftshift(x=Tensor([2, 4, 0],"float64"), )

[cuda error] paddle.fft.ifftshift(x=Tensor([2, 4, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:18.758011 test begin: paddle.fft.ifftshift(x=Tensor([4, 0, 4, 4],"complex128"), )

[cuda error] paddle.fft.ifftshift(x=Tensor([4, 0, 4, 4],"complex128"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:18.982873 test begin: paddle.fft.ifftshift(x=Tensor([4, 0, 4, 4],"complex128"), axes=3, )

[cuda error] paddle.fft.ifftshift(x=Tensor([4, 0, 4, 4],"complex128"), axes=3, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:19.206845 test begin: paddle.fft.ifftshift(x=Tensor([4, 0, 4, 4],"complex128"), axes=tuple(0,3,), )

[cuda error] paddle.fft.ifftshift(x=Tensor([4, 0, 4, 4],"complex128"), axes=tuple(0,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:19.405662 test begin: paddle.fft.ifftshift(x=Tensor([4, 5, 0, 4],"complex128"), )

[cuda error] paddle.fft.ifftshift(x=Tensor([4, 5, 0, 4],"complex128"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:19.610145 test begin: paddle.fft.ifftshift(x=Tensor([4, 5, 0, 4],"complex128"), axes=3, )

[cuda error] paddle.fft.ifftshift(x=Tensor([4, 5, 0, 4],"complex128"), axes=3, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:19.755116 test begin: paddle.fft.ifftshift(x=Tensor([4, 5, 0, 4],"complex128"), axes=tuple(0,3,), )

[cuda error] paddle.fft.ifftshift(x=Tensor([4, 5, 0, 4],"complex128"), axes=tuple(0,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:19.887051 test begin: paddle.fft.ifftshift(x=Tensor([4, 5, 4, 0],"complex128"), )

[cuda error] paddle.fft.ifftshift(x=Tensor([4, 5, 4, 0],"complex128"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:20.104235 test begin: paddle.fft.ifftshift(x=Tensor([4, 5, 4, 0],"complex128"), axes=3, )

[cuda error] paddle.fft.ifftshift(x=Tensor([4, 5, 4, 0],"complex128"), axes=3, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:20.311962 test begin: paddle.fft.ifftshift(x=Tensor([4, 5, 4, 0],"complex128"), axes=tuple(0,3,), )

[cuda error] paddle.fft.ifftshift(x=Tensor([4, 5, 4, 0],"complex128"), axes=tuple(0,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:20.949781 test begin: paddle.fft.ihfft2(x=Tensor([0, 3, 3],"float64"), s=tuple(1,2,), axes=tuple(0,2,), )

W0421 10:07:21.163617  1265 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.ihfft2(x=Tensor([0, 3, 3],"float64"), s=tuple(1,2,), axes=tuple(0,2,), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:07:21.164030 test begin: paddle.fft.ihfft2(x=Tensor([0, 3, 3],"float64"), s=tuple(1,2,), axes=tuple(0,2,), norm="forward", )

W0421 10:07:21.451925  1409 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.ihfft2(x=Tensor([0, 3, 3],"float64"), s=tuple(1,2,), axes=tuple(0,2,), norm="forward", ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:07:21.452392 test begin: paddle.fft.ihfft2(x=Tensor([0, 3, 3],"float64"), s=tuple(1,2,), axes=tuple(0,2,), norm="ortho", )

W0421 10:07:21.700659  1563 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.ihfft2(x=Tensor([0, 3, 3],"float64"), s=tuple(1,2,), axes=tuple(0,2,), norm="ortho", ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:07:21.705763 test begin: paddle.fft.ihfft2(x=Tensor([4, 0, 3],"float64"), s=tuple(1,2,), )

W0421 10:07:21.894145  1587 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.ihfft2(x=Tensor([4, 0, 3],"float64"), s=tuple(1,2,), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:07:21.897958 test begin: paddle.fft.ihfft2(x=Tensor([4, 3, 0],"float64"), s=tuple(1,2,), )

W0421 10:07:22.113075  1608 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.ihfft2(x=Tensor([4, 3, 0],"float64"), s=tuple(1,2,), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:07:22.113515 test begin: paddle.fft.ihfft2(x=Tensor([4, 3, 0],"float64"), s=tuple(1,2,), axes=tuple(0,2,), )

W0421 10:07:22.313238  1653 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.ihfft2(x=Tensor([4, 3, 0],"float64"), s=tuple(1,2,), axes=tuple(0,2,), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:07:22.313711 test begin: paddle.fft.ihfft2(x=Tensor([4, 3, 0],"float64"), s=tuple(1,2,), axes=tuple(0,2,), norm="forward", )

W0421 10:07:22.519520  1741 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.ihfft2(x=Tensor([4, 3, 0],"float64"), s=tuple(1,2,), axes=tuple(0,2,), norm="forward", ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:07:22.519884 test begin: paddle.fft.ihfft2(x=Tensor([4, 3, 0],"float64"), s=tuple(1,2,), axes=tuple(0,2,), norm="ortho", )

W0421 10:07:22.735020  1754 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.ihfft2(x=Tensor([4, 3, 0],"float64"), s=tuple(1,2,), axes=tuple(0,2,), norm="ortho", ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:07:22.747311 test begin: paddle.fft.ihfftn(Tensor([0, 3, 3],"float64"), tuple(1,2,), tuple(0,2,), "backward", None, )

W0421 10:07:23.003868  1908 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.ihfftn(Tensor([0, 3, 3],"float64"), tuple(1,2,), tuple(0,2,), "backward", None, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:07:23.004328 test begin: paddle.fft.ihfftn(Tensor([0, 3, 3],"float64"), tuple(1,2,), tuple(0,2,), "forward", None, )

W0421 10:07:23.216915  2078 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.ihfftn(Tensor([0, 3, 3],"float64"), tuple(1,2,), tuple(0,2,), "forward", None, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:07:23.217286 test begin: paddle.fft.ihfftn(Tensor([0, 3, 3],"float64"), tuple(1,2,), tuple(0,2,), "ortho", None, )

W0421 10:07:23.460587  2102 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.ihfftn(Tensor([0, 3, 3],"float64"), tuple(1,2,), tuple(0,2,), "ortho", None, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:07:23.467588 test begin: paddle.fft.ihfftn(Tensor([4, 0, 3],"float64"), tuple(1,2,), tuple(-2,-1,), "backward", None, )

W0421 10:07:23.704169  2275 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.ihfftn(Tensor([4, 0, 3],"float64"), tuple(1,2,), tuple(-2,-1,), "backward", None, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:07:23.708170 test begin: paddle.fft.ihfftn(Tensor([4, 3, 0],"float64"), tuple(1,2,), tuple(-2,-1,), "backward", None, )

W0421 10:07:23.913168  2296 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.ihfftn(Tensor([4, 3, 0],"float64"), tuple(1,2,), tuple(-2,-1,), "backward", None, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:07:23.913984 test begin: paddle.fft.ihfftn(Tensor([4, 3, 0],"float64"), tuple(1,2,), tuple(0,2,), "backward", None, )

W0421 10:07:24.141412  2392 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.ihfftn(Tensor([4, 3, 0],"float64"), tuple(1,2,), tuple(0,2,), "backward", None, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:07:24.141816 test begin: paddle.fft.ihfftn(Tensor([4, 3, 0],"float64"), tuple(1,2,), tuple(0,2,), "forward", None, )

W0421 10:07:24.356346  2547 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.ihfftn(Tensor([4, 3, 0],"float64"), tuple(1,2,), tuple(0,2,), "forward", None, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:07:24.356732 test begin: paddle.fft.ihfftn(Tensor([4, 3, 0],"float64"), tuple(1,2,), tuple(0,2,), "ortho", None, )

W0421 10:07:24.569456  2572 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.ihfftn(Tensor([4, 3, 0],"float64"), tuple(1,2,), tuple(0,2,), "ortho", None, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:07:25.400404 test begin: paddle.fft.ihfftn(x=Tensor([4, 3, 0],"float64"), s=tuple(1,2,), )

W0421 10:07:25.619419  2809 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.ihfftn(x=Tensor([4, 3, 0],"float64"), s=tuple(1,2,), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:07:25.619775 test begin: paddle.fft.ihfftn(x=Tensor([4, 3, 0],"float64"), s=tuple(1,2,), axes=tuple(0,2,), )

W0421 10:07:25.750625  2964 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.ihfftn(x=Tensor([4, 3, 0],"float64"), s=tuple(1,2,), axes=tuple(0,2,), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:07:25.751143 test begin: paddle.fft.ihfftn(x=Tensor([4, 3, 0],"float64"), s=tuple(1,2,), axes=tuple(0,2,), norm="forward", )

W0421 10:07:25.920248  3036 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.ihfftn(x=Tensor([4, 3, 0],"float64"), s=tuple(1,2,), axes=tuple(0,2,), norm="forward", ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:07:25.920649 test begin: paddle.fft.ihfftn(x=Tensor([4, 3, 0],"float64"), s=tuple(1,2,), axes=tuple(0,2,), norm="ortho", )

W0421 10:07:26.063848  3057 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.ihfftn(x=Tensor([4, 3, 0],"float64"), s=tuple(1,2,), axes=tuple(0,2,), norm="ortho", ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:07:33.544014 test begin: paddle.fft.rfft2(x=Tensor([2, 2, 0],"float64"), s=list[1,2,], )

W0421 10:07:33.751287  6147 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.rfft2(x=Tensor([2, 2, 0],"float64"), s=list[1,2,], ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:07:33.751653 test begin: paddle.fft.rfft2(x=Tensor([2, 2, 0],"float64"), s=list[1,2,], norm="backward", )

W0421 10:07:34.024564  6162 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.rfft2(x=Tensor([2, 2, 0],"float64"), s=list[1,2,], norm="backward", ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:07:34.024915 test begin: paddle.fft.rfft2(x=Tensor([2, 2, 0],"float64"), s=list[1,2,], norm="forward", )

W0421 10:07:34.239679  6318 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.rfft2(x=Tensor([2, 2, 0],"float64"), s=list[1,2,], norm="forward", ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:07:34.240003 test begin: paddle.fft.rfft2(x=Tensor([2, 2, 0],"float64"), s=list[1,2,], norm="ortho", )

W0421 10:07:34.444408  6336 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.rfft2(x=Tensor([2, 2, 0],"float64"), s=list[1,2,], norm="ortho", ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:07:35.431916 test begin: paddle.fft.rfftn(Tensor([2, 2, 0],"float64"), list[1,2,], None, "ortho", None, )

W0421 10:07:35.643589  6775 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.rfftn(Tensor([2, 2, 0],"float64"), list[1,2,], None, "ortho", None, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:07:35.643993 test begin: paddle.fft.rfftn(Tensor([2, 2, 0],"float64"), list[1,2,], tuple(-2,-1,), "backward", None, )

W0421 10:07:35.945216  6863 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.rfftn(Tensor([2, 2, 0],"float64"), list[1,2,], tuple(-2,-1,), "backward", None, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:07:35.945685 test begin: paddle.fft.rfftn(Tensor([2, 2, 0],"float64"), list[1,2,], tuple(-2,-1,), "forward", None, )

W0421 10:07:36.187031  7022 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.rfftn(Tensor([2, 2, 0],"float64"), list[1,2,], tuple(-2,-1,), "forward", None, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:07:36.187418 test begin: paddle.fft.rfftn(Tensor([2, 2, 0],"float64"), list[1,2,], tuple(-2,-1,), "ortho", None, )

W0421 10:07:36.488659  7107 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.rfftn(Tensor([2, 2, 0],"float64"), list[1,2,], tuple(-2,-1,), "ortho", None, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:08:07.916610 test begin: paddle.flip(Tensor([0, 2, 2],"float32"), list[0,1,], )

[cuda error] paddle.flip(Tensor([0, 2, 2],"float32"), list[0,1,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:08.117414 test begin: paddle.flip(Tensor([0, 2],"float32"), tuple(-2,-1,), )

[cuda error] paddle.flip(Tensor([0, 2],"float32"), tuple(-2,-1,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:08.303818 test begin: paddle.flip(Tensor([0, 3],"float32"), list[0,], )

[cuda error] paddle.flip(Tensor([0, 3],"float32"), list[0,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:08.520743 test begin: paddle.flip(Tensor([0, 4],"float32"), list[0,1,], )

[cuda error] paddle.flip(Tensor([0, 4],"float32"), list[0,1,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:08.722892 test begin: paddle.flip(Tensor([0, 8, 224, 224],"float32"), axis=list[3,], )

[cuda error] paddle.flip(Tensor([0, 8, 224, 224],"float32"), axis=list[3,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:08.862601 test begin: paddle.flip(Tensor([0],"int32"), axis=list[0,], )

[cuda error] paddle.flip(Tensor([0],"int32"), axis=list[0,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:09.000323 test begin: paddle.flip(Tensor([2, 0],"float32"), list[0,], )

[cuda error] paddle.flip(Tensor([2, 0],"float32"), list[0,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:09.240415 test begin: paddle.flip(Tensor([2, 0],"float32"), tuple(-2,-1,), )

[cuda error] paddle.flip(Tensor([2, 0],"float32"), tuple(-2,-1,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:09.438936 test begin: paddle.flip(Tensor([3, 0, 224, 224],"float32"), axis=list[3,], )

[cuda error] paddle.flip(Tensor([3, 0, 224, 224],"float32"), axis=list[3,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:09.643786 test begin: paddle.flip(Tensor([3, 0, 2],"float32"), list[0,1,], )

[cuda error] paddle.flip(Tensor([3, 0, 2],"float32"), list[0,1,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:09.882793 test begin: paddle.flip(Tensor([3, 2, 0],"float32"), list[0,1,], )

[cuda error] paddle.flip(Tensor([3, 2, 0],"float32"), list[0,1,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:10.183340 test begin: paddle.flip(Tensor([3, 8, 0, 224],"float32"), axis=list[3,], )

[cuda error] paddle.flip(Tensor([3, 8, 0, 224],"float32"), axis=list[3,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:10.453819 test begin: paddle.flip(Tensor([3, 8, 224, 0],"float32"), axis=list[3,], )

[cuda error] paddle.flip(Tensor([3, 8, 224, 0],"float32"), axis=list[3,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:10.587641 test begin: paddle.flip(Tensor([4, 0],"float32"), list[0,1,], )

[cuda error] paddle.flip(Tensor([4, 0],"float32"), list[0,1,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:10.782052 test begin: paddle.flip(x=Tensor([0, 3, 3, 3, 3, 3],"float64"), axis=list[-1,0,3,4,2,], )

[cuda error] paddle.flip(x=Tensor([0, 3, 3, 3, 3, 3],"float64"), axis=list[-1,0,3,4,2,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:11.023650 test begin: paddle.flip(x=Tensor([0, 3, 3],"bool"), axis=list[0,], )

element 0 of tensors does not require grad and does not have a grad_fn
[cuda error] paddle.flip(x=Tensor([0, 3, 3],"bool"), axis=list[0,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:11.225804 test begin: paddle.flip(x=Tensor([0, 3, 3],"float64"), axis=list[-1,0,1,], )

[cuda error] paddle.flip(x=Tensor([0, 3, 3],"float64"), axis=list[-1,0,1,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:11.465194 test begin: paddle.flip(x=Tensor([0, 3, 3],"float64"), axis=list[0,1,2,], )

[cuda error] paddle.flip(x=Tensor([0, 3, 3],"float64"), axis=list[0,1,2,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:11.670938 test begin: paddle.flip(x=Tensor([3, 0, 3, 3, 3, 3],"float64"), axis=list[-1,0,3,4,2,], )

[cuda error] paddle.flip(x=Tensor([3, 0, 3, 3, 3, 3],"float64"), axis=list[-1,0,3,4,2,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:11.896432 test begin: paddle.flip(x=Tensor([3, 0, 3],"bool"), axis=list[0,], )

element 0 of tensors does not require grad and does not have a grad_fn
[cuda error] paddle.flip(x=Tensor([3, 0, 3],"bool"), axis=list[0,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:12.139010 test begin: paddle.flip(x=Tensor([3, 0, 3],"float64"), axis=list[-1,0,1,], )

[cuda error] paddle.flip(x=Tensor([3, 0, 3],"float64"), axis=list[-1,0,1,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:12.339457 test begin: paddle.flip(x=Tensor([3, 0, 3],"float64"), axis=list[0,1,2,], )

[cuda error] paddle.flip(x=Tensor([3, 0, 3],"float64"), axis=list[0,1,2,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:12.543378 test begin: paddle.flip(x=Tensor([3, 3, 0, 3, 3, 3],"float64"), axis=list[-1,0,3,4,2,], )

[cuda error] paddle.flip(x=Tensor([3, 3, 0, 3, 3, 3],"float64"), axis=list[-1,0,3,4,2,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:12.748593 test begin: paddle.flip(x=Tensor([3, 3, 0],"bool"), axis=list[0,], )

element 0 of tensors does not require grad and does not have a grad_fn
[cuda error] paddle.flip(x=Tensor([3, 3, 0],"bool"), axis=list[0,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:13.028626 test begin: paddle.flip(x=Tensor([3, 3, 0],"float64"), axis=list[-1,0,1,], )

[cuda error] paddle.flip(x=Tensor([3, 3, 0],"float64"), axis=list[-1,0,1,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:13.243345 test begin: paddle.flip(x=Tensor([3, 3, 0],"float64"), axis=list[0,1,2,], )

[cuda error] paddle.flip(x=Tensor([3, 3, 0],"float64"), axis=list[0,1,2,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:13.449191 test begin: paddle.flip(x=Tensor([3, 3, 3, 0, 3, 3],"float64"), axis=list[-1,0,3,4,2,], )

[cuda error] paddle.flip(x=Tensor([3, 3, 3, 0, 3, 3],"float64"), axis=list[-1,0,3,4,2,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:13.753344 test begin: paddle.flip(x=Tensor([3, 3, 3, 3, 0, 3],"float64"), axis=list[-1,0,3,4,2,], )

[cuda error] paddle.flip(x=Tensor([3, 3, 3, 3, 0, 3],"float64"), axis=list[-1,0,3,4,2,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:14.010600 test begin: paddle.flip(x=Tensor([3, 3, 3, 3, 3, 0],"float64"), axis=list[-1,0,3,4,2,], )

[cuda error] paddle.flip(x=Tensor([3, 3, 3, 3, 3, 0],"float64"), axis=list[-1,0,3,4,2,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:19.149595 test begin: paddle.fmax(Tensor([0, 15],"float32"), Tensor([0, 15],"float32"), )

[cuda error] paddle.fmax(Tensor([0, 15],"float32"), Tensor([0, 15],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:19.345183 test begin: paddle.fmax(Tensor([0, 15],"float32"), Tensor([15],"float32"), )

[cuda error] paddle.fmax(Tensor([0, 15],"float32"), Tensor([15],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:19.473829 test begin: paddle.fmax(Tensor([0, 200, 40],"float32"), Tensor([0, 200, 40],"float32"), )

[cuda error] paddle.fmax(Tensor([0, 200, 40],"float32"), Tensor([0, 200, 40],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:19.892865 test begin: paddle.fmax(Tensor([10, 0],"float32"), Tensor([10, 0],"float32"), )

[cuda error] paddle.fmax(Tensor([10, 0],"float32"), Tensor([10, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:20.091179 test begin: paddle.fmax(Tensor([30, 0, 40],"float32"), Tensor([30, 0, 40],"float32"), )

[cuda error] paddle.fmax(Tensor([30, 0, 40],"float32"), Tensor([30, 0, 40],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:20.302121 test begin: paddle.fmax(Tensor([30, 200, 0],"float32"), Tensor([30, 200, 0],"float32"), )

[cuda error] paddle.fmax(Tensor([30, 200, 0],"float32"), Tensor([30, 200, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:20.521662 test begin: paddle.fmin(Tensor([0, 15],"float32"), Tensor([0, 15],"float32"), )

[cuda error] paddle.fmin(Tensor([0, 15],"float32"), Tensor([0, 15],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:20.737875 test begin: paddle.fmin(Tensor([0, 15],"float32"), Tensor([15],"float32"), )

[cuda error] paddle.fmin(Tensor([0, 15],"float32"), Tensor([15],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:20.954686 test begin: paddle.fmin(Tensor([0, 200, 40],"float32"), Tensor([0, 200, 40],"float32"), )

[cuda error] paddle.fmin(Tensor([0, 200, 40],"float32"), Tensor([0, 200, 40],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:21.617869 test begin: paddle.fmin(Tensor([10, 0],"float32"), Tensor([10, 0],"float32"), )

[cuda error] paddle.fmin(Tensor([10, 0],"float32"), Tensor([10, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:22.204528 test begin: paddle.fmin(Tensor([30, 0, 40],"float32"), Tensor([30, 0, 40],"float32"), )

[cuda error] paddle.fmin(Tensor([30, 0, 40],"float32"), Tensor([30, 0, 40],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:22.421620 test begin: paddle.fmin(Tensor([30, 200, 0],"float32"), Tensor([30, 200, 0],"float32"), )

[cuda error] paddle.fmin(Tensor([30, 200, 0],"float32"), Tensor([30, 200, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:22.670566 test begin: paddle.frac(Tensor([0, 20, 1],"float32"), )

[cuda error] paddle.frac(Tensor([0, 20, 1],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:22.871925 test begin: paddle.frac(Tensor([0, 3],"float32"), )

[cuda error] paddle.frac(Tensor([0, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:23.121327 test begin: paddle.frac(Tensor([10, 0, 1],"float32"), )

[cuda error] paddle.frac(Tensor([10, 0, 1],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:23.367272 test begin: paddle.frac(Tensor([10, 20, 0],"float32"), )

[cuda error] paddle.frac(Tensor([10, 20, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:23.570816 test begin: paddle.frac(Tensor([2, 0],"float32"), )

[cuda error] paddle.frac(Tensor([2, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:28.865831 test begin: paddle.imag(Tensor([0, 10, 10, 20],"complex128"), )

[cuda error] paddle.imag(Tensor([0, 10, 10, 20],"complex128"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:29.113008 test begin: paddle.imag(Tensor([0, 3],"complex128"), )

[cuda error] paddle.imag(Tensor([0, 3],"complex128"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:29.246876 test begin: paddle.imag(Tensor([10, 0, 10, 20],"complex128"), )

[cuda error] paddle.imag(Tensor([10, 0, 10, 20],"complex128"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:29.447049 test begin: paddle.imag(Tensor([10, 10, 0, 20],"complex128"), )

[cuda error] paddle.imag(Tensor([10, 10, 0, 20],"complex128"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:29.582384 test begin: paddle.imag(Tensor([10, 10, 10, 0],"complex128"), )

[cuda error] paddle.imag(Tensor([10, 10, 10, 0],"complex128"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:29.722407 test begin: paddle.imag(Tensor([2, 0],"complex128"), )

[cuda error] paddle.imag(Tensor([2, 0],"complex128"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:29.910899 test begin: paddle.imag(x=Tensor([0, 10],"complex64"), )

[cuda error] paddle.imag(x=Tensor([0, 10],"complex64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:30.115649 test begin: paddle.imag(x=Tensor([1, 0],"complex64"), )

[cuda error] paddle.imag(x=Tensor([1, 0],"complex64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:31.290774 test begin: paddle.inner(Tensor([5, 10, 10],"complex128"), Tensor([0, 10],"complex128"), )

W0421 10:09:32.643929 47388 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.inner(Tensor([5, 10, 10],"complex128"), Tensor([0, 10],"complex128"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:958)

2025-04-21 10:09:32.646490 test begin: paddle.inner(Tensor([5, 10, 10],"float64"), Tensor([0, 10],"float64"), )

W0421 10:09:32.845141 47745 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.inner(Tensor([5, 10, 10],"float64"), Tensor([0, 10],"float64"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:35.292466 test begin: paddle.inner(x=Tensor([0],"float64"), y=Tensor([0],"float64"), )

[paddle error] paddle.inner(x=Tensor([0],"float64"), y=Tensor([0],"float64"), ) 
 (InvalidArgument) can not reshape 0 to -1, 0, because the unspecified dimension 0 can be any number and is ambiguous
  [Hint: Expected unk_dim_idx == -1, but received unk_dim_idx:0 != -1:-1.] (at ../paddle/phi/infermeta/unary.cc:2209)

2025-04-21 10:09:36.407094 test begin: paddle.inner(x=Tensor([2, 5, 3, 0],"float64"), y=Tensor([3, 2, 5, 0],"float64"), )

[paddle error] paddle.inner(x=Tensor([2, 5, 3, 0],"float64"), y=Tensor([3, 2, 5, 0],"float64"), ) 
 (InvalidArgument) can not reshape 2, 5, 3, 0 to -1, 0, because the unspecified dimension 0 can be any number and is ambiguous
  [Hint: Expected unk_dim_idx == -1, but received unk_dim_idx:0 != -1:-1.] (at ../paddle/phi/infermeta/unary.cc:2209)

2025-04-21 10:09:36.603554 test begin: paddle.inner(x=Tensor([2, 5, 3, 4],"float64"), y=Tensor([0, 2, 5, 4],"float64"), )

W0421 10:09:36.850044 49041 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.inner(x=Tensor([2, 5, 3, 4],"float64"), y=Tensor([0, 2, 5, 4],"float64"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:36.850405 test begin: paddle.inner(x=Tensor([2, 5, 3, 4],"float64"), y=Tensor([3, 0, 5, 4],"float64"), )

W0421 10:09:37.094735 49059 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.inner(x=Tensor([2, 5, 3, 4],"float64"), y=Tensor([3, 0, 5, 4],"float64"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:37.095091 test begin: paddle.inner(x=Tensor([2, 5, 3, 4],"float64"), y=Tensor([3, 2, 0, 4],"float64"), )

W0421 10:09:37.367230 49218 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.inner(x=Tensor([2, 5, 3, 4],"float64"), y=Tensor([3, 2, 0, 4],"float64"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:37.372672 test begin: paddle.inner(x=Tensor([3, 0],"float64"), y=Tensor([5, 0],"float64"), )

[paddle error] paddle.inner(x=Tensor([3, 0],"float64"), y=Tensor([5, 0],"float64"), ) 
 (InvalidArgument) can not reshape 3, 0 to -1, 0, because the unspecified dimension 0 can be any number and is ambiguous
  [Hint: Expected unk_dim_idx == -1, but received unk_dim_idx:0 != -1:-1.] (at ../paddle/phi/infermeta/unary.cc:2209)

2025-04-21 10:09:37.588470 test begin: paddle.inner(x=Tensor([3, 4],"float64"), y=Tensor([0, 2, 4],"float64"), )

W0421 10:09:37.814349 49356 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.inner(x=Tensor([3, 4],"float64"), y=Tensor([0, 2, 4],"float64"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:37.814771 test begin: paddle.inner(x=Tensor([3, 4],"float64"), y=Tensor([0, 2, 5, 4],"float64"), )

W0421 10:09:38.066816 49459 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.inner(x=Tensor([3, 4],"float64"), y=Tensor([0, 2, 5, 4],"float64"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:38.067145 test begin: paddle.inner(x=Tensor([3, 4],"float64"), y=Tensor([0, 4],"float64"), )

W0421 10:09:38.276068 49612 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.inner(x=Tensor([3, 4],"float64"), y=Tensor([0, 4],"float64"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:38.276718 test begin: paddle.inner(x=Tensor([3, 4],"float64"), y=Tensor([3, 0, 4],"float64"), )

W0421 10:09:38.494930 49697 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.inner(x=Tensor([3, 4],"float64"), y=Tensor([3, 0, 4],"float64"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:38.495817 test begin: paddle.inner(x=Tensor([3, 4],"float64"), y=Tensor([3, 0, 5, 4],"float64"), )

W0421 10:09:38.810961 49978 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.inner(x=Tensor([3, 4],"float64"), y=Tensor([3, 0, 5, 4],"float64"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:38.811307 test begin: paddle.inner(x=Tensor([3, 4],"float64"), y=Tensor([3, 2, 0, 4],"float64"), )

W0421 10:09:39.020977 49999 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.inner(x=Tensor([3, 4],"float64"), y=Tensor([3, 2, 0, 4],"float64"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:39.055749 test begin: paddle.inner(x=Tensor([4, 0],"float32"), y=Tensor([4, 0],"float32"), )

[paddle error] paddle.inner(x=Tensor([4, 0],"float32"), y=Tensor([4, 0],"float32"), ) 
 (InvalidArgument) can not reshape 4, 0 to -1, 0, because the unspecified dimension 0 can be any number and is ambiguous
  [Hint: Expected unk_dim_idx == -1, but received unk_dim_idx:0 != -1:-1.] (at ../paddle/phi/infermeta/unary.cc:2209)

2025-04-21 10:09:39.352450 test begin: paddle.inner(x=Tensor([4, 4],"float32"), y=Tensor([0, 4],"float32"), )

W0421 10:09:39.857798 50181 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.inner(x=Tensor([4, 4],"float32"), y=Tensor([0, 4],"float32"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)

2025-04-21 10:09:40.717751 test begin: paddle.inner(x=Tensor([5, 3, 0],"float64"), y=Tensor([2, 5, 0],"float64"), )

[paddle error] paddle.inner(x=Tensor([5, 3, 0],"float64"), y=Tensor([2, 5, 0],"float64"), ) 
 (InvalidArgument) can not reshape 5, 3, 0 to -1, 0, because the unspecified dimension 0 can be any number and is ambiguous
  [Hint: Expected unk_dim_idx == -1, but received unk_dim_idx:0 != -1:-1.] (at ../paddle/phi/infermeta/unary.cc:2209)

2025-04-21 10:09:40.913565 test begin: paddle.inner(x=Tensor([5, 3, 4],"float64"), y=Tensor([0, 5, 4],"float64"), )

W0421 10:09:41.151711 50391 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.inner(x=Tensor([5, 3, 4],"float64"), y=Tensor([0, 5, 4],"float64"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:41.152647 test begin: paddle.inner(x=Tensor([5, 3, 4],"float64"), y=Tensor([2, 0, 4],"float64"), )

W0421 10:09:41.360669 50537 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.inner(x=Tensor([5, 3, 4],"float64"), y=Tensor([2, 0, 4],"float64"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:46.216326 test begin: paddle.isclose(Tensor([0, 10],"float64"), Tensor([0, 10],"float64"), rtol=1e-05, atol=1e-08, )

[cuda error] paddle.isclose(Tensor([0, 10],"float64"), Tensor([0, 10],"float64"), rtol=1e-05, atol=1e-08, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:46.415047 test begin: paddle.isclose(Tensor([10, 0],"float64"), Tensor([10, 0],"float64"), rtol=1e-05, atol=1e-08, )

[cuda error] paddle.isclose(Tensor([10, 0],"float64"), Tensor([10, 0],"float64"), rtol=1e-05, atol=1e-08, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:46.622807 test begin: paddle.isclose(x=Tensor([0, 4, 5],"float64"), y=Tensor([0, 4, 5],"float64"), )

[cuda error] paddle.isclose(x=Tensor([0, 4, 5],"float64"), y=Tensor([0, 4, 5],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:46.887141 test begin: paddle.isclose(x=Tensor([0],"float32"), y=Tensor([0],"float32"), )

[cuda error] paddle.isclose(x=Tensor([0],"float32"), y=Tensor([0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:47.089187 test begin: paddle.isclose(x=Tensor([0],"float64"), y=Tensor([0],"float64"), )

[cuda error] paddle.isclose(x=Tensor([0],"float64"), y=Tensor([0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:47.300035 test begin: paddle.isclose(x=Tensor([3, 0, 5],"float64"), y=Tensor([3, 0, 5],"float64"), )

[cuda error] paddle.isclose(x=Tensor([3, 0, 5],"float64"), y=Tensor([3, 0, 5],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:47.542678 test begin: paddle.isclose(x=Tensor([3, 4, 0],"float64"), y=Tensor([3, 4, 0],"float64"), )

[cuda error] paddle.isclose(x=Tensor([3, 4, 0],"float64"), y=Tensor([3, 4, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:47.816998 test begin: paddle.isfinite(Tensor([0, 17, 10],"int32"), )

[cuda error] paddle.isfinite(Tensor([0, 17, 10],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:48.042951 test begin: paddle.isfinite(Tensor([0, 17, 5, 6, 7],"float16"), )

[cuda error] paddle.isfinite(Tensor([0, 17, 5, 6, 7],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:48.234426 test begin: paddle.isfinite(Tensor([0, 17],"float32"), )

[cuda error] paddle.isfinite(Tensor([0, 17],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:48.427475 test begin: paddle.isfinite(Tensor([0, 280, 376, 25, 3],"float32"), )

[cuda error] paddle.isfinite(Tensor([0, 280, 376, 25, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:48.554933 test begin: paddle.isfinite(Tensor([0, 3, 4, 5],"float64"), )

[cuda error] paddle.isfinite(Tensor([0, 3, 4, 5],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:48.742785 test begin: paddle.isfinite(Tensor([0, 94, 311],"float32"), )

[cuda error] paddle.isfinite(Tensor([0, 94, 311],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:48.935559 test begin: paddle.isfinite(Tensor([0],"int64"), )

[cuda error] paddle.isfinite(Tensor([0],"int64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:49.104321 test begin: paddle.isfinite(Tensor([11, 0, 10],"int32"), )

[cuda error] paddle.isfinite(Tensor([11, 0, 10],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:49.230866 test begin: paddle.isfinite(Tensor([11, 0],"float32"), )

[cuda error] paddle.isfinite(Tensor([11, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:49.466965 test begin: paddle.isfinite(Tensor([11, 17, 0],"int32"), )

[cuda error] paddle.isfinite(Tensor([11, 17, 0],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:49.706273 test begin: paddle.isfinite(Tensor([2, 0, 4, 5],"float64"), )

[cuda error] paddle.isfinite(Tensor([2, 0, 4, 5],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:49.923895 test begin: paddle.isfinite(Tensor([2, 3, 0, 5],"float64"), )

[cuda error] paddle.isfinite(Tensor([2, 3, 0, 5],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:50.126710 test begin: paddle.isfinite(Tensor([2, 3, 4, 0],"float64"), )

[cuda error] paddle.isfinite(Tensor([2, 3, 4, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:50.259454 test begin: paddle.isfinite(Tensor([4, 0, 311],"float32"), )

[cuda error] paddle.isfinite(Tensor([4, 0, 311],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:50.445683 test begin: paddle.isfinite(Tensor([4, 0, 376, 25, 3],"float32"), )

[cuda error] paddle.isfinite(Tensor([4, 0, 376, 25, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:50.648709 test begin: paddle.isfinite(Tensor([4, 280, 0, 25, 3],"float32"), )

[cuda error] paddle.isfinite(Tensor([4, 280, 0, 25, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:50.851660 test begin: paddle.isfinite(Tensor([4, 280, 376, 0, 3],"float32"), )

[cuda error] paddle.isfinite(Tensor([4, 280, 376, 0, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:50.991489 test begin: paddle.isfinite(Tensor([4, 280, 376, 25, 0],"float32"), )

[cuda error] paddle.isfinite(Tensor([4, 280, 376, 25, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:51.115138 test begin: paddle.isfinite(Tensor([4, 94, 0],"float32"), )

[cuda error] paddle.isfinite(Tensor([4, 94, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:51.234526 test begin: paddle.isfinite(Tensor([8, 0, 5, 6, 7],"float16"), )

[cuda error] paddle.isfinite(Tensor([8, 0, 5, 6, 7],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:51.426028 test begin: paddle.isfinite(Tensor([8, 17, 0, 6, 7],"float16"), )

[cuda error] paddle.isfinite(Tensor([8, 17, 0, 6, 7],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:51.558692 test begin: paddle.isfinite(Tensor([8, 17, 5, 0, 7],"float16"), )

[cuda error] paddle.isfinite(Tensor([8, 17, 5, 0, 7],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:51.741917 test begin: paddle.isfinite(Tensor([8, 17, 5, 6, 0],"float16"), )

[cuda error] paddle.isfinite(Tensor([8, 17, 5, 6, 0],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:52.017698 test begin: paddle.isfinite(x=Tensor([0],"float64"), )

[cuda error] paddle.isfinite(x=Tensor([0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:54.114012 test begin: paddle.isin(Tensor([0, 8],"float16"), Tensor([0, 3],"float16"), False, False, )

[cuda error] paddle.isin(Tensor([0, 8],"float16"), Tensor([0, 3],"float16"), False, False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:54.254033 test begin: paddle.isin(Tensor([0, 8],"float16"), Tensor([0, 3],"float16"), False, True, )

[cuda error] paddle.isin(Tensor([0, 8],"float16"), Tensor([0, 3],"float16"), False, True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:54.412704 test begin: paddle.isin(Tensor([0, 8],"float16"), Tensor([2, 3],"float16"), False, False, )

[cuda error] paddle.isin(Tensor([0, 8],"float16"), Tensor([2, 3],"float16"), False, False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:54.655547 test begin: paddle.isin(Tensor([0, 8],"float16"), Tensor([2, 3],"float16"), False, True, )

[cuda error] paddle.isin(Tensor([0, 8],"float16"), Tensor([2, 3],"float16"), False, True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:54.854755 test begin: paddle.isin(Tensor([0, 8],"float32"), Tensor([0, 3],"float32"), False, False, )

[cuda error] paddle.isin(Tensor([0, 8],"float32"), Tensor([0, 3],"float32"), False, False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:54.988761 test begin: paddle.isin(Tensor([0, 8],"float32"), Tensor([0, 3],"float32"), False, True, )

[cuda error] paddle.isin(Tensor([0, 8],"float32"), Tensor([0, 3],"float32"), False, True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:55.223366 test begin: paddle.isin(Tensor([0, 8],"float32"), Tensor([2, 3],"float32"), False, False, )

[cuda error] paddle.isin(Tensor([0, 8],"float32"), Tensor([2, 3],"float32"), False, False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:55.428411 test begin: paddle.isin(Tensor([0, 8],"float32"), Tensor([2, 3],"float32"), False, True, )

[cuda error] paddle.isin(Tensor([0, 8],"float32"), Tensor([2, 3],"float32"), False, True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:02.937595 test begin: paddle.isin(Tensor([4, 0],"float16"), Tensor([2, 0],"float16"), False, False, )

[cuda error] paddle.isin(Tensor([4, 0],"float16"), Tensor([2, 0],"float16"), False, False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:03.139316 test begin: paddle.isin(Tensor([4, 0],"float16"), Tensor([2, 0],"float16"), False, True, )

[cuda error] paddle.isin(Tensor([4, 0],"float16"), Tensor([2, 0],"float16"), False, True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:03.346037 test begin: paddle.isin(Tensor([4, 0],"float16"), Tensor([2, 3],"float16"), False, False, )

[cuda error] paddle.isin(Tensor([4, 0],"float16"), Tensor([2, 3],"float16"), False, False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:03.588409 test begin: paddle.isin(Tensor([4, 0],"float16"), Tensor([2, 3],"float16"), False, True, )

[cuda error] paddle.isin(Tensor([4, 0],"float16"), Tensor([2, 3],"float16"), False, True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:03.860688 test begin: paddle.isin(Tensor([4, 0],"float32"), Tensor([2, 0],"float32"), False, False, )

[cuda error] paddle.isin(Tensor([4, 0],"float32"), Tensor([2, 0],"float32"), False, False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:04.118758 test begin: paddle.isin(Tensor([4, 0],"float32"), Tensor([2, 0],"float32"), False, True, )

[cuda error] paddle.isin(Tensor([4, 0],"float32"), Tensor([2, 0],"float32"), False, True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:04.353898 test begin: paddle.isin(Tensor([4, 0],"float32"), Tensor([2, 3],"float32"), False, False, )

[cuda error] paddle.isin(Tensor([4, 0],"float32"), Tensor([2, 3],"float32"), False, False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:04.617596 test begin: paddle.isin(Tensor([4, 0],"float32"), Tensor([2, 3],"float32"), False, True, )

[cuda error] paddle.isin(Tensor([4, 0],"float32"), Tensor([2, 3],"float32"), False, True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:06.590545 test begin: paddle.isinf(Tensor([0, 12],"float32"), )

[cuda error] paddle.isinf(Tensor([0, 12],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:06.801877 test begin: paddle.isinf(Tensor([0, 12],"float64"), )

[cuda error] paddle.isinf(Tensor([0, 12],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:06.939798 test begin: paddle.isinf(Tensor([0, 17, 10],"int16"), )

[cuda error] paddle.isinf(Tensor([0, 17, 10],"int16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:07.058631 test begin: paddle.isinf(Tensor([0, 17, 10],"int32"), )

[cuda error] paddle.isinf(Tensor([0, 17, 10],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:07.176928 test begin: paddle.isinf(Tensor([0, 17, 5, 6, 7],"float16"), )

[cuda error] paddle.isinf(Tensor([0, 17, 5, 6, 7],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:07.291265 test begin: paddle.isinf(Tensor([0, 3, 4, 5],"float64"), )

[cuda error] paddle.isinf(Tensor([0, 3, 4, 5],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:07.470436 test begin: paddle.isinf(Tensor([0, 3, 4, 5],"int8"), )

[cuda error] paddle.isinf(Tensor([0, 3, 4, 5],"int8"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:07.706337 test begin: paddle.isinf(Tensor([0],"int64"), )

[cuda error] paddle.isinf(Tensor([0],"int64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:07.965968 test begin: paddle.isinf(Tensor([0],"uint8"), )

[cuda error] paddle.isinf(Tensor([0],"uint8"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:08.180206 test begin: paddle.isinf(Tensor([10, 0],"float32"), )

[cuda error] paddle.isinf(Tensor([10, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:08.381780 test begin: paddle.isinf(Tensor([10, 0],"float64"), )

[cuda error] paddle.isinf(Tensor([10, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:08.616963 test begin: paddle.isinf(Tensor([11, 0, 10],"int16"), )

[cuda error] paddle.isinf(Tensor([11, 0, 10],"int16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:08.817620 test begin: paddle.isinf(Tensor([11, 0, 10],"int32"), )

[cuda error] paddle.isinf(Tensor([11, 0, 10],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:09.022646 test begin: paddle.isinf(Tensor([11, 17, 0],"int16"), )

[cuda error] paddle.isinf(Tensor([11, 17, 0],"int16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:09.243694 test begin: paddle.isinf(Tensor([11, 17, 0],"int32"), )

[cuda error] paddle.isinf(Tensor([11, 17, 0],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:09.575280 test begin: paddle.isinf(Tensor([2, 0, 4, 5],"float64"), )

[cuda error] paddle.isinf(Tensor([2, 0, 4, 5],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:09.779152 test begin: paddle.isinf(Tensor([2, 0, 4, 5],"int8"), )

[cuda error] paddle.isinf(Tensor([2, 0, 4, 5],"int8"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:10.057601 test begin: paddle.isinf(Tensor([2, 3, 0, 5],"float64"), )

[cuda error] paddle.isinf(Tensor([2, 3, 0, 5],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:10.233010 test begin: paddle.isinf(Tensor([2, 3, 0, 5],"int8"), )

[cuda error] paddle.isinf(Tensor([2, 3, 0, 5],"int8"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:10.454533 test begin: paddle.isinf(Tensor([2, 3, 4, 0],"float64"), )

[cuda error] paddle.isinf(Tensor([2, 3, 4, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:10.653563 test begin: paddle.isinf(Tensor([2, 3, 4, 0],"int8"), )

[cuda error] paddle.isinf(Tensor([2, 3, 4, 0],"int8"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:10.848749 test begin: paddle.isinf(Tensor([8, 0, 5, 6, 7],"float16"), )

[cuda error] paddle.isinf(Tensor([8, 0, 5, 6, 7],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:11.033898 test begin: paddle.isinf(Tensor([8, 17, 0, 6, 7],"float16"), )

[cuda error] paddle.isinf(Tensor([8, 17, 0, 6, 7],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:11.230205 test begin: paddle.isinf(Tensor([8, 17, 5, 0, 7],"float16"), )

[cuda error] paddle.isinf(Tensor([8, 17, 5, 0, 7],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:11.429268 test begin: paddle.isinf(Tensor([8, 17, 5, 6, 0],"float16"), )

[cuda error] paddle.isinf(Tensor([8, 17, 5, 6, 0],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:11.622687 test begin: paddle.isinf(x=Tensor([0],"float64"), )

[cuda error] paddle.isinf(x=Tensor([0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:11.822252 test begin: paddle.isnan(Tensor([0, 17, 10],"int32"), )

[cuda error] paddle.isnan(Tensor([0, 17, 10],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:12.026469 test begin: paddle.isnan(Tensor([0, 17, 5, 6, 7],"float16"), )

[cuda error] paddle.isnan(Tensor([0, 17, 5, 6, 7],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:12.231515 test begin: paddle.isnan(Tensor([0, 17],"float32"), )

[cuda error] paddle.isnan(Tensor([0, 17],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:12.364379 test begin: paddle.isnan(Tensor([0, 3, 4, 5],"float32"), )

[cuda error] paddle.isnan(Tensor([0, 3, 4, 5],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:12.551514 test begin: paddle.isnan(Tensor([0, 3, 4, 5],"float64"), )

[cuda error] paddle.isnan(Tensor([0, 3, 4, 5],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:12.753746 test begin: paddle.isnan(Tensor([0, 512],"float16"), )

[cuda error] paddle.isnan(Tensor([0, 512],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:12.889539 test begin: paddle.isnan(Tensor([0, 64, 16],"float32"), )

[cuda error] paddle.isnan(Tensor([0, 64, 16],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:13.091209 test begin: paddle.isnan(Tensor([0],"float16"), )

[cuda error] paddle.isnan(Tensor([0],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:13.346956 test begin: paddle.isnan(Tensor([0],"float32"), )

[cuda error] paddle.isnan(Tensor([0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:13.477948 test begin: paddle.isnan(Tensor([1024, 0],"float16"), )

[cuda error] paddle.isnan(Tensor([1024, 0],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:13.695969 test begin: paddle.isnan(Tensor([11, 0, 10],"int32"), )

[cuda error] paddle.isnan(Tensor([11, 0, 10],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:13.868197 test begin: paddle.isnan(Tensor([11, 0],"float32"), )

[cuda error] paddle.isnan(Tensor([11, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:14.052687 test begin: paddle.isnan(Tensor([11, 17, 0],"int32"), )

[cuda error] paddle.isnan(Tensor([11, 17, 0],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:14.252035 test begin: paddle.isnan(Tensor([14, 0, 16],"float32"), )

[cuda error] paddle.isnan(Tensor([14, 0, 16],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:14.470277 test begin: paddle.isnan(Tensor([14, 64, 0],"float32"), )

[cuda error] paddle.isnan(Tensor([14, 64, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:14.643583 test begin: paddle.isnan(Tensor([2, 0, 4, 5],"float32"), )

[cuda error] paddle.isnan(Tensor([2, 0, 4, 5],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:14.881450 test begin: paddle.isnan(Tensor([2, 0, 4, 5],"float64"), )

[cuda error] paddle.isnan(Tensor([2, 0, 4, 5],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:15.070850 test begin: paddle.isnan(Tensor([2, 3, 0, 5],"float32"), )

[cuda error] paddle.isnan(Tensor([2, 3, 0, 5],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:15.259257 test begin: paddle.isnan(Tensor([2, 3, 0, 5],"float64"), )

[cuda error] paddle.isnan(Tensor([2, 3, 0, 5],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:15.501913 test begin: paddle.isnan(Tensor([2, 3, 4, 0],"float32"), )

[cuda error] paddle.isnan(Tensor([2, 3, 4, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:15.695727 test begin: paddle.isnan(Tensor([2, 3, 4, 0],"float64"), )

[cuda error] paddle.isnan(Tensor([2, 3, 4, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:15.969777 test begin: paddle.isnan(Tensor([8, 0, 5, 6, 7],"float16"), )

[cuda error] paddle.isnan(Tensor([8, 0, 5, 6, 7],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:16.174429 test begin: paddle.isnan(Tensor([8, 17, 0, 6, 7],"float16"), )

[cuda error] paddle.isnan(Tensor([8, 17, 0, 6, 7],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:16.398591 test begin: paddle.isnan(Tensor([8, 17, 5, 0, 7],"float16"), )

[cuda error] paddle.isnan(Tensor([8, 17, 5, 0, 7],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:16.622187 test begin: paddle.isnan(Tensor([8, 17, 5, 6, 0],"float16"), )

[cuda error] paddle.isnan(Tensor([8, 17, 5, 6, 0],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:16.820175 test begin: paddle.isnan(x=Tensor([0],"float64"), )

[cuda error] paddle.isnan(x=Tensor([0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:17.027004 test begin: paddle.isneginf(Tensor([0, 17, 10],"int16"), )

[cuda error] paddle.isneginf(Tensor([0, 17, 10],"int16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:17.261236 test begin: paddle.isneginf(Tensor([0, 17, 10],"int32"), )

[cuda error] paddle.isneginf(Tensor([0, 17, 10],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:17.512391 test begin: paddle.isneginf(Tensor([0, 17, 5, 6, 7],"float16"), )

[cuda error] paddle.isneginf(Tensor([0, 17, 5, 6, 7],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:17.679600 test begin: paddle.isneginf(Tensor([0, 17],"float32"), )

[cuda error] paddle.isneginf(Tensor([0, 17],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:17.865405 test begin: paddle.isneginf(Tensor([0, 3, 4, 5],"float64"), )

[cuda error] paddle.isneginf(Tensor([0, 3, 4, 5],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:18.058270 test begin: paddle.isneginf(Tensor([0, 3, 4, 5],"int8"), )

[cuda error] paddle.isneginf(Tensor([0, 3, 4, 5],"int8"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:18.248700 test begin: paddle.isneginf(Tensor([0],"int64"), )

[cuda error] paddle.isneginf(Tensor([0],"int64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:18.501986 test begin: paddle.isneginf(Tensor([0],"uint8"), )

[cuda error] paddle.isneginf(Tensor([0],"uint8"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:18.733825 test begin: paddle.isneginf(Tensor([11, 0, 10],"int16"), )

[cuda error] paddle.isneginf(Tensor([11, 0, 10],"int16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:18.936036 test begin: paddle.isneginf(Tensor([11, 0, 10],"int32"), )

[cuda error] paddle.isneginf(Tensor([11, 0, 10],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:19.139262 test begin: paddle.isneginf(Tensor([11, 0],"float32"), )

[cuda error] paddle.isneginf(Tensor([11, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:19.403144 test begin: paddle.isneginf(Tensor([11, 17, 0],"int16"), )

[cuda error] paddle.isneginf(Tensor([11, 17, 0],"int16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:19.598798 test begin: paddle.isneginf(Tensor([11, 17, 0],"int32"), )

[cuda error] paddle.isneginf(Tensor([11, 17, 0],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:19.788855 test begin: paddle.isneginf(Tensor([2, 0, 4, 5],"float64"), )

[cuda error] paddle.isneginf(Tensor([2, 0, 4, 5],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:19.991585 test begin: paddle.isneginf(Tensor([2, 0, 4, 5],"int8"), )

[cuda error] paddle.isneginf(Tensor([2, 0, 4, 5],"int8"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:20.184883 test begin: paddle.isneginf(Tensor([2, 3, 0, 5],"float64"), )

[cuda error] paddle.isneginf(Tensor([2, 3, 0, 5],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:20.380580 test begin: paddle.isneginf(Tensor([2, 3, 0, 5],"int8"), )

[cuda error] paddle.isneginf(Tensor([2, 3, 0, 5],"int8"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:20.569805 test begin: paddle.isneginf(Tensor([2, 3, 4, 0],"float64"), )

[cuda error] paddle.isneginf(Tensor([2, 3, 4, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:20.815231 test begin: paddle.isneginf(Tensor([2, 3, 4, 0],"int8"), )

[cuda error] paddle.isneginf(Tensor([2, 3, 4, 0],"int8"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:21.059547 test begin: paddle.isneginf(Tensor([8, 0, 5, 6, 7],"float16"), )

[cuda error] paddle.isneginf(Tensor([8, 0, 5, 6, 7],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:21.289964 test begin: paddle.isneginf(Tensor([8, 17, 0, 6, 7],"float16"), )

[cuda error] paddle.isneginf(Tensor([8, 17, 0, 6, 7],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:21.504975 test begin: paddle.isneginf(Tensor([8, 17, 5, 0, 7],"float16"), )

[cuda error] paddle.isneginf(Tensor([8, 17, 5, 0, 7],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:21.705749 test begin: paddle.isneginf(Tensor([8, 17, 5, 6, 0],"float16"), )

[cuda error] paddle.isneginf(Tensor([8, 17, 5, 6, 0],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:21.984654 test begin: paddle.isposinf(Tensor([0, 17, 10],"int16"), )

[cuda error] paddle.isposinf(Tensor([0, 17, 10],"int16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:22.206879 test begin: paddle.isposinf(Tensor([0, 17, 10],"int32"), )

[cuda error] paddle.isposinf(Tensor([0, 17, 10],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:22.447259 test begin: paddle.isposinf(Tensor([0, 17, 5, 6, 7],"float16"), )

[cuda error] paddle.isposinf(Tensor([0, 17, 5, 6, 7],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:22.677161 test begin: paddle.isposinf(Tensor([0, 17],"float32"), )

[cuda error] paddle.isposinf(Tensor([0, 17],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:22.880067 test begin: paddle.isposinf(Tensor([0, 3, 4, 5],"float64"), )

[cuda error] paddle.isposinf(Tensor([0, 3, 4, 5],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:23.078213 test begin: paddle.isposinf(Tensor([0, 3, 4, 5],"int8"), )

[cuda error] paddle.isposinf(Tensor([0, 3, 4, 5],"int8"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:23.287489 test begin: paddle.isposinf(Tensor([0],"int64"), )

[cuda error] paddle.isposinf(Tensor([0],"int64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:23.492345 test begin: paddle.isposinf(Tensor([0],"uint8"), )

[cuda error] paddle.isposinf(Tensor([0],"uint8"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:23.688833 test begin: paddle.isposinf(Tensor([11, 0, 10],"int16"), )

[cuda error] paddle.isposinf(Tensor([11, 0, 10],"int16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:23.918894 test begin: paddle.isposinf(Tensor([11, 0, 10],"int32"), )

[cuda error] paddle.isposinf(Tensor([11, 0, 10],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:24.114460 test begin: paddle.isposinf(Tensor([11, 0],"float32"), )

[cuda error] paddle.isposinf(Tensor([11, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:24.348929 test begin: paddle.isposinf(Tensor([11, 17, 0],"int16"), )

[cuda error] paddle.isposinf(Tensor([11, 17, 0],"int16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:24.546805 test begin: paddle.isposinf(Tensor([11, 17, 0],"int32"), )

[cuda error] paddle.isposinf(Tensor([11, 17, 0],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:24.749632 test begin: paddle.isposinf(Tensor([2, 0, 4, 5],"float64"), )

[cuda error] paddle.isposinf(Tensor([2, 0, 4, 5],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:25.015714 test begin: paddle.isposinf(Tensor([2, 0, 4, 5],"int8"), )

[cuda error] paddle.isposinf(Tensor([2, 0, 4, 5],"int8"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:25.231109 test begin: paddle.isposinf(Tensor([2, 3, 0, 5],"float64"), )

[cuda error] paddle.isposinf(Tensor([2, 3, 0, 5],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:25.461164 test begin: paddle.isposinf(Tensor([2, 3, 0, 5],"int8"), )

[cuda error] paddle.isposinf(Tensor([2, 3, 0, 5],"int8"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:25.674793 test begin: paddle.isposinf(Tensor([2, 3, 4, 0],"float64"), )

[cuda error] paddle.isposinf(Tensor([2, 3, 4, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:25.910569 test begin: paddle.isposinf(Tensor([2, 3, 4, 0],"int8"), )

[cuda error] paddle.isposinf(Tensor([2, 3, 4, 0],"int8"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:26.163550 test begin: paddle.isposinf(Tensor([8, 0, 5, 6, 7],"float16"), )

[cuda error] paddle.isposinf(Tensor([8, 0, 5, 6, 7],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:26.360987 test begin: paddle.isposinf(Tensor([8, 17, 0, 6, 7],"float16"), )

[cuda error] paddle.isposinf(Tensor([8, 17, 0, 6, 7],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:26.619654 test begin: paddle.isposinf(Tensor([8, 17, 5, 0, 7],"float16"), )

[cuda error] paddle.isposinf(Tensor([8, 17, 5, 0, 7],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:26.911616 test begin: paddle.isposinf(Tensor([8, 17, 5, 6, 0],"float16"), )

[cuda error] paddle.isposinf(Tensor([8, 17, 5, 6, 0],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:27.987816 test begin: paddle.kron(Tensor([0, 10],"float32"), Tensor([5, 5, 4, 3, 2],"float32"), )

[cuda error] paddle.kron(Tensor([0, 10],"float32"), Tensor([5, 5, 4, 3, 2],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:28.190595 test begin: paddle.kron(Tensor([0, 10],"float32"), Tensor([5, 5, 4],"float32"), )

[cuda error] paddle.kron(Tensor([0, 10],"float32"), Tensor([5, 5, 4],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:28.452930 test begin: paddle.kron(Tensor([0, 10],"float64"), Tensor([0, 10],"float64"), )

[cuda error] paddle.kron(Tensor([0, 10],"float64"), Tensor([0, 10],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:28.651130 test begin: paddle.kron(Tensor([0, 10],"float64"), Tensor([10, 10],"float64"), )

[cuda error] paddle.kron(Tensor([0, 10],"float64"), Tensor([10, 10],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:28.856186 test begin: paddle.kron(Tensor([0, 2],"complex128"), Tensor([2, 2, 3],"float64"), )

[cuda error] paddle.kron(Tensor([0, 2],"complex128"), Tensor([2, 2, 3],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:29.060395 test begin: paddle.kron(Tensor([0, 5, 4, 3, 5, 6],"float32"), Tensor([3, 5, 4],"float32"), )

[cuda error] paddle.kron(Tensor([0, 5, 4, 3, 5, 6],"float32"), Tensor([3, 5, 4],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:29.218359 test begin: paddle.kron(Tensor([0, 8],"float16"), Tensor([0, 8],"float16"), )

[cuda error] paddle.kron(Tensor([0, 8],"float16"), Tensor([0, 8],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:29.413355 test begin: paddle.kron(Tensor([0, 8],"float16"), Tensor([16, 8],"float16"), )

[cuda error] paddle.kron(Tensor([0, 8],"float16"), Tensor([16, 8],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:29.622316 test begin: paddle.kron(Tensor([10, 0],"float32"), Tensor([5, 5, 4, 3, 2],"float32"), )

[cuda error] paddle.kron(Tensor([10, 0],"float32"), Tensor([5, 5, 4, 3, 2],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:29.824408 test begin: paddle.kron(Tensor([10, 0],"float32"), Tensor([5, 5, 4],"float32"), )

[cuda error] paddle.kron(Tensor([10, 0],"float32"), Tensor([5, 5, 4],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:29.962182 test begin: paddle.kron(Tensor([10, 0],"float64"), Tensor([10, 0],"float64"), )

[cuda error] paddle.kron(Tensor([10, 0],"float64"), Tensor([10, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:30.084760 test begin: paddle.kron(Tensor([10, 0],"float64"), Tensor([10, 10],"float64"), )

[cuda error] paddle.kron(Tensor([10, 0],"float64"), Tensor([10, 10],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:30.274865 test begin: paddle.kron(Tensor([10, 10],"float32"), Tensor([0, 5, 4, 3, 2],"float32"), )

[cuda error] paddle.kron(Tensor([10, 10],"float32"), Tensor([0, 5, 4, 3, 2],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:30.486682 test begin: paddle.kron(Tensor([10, 10],"float32"), Tensor([0, 5, 4],"float32"), )

[cuda error] paddle.kron(Tensor([10, 10],"float32"), Tensor([0, 5, 4],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:30.697059 test begin: paddle.kron(Tensor([10, 10],"float32"), Tensor([5, 0, 4, 3, 2],"float32"), )

[cuda error] paddle.kron(Tensor([10, 10],"float32"), Tensor([5, 0, 4, 3, 2],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:30.925231 test begin: paddle.kron(Tensor([10, 10],"float32"), Tensor([5, 0, 4],"float32"), )

[cuda error] paddle.kron(Tensor([10, 10],"float32"), Tensor([5, 0, 4],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:31.182953 test begin: paddle.kron(Tensor([10, 10],"float32"), Tensor([5, 5, 0, 3, 2],"float32"), )

[cuda error] paddle.kron(Tensor([10, 10],"float32"), Tensor([5, 5, 0, 3, 2],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:31.412688 test begin: paddle.kron(Tensor([10, 10],"float32"), Tensor([5, 5, 0],"float32"), )

[cuda error] paddle.kron(Tensor([10, 10],"float32"), Tensor([5, 5, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:31.564113 test begin: paddle.kron(Tensor([10, 10],"float32"), Tensor([5, 5, 4, 0, 2],"float32"), )

[cuda error] paddle.kron(Tensor([10, 10],"float32"), Tensor([5, 5, 4, 0, 2],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:31.746289 test begin: paddle.kron(Tensor([10, 10],"float32"), Tensor([5, 5, 4, 3, 0],"float32"), )

[cuda error] paddle.kron(Tensor([10, 10],"float32"), Tensor([5, 5, 4, 3, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:31.940496 test begin: paddle.kron(Tensor([10, 10],"float64"), Tensor([0, 10],"float64"), )

[cuda error] paddle.kron(Tensor([10, 10],"float64"), Tensor([0, 10],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:32.136361 test begin: paddle.kron(Tensor([10, 10],"float64"), Tensor([10, 0],"float64"), )

[cuda error] paddle.kron(Tensor([10, 10],"float64"), Tensor([10, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:32.346840 test begin: paddle.kron(Tensor([12, 0],"float16"), Tensor([16, 0],"float16"), )

[cuda error] paddle.kron(Tensor([12, 0],"float16"), Tensor([16, 0],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:32.558958 test begin: paddle.kron(Tensor([12, 0],"float16"), Tensor([16, 8],"float16"), )

[cuda error] paddle.kron(Tensor([12, 0],"float16"), Tensor([16, 8],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:32.755584 test begin: paddle.kron(Tensor([12, 8],"float16"), Tensor([0, 8],"float16"), )

[cuda error] paddle.kron(Tensor([12, 8],"float16"), Tensor([0, 8],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:32.906594 test begin: paddle.kron(Tensor([12, 8],"float16"), Tensor([16, 0],"float16"), )

[cuda error] paddle.kron(Tensor([12, 8],"float16"), Tensor([16, 0],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:33.153568 test begin: paddle.kron(Tensor([2, 0],"complex128"), Tensor([2, 2, 3],"float64"), )

[cuda error] paddle.kron(Tensor([2, 0],"complex128"), Tensor([2, 2, 3],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:33.394048 test begin: paddle.kron(Tensor([2, 2],"complex128"), Tensor([0, 2, 3],"float64"), )

[paddle error] paddle.kron(Tensor([2, 2],"complex128"), Tensor([0, 2, 3],"float64"), ) 
 (InvalidArgument) The type of data we are trying to retrieve (complex128) does not match the type of data (float64) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():11 != phi::CppTypeToDataType<T>::Type():13.] (at ../paddle/phi/core/dense_tensor.cc:153)

2025-04-21 10:10:33.629012 test begin: paddle.kron(Tensor([2, 2],"complex128"), Tensor([2, 0, 3],"float64"), )

[paddle error] paddle.kron(Tensor([2, 2],"complex128"), Tensor([2, 0, 3],"float64"), ) 
 (InvalidArgument) The type of data we are trying to retrieve (complex128) does not match the type of data (float64) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():11 != phi::CppTypeToDataType<T>::Type():13.] (at ../paddle/phi/core/dense_tensor.cc:153)

2025-04-21 10:10:33.828200 test begin: paddle.kron(Tensor([2, 2],"complex128"), Tensor([2, 2, 0],"float64"), )

[paddle error] paddle.kron(Tensor([2, 2],"complex128"), Tensor([2, 2, 0],"float64"), ) 
 (InvalidArgument) The type of data we are trying to retrieve (complex128) does not match the type of data (float64) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():11 != phi::CppTypeToDataType<T>::Type():13.] (at ../paddle/phi/core/dense_tensor.cc:153)

2025-04-21 10:10:34.031168 test begin: paddle.kron(Tensor([5, 0, 4, 3, 5, 6],"float32"), Tensor([3, 5, 4],"float32"), )

[cuda error] paddle.kron(Tensor([5, 0, 4, 3, 5, 6],"float32"), Tensor([3, 5, 4],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:34.229172 test begin: paddle.kron(Tensor([5, 5, 0, 3, 5, 6],"float32"), Tensor([3, 5, 4],"float32"), )

[cuda error] paddle.kron(Tensor([5, 5, 0, 3, 5, 6],"float32"), Tensor([3, 5, 4],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:34.484124 test begin: paddle.kron(Tensor([5, 5, 4, 0, 5, 6],"float32"), Tensor([3, 5, 4],"float32"), )

[cuda error] paddle.kron(Tensor([5, 5, 4, 0, 5, 6],"float32"), Tensor([3, 5, 4],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:34.695594 test begin: paddle.kron(Tensor([5, 5, 4, 3, 0, 6],"float32"), Tensor([3, 5, 4],"float32"), )

[cuda error] paddle.kron(Tensor([5, 5, 4, 3, 0, 6],"float32"), Tensor([3, 5, 4],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:34.921999 test begin: paddle.kron(Tensor([5, 5, 4, 3, 5, 0],"float32"), Tensor([3, 5, 4],"float32"), )

[cuda error] paddle.kron(Tensor([5, 5, 4, 3, 5, 0],"float32"), Tensor([3, 5, 4],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:35.172678 test begin: paddle.kron(Tensor([5, 5, 4, 3, 5, 6],"float32"), Tensor([0, 5, 4],"float32"), )

[cuda error] paddle.kron(Tensor([5, 5, 4, 3, 5, 6],"float32"), Tensor([0, 5, 4],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:35.370782 test begin: paddle.kron(Tensor([5, 5, 4, 3, 5, 6],"float32"), Tensor([3, 0, 4],"float32"), )

[cuda error] paddle.kron(Tensor([5, 5, 4, 3, 5, 6],"float32"), Tensor([3, 0, 4],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:35.616543 test begin: paddle.kron(Tensor([5, 5, 4, 3, 5, 6],"float32"), Tensor([3, 5, 0],"float32"), )

[cuda error] paddle.kron(Tensor([5, 5, 4, 3, 5, 6],"float32"), Tensor([3, 5, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:35.852046 test begin: paddle.kron(x=Tensor([0, 2],"float64"), y=Tensor([0, 3],"float64"), )

[cuda error] paddle.kron(x=Tensor([0, 2],"float64"), y=Tensor([0, 3],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:36.070360 test begin: paddle.kron(x=Tensor([0, 2],"float64"), y=Tensor([3, 3, 2],"float64"), )

[cuda error] paddle.kron(x=Tensor([0, 2],"float64"), y=Tensor([3, 3, 2],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:36.290735 test begin: paddle.kron(x=Tensor([0, 2],"float64"), y=Tensor([3, 3],"float64"), )

[cuda error] paddle.kron(x=Tensor([0, 2],"float64"), y=Tensor([3, 3],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:36.491870 test begin: paddle.kron(x=Tensor([0, 3],"float32"), y=Tensor([0, 3],"float32"), )

[cuda error] paddle.kron(x=Tensor([0, 3],"float32"), y=Tensor([0, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:36.684868 test begin: paddle.kron(x=Tensor([0, 3],"float32"), y=Tensor([3, 3],"float32"), )

[cuda error] paddle.kron(x=Tensor([0, 3],"float32"), y=Tensor([3, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:36.886249 test begin: paddle.kron(x=Tensor([0],"float64"), y=Tensor([3, 3],"float64"), )

[cuda error] paddle.kron(x=Tensor([0],"float64"), y=Tensor([3, 3],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:37.129298 test begin: paddle.kron(x=Tensor([1],"float64"), y=Tensor([0, 3],"float64"), )

[cuda error] paddle.kron(x=Tensor([1],"float64"), y=Tensor([0, 3],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:37.342689 test begin: paddle.kron(x=Tensor([1],"float64"), y=Tensor([3, 0],"float64"), )

[cuda error] paddle.kron(x=Tensor([1],"float64"), y=Tensor([3, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:37.562766 test begin: paddle.kron(x=Tensor([2, 0],"float32"), y=Tensor([3, 0],"float32"), )

[cuda error] paddle.kron(x=Tensor([2, 0],"float32"), y=Tensor([3, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:37.776337 test begin: paddle.kron(x=Tensor([2, 0],"float32"), y=Tensor([3, 3],"float32"), )

[cuda error] paddle.kron(x=Tensor([2, 0],"float32"), y=Tensor([3, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:38.005806 test begin: paddle.kron(x=Tensor([2, 0],"float64"), y=Tensor([3, 0],"float64"), )

[cuda error] paddle.kron(x=Tensor([2, 0],"float64"), y=Tensor([3, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:38.215328 test begin: paddle.kron(x=Tensor([2, 0],"float64"), y=Tensor([3, 3, 2],"float64"), )

[cuda error] paddle.kron(x=Tensor([2, 0],"float64"), y=Tensor([3, 3, 2],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:38.414033 test begin: paddle.kron(x=Tensor([2, 0],"float64"), y=Tensor([3, 3],"float64"), )

[cuda error] paddle.kron(x=Tensor([2, 0],"float64"), y=Tensor([3, 3],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:38.651013 test begin: paddle.kron(x=Tensor([2, 2],"float64"), y=Tensor([0, 3, 2],"float64"), )

[cuda error] paddle.kron(x=Tensor([2, 2],"float64"), y=Tensor([0, 3, 2],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:38.843497 test begin: paddle.kron(x=Tensor([2, 2],"float64"), y=Tensor([0, 3],"float64"), )

[cuda error] paddle.kron(x=Tensor([2, 2],"float64"), y=Tensor([0, 3],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:39.070949 test begin: paddle.kron(x=Tensor([2, 2],"float64"), y=Tensor([3, 0, 2],"float64"), )

[cuda error] paddle.kron(x=Tensor([2, 2],"float64"), y=Tensor([3, 0, 2],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:39.314351 test begin: paddle.kron(x=Tensor([2, 2],"float64"), y=Tensor([3, 0],"float64"), )

[cuda error] paddle.kron(x=Tensor([2, 2],"float64"), y=Tensor([3, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:39.510554 test begin: paddle.kron(x=Tensor([2, 2],"float64"), y=Tensor([3, 3, 0],"float64"), )

[cuda error] paddle.kron(x=Tensor([2, 2],"float64"), y=Tensor([3, 3, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:39.771071 test begin: paddle.kron(x=Tensor([2, 3],"float32"), y=Tensor([0, 3],"float32"), )

[cuda error] paddle.kron(x=Tensor([2, 3],"float32"), y=Tensor([0, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:39.983582 test begin: paddle.kron(x=Tensor([2, 3],"float32"), y=Tensor([3, 0],"float32"), )

[cuda error] paddle.kron(x=Tensor([2, 3],"float32"), y=Tensor([3, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:40.194189 test begin: paddle.lerp(Tensor([0, 1, 1, 1],"float32"), Tensor([0, 3, 8, 8],"float32"), 0.3, )

[paddle error] paddle.lerp(Tensor([0, 1, 1, 1],"float32"), Tensor([0, 3, 8, 8],"float32"), 0.3, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:40.443993 test begin: paddle.lerp(Tensor([0, 1, 1],"float32"), Tensor([0, 28, 28],"float32"), 0.36, )

[paddle error] paddle.lerp(Tensor([0, 1, 1],"float32"), Tensor([0, 28, 28],"float32"), 0.36, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:40.675525 test begin: paddle.lerp(Tensor([0, 1, 1],"float32"), Tensor([0, 8, 8],"float32"), 0.3, )

[paddle error] paddle.lerp(Tensor([0, 1, 1],"float32"), Tensor([0, 8, 8],"float32"), 0.3, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:40.889545 test begin: paddle.lerp(Tensor([0, 1, 8, 8],"float32"), Tensor([0, 3, 8, 8],"float32"), 1.1, )

[paddle error] paddle.lerp(Tensor([0, 1, 8, 8],"float32"), Tensor([0, 3, 8, 8],"float32"), 1.1, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:41.088519 test begin: paddle.lerp(Tensor([0, 1],"float32"), Tensor([0, 1],"float32"), Tensor([0, 1],"float32"), )

[paddle error] paddle.lerp(Tensor([0, 1],"float32"), Tensor([0, 1],"float32"), Tensor([0, 1],"float32"), ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:41.321681 test begin: paddle.lerp(Tensor([0, 28, 28],"float32"), Tensor([0, 28, 28],"float32"), 1.0, )

[paddle error] paddle.lerp(Tensor([0, 28, 28],"float32"), Tensor([0, 28, 28],"float32"), 1.0, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:41.523265 test begin: paddle.lerp(Tensor([0, 28, 28],"float32"), Tensor([0, 28, 28],"float32"), 1.2, )

[paddle error] paddle.lerp(Tensor([0, 28, 28],"float32"), Tensor([0, 28, 28],"float32"), 1.2, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:41.790978 test begin: paddle.lerp(Tensor([0, 3, 3],"float32"), Tensor([0, 3, 3],"float32"), Tensor([0, 3, 3],"float32"), )

[paddle error] paddle.lerp(Tensor([0, 3, 3],"float32"), Tensor([0, 3, 3],"float32"), Tensor([0, 3, 3],"float32"), ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:42.030213 test begin: paddle.lerp(Tensor([0, 3, 3],"float32"), Tensor([1, 3, 3],"float32"), Tensor([1, 3, 3],"float32"), )

[paddle error] paddle.lerp(Tensor([0, 3, 3],"float32"), Tensor([1, 3, 3],"float32"), Tensor([1, 3, 3],"float32"), ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:42.242429 test begin: paddle.lerp(Tensor([0, 3, 3],"float64"), Tensor([0, 3, 3],"float64"), Tensor([0, 3, 3],"float64"), )

[paddle error] paddle.lerp(Tensor([0, 3, 3],"float64"), Tensor([0, 3, 3],"float64"), Tensor([0, 3, 3],"float64"), ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:42.489816 test begin: paddle.lerp(Tensor([0, 3, 3],"float64"), Tensor([1, 3, 3],"float64"), Tensor([1, 3, 3],"float64"), )

[paddle error] paddle.lerp(Tensor([0, 3, 3],"float64"), Tensor([1, 3, 3],"float64"), Tensor([1, 3, 3],"float64"), ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:42.740870 test begin: paddle.lerp(Tensor([0, 3, 8, 8],"float32"), Tensor([0, 3, 8, 8],"float32"), 2.1, )

[paddle error] paddle.lerp(Tensor([0, 3, 8, 8],"float32"), Tensor([0, 3, 8, 8],"float32"), 2.1, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:42.945280 test begin: paddle.lerp(Tensor([0, 3],"float64"), Tensor([0, 3],"float64"), Tensor([0, 3],"float64"), )

[paddle error] paddle.lerp(Tensor([0, 3],"float64"), Tensor([0, 3],"float64"), Tensor([0, 3],"float64"), ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:43.139883 test begin: paddle.lerp(Tensor([0, 3],"float64"), Tensor([1, 3],"float64"), Tensor([1, 3],"float64"), )

[paddle error] paddle.lerp(Tensor([0, 3],"float64"), Tensor([1, 3],"float64"), Tensor([1, 3],"float64"), ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:43.352136 test begin: paddle.lerp(Tensor([0, 6, 3, 1, 2, 5],"float64"), Tensor([0, 6, 3, 1, 2, 5],"float64"), Tensor([0, 6, 3, 1, 2, 5],"float64"), )

[paddle error] paddle.lerp(Tensor([0, 6, 3, 1, 2, 5],"float64"), Tensor([0, 6, 3, 1, 2, 5],"float64"), Tensor([0, 6, 3, 1, 2, 5],"float64"), ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:43.554051 test begin: paddle.lerp(Tensor([0, 6, 3, 4, 1, 5],"float64"), Tensor([0, 6, 3, 4, 1, 5],"float64"), Tensor([0, 6, 3, 4, 1, 5],"float64"), )

[paddle error] paddle.lerp(Tensor([0, 6, 3, 4, 1, 5],"float64"), Tensor([0, 6, 3, 4, 1, 5],"float64"), Tensor([0, 6, 3, 4, 1, 5],"float64"), ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:43.692857 test begin: paddle.lerp(Tensor([0, 8, 8],"float32"), Tensor([0, 8, 8],"float32"), 1.1, )

[paddle error] paddle.lerp(Tensor([0, 8, 8],"float32"), Tensor([0, 8, 8],"float32"), 1.1, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:43.815409 test begin: paddle.lerp(Tensor([0, 8, 8],"float32"), Tensor([0, 8, 8],"float32"), 2.1, )

[paddle error] paddle.lerp(Tensor([0, 8, 8],"float32"), Tensor([0, 8, 8],"float32"), 2.1, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:43.964895 test begin: paddle.lerp(Tensor([0],"float32"), Tensor([0],"float32"), Tensor([0],"float32"), )

[paddle error] paddle.lerp(Tensor([0],"float32"), Tensor([0],"float32"), Tensor([0],"float32"), ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:44.158920 test begin: paddle.lerp(Tensor([0],"float32"), Tensor([1],"float32"), Tensor([1],"float32"), )

[paddle error] paddle.lerp(Tensor([0],"float32"), Tensor([1],"float32"), Tensor([1],"float32"), ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:44.394491 test begin: paddle.lerp(Tensor([0],"float64"), Tensor([0],"float64"), Tensor([0],"float64"), )

[paddle error] paddle.lerp(Tensor([0],"float64"), Tensor([0],"float64"), Tensor([0],"float64"), ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:44.664999 test begin: paddle.lerp(Tensor([0],"float64"), Tensor([1],"float64"), Tensor([1],"float64"), )

[paddle error] paddle.lerp(Tensor([0],"float64"), Tensor([1],"float64"), Tensor([1],"float64"), ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:44.894148 test begin: paddle.lerp(Tensor([1, 0, 1],"float32"), Tensor([3, 0, 28],"float32"), 0.36, )

[paddle error] paddle.lerp(Tensor([1, 0, 1],"float32"), Tensor([3, 0, 28],"float32"), 0.36, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:45.099590 test begin: paddle.lerp(Tensor([1, 0, 1],"float32"), Tensor([3, 0, 8],"float32"), 0.3, )

[paddle error] paddle.lerp(Tensor([1, 0, 1],"float32"), Tensor([3, 0, 8],"float32"), 0.3, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:45.295917 test begin: paddle.lerp(Tensor([1, 0, 28],"float32"), Tensor([3, 0, 28],"float32"), 1.0, )

[paddle error] paddle.lerp(Tensor([1, 0, 28],"float32"), Tensor([3, 0, 28],"float32"), 1.0, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:45.485737 test begin: paddle.lerp(Tensor([1, 0, 3],"float32"), Tensor([1, 0, 3],"float32"), Tensor([1, 0, 3],"float32"), )

[paddle error] paddle.lerp(Tensor([1, 0, 3],"float32"), Tensor([1, 0, 3],"float32"), Tensor([1, 0, 3],"float32"), ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:45.691200 test begin: paddle.lerp(Tensor([1, 0, 3],"float64"), Tensor([1, 0, 3],"float64"), Tensor([1, 0, 3],"float64"), )

[paddle error] paddle.lerp(Tensor([1, 0, 3],"float64"), Tensor([1, 0, 3],"float64"), Tensor([1, 0, 3],"float64"), ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:45.916563 test begin: paddle.lerp(Tensor([1, 0, 8],"float32"), Tensor([3, 0, 8],"float32"), 1.1, )

[paddle error] paddle.lerp(Tensor([1, 0, 8],"float32"), Tensor([3, 0, 8],"float32"), 1.1, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:46.126572 test begin: paddle.lerp(Tensor([1, 0],"float64"), Tensor([1, 0],"float64"), Tensor([1, 0],"float64"), )

[paddle error] paddle.lerp(Tensor([1, 0],"float64"), Tensor([1, 0],"float64"), Tensor([1, 0],"float64"), ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:46.392786 test begin: paddle.lerp(Tensor([1, 1, 0],"float32"), Tensor([3, 28, 0],"float32"), 0.36, )

[paddle error] paddle.lerp(Tensor([1, 1, 0],"float32"), Tensor([3, 28, 0],"float32"), 0.36, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:46.585328 test begin: paddle.lerp(Tensor([1, 1, 0],"float32"), Tensor([3, 8, 0],"float32"), 0.3, )

[paddle error] paddle.lerp(Tensor([1, 1, 0],"float32"), Tensor([3, 8, 0],"float32"), 0.3, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:46.864212 test begin: paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([0, 28, 28],"float32"), 0.36, )

[paddle error] paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([0, 28, 28],"float32"), 0.36, ) 
 (InvalidArgument) LerpKernel's input y must not empty.
  [Hint: Expected y.numel() > 0, but received y.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:55)

2025-04-21 10:10:47.132530 test begin: paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([0, 8, 8],"float32"), 0.3, )

[paddle error] paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([0, 8, 8],"float32"), 0.3, ) 
 (InvalidArgument) LerpKernel's input y must not empty.
  [Hint: Expected y.numel() > 0, but received y.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:55)

2025-04-21 10:10:47.393334 test begin: paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([3, 0, 28],"float32"), 0.36, )

[paddle error] paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([3, 0, 28],"float32"), 0.36, ) 
 (InvalidArgument) LerpKernel's input y must not empty.
  [Hint: Expected y.numel() > 0, but received y.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:55)

2025-04-21 10:10:47.624457 test begin: paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([3, 0, 8],"float32"), 0.3, )

[paddle error] paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([3, 0, 8],"float32"), 0.3, ) 
 (InvalidArgument) LerpKernel's input y must not empty.
  [Hint: Expected y.numel() > 0, but received y.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:55)

2025-04-21 10:10:47.862893 test begin: paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([3, 28, 0],"float32"), 0.36, )

[paddle error] paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([3, 28, 0],"float32"), 0.36, ) 
 (InvalidArgument) LerpKernel's input y must not empty.
  [Hint: Expected y.numel() > 0, but received y.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:55)

2025-04-21 10:10:48.070435 test begin: paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([3, 8, 0],"float32"), 0.3, )

[paddle error] paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([3, 8, 0],"float32"), 0.3, ) 
 (InvalidArgument) LerpKernel's input y must not empty.
  [Hint: Expected y.numel() > 0, but received y.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:55)

2025-04-21 10:10:48.314236 test begin: paddle.lerp(Tensor([1, 28, 0],"float32"), Tensor([3, 28, 0],"float32"), 1.0, )

[paddle error] paddle.lerp(Tensor([1, 28, 0],"float32"), Tensor([3, 28, 0],"float32"), 1.0, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:48.549612 test begin: paddle.lerp(Tensor([1, 28, 28],"float32"), Tensor([0, 28, 28],"float32"), 1.0, )

[paddle error] paddle.lerp(Tensor([1, 28, 28],"float32"), Tensor([0, 28, 28],"float32"), 1.0, ) 
 (InvalidArgument) LerpKernel's input y must not empty.
  [Hint: Expected y.numel() > 0, but received y.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:55)

2025-04-21 10:10:48.788829 test begin: paddle.lerp(Tensor([1, 3, 0],"float32"), Tensor([1, 3, 0],"float32"), Tensor([1, 3, 0],"float32"), )

[paddle error] paddle.lerp(Tensor([1, 3, 0],"float32"), Tensor([1, 3, 0],"float32"), Tensor([1, 3, 0],"float32"), ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:49.018213 test begin: paddle.lerp(Tensor([1, 3, 0],"float64"), Tensor([1, 3, 0],"float64"), Tensor([1, 3, 0],"float64"), )

[paddle error] paddle.lerp(Tensor([1, 3, 0],"float64"), Tensor([1, 3, 0],"float64"), Tensor([1, 3, 0],"float64"), ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:49.212122 test begin: paddle.lerp(Tensor([1, 3, 3],"float32"), Tensor([0, 3, 3],"float32"), Tensor([1, 3, 3],"float32"), )

[paddle error] paddle.lerp(Tensor([1, 3, 3],"float32"), Tensor([0, 3, 3],"float32"), Tensor([1, 3, 3],"float32"), ) 
 (InvalidArgument) LerpKernel's input y must not empty.
  [Hint: Expected y.numel() > 0, but received y.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:55)

2025-04-21 10:10:49.419464 test begin: paddle.lerp(Tensor([1, 3, 3],"float32"), Tensor([1, 3, 3],"float32"), Tensor([0, 3, 3],"float32"), )

[cuda error] paddle.lerp(Tensor([1, 3, 3],"float32"), Tensor([1, 3, 3],"float32"), Tensor([0, 3, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:49.647507 test begin: paddle.lerp(Tensor([1, 3, 3],"float64"), Tensor([0, 3, 3],"float64"), Tensor([1, 3, 3],"float64"), )

[paddle error] paddle.lerp(Tensor([1, 3, 3],"float64"), Tensor([0, 3, 3],"float64"), Tensor([1, 3, 3],"float64"), ) 
 (InvalidArgument) LerpKernel's input y must not empty.
  [Hint: Expected y.numel() > 0, but received y.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:55)

2025-04-21 10:10:49.880280 test begin: paddle.lerp(Tensor([1, 3, 3],"float64"), Tensor([1, 3, 3],"float64"), Tensor([0, 3, 3],"float64"), )

[cuda error] paddle.lerp(Tensor([1, 3, 3],"float64"), Tensor([1, 3, 3],"float64"), Tensor([0, 3, 3],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:50.108885 test begin: paddle.lerp(Tensor([1, 3],"float64"), Tensor([0, 3],"float64"), Tensor([1, 3],"float64"), )

[paddle error] paddle.lerp(Tensor([1, 3],"float64"), Tensor([0, 3],"float64"), Tensor([1, 3],"float64"), ) 
 (InvalidArgument) LerpKernel's input y must not empty.
  [Hint: Expected y.numel() > 0, but received y.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:55)

2025-04-21 10:10:50.343441 test begin: paddle.lerp(Tensor([1, 3],"float64"), Tensor([1, 3],"float64"), Tensor([0, 3],"float64"), )

[cuda error] paddle.lerp(Tensor([1, 3],"float64"), Tensor([1, 3],"float64"), Tensor([0, 3],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:50.597918 test begin: paddle.lerp(Tensor([1, 8, 0],"float32"), Tensor([3, 8, 0],"float32"), 1.1, )

[paddle error] paddle.lerp(Tensor([1, 8, 0],"float32"), Tensor([3, 8, 0],"float32"), 1.1, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:50.832595 test begin: paddle.lerp(Tensor([1, 8, 8],"float32"), Tensor([0, 8, 8],"float32"), 1.1, )

[paddle error] paddle.lerp(Tensor([1, 8, 8],"float32"), Tensor([0, 8, 8],"float32"), 1.1, ) 
 (InvalidArgument) LerpKernel's input y must not empty.
  [Hint: Expected y.numel() > 0, but received y.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:55)

2025-04-21 10:10:51.127919 test begin: paddle.lerp(Tensor([10, 1, 0, 5, 5],"float32"), Tensor([10, 5, 1, 5, 5],"float32"), Tensor([1],"float32"), )

[paddle error] paddle.lerp(Tensor([10, 1, 0, 5, 5],"float32"), Tensor([10, 5, 1, 5, 5],"float32"), Tensor([1],"float32"), ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:51.338917 test begin: paddle.lerp(Tensor([10, 1, 10, 5, 5],"float32"), Tensor([10, 0, 1, 5, 5],"float32"), Tensor([1],"float32"), )

[paddle error] paddle.lerp(Tensor([10, 1, 10, 5, 5],"float32"), Tensor([10, 0, 1, 5, 5],"float32"), Tensor([1],"float32"), ) 
 (InvalidArgument) LerpKernel's input y must not empty.
  [Hint: Expected y.numel() > 0, but received y.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:55)

2025-04-21 10:10:51.597875 test begin: paddle.lerp(Tensor([10, 5, 10, 1, 0],"float32"), Tensor([10, 5, 10, 5, 1],"float32"), Tensor([1],"float32"), )

[paddle error] paddle.lerp(Tensor([10, 5, 10, 1, 0],"float32"), Tensor([10, 5, 10, 5, 1],"float32"), Tensor([1],"float32"), ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:51.803735 test begin: paddle.lerp(Tensor([10, 5, 10, 1, 5],"float32"), Tensor([10, 5, 10, 0, 1],"float32"), Tensor([1],"float32"), )

[paddle error] paddle.lerp(Tensor([10, 5, 10, 1, 5],"float32"), Tensor([10, 5, 10, 0, 1],"float32"), Tensor([1],"float32"), ) 
 (InvalidArgument) LerpKernel's input y must not empty.
  [Hint: Expected y.numel() > 0, but received y.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:55)

2025-04-21 10:10:52.059267 test begin: paddle.lerp(Tensor([1],"float32"), Tensor([0],"float32"), Tensor([1],"float32"), )

[paddle error] paddle.lerp(Tensor([1],"float32"), Tensor([0],"float32"), Tensor([1],"float32"), ) 
 (InvalidArgument) LerpKernel's input y must not empty.
  [Hint: Expected y.numel() > 0, but received y.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:55)

2025-04-21 10:10:52.270995 test begin: paddle.lerp(Tensor([1],"float32"), Tensor([1],"float32"), Tensor([0],"float32"), )

[cuda error] paddle.lerp(Tensor([1],"float32"), Tensor([1],"float32"), Tensor([0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:52.464037 test begin: paddle.lerp(Tensor([1],"float64"), Tensor([0],"float64"), Tensor([1],"float64"), )

[paddle error] paddle.lerp(Tensor([1],"float64"), Tensor([0],"float64"), Tensor([1],"float64"), ) 
 (InvalidArgument) LerpKernel's input y must not empty.
  [Hint: Expected y.numel() > 0, but received y.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:55)

2025-04-21 10:10:52.645587 test begin: paddle.lerp(Tensor([1],"float64"), Tensor([1],"float64"), Tensor([0],"float64"), )

[cuda error] paddle.lerp(Tensor([1],"float64"), Tensor([1],"float64"), Tensor([0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:52.868554 test begin: paddle.lerp(Tensor([2, 0, 1, 1],"float32"), Tensor([2, 0, 8, 8],"float32"), 0.3, )

[paddle error] paddle.lerp(Tensor([2, 0, 1, 1],"float32"), Tensor([2, 0, 8, 8],"float32"), 0.3, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:53.010839 test begin: paddle.lerp(Tensor([2, 0, 5],"float32"), Tensor([3, 2, 1, 5],"float32"), 0.5, )

[paddle error] paddle.lerp(Tensor([2, 0, 5],"float32"), Tensor([3, 2, 1, 5],"float32"), 0.5, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:53.173158 test begin: paddle.lerp(Tensor([2, 0, 8, 8],"float32"), Tensor([2, 0, 8, 8],"float32"), 1.1, )

[paddle error] paddle.lerp(Tensor([2, 0, 8, 8],"float32"), Tensor([2, 0, 8, 8],"float32"), 1.1, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:53.351407 test begin: paddle.lerp(Tensor([2, 0, 8, 8],"float32"), Tensor([2, 0, 8, 8],"float32"), 2.1, )

[paddle error] paddle.lerp(Tensor([2, 0, 8, 8],"float32"), Tensor([2, 0, 8, 8],"float32"), 2.1, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:53.545942 test begin: paddle.lerp(Tensor([2, 0],"float32"), Tensor([2, 0],"float32"), Tensor([2, 0],"float32"), )

[paddle error] paddle.lerp(Tensor([2, 0],"float32"), Tensor([2, 0],"float32"), Tensor([2, 0],"float32"), ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:53.690286 test begin: paddle.lerp(Tensor([2, 0],"float32"), Tensor([2, 1],"float32"), Tensor([2, 1],"float32"), )

[paddle error] paddle.lerp(Tensor([2, 0],"float32"), Tensor([2, 1],"float32"), Tensor([2, 1],"float32"), ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:53.870729 test begin: paddle.lerp(Tensor([2, 1, 0, 1],"float32"), Tensor([2, 3, 0, 8],"float32"), 0.3, )

[paddle error] paddle.lerp(Tensor([2, 1, 0, 1],"float32"), Tensor([2, 3, 0, 8],"float32"), 0.3, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:54.090099 test begin: paddle.lerp(Tensor([2, 1, 0, 8],"float32"), Tensor([2, 3, 0, 8],"float32"), 1.1, )

[paddle error] paddle.lerp(Tensor([2, 1, 0, 8],"float32"), Tensor([2, 3, 0, 8],"float32"), 1.1, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:54.244512 test begin: paddle.lerp(Tensor([2, 1, 1, 0],"float32"), Tensor([2, 3, 8, 0],"float32"), 0.3, )

[paddle error] paddle.lerp(Tensor([2, 1, 1, 0],"float32"), Tensor([2, 3, 8, 0],"float32"), 0.3, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:54.426023 test begin: paddle.lerp(Tensor([2, 1, 1, 1],"float32"), Tensor([2, 0, 8, 8],"float32"), 0.3, )

[paddle error] paddle.lerp(Tensor([2, 1, 1, 1],"float32"), Tensor([2, 0, 8, 8],"float32"), 0.3, ) 
 (InvalidArgument) LerpKernel's input y must not empty.
  [Hint: Expected y.numel() > 0, but received y.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:55)

2025-04-21 10:10:54.668912 test begin: paddle.lerp(Tensor([2, 1, 1, 1],"float32"), Tensor([2, 3, 0, 8],"float32"), 0.3, )

[paddle error] paddle.lerp(Tensor([2, 1, 1, 1],"float32"), Tensor([2, 3, 0, 8],"float32"), 0.3, ) 
 (InvalidArgument) LerpKernel's input y must not empty.
  [Hint: Expected y.numel() > 0, but received y.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:55)

2025-04-21 10:10:54.881985 test begin: paddle.lerp(Tensor([2, 1, 1, 1],"float32"), Tensor([2, 3, 8, 0],"float32"), 0.3, )

[paddle error] paddle.lerp(Tensor([2, 1, 1, 1],"float32"), Tensor([2, 3, 8, 0],"float32"), 0.3, ) 
 (InvalidArgument) LerpKernel's input y must not empty.
  [Hint: Expected y.numel() > 0, but received y.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:55)

2025-04-21 10:10:55.124065 test begin: paddle.lerp(Tensor([2, 1, 8, 0],"float32"), Tensor([2, 3, 8, 0],"float32"), 1.1, )

[paddle error] paddle.lerp(Tensor([2, 1, 8, 0],"float32"), Tensor([2, 3, 8, 0],"float32"), 1.1, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:55.331994 test begin: paddle.lerp(Tensor([2, 1, 8, 8],"float32"), Tensor([2, 0, 8, 8],"float32"), 1.1, )

[paddle error] paddle.lerp(Tensor([2, 1, 8, 8],"float32"), Tensor([2, 0, 8, 8],"float32"), 1.1, ) 
 (InvalidArgument) LerpKernel's input y must not empty.
  [Hint: Expected y.numel() > 0, but received y.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:55)

2025-04-21 10:10:55.544028 test begin: paddle.lerp(Tensor([2, 1],"float32"), Tensor([2, 0],"float32"), Tensor([2, 1],"float32"), )

[paddle error] paddle.lerp(Tensor([2, 1],"float32"), Tensor([2, 0],"float32"), Tensor([2, 1],"float32"), ) 
 (InvalidArgument) LerpKernel's input y must not empty.
  [Hint: Expected y.numel() > 0, but received y.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:55)

2025-04-21 10:10:55.693845 test begin: paddle.lerp(Tensor([2, 1],"float32"), Tensor([2, 1],"float32"), Tensor([2, 0],"float32"), )

[cuda error] paddle.lerp(Tensor([2, 1],"float32"), Tensor([2, 1],"float32"), Tensor([2, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:55.887737 test begin: paddle.lerp(Tensor([2, 2, 5],"float32"), Tensor([0, 2, 1, 5],"float32"), 0.5, )

[paddle error] paddle.lerp(Tensor([2, 2, 5],"float32"), Tensor([0, 2, 1, 5],"float32"), 0.5, ) 
 (InvalidArgument) LerpKernel's input y must not empty.
  [Hint: Expected y.numel() > 0, but received y.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:55)

2025-04-21 10:10:56.111179 test begin: paddle.lerp(Tensor([2, 3, 0, 8],"float32"), Tensor([2, 3, 0, 8],"float32"), 2.1, )

[paddle error] paddle.lerp(Tensor([2, 3, 0, 8],"float32"), Tensor([2, 3, 0, 8],"float32"), 2.1, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:56.258317 test begin: paddle.lerp(Tensor([2, 3, 8, 0],"float32"), Tensor([2, 3, 8, 0],"float32"), 2.1, )

[paddle error] paddle.lerp(Tensor([2, 3, 8, 0],"float32"), Tensor([2, 3, 8, 0],"float32"), 2.1, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:56.471206 test begin: paddle.lerp(Tensor([2, 5],"float32"), Tensor([2, 2, 5],"float32"), Tensor([0, 2, 2, 5],"float32"), )

[cuda error] paddle.lerp(Tensor([2, 5],"float32"), Tensor([2, 2, 5],"float32"), Tensor([0, 2, 2, 5],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:56.716845 test begin: paddle.lerp(Tensor([3, 0, 28],"float32"), Tensor([3, 0, 28],"float32"), 1.2, )

[paddle error] paddle.lerp(Tensor([3, 0, 28],"float32"), Tensor([3, 0, 28],"float32"), 1.2, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:56.907436 test begin: paddle.lerp(Tensor([3, 0, 3, 1, 2, 5],"float64"), Tensor([3, 0, 3, 1, 2, 5],"float64"), Tensor([3, 0, 3, 1, 2, 5],"float64"), )

[paddle error] paddle.lerp(Tensor([3, 0, 3, 1, 2, 5],"float64"), Tensor([3, 0, 3, 1, 2, 5],"float64"), Tensor([3, 0, 3, 1, 2, 5],"float64"), ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:57.093964 test begin: paddle.lerp(Tensor([3, 0, 3, 4, 1, 5],"float64"), Tensor([3, 0, 3, 4, 1, 5],"float64"), Tensor([3, 0, 3, 4, 1, 5],"float64"), )

[paddle error] paddle.lerp(Tensor([3, 0, 3, 4, 1, 5],"float64"), Tensor([3, 0, 3, 4, 1, 5],"float64"), Tensor([3, 0, 3, 4, 1, 5],"float64"), ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:57.293381 test begin: paddle.lerp(Tensor([3, 0, 8],"float32"), Tensor([3, 0, 8],"float32"), 2.1, )

[paddle error] paddle.lerp(Tensor([3, 0, 8],"float32"), Tensor([3, 0, 8],"float32"), 2.1, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:57.536260 test begin: paddle.lerp(Tensor([3, 28, 0],"float32"), Tensor([3, 28, 0],"float32"), 1.2, )

[paddle error] paddle.lerp(Tensor([3, 28, 0],"float32"), Tensor([3, 28, 0],"float32"), 1.2, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:57.684563 test begin: paddle.lerp(Tensor([3, 6, 0, 1, 2, 5],"float64"), Tensor([3, 6, 0, 1, 2, 5],"float64"), Tensor([3, 6, 0, 1, 2, 5],"float64"), )

[paddle error] paddle.lerp(Tensor([3, 6, 0, 1, 2, 5],"float64"), Tensor([3, 6, 0, 1, 2, 5],"float64"), Tensor([3, 6, 0, 1, 2, 5],"float64"), ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:57.883757 test begin: paddle.lerp(Tensor([3, 6, 0, 4, 1, 5],"float64"), Tensor([3, 6, 0, 4, 1, 5],"float64"), Tensor([3, 6, 0, 4, 1, 5],"float64"), )

[paddle error] paddle.lerp(Tensor([3, 6, 0, 4, 1, 5],"float64"), Tensor([3, 6, 0, 4, 1, 5],"float64"), Tensor([3, 6, 0, 4, 1, 5],"float64"), ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:58.089468 test begin: paddle.lerp(Tensor([3, 6, 3, 0, 1, 5],"float64"), Tensor([3, 6, 3, 0, 1, 5],"float64"), Tensor([3, 6, 3, 0, 1, 5],"float64"), )

[paddle error] paddle.lerp(Tensor([3, 6, 3, 0, 1, 5],"float64"), Tensor([3, 6, 3, 0, 1, 5],"float64"), Tensor([3, 6, 3, 0, 1, 5],"float64"), ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:58.238138 test begin: paddle.lerp(Tensor([3, 6, 3, 0, 2, 5],"float64"), Tensor([3, 6, 3, 0, 2, 5],"float64"), Tensor([3, 6, 3, 0, 2, 5],"float64"), )

[paddle error] paddle.lerp(Tensor([3, 6, 3, 0, 2, 5],"float64"), Tensor([3, 6, 3, 0, 2, 5],"float64"), Tensor([3, 6, 3, 0, 2, 5],"float64"), ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:58.482165 test begin: paddle.lerp(Tensor([3, 6, 3, 0, 2, 5],"float64"), Tensor([3, 6, 3, 1, 2, 5],"float64"), Tensor([3, 6, 3, 1, 2, 5],"float64"), )

[paddle error] paddle.lerp(Tensor([3, 6, 3, 0, 2, 5],"float64"), Tensor([3, 6, 3, 1, 2, 5],"float64"), Tensor([3, 6, 3, 1, 2, 5],"float64"), ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:58.671847 test begin: paddle.lerp(Tensor([3, 6, 3, 1, 0, 5],"float64"), Tensor([3, 6, 3, 1, 0, 5],"float64"), Tensor([3, 6, 3, 1, 0, 5],"float64"), )

[paddle error] paddle.lerp(Tensor([3, 6, 3, 1, 0, 5],"float64"), Tensor([3, 6, 3, 1, 0, 5],"float64"), Tensor([3, 6, 3, 1, 0, 5],"float64"), ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:58.876262 test begin: paddle.lerp(Tensor([3, 6, 3, 1, 2, 0],"float64"), Tensor([3, 6, 3, 1, 2, 0],"float64"), Tensor([3, 6, 3, 1, 2, 0],"float64"), )

[paddle error] paddle.lerp(Tensor([3, 6, 3, 1, 2, 0],"float64"), Tensor([3, 6, 3, 1, 2, 0],"float64"), Tensor([3, 6, 3, 1, 2, 0],"float64"), ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:59.030514 test begin: paddle.lerp(Tensor([3, 6, 3, 1, 2, 5],"float64"), Tensor([3, 6, 3, 0, 2, 5],"float64"), Tensor([3, 6, 3, 1, 2, 5],"float64"), )

[paddle error] paddle.lerp(Tensor([3, 6, 3, 1, 2, 5],"float64"), Tensor([3, 6, 3, 0, 2, 5],"float64"), Tensor([3, 6, 3, 1, 2, 5],"float64"), ) 
 (InvalidArgument) LerpKernel's input y must not empty.
  [Hint: Expected y.numel() > 0, but received y.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:55)

2025-04-21 10:10:59.283095 test begin: paddle.lerp(Tensor([3, 6, 3, 1, 2, 5],"float64"), Tensor([3, 6, 3, 1, 2, 5],"float64"), Tensor([3, 6, 3, 0, 2, 5],"float64"), )

[cuda error] paddle.lerp(Tensor([3, 6, 3, 1, 2, 5],"float64"), Tensor([3, 6, 3, 1, 2, 5],"float64"), Tensor([3, 6, 3, 0, 2, 5],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:59.539514 test begin: paddle.lerp(Tensor([3, 6, 3, 4, 0, 5],"float64"), Tensor([3, 6, 3, 4, 0, 5],"float64"), Tensor([3, 6, 3, 4, 0, 5],"float64"), )

[paddle error] paddle.lerp(Tensor([3, 6, 3, 4, 0, 5],"float64"), Tensor([3, 6, 3, 4, 0, 5],"float64"), Tensor([3, 6, 3, 4, 0, 5],"float64"), ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:59.804762 test begin: paddle.lerp(Tensor([3, 6, 3, 4, 0, 5],"float64"), Tensor([3, 6, 3, 4, 1, 5],"float64"), Tensor([3, 6, 3, 4, 1, 5],"float64"), )

[paddle error] paddle.lerp(Tensor([3, 6, 3, 4, 0, 5],"float64"), Tensor([3, 6, 3, 4, 1, 5],"float64"), Tensor([3, 6, 3, 4, 1, 5],"float64"), ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:11:00.018180 test begin: paddle.lerp(Tensor([3, 6, 3, 4, 1, 0],"float64"), Tensor([3, 6, 3, 4, 1, 0],"float64"), Tensor([3, 6, 3, 4, 1, 0],"float64"), )

[paddle error] paddle.lerp(Tensor([3, 6, 3, 4, 1, 0],"float64"), Tensor([3, 6, 3, 4, 1, 0],"float64"), Tensor([3, 6, 3, 4, 1, 0],"float64"), ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:11:00.300708 test begin: paddle.lerp(Tensor([3, 6, 3, 4, 1, 5],"float64"), Tensor([3, 6, 3, 4, 0, 5],"float64"), Tensor([3, 6, 3, 4, 1, 5],"float64"), )

[paddle error] paddle.lerp(Tensor([3, 6, 3, 4, 1, 5],"float64"), Tensor([3, 6, 3, 4, 0, 5],"float64"), Tensor([3, 6, 3, 4, 1, 5],"float64"), ) 
 (InvalidArgument) LerpKernel's input y must not empty.
  [Hint: Expected y.numel() > 0, but received y.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:55)

2025-04-21 10:11:00.452427 test begin: paddle.lerp(Tensor([3, 6, 3, 4, 1, 5],"float64"), Tensor([3, 6, 3, 4, 1, 5],"float64"), Tensor([3, 6, 3, 4, 0, 5],"float64"), )

[cuda error] paddle.lerp(Tensor([3, 6, 3, 4, 1, 5],"float64"), Tensor([3, 6, 3, 4, 1, 5],"float64"), Tensor([3, 6, 3, 4, 0, 5],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:00.662391 test begin: paddle.lerp(Tensor([3, 8, 0],"float32"), Tensor([3, 8, 0],"float32"), 2.1, )

[paddle error] paddle.lerp(Tensor([3, 8, 0],"float32"), Tensor([3, 8, 0],"float32"), 2.1, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:11:00.942427 test begin: paddle.lerp(x=Tensor([0, 5, 4, 3],"float64"), y=Tensor([0, 5, 4, 3],"float64"), weight=0.0, )

[paddle error] paddle.lerp(x=Tensor([0, 5, 4, 3],"float64"), y=Tensor([0, 5, 4, 3],"float64"), weight=0.0, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:11:01.201721 test begin: paddle.lerp(x=Tensor([0, 5, 4, 3],"float64"), y=Tensor([0, 5, 4, 3],"float64"), weight=0.5, )

[paddle error] paddle.lerp(x=Tensor([0, 5, 4, 3],"float64"), y=Tensor([0, 5, 4, 3],"float64"), weight=0.5, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:11:01.341219 test begin: paddle.lerp(x=Tensor([0, 5, 4, 3],"float64"), y=Tensor([0, 5, 4, 3],"float64"), weight=1.0, )

[paddle error] paddle.lerp(x=Tensor([0, 5, 4, 3],"float64"), y=Tensor([0, 5, 4, 3],"float64"), weight=1.0, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:11:01.532327 test begin: paddle.lerp(x=Tensor([0, 5, 4],"float64"), y=Tensor([0, 5, 4],"float64"), weight=0.5, )

[paddle error] paddle.lerp(x=Tensor([0, 5, 4],"float64"), y=Tensor([0, 5, 4],"float64"), weight=0.5, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:11:01.672730 test begin: paddle.lerp(x=Tensor([0, 5],"float64"), y=Tensor([0, 5],"float64"), weight=0.5, )

[paddle error] paddle.lerp(x=Tensor([0, 5],"float64"), y=Tensor([0, 5],"float64"), weight=0.5, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:11:01.860864 test begin: paddle.lerp(x=Tensor([0, 5],"float64"), y=Tensor([1],"float64"), weight=0.2, )

[paddle error] paddle.lerp(x=Tensor([0, 5],"float64"), y=Tensor([1],"float64"), weight=0.2, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:11:02.070357 test begin: paddle.lerp(x=Tensor([0],"float32"), y=Tensor([0],"float32"), weight=0.5, )

[paddle error] paddle.lerp(x=Tensor([0],"float32"), y=Tensor([0],"float32"), weight=0.5, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:11:02.327787 test begin: paddle.lerp(x=Tensor([0],"float64"), y=Tensor([0],"float64"), weight=0.5, )

[paddle error] paddle.lerp(x=Tensor([0],"float64"), y=Tensor([0],"float64"), weight=0.5, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:11:02.558351 test begin: paddle.lerp(x=Tensor([4, 0, 4, 3],"float64"), y=Tensor([4, 0, 4, 3],"float64"), weight=0.0, )

[paddle error] paddle.lerp(x=Tensor([4, 0, 4, 3],"float64"), y=Tensor([4, 0, 4, 3],"float64"), weight=0.0, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:11:02.760097 test begin: paddle.lerp(x=Tensor([4, 0, 4, 3],"float64"), y=Tensor([4, 0, 4, 3],"float64"), weight=0.5, )

[paddle error] paddle.lerp(x=Tensor([4, 0, 4, 3],"float64"), y=Tensor([4, 0, 4, 3],"float64"), weight=0.5, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:11:02.993527 test begin: paddle.lerp(x=Tensor([4, 0, 4, 3],"float64"), y=Tensor([4, 0, 4, 3],"float64"), weight=1.0, )

[paddle error] paddle.lerp(x=Tensor([4, 0, 4, 3],"float64"), y=Tensor([4, 0, 4, 3],"float64"), weight=1.0, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:11:03.230332 test begin: paddle.lerp(x=Tensor([4, 0, 4],"float64"), y=Tensor([4, 0, 4],"float64"), weight=0.5, )

[paddle error] paddle.lerp(x=Tensor([4, 0, 4],"float64"), y=Tensor([4, 0, 4],"float64"), weight=0.5, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:11:03.469898 test begin: paddle.lerp(x=Tensor([4, 0],"float64"), y=Tensor([1],"float64"), weight=0.2, )

[paddle error] paddle.lerp(x=Tensor([4, 0],"float64"), y=Tensor([1],"float64"), weight=0.2, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:11:03.661784 test begin: paddle.lerp(x=Tensor([4, 0],"float64"), y=Tensor([4, 0],"float64"), weight=0.5, )

[paddle error] paddle.lerp(x=Tensor([4, 0],"float64"), y=Tensor([4, 0],"float64"), weight=0.5, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:11:03.883101 test begin: paddle.lerp(x=Tensor([4, 5, 0, 3],"float64"), y=Tensor([4, 5, 0, 3],"float64"), weight=0.0, )

[paddle error] paddle.lerp(x=Tensor([4, 5, 0, 3],"float64"), y=Tensor([4, 5, 0, 3],"float64"), weight=0.0, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:11:04.084833 test begin: paddle.lerp(x=Tensor([4, 5, 0, 3],"float64"), y=Tensor([4, 5, 0, 3],"float64"), weight=0.5, )

[paddle error] paddle.lerp(x=Tensor([4, 5, 0, 3],"float64"), y=Tensor([4, 5, 0, 3],"float64"), weight=0.5, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:11:04.258489 test begin: paddle.lerp(x=Tensor([4, 5, 0, 3],"float64"), y=Tensor([4, 5, 0, 3],"float64"), weight=1.0, )

[paddle error] paddle.lerp(x=Tensor([4, 5, 0, 3],"float64"), y=Tensor([4, 5, 0, 3],"float64"), weight=1.0, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:11:04.454664 test begin: paddle.lerp(x=Tensor([4, 5, 0],"float64"), y=Tensor([4, 5, 0],"float64"), weight=0.5, )

[paddle error] paddle.lerp(x=Tensor([4, 5, 0],"float64"), y=Tensor([4, 5, 0],"float64"), weight=0.5, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:11:04.645957 test begin: paddle.lerp(x=Tensor([4, 5, 4, 0],"float64"), y=Tensor([4, 5, 4, 0],"float64"), weight=0.0, )

[paddle error] paddle.lerp(x=Tensor([4, 5, 4, 0],"float64"), y=Tensor([4, 5, 4, 0],"float64"), weight=0.0, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:11:04.812914 test begin: paddle.lerp(x=Tensor([4, 5, 4, 0],"float64"), y=Tensor([4, 5, 4, 0],"float64"), weight=0.5, )

[paddle error] paddle.lerp(x=Tensor([4, 5, 4, 0],"float64"), y=Tensor([4, 5, 4, 0],"float64"), weight=0.5, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:11:04.942201 test begin: paddle.lerp(x=Tensor([4, 5, 4, 0],"float64"), y=Tensor([4, 5, 4, 0],"float64"), weight=1.0, )

[paddle error] paddle.lerp(x=Tensor([4, 5, 4, 0],"float64"), y=Tensor([4, 5, 4, 0],"float64"), weight=1.0, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:11:05.177732 test begin: paddle.lgamma(Tensor([0, 1, 1, 1],"float32"), )

[cuda error] paddle.lgamma(Tensor([0, 1, 1, 1],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:05.363434 test begin: paddle.lgamma(Tensor([0, 1, 1],"float32"), )

[cuda error] paddle.lgamma(Tensor([0, 1, 1],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:05.566329 test begin: paddle.lgamma(Tensor([0, 10, 10, 2],"float64"), )

[cuda error] paddle.lgamma(Tensor([0, 10, 10, 2],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:05.770177 test begin: paddle.lgamma(Tensor([0, 1],"float32"), )

[cuda error] paddle.lgamma(Tensor([0, 1],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:05.927573 test begin: paddle.lgamma(Tensor([0, 2, 2],"float32"), )

[cuda error] paddle.lgamma(Tensor([0, 2, 2],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:06.120653 test begin: paddle.lgamma(Tensor([0, 2],"float32"), )

[cuda error] paddle.lgamma(Tensor([0, 2],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:06.327506 test begin: paddle.lgamma(Tensor([0],"float32"), )

[cuda error] paddle.lgamma(Tensor([0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:06.483017 test begin: paddle.lgamma(Tensor([0],"float64"), )

[cuda error] paddle.lgamma(Tensor([0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:06.613526 test begin: paddle.lgamma(Tensor([1, 0, 2],"float32"), )

[cuda error] paddle.lgamma(Tensor([1, 0, 2],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:06.856089 test begin: paddle.lgamma(Tensor([1, 0],"float32"), )

[cuda error] paddle.lgamma(Tensor([1, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:07.053088 test begin: paddle.lgamma(Tensor([1, 2, 0],"float32"), )

[cuda error] paddle.lgamma(Tensor([1, 2, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:07.263794 test begin: paddle.lgamma(Tensor([10, 0, 1, 1],"float32"), )

[cuda error] paddle.lgamma(Tensor([10, 0, 1, 1],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:07.531727 test begin: paddle.lgamma(Tensor([10, 0, 10, 2],"float64"), )

[cuda error] paddle.lgamma(Tensor([10, 0, 10, 2],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:07.760437 test begin: paddle.lgamma(Tensor([10, 0, 1],"float32"), )

[cuda error] paddle.lgamma(Tensor([10, 0, 1],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:08.050285 test begin: paddle.lgamma(Tensor([10, 1, 0, 1],"float32"), )

[cuda error] paddle.lgamma(Tensor([10, 1, 0, 1],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:08.295140 test begin: paddle.lgamma(Tensor([10, 1, 0],"float32"), )

[cuda error] paddle.lgamma(Tensor([10, 1, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:08.579383 test begin: paddle.lgamma(Tensor([10, 1, 1, 0],"float32"), )

[cuda error] paddle.lgamma(Tensor([10, 1, 1, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:08.784598 test begin: paddle.lgamma(Tensor([10, 10, 0, 2],"float64"), )

[cuda error] paddle.lgamma(Tensor([10, 10, 0, 2],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:09.124333 test begin: paddle.lgamma(Tensor([10, 10, 10, 0],"float64"), )

[cuda error] paddle.lgamma(Tensor([10, 10, 10, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:09.366020 test begin: paddle.lgamma(x=Tensor([0, 3],"float32"), )

[cuda error] paddle.lgamma(x=Tensor([0, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:09.596220 test begin: paddle.lgamma(x=Tensor([0, 3],"float64"), )

[cuda error] paddle.lgamma(x=Tensor([0, 3],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:09.799324 test begin: paddle.lgamma(x=Tensor([0, 6, 6],"float64"), )

[cuda error] paddle.lgamma(x=Tensor([0, 6, 6],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:10.013573 test begin: paddle.lgamma(x=Tensor([3, 0],"float32"), )

[cuda error] paddle.lgamma(x=Tensor([3, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:10.309179 test begin: paddle.lgamma(x=Tensor([3, 0],"float64"), )

[cuda error] paddle.lgamma(x=Tensor([3, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:10.515524 test begin: paddle.lgamma(x=Tensor([6, 0, 6],"float64"), )

[cuda error] paddle.lgamma(x=Tensor([6, 0, 6],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:10.714129 test begin: paddle.lgamma(x=Tensor([6, 6, 0],"float64"), )

[cuda error] paddle.lgamma(x=Tensor([6, 6, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:10.918459 test begin: paddle.linalg.cholesky_solve(Tensor([0, 30, 2],"float64"), Tensor([0, 30, 30],"float64"), upper=True, )

[paddle error] paddle.linalg.cholesky_solve(Tensor([0, 30, 2],"float64"), Tensor([0, 30, 30],"float64"), upper=True, ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:13.197708 test begin: paddle.linalg.cholesky_solve(Tensor([1, 30, 0],"float64"), Tensor([2, 30, 30],"float64"), upper=True, )

[paddle error] paddle.linalg.cholesky_solve(Tensor([1, 30, 0],"float64"), Tensor([2, 30, 30],"float64"), upper=True, ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:13.469083 test begin: paddle.linalg.cholesky_solve(Tensor([1, 30, 2],"float64"), Tensor([0, 30, 30],"float64"), upper=True, )

[paddle error] paddle.linalg.cholesky_solve(Tensor([1, 30, 2],"float64"), Tensor([0, 30, 30],"float64"), upper=True, ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:13.689061 test begin: paddle.linalg.cholesky_solve(Tensor([20, 0],"float64"), Tensor([20, 20],"float64"), upper=True, )

[paddle error] paddle.linalg.cholesky_solve(Tensor([20, 0],"float64"), Tensor([20, 20],"float64"), upper=True, ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:14.210023 test begin: paddle.linalg.cholesky_solve(x=Tensor([0, 2, 4, 3],"float64"), y=Tensor([0, 2, 4, 4],"float64"), )

[paddle error] paddle.linalg.cholesky_solve(x=Tensor([0, 2, 4, 3],"float64"), y=Tensor([0, 2, 4, 4],"float64"), ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:14.442067 test begin: paddle.linalg.cholesky_solve(x=Tensor([0, 2, 4, 3],"float64"), y=Tensor([0, 2, 4, 4],"float64"), upper=True, )

/usr/local/lib/python3.9/dist-packages/torch/utils/_device.py:106: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:135.)
  return func(*args, **kwargs)
[paddle error] paddle.linalg.cholesky_solve(x=Tensor([0, 2, 4, 3],"float64"), y=Tensor([0, 2, 4, 4],"float64"), upper=True, ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:14.643353 test begin: paddle.linalg.cholesky_solve(x=Tensor([0, 4, 3],"float64"), y=Tensor([0, 4, 4],"float64"), )

[paddle error] paddle.linalg.cholesky_solve(x=Tensor([0, 4, 3],"float64"), y=Tensor([0, 4, 4],"float64"), ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:14.890096 test begin: paddle.linalg.cholesky_solve(x=Tensor([4, 0],"float32"), y=Tensor([4, 4],"float32"), )

[paddle error] paddle.linalg.cholesky_solve(x=Tensor([4, 0],"float32"), y=Tensor([4, 4],"float32"), ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:15.203771 test begin: paddle.linalg.cholesky_solve(x=Tensor([4, 0],"float64"), y=Tensor([4, 4],"float64"), )

[paddle error] paddle.linalg.cholesky_solve(x=Tensor([4, 0],"float64"), y=Tensor([4, 4],"float64"), ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:15.413479 test begin: paddle.linalg.cholesky_solve(x=Tensor([5, 0, 4, 3],"float64"), y=Tensor([5, 0, 4, 4],"float64"), )

[paddle error] paddle.linalg.cholesky_solve(x=Tensor([5, 0, 4, 3],"float64"), y=Tensor([5, 0, 4, 4],"float64"), ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:15.615081 test begin: paddle.linalg.cholesky_solve(x=Tensor([5, 0, 4, 3],"float64"), y=Tensor([5, 0, 4, 4],"float64"), upper=True, )

[paddle error] paddle.linalg.cholesky_solve(x=Tensor([5, 0, 4, 3],"float64"), y=Tensor([5, 0, 4, 4],"float64"), upper=True, ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:15.834711 test begin: paddle.linalg.cholesky_solve(x=Tensor([5, 2, 4, 0],"float64"), y=Tensor([5, 2, 4, 4],"float64"), )

[paddle error] paddle.linalg.cholesky_solve(x=Tensor([5, 2, 4, 0],"float64"), y=Tensor([5, 2, 4, 4],"float64"), ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:16.035990 test begin: paddle.linalg.cholesky_solve(x=Tensor([5, 2, 4, 0],"float64"), y=Tensor([5, 2, 4, 4],"float64"), upper=True, )

[paddle error] paddle.linalg.cholesky_solve(x=Tensor([5, 2, 4, 0],"float64"), y=Tensor([5, 2, 4, 4],"float64"), upper=True, ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:16.297187 test begin: paddle.linalg.cholesky_solve(x=Tensor([5, 4, 0],"float64"), y=Tensor([5, 4, 4],"float64"), )

[paddle error] paddle.linalg.cholesky_solve(x=Tensor([5, 4, 0],"float64"), y=Tensor([5, 4, 4],"float64"), ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:16.529766 test begin: paddle.linalg.cond(Tensor([0, 3],"float32"), )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([0, 3],"float32"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [0, 3], X's size = 0, 'shape' is [], the capacity of 'shape' is 1.
  [Hint: Expected capacity == in_size, but received capacity:1 != in_size:0.] (at ../paddle/phi/infermeta/unary.cc:2245)

2025-04-21 10:11:16.772679 test begin: paddle.linalg.cond(Tensor([0, 3],"float32"), p=-1, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([0, 3],"float32"), p=-1, ) 
 only support p is -1 when input is a square matrix or batches of square matrices
2025-04-21 10:11:17.041167 test begin: paddle.linalg.cond(Tensor([0, 3],"float32"), p=1, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([0, 3],"float32"), p=1, ) 
 only support p is 1 when input is a square matrix or batches of square matrices
2025-04-21 10:11:17.249500 test begin: paddle.linalg.cond(Tensor([0, 3],"float32"), p=2, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([0, 3],"float32"), p=2, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [0, 3], X's size = 0, 'shape' is [], the capacity of 'shape' is 1.
  [Hint: Expected capacity == in_size, but received capacity:1 != in_size:0.] (at ../paddle/phi/infermeta/unary.cc:2245)

2025-04-21 10:11:17.485264 test begin: paddle.linalg.cond(Tensor([0, 3],"float32"), p=math.inf, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([0, 3],"float32"), p=math.inf, ) 
 only support p is inf when input is a square matrix or batches of square matrices
2025-04-21 10:11:19.616698 test begin: paddle.linalg.cond(Tensor([0, 5],"float32"), -1, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([0, 5],"float32"), -1, ) 
 only support p is -1 when input is a square matrix or batches of square matrices
2025-04-21 10:11:19.845066 test begin: paddle.linalg.cond(Tensor([0, 5],"float32"), -2, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([0, 5],"float32"), -2, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [0, 5], X's size = 0, 'shape' is [], the capacity of 'shape' is 1.
  [Hint: Expected capacity == in_size, but received capacity:1 != in_size:0.] (at ../paddle/phi/infermeta/unary.cc:2245)

2025-04-21 10:11:20.135738 test begin: paddle.linalg.cond(Tensor([0, 5],"float32"), -math.inf, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([0, 5],"float32"), -math.inf, ) 
 only support p is -inf when input is a square matrix or batches of square matrices
2025-04-21 10:11:20.344233 test begin: paddle.linalg.cond(Tensor([0, 5],"float32"), 1, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([0, 5],"float32"), 1, ) 
 only support p is 1 when input is a square matrix or batches of square matrices
2025-04-21 10:11:20.572884 test begin: paddle.linalg.cond(Tensor([0, 5],"float32"), 2, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([0, 5],"float32"), 2, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [0, 5], X's size = 0, 'shape' is [], the capacity of 'shape' is 1.
  [Hint: Expected capacity == in_size, but received capacity:1 != in_size:0.] (at ../paddle/phi/infermeta/unary.cc:2245)

2025-04-21 10:11:20.738430 test begin: paddle.linalg.cond(Tensor([0, 5],"float32"), None, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([0, 5],"float32"), None, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [0, 5], X's size = 0, 'shape' is [], the capacity of 'shape' is 1.
  [Hint: Expected capacity == in_size, but received capacity:1 != in_size:0.] (at ../paddle/phi/infermeta/unary.cc:2245)

2025-04-21 10:11:21.004222 test begin: paddle.linalg.cond(Tensor([0, 5],"float32"), math.inf, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([0, 5],"float32"), math.inf, ) 
 only support p is inf when input is a square matrix or batches of square matrices
2025-04-21 10:11:21.199095 test begin: paddle.linalg.cond(Tensor([0, 7],"float64"), -2, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([0, 7],"float64"), -2, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [0, 7], X's size = 0, 'shape' is [], the capacity of 'shape' is 1.
  [Hint: Expected capacity == in_size, but received capacity:1 != in_size:0.] (at ../paddle/phi/infermeta/unary.cc:2245)

2025-04-21 10:11:21.442222 test begin: paddle.linalg.cond(Tensor([0, 7],"float64"), 2, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([0, 7],"float64"), 2, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [0, 7], X's size = 0, 'shape' is [], the capacity of 'shape' is 1.
  [Hint: Expected capacity == in_size, but received capacity:1 != in_size:0.] (at ../paddle/phi/infermeta/unary.cc:2245)

2025-04-21 10:11:21.678294 test begin: paddle.linalg.cond(Tensor([0, 7],"float64"), None, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([0, 7],"float64"), None, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [0, 7], X's size = 0, 'shape' is [], the capacity of 'shape' is 1.
  [Hint: Expected capacity == in_size, but received capacity:1 != in_size:0.] (at ../paddle/phi/infermeta/unary.cc:2245)

2025-04-21 10:11:23.942392 test begin: paddle.linalg.cond(Tensor([2, 4, 0, 3],"float32"), -1, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([2, 4, 0, 3],"float32"), -1, ) 
 only support p is -1 when input is a square matrix or batches of square matrices
2025-04-21 10:11:24.228700 test begin: paddle.linalg.cond(Tensor([2, 4, 0, 3],"float32"), -2, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([2, 4, 0, 3],"float32"), -2, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [2, 4, 0, 3], X's size = 0, 'shape' is [2, 4], the capacity of 'shape' is 8.
  [Hint: Expected capacity == in_size, but received capacity:8 != in_size:0.] (at ../paddle/phi/infermeta/unary.cc:2245)

2025-04-21 10:11:24.462465 test begin: paddle.linalg.cond(Tensor([2, 4, 0, 3],"float32"), -math.inf, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([2, 4, 0, 3],"float32"), -math.inf, ) 
 only support p is -inf when input is a square matrix or batches of square matrices
2025-04-21 10:11:24.725238 test begin: paddle.linalg.cond(Tensor([2, 4, 0, 3],"float32"), 1, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([2, 4, 0, 3],"float32"), 1, ) 
 only support p is 1 when input is a square matrix or batches of square matrices
2025-04-21 10:11:24.979381 test begin: paddle.linalg.cond(Tensor([2, 4, 0, 3],"float32"), 2, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([2, 4, 0, 3],"float32"), 2, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [2, 4, 0, 3], X's size = 0, 'shape' is [2, 4], the capacity of 'shape' is 8.
  [Hint: Expected capacity == in_size, but received capacity:8 != in_size:0.] (at ../paddle/phi/infermeta/unary.cc:2245)

2025-04-21 10:11:25.276084 test begin: paddle.linalg.cond(Tensor([2, 4, 0, 3],"float32"), None, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([2, 4, 0, 3],"float32"), None, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [2, 4, 0, 3], X's size = 0, 'shape' is [2, 4], the capacity of 'shape' is 8.
  [Hint: Expected capacity == in_size, but received capacity:8 != in_size:0.] (at ../paddle/phi/infermeta/unary.cc:2245)

2025-04-21 10:11:25.482695 test begin: paddle.linalg.cond(Tensor([2, 4, 0, 3],"float32"), math.inf, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([2, 4, 0, 3],"float32"), math.inf, ) 
 only support p is inf when input is a square matrix or batches of square matrices
2025-04-21 10:11:25.638418 test begin: paddle.linalg.cond(Tensor([2, 4, 3, 0],"float32"), -1, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([2, 4, 3, 0],"float32"), -1, ) 
 only support p is -1 when input is a square matrix or batches of square matrices
2025-04-21 10:11:25.854893 test begin: paddle.linalg.cond(Tensor([2, 4, 3, 0],"float32"), -2, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([2, 4, 3, 0],"float32"), -2, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [2, 4, 3, 0], X's size = 0, 'shape' is [2, 4], the capacity of 'shape' is 8.
  [Hint: Expected capacity == in_size, but received capacity:8 != in_size:0.] (at ../paddle/phi/infermeta/unary.cc:2245)

2025-04-21 10:11:25.989885 test begin: paddle.linalg.cond(Tensor([2, 4, 3, 0],"float32"), -math.inf, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([2, 4, 3, 0],"float32"), -math.inf, ) 
 only support p is -inf when input is a square matrix or batches of square matrices
2025-04-21 10:11:26.176115 test begin: paddle.linalg.cond(Tensor([2, 4, 3, 0],"float32"), 1, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([2, 4, 3, 0],"float32"), 1, ) 
 only support p is 1 when input is a square matrix or batches of square matrices
2025-04-21 10:11:26.380407 test begin: paddle.linalg.cond(Tensor([2, 4, 3, 0],"float32"), 2, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([2, 4, 3, 0],"float32"), 2, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [2, 4, 3, 0], X's size = 0, 'shape' is [2, 4], the capacity of 'shape' is 8.
  [Hint: Expected capacity == in_size, but received capacity:8 != in_size:0.] (at ../paddle/phi/infermeta/unary.cc:2245)

2025-04-21 10:11:26.598403 test begin: paddle.linalg.cond(Tensor([2, 4, 3, 0],"float32"), None, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([2, 4, 3, 0],"float32"), None, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [2, 4, 3, 0], X's size = 0, 'shape' is [2, 4], the capacity of 'shape' is 8.
  [Hint: Expected capacity == in_size, but received capacity:8 != in_size:0.] (at ../paddle/phi/infermeta/unary.cc:2245)

2025-04-21 10:11:26.837628 test begin: paddle.linalg.cond(Tensor([2, 4, 3, 0],"float32"), math.inf, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([2, 4, 3, 0],"float32"), math.inf, ) 
 only support p is inf when input is a square matrix or batches of square matrices
2025-04-21 10:11:27.056334 test begin: paddle.linalg.cond(Tensor([3, 0],"float32"), )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([3, 0],"float32"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [3, 0], X's size = 0, 'shape' is [], the capacity of 'shape' is 1.
  [Hint: Expected capacity == in_size, but received capacity:1 != in_size:0.] (at ../paddle/phi/infermeta/unary.cc:2245)

2025-04-21 10:11:27.309625 test begin: paddle.linalg.cond(Tensor([3, 0],"float32"), p=-1, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([3, 0],"float32"), p=-1, ) 
 only support p is -1 when input is a square matrix or batches of square matrices
2025-04-21 10:11:27.504127 test begin: paddle.linalg.cond(Tensor([3, 0],"float32"), p=1, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([3, 0],"float32"), p=1, ) 
 only support p is 1 when input is a square matrix or batches of square matrices
2025-04-21 10:11:27.718069 test begin: paddle.linalg.cond(Tensor([3, 0],"float32"), p=2, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([3, 0],"float32"), p=2, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [3, 0], X's size = 0, 'shape' is [], the capacity of 'shape' is 1.
  [Hint: Expected capacity == in_size, but received capacity:1 != in_size:0.] (at ../paddle/phi/infermeta/unary.cc:2245)

2025-04-21 10:11:27.933597 test begin: paddle.linalg.cond(Tensor([3, 0],"float32"), p=math.inf, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([3, 0],"float32"), p=math.inf, ) 
 only support p is inf when input is a square matrix or batches of square matrices
2025-04-21 10:11:28.123413 test begin: paddle.linalg.cond(Tensor([5, 0],"float32"), -1, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([5, 0],"float32"), -1, ) 
 only support p is -1 when input is a square matrix or batches of square matrices
2025-04-21 10:11:28.328227 test begin: paddle.linalg.cond(Tensor([5, 0],"float32"), -2, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([5, 0],"float32"), -2, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [5, 0], X's size = 0, 'shape' is [], the capacity of 'shape' is 1.
  [Hint: Expected capacity == in_size, but received capacity:1 != in_size:0.] (at ../paddle/phi/infermeta/unary.cc:2245)

2025-04-21 10:11:28.547061 test begin: paddle.linalg.cond(Tensor([5, 0],"float32"), -math.inf, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([5, 0],"float32"), -math.inf, ) 
 only support p is -inf when input is a square matrix or batches of square matrices
2025-04-21 10:11:28.729760 test begin: paddle.linalg.cond(Tensor([5, 0],"float32"), 1, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([5, 0],"float32"), 1, ) 
 only support p is 1 when input is a square matrix or batches of square matrices
2025-04-21 10:11:28.931181 test begin: paddle.linalg.cond(Tensor([5, 0],"float32"), 2, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([5, 0],"float32"), 2, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [5, 0], X's size = 0, 'shape' is [], the capacity of 'shape' is 1.
  [Hint: Expected capacity == in_size, but received capacity:1 != in_size:0.] (at ../paddle/phi/infermeta/unary.cc:2245)

2025-04-21 10:11:29.255958 test begin: paddle.linalg.cond(Tensor([5, 0],"float32"), None, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([5, 0],"float32"), None, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [5, 0], X's size = 0, 'shape' is [], the capacity of 'shape' is 1.
  [Hint: Expected capacity == in_size, but received capacity:1 != in_size:0.] (at ../paddle/phi/infermeta/unary.cc:2245)

2025-04-21 10:11:29.460471 test begin: paddle.linalg.cond(Tensor([5, 0],"float32"), math.inf, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([5, 0],"float32"), math.inf, ) 
 only support p is inf when input is a square matrix or batches of square matrices
2025-04-21 10:11:29.678571 test begin: paddle.linalg.cond(Tensor([9, 0],"float64"), -2, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([9, 0],"float64"), -2, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [9, 0], X's size = 0, 'shape' is [], the capacity of 'shape' is 1.
  [Hint: Expected capacity == in_size, but received capacity:1 != in_size:0.] (at ../paddle/phi/infermeta/unary.cc:2245)

2025-04-21 10:11:29.921927 test begin: paddle.linalg.cond(Tensor([9, 0],"float64"), 2, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([9, 0],"float64"), 2, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [9, 0], X's size = 0, 'shape' is [], the capacity of 'shape' is 1.
  [Hint: Expected capacity == in_size, but received capacity:1 != in_size:0.] (at ../paddle/phi/infermeta/unary.cc:2245)

2025-04-21 10:11:30.126275 test begin: paddle.linalg.cond(Tensor([9, 0],"float64"), None, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(Tensor([9, 0],"float64"), None, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [9, 0], X's size = 0, 'shape' is [], the capacity of 'shape' is 1.
  [Hint: Expected capacity == in_size, but received capacity:1 != in_size:0.] (at ../paddle/phi/infermeta/unary.cc:2245)

2025-04-21 10:11:31.177486 test begin: paddle.linalg.cond(x=Tensor([0, 3],"float32"), )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(x=Tensor([0, 3],"float32"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [0, 3], X's size = 0, 'shape' is [], the capacity of 'shape' is 1.
  [Hint: Expected capacity == in_size, but received capacity:1 != in_size:0.] (at ../paddle/phi/infermeta/unary.cc:2245)

2025-04-21 10:11:31.377815 test begin: paddle.linalg.cond(x=Tensor([0, 3],"float64"), )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(x=Tensor([0, 3],"float64"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [0, 3], X's size = 0, 'shape' is [], the capacity of 'shape' is 1.
  [Hint: Expected capacity == in_size, but received capacity:1 != in_size:0.] (at ../paddle/phi/infermeta/unary.cc:2245)

2025-04-21 10:11:31.612694 test begin: paddle.linalg.cond(x=Tensor([0, 4],"float64"), p=-2, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(x=Tensor([0, 4],"float64"), p=-2, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [0, 4], X's size = 0, 'shape' is [], the capacity of 'shape' is 1.
  [Hint: Expected capacity == in_size, but received capacity:1 != in_size:0.] (at ../paddle/phi/infermeta/unary.cc:2245)

2025-04-21 10:11:31.815349 test begin: paddle.linalg.cond(x=Tensor([0, 4],"float64"), p=1, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(x=Tensor([0, 4],"float64"), p=1, ) 
 only support p is 1 when input is a square matrix or batches of square matrices
2025-04-21 10:11:32.001981 test begin: paddle.linalg.cond(x=Tensor([3, 0],"float32"), )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(x=Tensor([3, 0],"float32"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [3, 0], X's size = 0, 'shape' is [], the capacity of 'shape' is 1.
  [Hint: Expected capacity == in_size, but received capacity:1 != in_size:0.] (at ../paddle/phi/infermeta/unary.cc:2245)

2025-04-21 10:11:32.186565 test begin: paddle.linalg.cond(x=Tensor([3, 0],"float64"), )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(x=Tensor([3, 0],"float64"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [3, 0], X's size = 0, 'shape' is [], the capacity of 'shape' is 1.
  [Hint: Expected capacity == in_size, but received capacity:1 != in_size:0.] (at ../paddle/phi/infermeta/unary.cc:2245)

2025-04-21 10:11:33.261764 test begin: paddle.linalg.cond(x=Tensor([4, 0],"float64"), p=-2, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(x=Tensor([4, 0],"float64"), p=-2, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [4, 0], X's size = 0, 'shape' is [], the capacity of 'shape' is 1.
  [Hint: Expected capacity == in_size, but received capacity:1 != in_size:0.] (at ../paddle/phi/infermeta/unary.cc:2245)

2025-04-21 10:11:33.551217 test begin: paddle.linalg.cond(x=Tensor([4, 0],"float64"), p=1, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(x=Tensor([4, 0],"float64"), p=1, ) 
 only support p is 1 when input is a square matrix or batches of square matrices
2025-04-21 10:11:33.752607 test begin: paddle.linalg.cond(x=Tensor([4, 2, 0, 4],"float64"), p=-1, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(x=Tensor([4, 2, 0, 4],"float64"), p=-1, ) 
 only support p is -1 when input is a square matrix or batches of square matrices
2025-04-21 10:11:33.946771 test begin: paddle.linalg.cond(x=Tensor([4, 2, 0, 4],"float64"), p=-math.inf, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(x=Tensor([4, 2, 0, 4],"float64"), p=-math.inf, ) 
 only support p is -inf when input is a square matrix or batches of square matrices
2025-04-21 10:11:34.139572 test begin: paddle.linalg.cond(x=Tensor([4, 2, 0, 4],"float64"), p=math.inf, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(x=Tensor([4, 2, 0, 4],"float64"), p=math.inf, ) 
 only support p is inf when input is a square matrix or batches of square matrices
2025-04-21 10:11:34.404495 test begin: paddle.linalg.cond(x=Tensor([4, 2, 4, 0],"float64"), p=-1, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(x=Tensor([4, 2, 4, 0],"float64"), p=-1, ) 
 only support p is -1 when input is a square matrix or batches of square matrices
2025-04-21 10:11:34.626284 test begin: paddle.linalg.cond(x=Tensor([4, 2, 4, 0],"float64"), p=-math.inf, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(x=Tensor([4, 2, 4, 0],"float64"), p=-math.inf, ) 
 only support p is -inf when input is a square matrix or batches of square matrices
2025-04-21 10:11:34.831522 test begin: paddle.linalg.cond(x=Tensor([4, 2, 4, 0],"float64"), p=math.inf, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(x=Tensor([4, 2, 4, 0],"float64"), p=math.inf, ) 
 only support p is inf when input is a square matrix or batches of square matrices
2025-04-21 10:11:35.264416 test begin: paddle.linalg.cond(x=Tensor([6, 0],"float64"), p=-2, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(x=Tensor([6, 0],"float64"), p=-2, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [6, 0], X's size = 0, 'shape' is [], the capacity of 'shape' is 1.
  [Hint: Expected capacity == in_size, but received capacity:1 != in_size:0.] (at ../paddle/phi/infermeta/unary.cc:2245)

2025-04-21 10:11:35.701361 test begin: paddle.linalg.cond(x=Tensor([6, 2, 4, 0, 4],"float64"), )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(x=Tensor([6, 2, 4, 0, 4],"float64"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [6, 2, 4, 0, 4], X's size = 0, 'shape' is [6, 2, 4], the capacity of 'shape' is 48.
  [Hint: Expected capacity == in_size, but received capacity:48 != in_size:0.] (at ../paddle/phi/infermeta/unary.cc:2245)

2025-04-21 10:11:35.973522 test begin: paddle.linalg.cond(x=Tensor([6, 2, 4, 3, 0],"float64"), )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.linalg.cond(x=Tensor([6, 2, 4, 3, 0],"float64"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [6, 2, 4, 3, 0], X's size = 0, 'shape' is [6, 2, 4], the capacity of 'shape' is 48.
  [Hint: Expected capacity == in_size, but received capacity:48 != in_size:0.] (at ../paddle/phi/infermeta/unary.cc:2245)

2025-04-21 10:11:36.219999 test begin: paddle.linalg.cov(Tensor([0, 4],"float32"), )

W0421 10:11:36.423849 91597 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.cov(Tensor([0, 4],"float32"), ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:11:36.424267 test begin: paddle.linalg.cov(Tensor([0],"float32"), )

/usr/local/lib/python3.9/dist-packages/torch/utils/_device.py:106: UserWarning: cov(): degrees of freedom is <= 0. Correction should be strictly less than the number of observations. (Triggered internally at ../aten/src/ATen/native/Correlation.cpp:116.)
  return func(*args, **kwargs)
[accuracy error] paddle.linalg.cov(Tensor([0],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array(-inf, dtype=float32)
 y: array(nan, dtype=float32)
2025-04-21 10:11:36.622916 test begin: paddle.linalg.cov(Tensor([3, 0],"float32"), )

[accuracy error] paddle.linalg.cov(Tensor([3, 0],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[-inf, -inf,  inf],
       [-inf,  inf, -inf],
       [-inf,  inf, -inf]], dtype=float32)
 y: array([[nan, nan, nan],
       [nan, nan, nan],
       [nan, nan, nan]], dtype=float32)
2025-04-21 10:11:36.848998 test begin: paddle.linalg.cov(x=Tensor([0, 12],"float64"), )

W0421 10:11:37.139050 91914 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.cov(x=Tensor([0, 12],"float64"), ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:11:37.139475 test begin: paddle.linalg.cov(x=Tensor([0, 2],"float32"), )

W0421 10:11:37.361721 92020 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.cov(x=Tensor([0, 2],"float32"), ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:11:37.362102 test begin: paddle.linalg.cov(x=Tensor([4, 0],"float32"), )

[accuracy error] paddle.linalg.cov(x=Tensor([4, 0],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[-inf, -inf,  inf, -inf],
       [ inf, -inf, -inf,  inf],
       [-inf,  inf, -inf, -inf],
       [ inf, -inf, -inf, -inf]], dtype=float32)
 y: array([[nan, nan, nan, nan],
       [nan, nan, nan, nan],
       [nan, nan, nan, nan],
       [nan, nan, nan, nan]], dtype=float32)
2025-04-21 10:11:37.573685 test begin: paddle.linalg.cov(x=Tensor([4, 0],"float64"), )

[accuracy error] paddle.linalg.cov(x=Tensor([4, 0],"float64"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[-inf, -inf, -inf,  inf],
       [ inf, -inf, -inf, -inf],
       [-inf,  inf,  inf, -inf],
       [-inf, -inf, -inf, -inf]])
 y: array([[nan, nan, nan, nan],
       [nan, nan, nan, nan],
       [nan, nan, nan, nan],
       [nan, nan, nan, nan]])
2025-04-21 10:11:37.775434 test begin: paddle.linalg.det(Tensor([0, 1, 4, 3, 6, 6],"complex64"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[cuda error] paddle.linalg.det(Tensor([0, 1, 4, 3, 6, 6],"complex64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:38.273755 test begin: paddle.linalg.det(Tensor([0, 3, 5, 5],"complex128"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[cuda error] paddle.linalg.det(Tensor([0, 3, 5, 5],"complex128"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:38.767847 test begin: paddle.linalg.det(Tensor([2, 0, 4, 3, 6, 6],"complex64"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[cuda error] paddle.linalg.det(Tensor([2, 0, 4, 3, 6, 6],"complex64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:38.914501 test begin: paddle.linalg.det(Tensor([2, 1, 0, 3, 6, 6],"complex64"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[cuda error] paddle.linalg.det(Tensor([2, 1, 0, 3, 6, 6],"complex64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:39.118142 test begin: paddle.linalg.det(Tensor([2, 1, 4, 0, 6, 6],"complex64"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[cuda error] paddle.linalg.det(Tensor([2, 1, 4, 0, 6, 6],"complex64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:39.333126 test begin: paddle.linalg.det(Tensor([3, 0, 5, 5],"complex128"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[cuda error] paddle.linalg.det(Tensor([3, 0, 5, 5],"complex128"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:40.582086 test begin: paddle.linalg.lstsq(Tensor([0, 10],"float64"), Tensor([0, 8],"float64"), rcond=1e-15, driver="gels", )

[paddle error] paddle.linalg.lstsq(Tensor([0, 10],"float64"), Tensor([0, 8],"float64"), rcond=1e-15, driver="gels", ) 
 (InvalidArgument) C++ Slice Operation Not Support End < Start
  [Hint: Expected ed > st, but received ed:0 <= st:0.] (at ../paddle/phi/kernels/funcs/slice.h:93)

2025-04-21 10:11:40.825051 test begin: paddle.linalg.lstsq(Tensor([0, 2, 8],"float32"), Tensor([0, 2, 15],"float32"), rcond=None, driver="gels", )

[paddle error] paddle.linalg.lstsq(Tensor([0, 2, 8],"float32"), Tensor([0, 2, 15],"float32"), rcond=None, driver="gels", ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:41.030777 test begin: paddle.linalg.lstsq(Tensor([0, 5],"float32"), Tensor([0, 8],"float32"), rcond=None, driver="gels", )

[paddle error] paddle.linalg.lstsq(Tensor([0, 5],"float32"), Tensor([0, 8],"float32"), rcond=None, driver="gels", ) 
 (InvalidArgument) C++ Slice Operation Not Support End < Start
  [Hint: Expected ed > st, but received ed:0 <= st:0.] (at ../paddle/phi/kernels/funcs/slice.h:93)

2025-04-21 10:11:41.223976 test begin: paddle.linalg.lstsq(Tensor([0, 7, 3],"float64"), Tensor([0, 7, 6],"float64"), rcond=1e-15, driver="gels", )

[paddle error] paddle.linalg.lstsq(Tensor([0, 7, 3],"float64"), Tensor([0, 7, 6],"float64"), rcond=1e-15, driver="gels", ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:41.396559 test begin: paddle.linalg.lstsq(Tensor([0, 8, 6],"float64"), Tensor([0, 8, 10],"float64"), rcond=1e-15, driver="gels", )

[paddle error] paddle.linalg.lstsq(Tensor([0, 8, 6],"float64"), Tensor([0, 8, 10],"float64"), rcond=1e-15, driver="gels", ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:41.645877 test begin: paddle.linalg.lstsq(Tensor([0, 9],"float32"), Tensor([0, 5],"float32"), rcond=1e-15, driver="gels", )

[paddle error] paddle.linalg.lstsq(Tensor([0, 9],"float32"), Tensor([0, 5],"float32"), rcond=1e-15, driver="gels", ) 
 (InvalidArgument) C++ Slice Operation Not Support End < Start
  [Hint: Expected ed > st, but received ed:0 <= st:0.] (at ../paddle/phi/kernels/funcs/slice.h:93)

2025-04-21 10:11:41.860048 test begin: paddle.linalg.lstsq(Tensor([10, 0, 3],"float64"), Tensor([10, 0, 6],"float64"), rcond=1e-15, driver="gels", )

[paddle error] paddle.linalg.lstsq(Tensor([10, 0, 3],"float64"), Tensor([10, 0, 6],"float64"), rcond=1e-15, driver="gels", ) 
 (InvalidArgument) C++ Slice Operation Not Support End < Start
  [Hint: Expected ed > st, but received ed:0 <= st:0.] (at ../paddle/phi/kernels/funcs/slice.h:93)

2025-04-21 10:11:42.059311 test begin: paddle.linalg.lstsq(Tensor([10, 0, 6],"float64"), Tensor([10, 0, 10],"float64"), rcond=1e-15, driver="gels", )

[paddle error] paddle.linalg.lstsq(Tensor([10, 0, 6],"float64"), Tensor([10, 0, 10],"float64"), rcond=1e-15, driver="gels", ) 
 (InvalidArgument) C++ Slice Operation Not Support End < Start
  [Hint: Expected ed > st, but received ed:0 <= st:0.] (at ../paddle/phi/kernels/funcs/slice.h:93)

2025-04-21 10:11:42.271630 test begin: paddle.linalg.lstsq(Tensor([10, 0],"float32"), Tensor([10, 0],"float32"), rcond=None, driver="gels", )

[paddle error] paddle.linalg.lstsq(Tensor([10, 0],"float32"), Tensor([10, 0],"float32"), rcond=None, driver="gels", ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:42.489814 test begin: paddle.linalg.lstsq(Tensor([10, 0],"float32"), Tensor([10, 8],"float32"), rcond=None, driver="gels", )

[paddle error] paddle.linalg.lstsq(Tensor([10, 0],"float32"), Tensor([10, 8],"float32"), rcond=None, driver="gels", ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:42.840143 test begin: paddle.linalg.lstsq(Tensor([10, 5],"float32"), Tensor([10, 0],"float32"), rcond=None, driver="gels", )

[paddle error] paddle.linalg.lstsq(Tensor([10, 5],"float32"), Tensor([10, 0],"float32"), rcond=None, driver="gels", ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:43.011704 test begin: paddle.linalg.lstsq(Tensor([10, 7, 0],"float64"), Tensor([10, 7, 0],"float64"), rcond=1e-15, driver="gels", )

[paddle error] paddle.linalg.lstsq(Tensor([10, 7, 0],"float64"), Tensor([10, 7, 0],"float64"), rcond=1e-15, driver="gels", ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:43.196713 test begin: paddle.linalg.lstsq(Tensor([10, 7, 0],"float64"), Tensor([10, 7, 6],"float64"), rcond=1e-15, driver="gels", )

[paddle error] paddle.linalg.lstsq(Tensor([10, 7, 0],"float64"), Tensor([10, 7, 6],"float64"), rcond=1e-15, driver="gels", ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:43.433384 test begin: paddle.linalg.lstsq(Tensor([10, 7, 3],"float64"), Tensor([10, 7, 0],"float64"), rcond=1e-15, driver="gels", )

[paddle error] paddle.linalg.lstsq(Tensor([10, 7, 3],"float64"), Tensor([10, 7, 0],"float64"), rcond=1e-15, driver="gels", ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:43.617133 test begin: paddle.linalg.lstsq(Tensor([10, 8, 0],"float64"), Tensor([10, 8, 0],"float64"), rcond=1e-15, driver="gels", )

[paddle error] paddle.linalg.lstsq(Tensor([10, 8, 0],"float64"), Tensor([10, 8, 0],"float64"), rcond=1e-15, driver="gels", ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:43.866102 test begin: paddle.linalg.lstsq(Tensor([10, 8, 0],"float64"), Tensor([10, 8, 10],"float64"), rcond=1e-15, driver="gels", )

[paddle error] paddle.linalg.lstsq(Tensor([10, 8, 0],"float64"), Tensor([10, 8, 10],"float64"), rcond=1e-15, driver="gels", ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:44.184746 test begin: paddle.linalg.lstsq(Tensor([10, 8, 6],"float64"), Tensor([10, 8, 0],"float64"), rcond=1e-15, driver="gels", )

[paddle error] paddle.linalg.lstsq(Tensor([10, 8, 6],"float64"), Tensor([10, 8, 0],"float64"), rcond=1e-15, driver="gels", ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:44.436775 test begin: paddle.linalg.lstsq(Tensor([3, 0, 8],"float32"), Tensor([3, 0, 15],"float32"), rcond=None, driver="gels", )

[paddle error] paddle.linalg.lstsq(Tensor([3, 0, 8],"float32"), Tensor([3, 0, 15],"float32"), rcond=None, driver="gels", ) 
 (InvalidArgument) C++ Slice Operation Not Support End < Start
  [Hint: Expected ed > st, but received ed:0 <= st:0.] (at ../paddle/phi/kernels/funcs/slice.h:93)

2025-04-21 10:11:44.647642 test begin: paddle.linalg.lstsq(Tensor([3, 2, 0],"float32"), Tensor([3, 2, 0],"float32"), rcond=None, driver="gels", )

[paddle error] paddle.linalg.lstsq(Tensor([3, 2, 0],"float32"), Tensor([3, 2, 0],"float32"), rcond=None, driver="gels", ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:44.854106 test begin: paddle.linalg.lstsq(Tensor([3, 2, 0],"float32"), Tensor([3, 2, 15],"float32"), rcond=None, driver="gels", )

[paddle error] paddle.linalg.lstsq(Tensor([3, 2, 0],"float32"), Tensor([3, 2, 15],"float32"), rcond=None, driver="gels", ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:45.091533 test begin: paddle.linalg.lstsq(Tensor([3, 2, 8],"float32"), Tensor([3, 2, 0],"float32"), rcond=None, driver="gels", )

[accuracy error] paddle.linalg.lstsq(Tensor([3, 2, 8],"float32"), Tensor([3, 2, 0],"float32"), rcond=None, driver="gels", ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (3, 8, 2, 0), (3, 8, 0) mismatch)
 x: array([], shape=(3, 8, 2, 0), dtype=float32)
 y: array([], shape=(3, 8, 0), dtype=float32)
2025-04-21 10:11:45.333040 test begin: paddle.linalg.lstsq(Tensor([5, 0],"float64"), Tensor([5, 0],"float64"), rcond=1e-15, driver="gels", )

[paddle error] paddle.linalg.lstsq(Tensor([5, 0],"float64"), Tensor([5, 0],"float64"), rcond=1e-15, driver="gels", ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:45.630500 test begin: paddle.linalg.lstsq(Tensor([5, 0],"float64"), Tensor([5, 8],"float64"), rcond=1e-15, driver="gels", )

[paddle error] paddle.linalg.lstsq(Tensor([5, 0],"float64"), Tensor([5, 8],"float64"), rcond=1e-15, driver="gels", ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:46.078687 test begin: paddle.linalg.lstsq(Tensor([9, 0],"float32"), Tensor([9, 0],"float32"), rcond=1e-15, driver="gels", )

[paddle error] paddle.linalg.lstsq(Tensor([9, 0],"float32"), Tensor([9, 0],"float32"), rcond=1e-15, driver="gels", ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:46.272545 test begin: paddle.linalg.lstsq(Tensor([9, 0],"float32"), Tensor([9, 5],"float32"), rcond=1e-15, driver="gels", )

[paddle error] paddle.linalg.lstsq(Tensor([9, 0],"float32"), Tensor([9, 5],"float32"), rcond=1e-15, driver="gels", ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:46.506203 test begin: paddle.linalg.lstsq(Tensor([9, 9],"float32"), Tensor([9, 0],"float32"), rcond=1e-15, driver="gels", )

[paddle error] paddle.linalg.lstsq(Tensor([9, 9],"float32"), Tensor([9, 0],"float32"), rcond=1e-15, driver="gels", ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:46.751819 test begin: paddle.linalg.matrix_norm(x=Tensor([0, 3, 4, 5],"float64"), p=-2, axis=list[1,2,], keepdim=False, )

[paddle error] paddle.linalg.matrix_norm(x=Tensor([0, 3, 4, 5],"float64"), p=-2, axis=list[1,2,], keepdim=False, ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:46.942916 test begin: paddle.linalg.matrix_norm(x=Tensor([0, 3, 4, 5],"float64"), p=-2, axis=list[1,2,], keepdim=True, )

[paddle error] paddle.linalg.matrix_norm(x=Tensor([0, 3, 4, 5],"float64"), p=-2, axis=list[1,2,], keepdim=True, ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:47.094954 test begin: paddle.linalg.matrix_norm(x=Tensor([0, 3, 4],"float64"), p="fro", axis=list[0,1,], keepdim=False, )

[paddle error] paddle.linalg.matrix_norm(x=Tensor([0, 3, 4],"float64"), p="fro", axis=list[0,1,], keepdim=False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:11:47.321419 test begin: paddle.linalg.matrix_norm(x=Tensor([0, 3, 4],"float64"), p="fro", axis=list[0,1,], keepdim=True, )

[paddle error] paddle.linalg.matrix_norm(x=Tensor([0, 3, 4],"float64"), p="fro", axis=list[0,1,], keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:11:47.526061 test begin: paddle.linalg.matrix_norm(x=Tensor([0, 3, 4],"float64"), p="nuc", axis=list[0,1,], keepdim=False, )

[paddle error] paddle.linalg.matrix_norm(x=Tensor([0, 3, 4],"float64"), p="nuc", axis=list[0,1,], keepdim=False, ) 
 (InvalidArgument) The row of Input(X) should be greater than 0.
  [Hint: Expected 0 < m, but received 0:0 >= m:0.] (at ../paddle/phi/kernels/gpu/svd_kernel.cu:220)

2025-04-21 10:11:47.792429 test begin: paddle.linalg.matrix_norm(x=Tensor([0, 3, 4],"float64"), p="nuc", axis=list[0,1,], keepdim=True, )

[paddle error] paddle.linalg.matrix_norm(x=Tensor([0, 3, 4],"float64"), p="nuc", axis=list[0,1,], keepdim=True, ) 
 (InvalidArgument) The row of Input(X) should be greater than 0.
  [Hint: Expected 0 < m, but received 0:0 >= m:0.] (at ../paddle/phi/kernels/gpu/svd_kernel.cu:220)

2025-04-21 10:11:47.940073 test begin: paddle.linalg.matrix_norm(x=Tensor([2, 0, 4],"float64"), p="fro", axis=list[0,1,], keepdim=False, )

[paddle error] paddle.linalg.matrix_norm(x=Tensor([2, 0, 4],"float64"), p="fro", axis=list[0,1,], keepdim=False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:11:48.069691 test begin: paddle.linalg.matrix_norm(x=Tensor([2, 0, 4],"float64"), p="fro", axis=list[0,1,], keepdim=True, )

[paddle error] paddle.linalg.matrix_norm(x=Tensor([2, 0, 4],"float64"), p="fro", axis=list[0,1,], keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:11:48.238124 test begin: paddle.linalg.matrix_norm(x=Tensor([2, 0, 4],"float64"), p="nuc", axis=list[0,1,], keepdim=False, )

[paddle error] paddle.linalg.matrix_norm(x=Tensor([2, 0, 4],"float64"), p="nuc", axis=list[0,1,], keepdim=False, ) 
 (InvalidArgument) The col of Input(X) should be greater than 0.
  [Hint: Expected 0 < n, but received 0:0 >= n:0.] (at ../paddle/phi/kernels/gpu/svd_kernel.cu:224)

2025-04-21 10:11:48.384194 test begin: paddle.linalg.matrix_norm(x=Tensor([2, 0, 4],"float64"), p="nuc", axis=list[0,1,], keepdim=True, )

[paddle error] paddle.linalg.matrix_norm(x=Tensor([2, 0, 4],"float64"), p="nuc", axis=list[0,1,], keepdim=True, ) 
 (InvalidArgument) The col of Input(X) should be greater than 0.
  [Hint: Expected 0 < n, but received 0:0 >= n:0.] (at ../paddle/phi/kernels/gpu/svd_kernel.cu:224)

2025-04-21 10:11:48.888978 test begin: paddle.linalg.matrix_norm(x=Tensor([2, 3, 0],"float64"), p="fro", axis=list[0,1,], keepdim=False, )

[paddle error] paddle.linalg.matrix_norm(x=Tensor([2, 3, 0],"float64"), p="fro", axis=list[0,1,], keepdim=False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:11:49.080583 test begin: paddle.linalg.matrix_norm(x=Tensor([2, 3, 0],"float64"), p="fro", axis=list[0,1,], keepdim=True, )

[paddle error] paddle.linalg.matrix_norm(x=Tensor([2, 3, 0],"float64"), p="fro", axis=list[0,1,], keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:11:49.258857 test begin: paddle.linalg.matrix_norm(x=Tensor([2, 3, 0],"float64"), p="nuc", axis=list[0,1,], keepdim=False, )

[paddle error] paddle.linalg.matrix_norm(x=Tensor([2, 3, 0],"float64"), p="nuc", axis=list[0,1,], keepdim=False, ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:49.504734 test begin: paddle.linalg.matrix_norm(x=Tensor([2, 3, 0],"float64"), p="nuc", axis=list[0,1,], keepdim=True, )

[paddle error] paddle.linalg.matrix_norm(x=Tensor([2, 3, 0],"float64"), p="nuc", axis=list[0,1,], keepdim=True, ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:49.747098 test begin: paddle.linalg.matrix_norm(x=Tensor([2, 3, 0],"float64"), p=-math.inf, axis=list[0,1,], keepdim=False, )

[paddle error] paddle.linalg.matrix_norm(x=Tensor([2, 3, 0],"float64"), p=-math.inf, axis=list[0,1,], keepdim=False, ) 
 (InvalidArgument) Zero-size tensor to reduction operation minimum which has no identity.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/reduce_min_kernel.cc:30)

2025-04-21 10:11:49.985383 test begin: paddle.linalg.matrix_norm(x=Tensor([2, 3, 0],"float64"), p=-math.inf, axis=list[0,1,], keepdim=True, )

[paddle error] paddle.linalg.matrix_norm(x=Tensor([2, 3, 0],"float64"), p=-math.inf, axis=list[0,1,], keepdim=True, ) 
 (InvalidArgument) Zero-size tensor to reduction operation minimum which has no identity.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/reduce_min_kernel.cc:30)

2025-04-21 10:11:50.187335 test begin: paddle.linalg.matrix_norm(x=Tensor([2, 3, 4, 0],"float64"), p=-2, axis=list[1,2,], keepdim=False, )

[paddle error] paddle.linalg.matrix_norm(x=Tensor([2, 3, 4, 0],"float64"), p=-2, axis=list[1,2,], keepdim=False, ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:50.407605 test begin: paddle.linalg.matrix_norm(x=Tensor([2, 3, 4, 0],"float64"), p=-2, axis=list[1,2,], keepdim=True, )

[paddle error] paddle.linalg.matrix_norm(x=Tensor([2, 3, 4, 0],"float64"), p=-2, axis=list[1,2,], keepdim=True, ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:11:50.649100 test begin: paddle.linalg.matrix_power(x=Tensor([0, 2, 10, 10],"float64"), n=64, )

W0421 10:11:50.797348 96748 backward.cc:437] While running Node (MatrixPowerGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.matrix_power(x=Tensor([0, 2, 10, 10],"float64"), n=64, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:50.797771 test begin: paddle.linalg.matrix_power(x=Tensor([0, 2, 2, 7, 6, 1, 11, 4, 4],"float64"), n=3, )

W0421 10:11:51.024705 96897 backward.cc:437] While running Node (MatrixPowerGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.matrix_power(x=Tensor([0, 2, 2, 7, 6, 1, 11, 4, 4],"float64"), n=3, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:51.024999 test begin: paddle.linalg.matrix_power(x=Tensor([0, 2, 3, 2, 1, 32, 32],"float64"), n=-10, )

W0421 10:11:51.219718 96991 backward.cc:437] While running Node (MatrixPowerGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.matrix_power(x=Tensor([0, 2, 3, 2, 1, 32, 32],"float64"), n=-10, ) 
 (InvalidArgument) cuBLAS function 'cublas<S/D>getrfBatched' cannot be executed in-place. The memory space of output matrix (address: 0) cannot overlap memory space of input matrix (address: 0).
  [Hint: Expected a_inv != a, but received a_inv:0 == a:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:2549)

2025-04-21 10:11:51.220082 test begin: paddle.linalg.matrix_power(x=Tensor([0, 2, 3, 2, 1, 32, 32],"float64"), n=-2, )

W0421 10:11:51.421650 97070 backward.cc:437] While running Node (MatrixPowerGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.matrix_power(x=Tensor([0, 2, 3, 2, 1, 32, 32],"float64"), n=-2, ) 
 (InvalidArgument) cuBLAS function 'cublas<S/D>getrfBatched' cannot be executed in-place. The memory space of output matrix (address: 0) cannot overlap memory space of input matrix (address: 0).
  [Hint: Expected a_inv != a, but received a_inv:0 == a:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:2549)

2025-04-21 10:11:51.422064 test begin: paddle.linalg.matrix_power(x=Tensor([0, 2, 3, 2, 1, 32, 32],"float64"), n=10, )

W0421 10:11:51.628794 97152 backward.cc:437] While running Node (MatrixPowerGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.matrix_power(x=Tensor([0, 2, 3, 2, 1, 32, 32],"float64"), n=10, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:51.629110 test begin: paddle.linalg.matrix_power(x=Tensor([0, 2, 32, 32],"float64"), n=-10, )

W0421 10:11:51.864792 97298 backward.cc:437] While running Node (MatrixPowerGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.matrix_power(x=Tensor([0, 2, 32, 32],"float64"), n=-10, ) 
 (InvalidArgument) cuBLAS function 'cublas<S/D>getrfBatched' cannot be executed in-place. The memory space of output matrix (address: 0) cannot overlap memory space of input matrix (address: 0).
  [Hint: Expected a_inv != a, but received a_inv:0 == a:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:2549)

2025-04-21 10:11:52.142321 test begin: paddle.linalg.matrix_power(x=Tensor([0, 2, 32, 32],"float64"), n=10, )

W0421 10:11:52.386041 97359 backward.cc:437] While running Node (MatrixPowerGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.matrix_power(x=Tensor([0, 2, 32, 32],"float64"), n=10, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:52.386377 test begin: paddle.linalg.matrix_power(x=Tensor([0, 2, 4, 4],"float64"), n=64, )

W0421 10:11:52.624408 97442 backward.cc:437] While running Node (MatrixPowerGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.matrix_power(x=Tensor([0, 2, 4, 4],"float64"), n=64, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:52.624802 test begin: paddle.linalg.matrix_power(x=Tensor([0, 2, 4, 4],"float64"), n=8, )

W0421 10:11:52.811761 97519 backward.cc:437] While running Node (MatrixPowerGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.matrix_power(x=Tensor([0, 2, 4, 4],"float64"), n=8, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:52.812126 test begin: paddle.linalg.matrix_power(x=Tensor([0, 4, 4],"float32"), n=3, )

W0421 10:11:52.999511 97526 backward.cc:437] While running Node (MatrixPowerGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.matrix_power(x=Tensor([0, 4, 4],"float32"), n=3, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)

2025-04-21 10:11:52.999899 test begin: paddle.linalg.matrix_power(x=Tensor([0, 4, 4],"float64"), n=3, )

W0421 10:11:53.206480 97537 backward.cc:437] While running Node (MatrixPowerGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.matrix_power(x=Tensor([0, 4, 4],"float64"), n=3, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:53.206796 test begin: paddle.linalg.matrix_power(x=Tensor([3, 0, 10, 10],"float64"), n=64, )

W0421 10:11:53.438645 97629 backward.cc:437] While running Node (MatrixPowerGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.matrix_power(x=Tensor([3, 0, 10, 10],"float64"), n=64, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:53.439036 test begin: paddle.linalg.matrix_power(x=Tensor([3, 0, 2, 7, 6, 1, 11, 4, 4],"float64"), n=3, )

W0421 10:11:53.688527 97655 backward.cc:437] While running Node (MatrixPowerGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.matrix_power(x=Tensor([3, 0, 2, 7, 6, 1, 11, 4, 4],"float64"), n=3, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:53.688991 test begin: paddle.linalg.matrix_power(x=Tensor([3, 0, 3, 2, 1, 32, 32],"float64"), n=-10, )

W0421 10:11:53.884864 97805 backward.cc:437] While running Node (MatrixPowerGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.matrix_power(x=Tensor([3, 0, 3, 2, 1, 32, 32],"float64"), n=-10, ) 
 (InvalidArgument) cuBLAS function 'cublas<S/D>getrfBatched' cannot be executed in-place. The memory space of output matrix (address: 0) cannot overlap memory space of input matrix (address: 0).
  [Hint: Expected a_inv != a, but received a_inv:0 == a:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:2549)

2025-04-21 10:11:53.885944 test begin: paddle.linalg.matrix_power(x=Tensor([3, 0, 3, 2, 1, 32, 32],"float64"), n=-2, )

W0421 10:11:54.204604 97946 backward.cc:437] While running Node (MatrixPowerGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.matrix_power(x=Tensor([3, 0, 3, 2, 1, 32, 32],"float64"), n=-2, ) 
 (InvalidArgument) cuBLAS function 'cublas<S/D>getrfBatched' cannot be executed in-place. The memory space of output matrix (address: 0) cannot overlap memory space of input matrix (address: 0).
  [Hint: Expected a_inv != a, but received a_inv:0 == a:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:2549)

2025-04-21 10:11:54.204967 test begin: paddle.linalg.matrix_power(x=Tensor([3, 0, 3, 2, 1, 32, 32],"float64"), n=10, )

W0421 10:11:54.451597 98028 backward.cc:437] While running Node (MatrixPowerGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.matrix_power(x=Tensor([3, 0, 3, 2, 1, 32, 32],"float64"), n=10, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:54.452245 test begin: paddle.linalg.matrix_power(x=Tensor([3, 0, 32, 32],"float64"), n=-10, )

W0421 10:11:54.637082 98126 backward.cc:437] While running Node (MatrixPowerGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.matrix_power(x=Tensor([3, 0, 32, 32],"float64"), n=-10, ) 
 (InvalidArgument) cuBLAS function 'cublas<S/D>getrfBatched' cannot be executed in-place. The memory space of output matrix (address: 0) cannot overlap memory space of input matrix (address: 0).
  [Hint: Expected a_inv != a, but received a_inv:0 == a:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:2549)

2025-04-21 10:11:54.871704 test begin: paddle.linalg.matrix_power(x=Tensor([3, 0, 32, 32],"float64"), n=10, )

W0421 10:11:55.082901 98288 backward.cc:437] While running Node (MatrixPowerGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.matrix_power(x=Tensor([3, 0, 32, 32],"float64"), n=10, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:55.083319 test begin: paddle.linalg.matrix_power(x=Tensor([3, 0, 4, 4],"float64"), n=64, )

W0421 10:11:55.225481 98311 backward.cc:437] While running Node (MatrixPowerGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.matrix_power(x=Tensor([3, 0, 4, 4],"float64"), n=64, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:55.225831 test begin: paddle.linalg.matrix_power(x=Tensor([3, 0, 4, 4],"float64"), n=8, )

W0421 10:11:55.404925 98454 backward.cc:437] While running Node (MatrixPowerGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.matrix_power(x=Tensor([3, 0, 4, 4],"float64"), n=8, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:55.407825 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 0, 2, 1, 32, 32],"float64"), n=-10, )

W0421 10:11:55.605020 98535 backward.cc:437] While running Node (MatrixPowerGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.matrix_power(x=Tensor([3, 2, 0, 2, 1, 32, 32],"float64"), n=-10, ) 
 (InvalidArgument) cuBLAS function 'cublas<S/D>getrfBatched' cannot be executed in-place. The memory space of output matrix (address: 0) cannot overlap memory space of input matrix (address: 0).
  [Hint: Expected a_inv != a, but received a_inv:0 == a:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:2549)

2025-04-21 10:11:55.605360 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 0, 2, 1, 32, 32],"float64"), n=-2, )

W0421 10:11:55.854956 98616 backward.cc:437] While running Node (MatrixPowerGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.matrix_power(x=Tensor([3, 2, 0, 2, 1, 32, 32],"float64"), n=-2, ) 
 (InvalidArgument) cuBLAS function 'cublas<S/D>getrfBatched' cannot be executed in-place. The memory space of output matrix (address: 0) cannot overlap memory space of input matrix (address: 0).
  [Hint: Expected a_inv != a, but received a_inv:0 == a:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:2549)

2025-04-21 10:11:55.855363 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 0, 2, 1, 32, 32],"float64"), n=10, )

W0421 10:11:56.092680 98766 backward.cc:437] While running Node (MatrixPowerGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.matrix_power(x=Tensor([3, 2, 0, 2, 1, 32, 32],"float64"), n=10, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:56.096879 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 0, 7, 6, 1, 11, 4, 4],"float64"), n=3, )

W0421 10:11:56.231330 98861 backward.cc:437] While running Node (MatrixPowerGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.matrix_power(x=Tensor([3, 2, 0, 7, 6, 1, 11, 4, 4],"float64"), n=3, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:56.232675 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 2, 0, 6, 1, 11, 4, 4],"float64"), n=3, )

W0421 10:11:56.356892 98867 backward.cc:437] While running Node (MatrixPowerGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.matrix_power(x=Tensor([3, 2, 2, 0, 6, 1, 11, 4, 4],"float64"), n=3, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:56.357191 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 2, 7, 0, 1, 11, 4, 4],"float64"), n=3, )

W0421 10:11:56.480021 98881 backward.cc:437] While running Node (MatrixPowerGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.matrix_power(x=Tensor([3, 2, 2, 7, 0, 1, 11, 4, 4],"float64"), n=3, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:56.480302 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 2, 7, 6, 0, 11, 4, 4],"float64"), n=3, )

W0421 10:11:56.605379 98955 backward.cc:437] While running Node (MatrixPowerGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.matrix_power(x=Tensor([3, 2, 2, 7, 6, 0, 11, 4, 4],"float64"), n=3, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:56.605906 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 2, 7, 6, 1, 0, 4, 4],"float64"), n=3, )

W0421 10:11:56.775180 99031 backward.cc:437] While running Node (MatrixPowerGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.matrix_power(x=Tensor([3, 2, 2, 7, 6, 1, 0, 4, 4],"float64"), n=3, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:56.777743 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 3, 0, 1, 32, 32],"float64"), n=-10, )

W0421 10:11:56.915267 99039 backward.cc:437] While running Node (MatrixPowerGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.matrix_power(x=Tensor([3, 2, 3, 0, 1, 32, 32],"float64"), n=-10, ) 
 (InvalidArgument) cuBLAS function 'cublas<S/D>getrfBatched' cannot be executed in-place. The memory space of output matrix (address: 0) cannot overlap memory space of input matrix (address: 0).
  [Hint: Expected a_inv != a, but received a_inv:0 == a:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:2549)

2025-04-21 10:11:56.915577 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 3, 0, 1, 32, 32],"float64"), n=-2, )

W0421 10:11:57.043222 99047 backward.cc:437] While running Node (MatrixPowerGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.matrix_power(x=Tensor([3, 2, 3, 0, 1, 32, 32],"float64"), n=-2, ) 
 (InvalidArgument) cuBLAS function 'cublas<S/D>getrfBatched' cannot be executed in-place. The memory space of output matrix (address: 0) cannot overlap memory space of input matrix (address: 0).
  [Hint: Expected a_inv != a, but received a_inv:0 == a:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:2549)

2025-04-21 10:11:57.043529 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 3, 0, 1, 32, 32],"float64"), n=10, )

W0421 10:11:57.242614 99060 backward.cc:437] While running Node (MatrixPowerGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.matrix_power(x=Tensor([3, 2, 3, 0, 1, 32, 32],"float64"), n=10, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:57.242994 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 3, 2, 0, 32, 32],"float64"), n=-10, )

W0421 10:11:57.391927 99102 backward.cc:437] While running Node (MatrixPowerGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.matrix_power(x=Tensor([3, 2, 3, 2, 0, 32, 32],"float64"), n=-10, ) 
 (InvalidArgument) cuBLAS function 'cublas<S/D>getrfBatched' cannot be executed in-place. The memory space of output matrix (address: 0) cannot overlap memory space of input matrix (address: 0).
  [Hint: Expected a_inv != a, but received a_inv:0 == a:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:2549)

2025-04-21 10:11:57.392223 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 3, 2, 0, 32, 32],"float64"), n=-2, )

W0421 10:11:57.525426 99112 backward.cc:437] While running Node (MatrixPowerGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.matrix_power(x=Tensor([3, 2, 3, 2, 0, 32, 32],"float64"), n=-2, ) 
 (InvalidArgument) cuBLAS function 'cublas<S/D>getrfBatched' cannot be executed in-place. The memory space of output matrix (address: 0) cannot overlap memory space of input matrix (address: 0).
  [Hint: Expected a_inv != a, but received a_inv:0 == a:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:2549)

2025-04-21 10:11:57.525756 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 3, 2, 0, 32, 32],"float64"), n=10, )

W0421 10:11:57.784997 99121 backward.cc:437] While running Node (MatrixPowerGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.matrix_power(x=Tensor([3, 2, 3, 2, 0, 32, 32],"float64"), n=10, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:57.798399 test begin: paddle.linalg.matrix_rank(Tensor([0, 10],"float32"), )

[paddle error] paddle.linalg.matrix_rank(Tensor([0, 10],"float32"), ) 
 (InvalidArgument) The input Tensor x's shape[-2] should not be 0, but received 0, 10 now.
  [Hint: Expected rows != 0, but received rows:0 == 0:0.] (at ../paddle/phi/kernels/gpu/matrix_rank_tol_kernel.cu:345)

2025-04-21 10:11:57.988238 test begin: paddle.linalg.matrix_rank(Tensor([0, 10],"float32"), hermitian=True, )

[paddle error] paddle.linalg.matrix_rank(Tensor([0, 10],"float32"), hermitian=True, ) 
 (InvalidArgument) if hermitian == true, matrix should be n*n
  [Hint: Expected rows == cols, but received rows:0 != cols:10.] (at ../paddle/phi/infermeta/unary.cc:2510)

2025-04-21 10:11:58.200621 test begin: paddle.linalg.matrix_rank(Tensor([0, 10],"float32"), hermitian=True, atol=0.015, rtol=None, )

[paddle error] paddle.linalg.matrix_rank(Tensor([0, 10],"float32"), hermitian=True, atol=0.015, rtol=None, ) 
 (InvalidArgument) if hermitian == true, matrix should be n*n
  [Hint: Expected rows == cols, but received rows:0 != cols:10.] (at ../paddle/phi/infermeta/binary.cc:3146)

2025-04-21 10:11:58.396423 test begin: paddle.linalg.matrix_rank(Tensor([0, 10],"float32"), hermitian=True, atol=0.2, rtol=0.05, )

[paddle error] paddle.linalg.matrix_rank(Tensor([0, 10],"float32"), hermitian=True, atol=0.2, rtol=0.05, ) 
 (InvalidArgument) if hermitian == true, matrix should be n*n
  [Hint: Expected rows == cols, but received rows:0 != cols:10.] (at ../paddle/phi/infermeta/binary.cc:3146)

2025-04-21 10:11:58.548298 test begin: paddle.linalg.matrix_rank(Tensor([0, 10],"float32"), hermitian=True, atol=None, rtol=1.1, )

[paddle error] paddle.linalg.matrix_rank(Tensor([0, 10],"float32"), hermitian=True, atol=None, rtol=1.1, ) 
 (InvalidArgument) if hermitian == true, matrix should be n*n
  [Hint: Expected rows == cols, but received rows:0 != cols:10.] (at ../paddle/phi/infermeta/binary.cc:3146)

2025-04-21 10:11:58.747675 test begin: paddle.linalg.matrix_rank(Tensor([0, 10],"float32"), tol=0.1, )

[paddle error] paddle.linalg.matrix_rank(Tensor([0, 10],"float32"), tol=0.1, ) 
 (InvalidArgument) The input Tensor x's shape[-2] should not be 0, but received 0, 10 now.
  [Hint: Expected rows != 0, but received rows:0 == 0:0.] (at ../paddle/phi/kernels/gpu/matrix_rank_tol_kernel.cu:345)

2025-04-21 10:11:58.901337 test begin: paddle.linalg.matrix_rank(Tensor([0, 10],"float32"), tol=Tensor([2],"float32"), )

[paddle error] paddle.linalg.matrix_rank(Tensor([0, 10],"float32"), tol=Tensor([2],"float32"), ) 
 (InvalidArgument) The input Tensor x's shape[-2] should not be 0, but received 0, 10 now.
  [Hint: Expected rows != 0, but received rows:0 == 0:0.] (at ../paddle/phi/kernels/gpu/matrix_rank_tol_kernel.cu:345)

2025-04-21 10:11:59.090210 test begin: paddle.linalg.matrix_rank(Tensor([0, 1],"float64"), Tensor([0, 4],"float64"), False, )

[paddle error] paddle.linalg.matrix_rank(Tensor([0, 1],"float64"), Tensor([0, 4],"float64"), False, ) 
 (InvalidArgument) The input Tensor x's shape[-2] should not be 0, but received 0, 1 now.
  [Hint: Expected rows != 0, but received rows:0 == 0:0.] (at ../paddle/phi/kernels/gpu/matrix_rank_tol_kernel.cu:345)

2025-04-21 10:11:59.269501 test begin: paddle.linalg.matrix_rank(Tensor([0, 1],"float64"), Tensor([1, 4],"float64"), False, )

[paddle error] paddle.linalg.matrix_rank(Tensor([0, 1],"float64"), Tensor([1, 4],"float64"), False, ) 
 (InvalidArgument) The input Tensor x's shape[-2] should not be 0, but received 0, 1 now.
  [Hint: Expected rows != 0, but received rows:0 == 0:0.] (at ../paddle/phi/kernels/gpu/matrix_rank_tol_kernel.cu:345)

2025-04-21 10:11:59.480255 test begin: paddle.linalg.matrix_rank(Tensor([0, 200],"float64"), Tensor([0, 200],"float64"), True, )

[paddle error] paddle.linalg.matrix_rank(Tensor([0, 200],"float64"), Tensor([0, 200],"float64"), True, ) 
 (InvalidArgument) if hermitian == true, matrix should be n*n
  [Hint: Expected rows == cols, but received rows:0 != cols:200.] (at ../paddle/phi/infermeta/binary.cc:3146)

2025-04-21 10:11:59.755815 test begin: paddle.linalg.matrix_rank(Tensor([0, 200],"float64"), Tensor([200, 200],"float64"), True, )

[paddle error] paddle.linalg.matrix_rank(Tensor([0, 200],"float64"), Tensor([200, 200],"float64"), True, ) 
 (InvalidArgument) if hermitian == true, matrix should be n*n
  [Hint: Expected rows == cols, but received rows:0 != cols:200.] (at ../paddle/phi/infermeta/binary.cc:3146)

2025-04-21 10:11:59.964324 test begin: paddle.linalg.matrix_rank(Tensor([0, 3],"float32"), 0.1, True, )

[paddle error] paddle.linalg.matrix_rank(Tensor([0, 3],"float32"), 0.1, True, ) 
 (InvalidArgument) if hermitian == true, matrix should be n*n
  [Hint: Expected rows == cols, but received rows:0 != cols:3.] (at ../paddle/phi/infermeta/unary.cc:2510)

2025-04-21 10:12:00.167884 test begin: paddle.linalg.matrix_rank(Tensor([0, 4, 5, 5],"float64"), hermitian=False, atol=Tensor([3, 4],"float64"), rtol=None, )

[paddle error] paddle.linalg.matrix_rank(Tensor([0, 4, 5, 5],"float64"), hermitian=False, atol=Tensor([3, 4],"float64"), rtol=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 4] and the shape of Y = [3, 4]. Received [0] in X is not equal to [3] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:71)

2025-04-21 10:12:00.503590 test begin: paddle.linalg.matrix_rank(Tensor([0, 4, 5, 6],"float32"), Tensor([3, 4],"float32"), False, )

[paddle error] paddle.linalg.matrix_rank(Tensor([0, 4, 5, 6],"float32"), Tensor([3, 4],"float32"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 4] and the shape of Y = [3, 4]. Received [0] in X is not equal to [3] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:71)

2025-04-21 10:12:01.282244 test begin: paddle.linalg.matrix_rank(Tensor([0, 4, 7, 8],"float64"), Tensor([3, 4],"float32"), hermitian=False, )

[paddle error] paddle.linalg.matrix_rank(Tensor([0, 4, 7, 8],"float64"), Tensor([3, 4],"float32"), hermitian=False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 4] and the shape of Y = [3, 4]. Received [0] in X is not equal to [3] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:71)

2025-04-21 10:12:01.470745 test begin: paddle.linalg.matrix_rank(Tensor([0, 4, 7, 8],"float64"), hermitian=False, atol=Tensor([3, 1],"float32"), rtol=Tensor([3, 1],"float32"), )

[paddle error] paddle.linalg.matrix_rank(Tensor([0, 4, 7, 8],"float64"), hermitian=False, atol=Tensor([3, 1],"float32"), rtol=Tensor([3, 1],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 4] and the shape of Y = [3, 1]. Received [0] in X is not equal to [3] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:71)

2025-04-21 10:12:01.685579 test begin: paddle.linalg.matrix_rank(Tensor([0, 4, 7, 8],"float64"), hermitian=False, atol=Tensor([3, 4],"float32"), rtol=Tensor([3, 4],"float32"), )

[paddle error] paddle.linalg.matrix_rank(Tensor([0, 4, 7, 8],"float64"), hermitian=False, atol=Tensor([3, 4],"float32"), rtol=Tensor([3, 4],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 4] and the shape of Y = [3, 4]. Received [0] in X is not equal to [3] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:71)

2025-04-21 10:12:01.891685 test begin: paddle.linalg.matrix_rank(Tensor([10, 0],"float32"), )

[paddle error] paddle.linalg.matrix_rank(Tensor([10, 0],"float32"), ) 
 (InvalidArgument) The input Tensor x's shape[-1] should not be 0, but received 10, 0 now.
  [Hint: Expected cols != 0, but received cols:0 == 0:0.] (at ../paddle/phi/kernels/gpu/matrix_rank_tol_kernel.cu:351)

2025-04-21 10:12:02.085320 test begin: paddle.linalg.matrix_rank(Tensor([10, 0],"float32"), hermitian=True, )

[paddle error] paddle.linalg.matrix_rank(Tensor([10, 0],"float32"), hermitian=True, ) 
 (InvalidArgument) if hermitian == true, matrix should be n*n
  [Hint: Expected rows == cols, but received rows:10 != cols:0.] (at ../paddle/phi/infermeta/unary.cc:2510)

2025-04-21 10:12:02.320802 test begin: paddle.linalg.matrix_rank(Tensor([10, 0],"float32"), hermitian=True, atol=0.015, rtol=None, )

[paddle error] paddle.linalg.matrix_rank(Tensor([10, 0],"float32"), hermitian=True, atol=0.015, rtol=None, ) 
 (InvalidArgument) if hermitian == true, matrix should be n*n
  [Hint: Expected rows == cols, but received rows:10 != cols:0.] (at ../paddle/phi/infermeta/binary.cc:3146)

2025-04-21 10:12:02.523627 test begin: paddle.linalg.matrix_rank(Tensor([10, 0],"float32"), hermitian=True, atol=0.2, rtol=0.05, )

[paddle error] paddle.linalg.matrix_rank(Tensor([10, 0],"float32"), hermitian=True, atol=0.2, rtol=0.05, ) 
 (InvalidArgument) if hermitian == true, matrix should be n*n
  [Hint: Expected rows == cols, but received rows:10 != cols:0.] (at ../paddle/phi/infermeta/binary.cc:3146)

2025-04-21 10:12:02.721729 test begin: paddle.linalg.matrix_rank(Tensor([10, 0],"float32"), hermitian=True, atol=None, rtol=1.1, )

[paddle error] paddle.linalg.matrix_rank(Tensor([10, 0],"float32"), hermitian=True, atol=None, rtol=1.1, ) 
 (InvalidArgument) if hermitian == true, matrix should be n*n
  [Hint: Expected rows == cols, but received rows:10 != cols:0.] (at ../paddle/phi/infermeta/binary.cc:3146)

2025-04-21 10:12:02.954926 test begin: paddle.linalg.matrix_rank(Tensor([10, 0],"float32"), tol=0.1, )

[paddle error] paddle.linalg.matrix_rank(Tensor([10, 0],"float32"), tol=0.1, ) 
 (InvalidArgument) The input Tensor x's shape[-1] should not be 0, but received 10, 0 now.
  [Hint: Expected cols != 0, but received cols:0 == 0:0.] (at ../paddle/phi/kernels/gpu/matrix_rank_tol_kernel.cu:351)

2025-04-21 10:12:03.157201 test begin: paddle.linalg.matrix_rank(Tensor([10, 0],"float32"), tol=Tensor([2],"float32"), )

[paddle error] paddle.linalg.matrix_rank(Tensor([10, 0],"float32"), tol=Tensor([2],"float32"), ) 
 (InvalidArgument) The input Tensor x's shape[-1] should not be 0, but received 10, 0 now.
  [Hint: Expected cols != 0, but received cols:0 == 0:0.] (at ../paddle/phi/kernels/gpu/matrix_rank_tol_kernel.cu:351)

2025-04-21 10:12:03.510728 test begin: paddle.linalg.matrix_rank(Tensor([200, 0],"float64"), Tensor([200, 0],"float64"), True, )

[paddle error] paddle.linalg.matrix_rank(Tensor([200, 0],"float64"), Tensor([200, 0],"float64"), True, ) 
 (InvalidArgument) if hermitian == true, matrix should be n*n
  [Hint: Expected rows == cols, but received rows:200 != cols:0.] (at ../paddle/phi/infermeta/binary.cc:3146)

2025-04-21 10:12:03.698147 test begin: paddle.linalg.matrix_rank(Tensor([200, 0],"float64"), Tensor([200, 200],"float64"), True, )

[paddle error] paddle.linalg.matrix_rank(Tensor([200, 0],"float64"), Tensor([200, 200],"float64"), True, ) 
 (InvalidArgument) if hermitian == true, matrix should be n*n
  [Hint: Expected rows == cols, but received rows:200 != cols:0.] (at ../paddle/phi/infermeta/binary.cc:3146)

2025-04-21 10:12:04.210957 test begin: paddle.linalg.matrix_rank(Tensor([3, 0, 5, 5],"float64"), hermitian=False, atol=Tensor([3, 4],"float64"), rtol=None, )

[paddle error] paddle.linalg.matrix_rank(Tensor([3, 0, 5, 5],"float64"), hermitian=False, atol=Tensor([3, 4],"float64"), rtol=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 0] and the shape of Y = [3, 4]. Received [0] in X is not equal to [4] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:71)

2025-04-21 10:12:04.622941 test begin: paddle.linalg.matrix_rank(Tensor([3, 0, 5, 6],"float32"), Tensor([3, 4],"float32"), False, )

[paddle error] paddle.linalg.matrix_rank(Tensor([3, 0, 5, 6],"float32"), Tensor([3, 4],"float32"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 0] and the shape of Y = [3, 4]. Received [0] in X is not equal to [4] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:71)

2025-04-21 10:12:04.761707 test begin: paddle.linalg.matrix_rank(Tensor([3, 0, 5],"float32"), )

[paddle error] paddle.linalg.matrix_rank(Tensor([3, 0, 5],"float32"), ) 
 (InvalidArgument) The input Tensor x's shape[-2] should not be 0, but received 3, 0, 5 now.
  [Hint: Expected rows != 0, but received rows:0 == 0:0.] (at ../paddle/phi/kernels/gpu/matrix_rank_tol_kernel.cu:345)

2025-04-21 10:12:04.906198 test begin: paddle.linalg.matrix_rank(Tensor([3, 0, 5],"float32"), tol=0.1, )

[paddle error] paddle.linalg.matrix_rank(Tensor([3, 0, 5],"float32"), tol=0.1, ) 
 (InvalidArgument) The input Tensor x's shape[-2] should not be 0, but received 3, 0, 5 now.
  [Hint: Expected rows != 0, but received rows:0 == 0:0.] (at ../paddle/phi/kernels/gpu/matrix_rank_tol_kernel.cu:345)

2025-04-21 10:12:05.213424 test begin: paddle.linalg.matrix_rank(Tensor([3, 0, 7, 8],"float64"), Tensor([3, 4],"float32"), hermitian=False, )

[paddle error] paddle.linalg.matrix_rank(Tensor([3, 0, 7, 8],"float64"), Tensor([3, 4],"float32"), hermitian=False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 0] and the shape of Y = [3, 4]. Received [0] in X is not equal to [4] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:71)

2025-04-21 10:12:05.590819 test begin: paddle.linalg.matrix_rank(Tensor([3, 0, 7, 8],"float64"), hermitian=False, atol=Tensor([3, 4],"float32"), rtol=Tensor([3, 4],"float32"), )

[paddle error] paddle.linalg.matrix_rank(Tensor([3, 0, 7, 8],"float64"), hermitian=False, atol=Tensor([3, 4],"float32"), rtol=Tensor([3, 4],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 0] and the shape of Y = [3, 4]. Received [0] in X is not equal to [4] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:71)

2025-04-21 10:12:05.779405 test begin: paddle.linalg.matrix_rank(Tensor([3, 0],"float32"), 0.1, True, )

[paddle error] paddle.linalg.matrix_rank(Tensor([3, 0],"float32"), 0.1, True, ) 
 (InvalidArgument) if hermitian == true, matrix should be n*n
  [Hint: Expected rows == cols, but received rows:3 != cols:0.] (at ../paddle/phi/infermeta/unary.cc:2510)

2025-04-21 10:12:05.979387 test begin: paddle.linalg.matrix_rank(Tensor([3, 4, 0, 5],"float64"), hermitian=False, atol=Tensor([3, 4],"float64"), rtol=None, )

[paddle error] paddle.linalg.matrix_rank(Tensor([3, 4, 0, 5],"float64"), hermitian=False, atol=Tensor([3, 4],"float64"), rtol=None, ) 
 (InvalidArgument) The input Tensor x's shape[-2] should not be 0, but received 3, 4, 0, 5 now.
  [Hint: Expected rows != 0, but received rows:0 == 0:0.] (at ../paddle/phi/kernels/gpu/matrix_rank_tol_kernel.cu:473)

2025-04-21 10:12:06.173504 test begin: paddle.linalg.matrix_rank(Tensor([3, 4, 0, 5],"float64"), hermitian=True, atol=0.5, rtol=None, )

[paddle error] paddle.linalg.matrix_rank(Tensor([3, 4, 0, 5],"float64"), hermitian=True, atol=0.5, rtol=None, ) 
 (InvalidArgument) if hermitian == true, matrix should be n*n
  [Hint: Expected rows == cols, but received rows:0 != cols:5.] (at ../paddle/phi/infermeta/binary.cc:3146)

2025-04-21 10:12:06.420585 test begin: paddle.linalg.matrix_rank(Tensor([3, 4, 0, 6],"float32"), Tensor([3, 4],"float32"), False, )

[paddle error] paddle.linalg.matrix_rank(Tensor([3, 4, 0, 6],"float32"), Tensor([3, 4],"float32"), False, ) 
 (InvalidArgument) The input Tensor x's shape[-2] should not be 0, but received 3, 4, 0, 6 now.
  [Hint: Expected rows != 0, but received rows:0 == 0:0.] (at ../paddle/phi/kernels/gpu/matrix_rank_tol_kernel.cu:345)

2025-04-21 10:12:06.625653 test begin: paddle.linalg.matrix_rank(Tensor([3, 4, 0, 8],"float64"), 0.1, hermitian=False, )

[paddle error] paddle.linalg.matrix_rank(Tensor([3, 4, 0, 8],"float64"), 0.1, hermitian=False, ) 
 (InvalidArgument) The input Tensor x's shape[-2] should not be 0, but received 3, 4, 0, 8 now.
  [Hint: Expected rows != 0, but received rows:0 == 0:0.] (at ../paddle/phi/kernels/gpu/matrix_rank_tol_kernel.cu:345)

2025-04-21 10:12:06.844696 test begin: paddle.linalg.matrix_rank(Tensor([3, 4, 0, 8],"float64"), Tensor([3, 4],"float32"), hermitian=False, )

[paddle error] paddle.linalg.matrix_rank(Tensor([3, 4, 0, 8],"float64"), Tensor([3, 4],"float32"), hermitian=False, ) 
 (InvalidArgument) The input Tensor x's shape[-2] should not be 0, but received 3, 4, 0, 8 now.
  [Hint: Expected rows != 0, but received rows:0 == 0:0.] (at ../paddle/phi/kernels/gpu/matrix_rank_tol_kernel.cu:345)

2025-04-21 10:12:06.986006 test begin: paddle.linalg.matrix_rank(Tensor([3, 4, 0, 8],"float64"), hermitian=False, atol=Tensor([3, 1],"float32"), rtol=Tensor([3, 1],"float32"), )

[paddle error] paddle.linalg.matrix_rank(Tensor([3, 4, 0, 8],"float64"), hermitian=False, atol=Tensor([3, 1],"float32"), rtol=Tensor([3, 1],"float32"), ) 
 (InvalidArgument) The input Tensor x's shape[-2] should not be 0, but received 3, 4, 0, 8 now.
  [Hint: Expected rows != 0, but received rows:0 == 0:0.] (at ../paddle/phi/kernels/gpu/matrix_rank_tol_kernel.cu:473)

2025-04-21 10:12:07.199381 test begin: paddle.linalg.matrix_rank(Tensor([3, 4, 0, 8],"float64"), hermitian=False, atol=Tensor([3, 4],"float32"), rtol=Tensor([3, 4],"float32"), )

[paddle error] paddle.linalg.matrix_rank(Tensor([3, 4, 0, 8],"float64"), hermitian=False, atol=Tensor([3, 4],"float32"), rtol=Tensor([3, 4],"float32"), ) 
 (InvalidArgument) The input Tensor x's shape[-2] should not be 0, but received 3, 4, 0, 8 now.
  [Hint: Expected rows != 0, but received rows:0 == 0:0.] (at ../paddle/phi/kernels/gpu/matrix_rank_tol_kernel.cu:473)

2025-04-21 10:12:07.450152 test begin: paddle.linalg.matrix_rank(Tensor([3, 4, 0],"float32"), )

[paddle error] paddle.linalg.matrix_rank(Tensor([3, 4, 0],"float32"), ) 
 (InvalidArgument) The input Tensor x's shape[-1] should not be 0, but received 3, 4, 0 now.
  [Hint: Expected cols != 0, but received cols:0 == 0:0.] (at ../paddle/phi/kernels/gpu/matrix_rank_tol_kernel.cu:351)

2025-04-21 10:12:07.682479 test begin: paddle.linalg.matrix_rank(Tensor([3, 4, 0],"float32"), tol=0.1, )

[paddle error] paddle.linalg.matrix_rank(Tensor([3, 4, 0],"float32"), tol=0.1, ) 
 (InvalidArgument) The input Tensor x's shape[-1] should not be 0, but received 3, 4, 0 now.
  [Hint: Expected cols != 0, but received cols:0 == 0:0.] (at ../paddle/phi/kernels/gpu/matrix_rank_tol_kernel.cu:351)

2025-04-21 10:12:07.883689 test begin: paddle.linalg.matrix_rank(Tensor([3, 4, 5, 0],"float32"), Tensor([3, 4],"float32"), False, )

[paddle error] paddle.linalg.matrix_rank(Tensor([3, 4, 5, 0],"float32"), Tensor([3, 4],"float32"), False, ) 
 (InvalidArgument) The input Tensor x's shape[-1] should not be 0, but received 3, 4, 5, 0 now.
  [Hint: Expected cols != 0, but received cols:0 == 0:0.] (at ../paddle/phi/kernels/gpu/matrix_rank_tol_kernel.cu:351)

2025-04-21 10:12:08.104917 test begin: paddle.linalg.matrix_rank(Tensor([3, 4, 5, 0],"float64"), hermitian=False, atol=Tensor([3, 4],"float64"), rtol=None, )

[paddle error] paddle.linalg.matrix_rank(Tensor([3, 4, 5, 0],"float64"), hermitian=False, atol=Tensor([3, 4],"float64"), rtol=None, ) 
 (InvalidArgument) The input Tensor x's shape[-1] should not be 0, but received 3, 4, 5, 0 now.
  [Hint: Expected cols != 0, but received cols:0 == 0:0.] (at ../paddle/phi/kernels/gpu/matrix_rank_tol_kernel.cu:479)

2025-04-21 10:12:08.253099 test begin: paddle.linalg.matrix_rank(Tensor([3, 4, 5, 0],"float64"), hermitian=True, atol=0.5, rtol=None, )

[paddle error] paddle.linalg.matrix_rank(Tensor([3, 4, 5, 0],"float64"), hermitian=True, atol=0.5, rtol=None, ) 
 (InvalidArgument) if hermitian == true, matrix should be n*n
  [Hint: Expected rows == cols, but received rows:5 != cols:0.] (at ../paddle/phi/infermeta/binary.cc:3146)

2025-04-21 10:12:08.418078 test begin: paddle.linalg.matrix_rank(Tensor([3, 4, 7, 0],"float64"), 0.1, hermitian=False, )

[paddle error] paddle.linalg.matrix_rank(Tensor([3, 4, 7, 0],"float64"), 0.1, hermitian=False, ) 
 (InvalidArgument) The input Tensor x's shape[-1] should not be 0, but received 3, 4, 7, 0 now.
  [Hint: Expected cols != 0, but received cols:0 == 0:0.] (at ../paddle/phi/kernels/gpu/matrix_rank_tol_kernel.cu:351)

2025-04-21 10:12:08.545454 test begin: paddle.linalg.matrix_rank(Tensor([3, 4, 7, 0],"float64"), Tensor([3, 4],"float32"), hermitian=False, )

[paddle error] paddle.linalg.matrix_rank(Tensor([3, 4, 7, 0],"float64"), Tensor([3, 4],"float32"), hermitian=False, ) 
 (InvalidArgument) The input Tensor x's shape[-1] should not be 0, but received 3, 4, 7, 0 now.
  [Hint: Expected cols != 0, but received cols:0 == 0:0.] (at ../paddle/phi/kernels/gpu/matrix_rank_tol_kernel.cu:351)

2025-04-21 10:12:08.782569 test begin: paddle.linalg.matrix_rank(Tensor([3, 4, 7, 0],"float64"), hermitian=False, atol=Tensor([3, 1],"float32"), rtol=Tensor([3, 1],"float32"), )

[paddle error] paddle.linalg.matrix_rank(Tensor([3, 4, 7, 0],"float64"), hermitian=False, atol=Tensor([3, 1],"float32"), rtol=Tensor([3, 1],"float32"), ) 
 (InvalidArgument) The input Tensor x's shape[-1] should not be 0, but received 3, 4, 7, 0 now.
  [Hint: Expected cols != 0, but received cols:0 == 0:0.] (at ../paddle/phi/kernels/gpu/matrix_rank_tol_kernel.cu:479)

2025-04-21 10:12:08.988209 test begin: paddle.linalg.matrix_rank(Tensor([3, 4, 7, 0],"float64"), hermitian=False, atol=Tensor([3, 4],"float32"), rtol=Tensor([3, 4],"float32"), )

[paddle error] paddle.linalg.matrix_rank(Tensor([3, 4, 7, 0],"float64"), hermitian=False, atol=Tensor([3, 4],"float32"), rtol=Tensor([3, 4],"float32"), ) 
 (InvalidArgument) The input Tensor x's shape[-1] should not be 0, but received 3, 4, 7, 0 now.
  [Hint: Expected cols != 0, but received cols:0 == 0:0.] (at ../paddle/phi/kernels/gpu/matrix_rank_tol_kernel.cu:479)

2025-04-21 10:12:09.247803 test begin: paddle.linalg.matrix_rank(Tensor([5, 0],"float64"), Tensor([1, 0],"float64"), False, )

[paddle error] paddle.linalg.matrix_rank(Tensor([5, 0],"float64"), Tensor([1, 0],"float64"), False, ) 
 (InvalidArgument) The input Tensor x's shape[-1] should not be 0, but received 5, 0 now.
  [Hint: Expected cols != 0, but received cols:0 == 0:0.] (at ../paddle/phi/kernels/gpu/matrix_rank_tol_kernel.cu:351)

2025-04-21 10:12:09.435638 test begin: paddle.linalg.matrix_rank(Tensor([5, 0],"float64"), Tensor([1, 4],"float64"), False, )

[paddle error] paddle.linalg.matrix_rank(Tensor([5, 0],"float64"), Tensor([1, 4],"float64"), False, ) 
 (InvalidArgument) The input Tensor x's shape[-1] should not be 0, but received 5, 0 now.
  [Hint: Expected cols != 0, but received cols:0 == 0:0.] (at ../paddle/phi/kernels/gpu/matrix_rank_tol_kernel.cu:351)

2025-04-21 10:12:10.484235 test begin: paddle.linalg.matrix_rank(x=Tensor([0, 3, 4, 4],"float64"), tol=Tensor([2, 3],"float64"), hermitian=True, )

[paddle error] paddle.linalg.matrix_rank(x=Tensor([0, 3, 4, 4],"float64"), tol=Tensor([2, 3],"float64"), hermitian=True, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 3] and the shape of Y = [2, 3]. Received [0] in X is not equal to [2] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:71)

2025-04-21 10:12:10.729270 test begin: paddle.linalg.matrix_rank(x=Tensor([0, 4, 4, 5],"float64"), tol=Tensor([2, 1],"float64"), hermitian=False, )

[paddle error] paddle.linalg.matrix_rank(x=Tensor([0, 4, 4, 5],"float64"), tol=Tensor([2, 1],"float64"), hermitian=False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 4] and the shape of Y = [2, 1]. Received [0] in X is not equal to [2] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:71)

2025-04-21 10:12:10.934471 test begin: paddle.linalg.matrix_rank(x=Tensor([0, 4],"float64"), tol=4.4, hermitian=True, )

[paddle error] paddle.linalg.matrix_rank(x=Tensor([0, 4],"float64"), tol=4.4, hermitian=True, ) 
 (InvalidArgument) if hermitian == true, matrix should be n*n
  [Hint: Expected rows == cols, but received rows:0 != cols:4.] (at ../paddle/phi/infermeta/unary.cc:2510)

2025-04-21 10:12:11.254190 test begin: paddle.linalg.matrix_rank(x=Tensor([0, 4],"float64"), tol=Tensor([1],"float64"), hermitian=True, )

[paddle error] paddle.linalg.matrix_rank(x=Tensor([0, 4],"float64"), tol=Tensor([1],"float64"), hermitian=True, ) 
 (InvalidArgument) if hermitian == true, matrix should be n*n
  [Hint: Expected rows == cols, but received rows:0 != cols:4.] (at ../paddle/phi/infermeta/binary.cc:3146)

2025-04-21 10:12:11.516181 test begin: paddle.linalg.matrix_rank(x=Tensor([0, 5],"float64"), tol=4.4, hermitian=False, )

[paddle error] paddle.linalg.matrix_rank(x=Tensor([0, 5],"float64"), tol=4.4, hermitian=False, ) 
 (InvalidArgument) The input Tensor x's shape[-2] should not be 0, but received 0, 5 now.
  [Hint: Expected rows != 0, but received rows:0 == 0:0.] (at ../paddle/phi/kernels/gpu/matrix_rank_tol_kernel.cu:345)

2025-04-21 10:12:12.293328 test begin: paddle.linalg.matrix_rank(x=Tensor([2, 0, 4, 4],"float64"), tol=Tensor([2, 3],"float64"), hermitian=True, )

[paddle error] paddle.linalg.matrix_rank(x=Tensor([2, 0, 4, 4],"float64"), tol=Tensor([2, 3],"float64"), hermitian=True, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 0] and the shape of Y = [2, 3]. Received [0] in X is not equal to [3] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:71)

2025-04-21 10:12:12.822153 test begin: paddle.linalg.matrix_rank(x=Tensor([2, 2, 0, 4],"float64"), tol=Tensor([1, 1],"float64"), hermitian=True, )

[paddle error] paddle.linalg.matrix_rank(x=Tensor([2, 2, 0, 4],"float64"), tol=Tensor([1, 1],"float64"), hermitian=True, ) 
 (InvalidArgument) if hermitian == true, matrix should be n*n
  [Hint: Expected rows == cols, but received rows:0 != cols:4.] (at ../paddle/phi/infermeta/binary.cc:3146)

2025-04-21 10:12:13.015761 test begin: paddle.linalg.matrix_rank(x=Tensor([2, 2, 4, 0],"float64"), tol=Tensor([1, 1],"float64"), hermitian=True, )

[paddle error] paddle.linalg.matrix_rank(x=Tensor([2, 2, 4, 0],"float64"), tol=Tensor([1, 1],"float64"), hermitian=True, ) 
 (InvalidArgument) if hermitian == true, matrix should be n*n
  [Hint: Expected rows == cols, but received rows:4 != cols:0.] (at ../paddle/phi/infermeta/binary.cc:3146)

2025-04-21 10:12:13.224126 test begin: paddle.linalg.matrix_rank(x=Tensor([2, 3, 0, 4],"float64"), tol=Tensor([1],"float64"), hermitian=True, )

[paddle error] paddle.linalg.matrix_rank(x=Tensor([2, 3, 0, 4],"float64"), tol=Tensor([1],"float64"), hermitian=True, ) 
 (InvalidArgument) if hermitian == true, matrix should be n*n
  [Hint: Expected rows == cols, but received rows:0 != cols:4.] (at ../paddle/phi/infermeta/binary.cc:3146)

2025-04-21 10:12:13.425610 test begin: paddle.linalg.matrix_rank(x=Tensor([2, 3, 0, 4],"float64"), tol=Tensor([2, 3],"float64"), hermitian=True, )

[paddle error] paddle.linalg.matrix_rank(x=Tensor([2, 3, 0, 4],"float64"), tol=Tensor([2, 3],"float64"), hermitian=True, ) 
 (InvalidArgument) if hermitian == true, matrix should be n*n
  [Hint: Expected rows == cols, but received rows:0 != cols:4.] (at ../paddle/phi/infermeta/binary.cc:3146)

2025-04-21 10:12:13.683827 test begin: paddle.linalg.matrix_rank(x=Tensor([2, 3, 4, 0],"float64"), tol=Tensor([1],"float64"), hermitian=True, )

[paddle error] paddle.linalg.matrix_rank(x=Tensor([2, 3, 4, 0],"float64"), tol=Tensor([1],"float64"), hermitian=True, ) 
 (InvalidArgument) if hermitian == true, matrix should be n*n
  [Hint: Expected rows == cols, but received rows:4 != cols:0.] (at ../paddle/phi/infermeta/binary.cc:3146)

2025-04-21 10:12:13.867510 test begin: paddle.linalg.matrix_rank(x=Tensor([2, 3, 4, 0],"float64"), tol=Tensor([2, 3],"float64"), hermitian=True, )

[paddle error] paddle.linalg.matrix_rank(x=Tensor([2, 3, 4, 0],"float64"), tol=Tensor([2, 3],"float64"), hermitian=True, ) 
 (InvalidArgument) if hermitian == true, matrix should be n*n
  [Hint: Expected rows == cols, but received rows:4 != cols:0.] (at ../paddle/phi/infermeta/binary.cc:3146)

2025-04-21 10:12:14.079586 test begin: paddle.linalg.matrix_rank(x=Tensor([2, 4, 0, 5],"float64"), tol=Tensor([2, 1],"float64"), hermitian=False, )

[paddle error] paddle.linalg.matrix_rank(x=Tensor([2, 4, 0, 5],"float64"), tol=Tensor([2, 1],"float64"), hermitian=False, ) 
 (InvalidArgument) The input Tensor x's shape[-2] should not be 0, but received 2, 4, 0, 5 now.
  [Hint: Expected rows != 0, but received rows:0 == 0:0.] (at ../paddle/phi/kernels/gpu/matrix_rank_tol_kernel.cu:345)

2025-04-21 10:12:14.294083 test begin: paddle.linalg.matrix_rank(x=Tensor([2, 4, 4, 0],"float64"), tol=Tensor([2, 1],"float64"), hermitian=False, )

[paddle error] paddle.linalg.matrix_rank(x=Tensor([2, 4, 4, 0],"float64"), tol=Tensor([2, 1],"float64"), hermitian=False, ) 
 (InvalidArgument) The input Tensor x's shape[-1] should not be 0, but received 2, 4, 4, 0 now.
  [Hint: Expected cols != 0, but received cols:0 == 0:0.] (at ../paddle/phi/kernels/gpu/matrix_rank_tol_kernel.cu:351)

2025-04-21 10:12:14.516609 test begin: paddle.linalg.matrix_rank(x=Tensor([4, 0],"float64"), tol=4.4, hermitian=False, )

[paddle error] paddle.linalg.matrix_rank(x=Tensor([4, 0],"float64"), tol=4.4, hermitian=False, ) 
 (InvalidArgument) The input Tensor x's shape[-1] should not be 0, but received 4, 0 now.
  [Hint: Expected cols != 0, but received cols:0 == 0:0.] (at ../paddle/phi/kernels/gpu/matrix_rank_tol_kernel.cu:351)

2025-04-21 10:12:14.734629 test begin: paddle.linalg.matrix_rank(x=Tensor([4, 0],"float64"), tol=4.4, hermitian=True, )

[paddle error] paddle.linalg.matrix_rank(x=Tensor([4, 0],"float64"), tol=4.4, hermitian=True, ) 
 (InvalidArgument) if hermitian == true, matrix should be n*n
  [Hint: Expected rows == cols, but received rows:4 != cols:0.] (at ../paddle/phi/infermeta/unary.cc:2510)

2025-04-21 10:12:15.053073 test begin: paddle.linalg.matrix_rank(x=Tensor([4, 0],"float64"), tol=Tensor([1],"float64"), hermitian=True, )

[paddle error] paddle.linalg.matrix_rank(x=Tensor([4, 0],"float64"), tol=Tensor([1],"float64"), hermitian=True, ) 
 (InvalidArgument) if hermitian == true, matrix should be n*n
  [Hint: Expected rows == cols, but received rows:4 != cols:0.] (at ../paddle/phi/infermeta/binary.cc:3146)

2025-04-21 10:12:17.438631 test begin: paddle.linalg.multi_dot(list[Tensor([0],"float64"),Tensor([0],"float64"),], )

 ** On entry to DGEMM  parameter number 10 had an illegal value
[paddle error] paddle.linalg.multi_dot(list[Tensor([0],"float64"),Tensor([0],"float64"),], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:12:17.736620 test begin: paddle.linalg.multi_dot(list[Tensor([2, 10],"float64"),Tensor([10, 4],"float64"),Tensor([4, 0],"float64"),], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
[paddle error] paddle.linalg.multi_dot(list[Tensor([2, 10],"float64"),Tensor([10, 4],"float64"),Tensor([4, 0],"float64"),], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:12:17.926503 test begin: paddle.linalg.multi_dot(list[Tensor([2, 4],"float64"),Tensor([4, 0],"float64"),], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
[paddle error] paddle.linalg.multi_dot(list[Tensor([2, 4],"float64"),Tensor([4, 0],"float64"),], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:12:18.146094 test begin: paddle.linalg.multi_dot(list[Tensor([2, 8],"float16"),Tensor([8, 0],"float16"),], )

 ** On entry to GEMM_EX  parameter number 9 had an illegal value
[paddle error] paddle.linalg.multi_dot(list[Tensor([2, 8],"float16"),Tensor([8, 0],"float16"),], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:426)

2025-04-21 10:12:18.373443 test begin: paddle.linalg.multi_dot(list[Tensor([3, 4],"float64"),Tensor([4, 8],"float64"),Tensor([8, 0],"float64"),], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
[paddle error] paddle.linalg.multi_dot(list[Tensor([3, 4],"float64"),Tensor([4, 8],"float64"),Tensor([8, 0],"float64"),], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:12:18.558610 test begin: paddle.linalg.multi_dot(list[Tensor([4],"float64"),Tensor([4, 0],"float64"),], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
[paddle error] paddle.linalg.multi_dot(list[Tensor([4],"float64"),Tensor([4, 0],"float64"),], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:12:18.772187 test begin: paddle.linalg.multi_dot(list[Tensor([4],"float64"),Tensor([4, 3],"float64"),Tensor([3, 0],"float64"),], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
[paddle error] paddle.linalg.multi_dot(list[Tensor([4],"float64"),Tensor([4, 3],"float64"),Tensor([3, 0],"float64"),], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:12:19.026889 test begin: paddle.linalg.multi_dot(list[Tensor([4],"float64"),Tensor([4, 3],"float64"),Tensor([3, 4],"float64"),Tensor([4, 0],"float64"),], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
[paddle error] paddle.linalg.multi_dot(list[Tensor([4],"float64"),Tensor([4, 3],"float64"),Tensor([3, 4],"float64"),Tensor([4, 0],"float64"),], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:12:19.220561 test begin: paddle.linalg.multi_dot(list[Tensor([8, 6],"float64"),Tensor([6, 3],"float64"),Tensor([3, 4],"float64"),Tensor([4, 0],"float64"),], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
[paddle error] paddle.linalg.multi_dot(list[Tensor([8, 6],"float64"),Tensor([6, 3],"float64"),Tensor([3, 4],"float64"),Tensor([4, 0],"float64"),], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:12:22.419691 test begin: paddle.linalg.norm(Tensor([0, 3, 4, 5],"float64"), p="fro", axis=list[1,2,], keepdim=False, )

[paddle error] paddle.linalg.norm(Tensor([0, 3, 4, 5],"float64"), p="fro", axis=list[1,2,], keepdim=False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:12:22.604710 test begin: paddle.linalg.norm(Tensor([0, 5, 5],"float32"), p="fro", axis=list[0,1,], keepdim=True, )

[paddle error] paddle.linalg.norm(Tensor([0, 5, 5],"float32"), p="fro", axis=list[0,1,], keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:12:26.854075 test begin: paddle.linalg.norm(Tensor([2, 0, 4, 5],"float64"), p="fro", axis=list[1,2,], keepdim=False, )

[paddle error] paddle.linalg.norm(Tensor([2, 0, 4, 5],"float64"), p="fro", axis=list[1,2,], keepdim=False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:12:27.287587 test begin: paddle.linalg.norm(Tensor([2, 3, 0, 5],"float64"), p="fro", axis=list[1,2,], keepdim=False, )

[paddle error] paddle.linalg.norm(Tensor([2, 3, 0, 5],"float64"), p="fro", axis=list[1,2,], keepdim=False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:12:27.740948 test begin: paddle.linalg.norm(Tensor([2, 3, 4, 0],"float64"), p="fro", axis=list[1,2,], keepdim=False, )

[paddle error] paddle.linalg.norm(Tensor([2, 3, 4, 0],"float64"), p="fro", axis=list[1,2,], keepdim=False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:12:30.791763 test begin: paddle.linalg.norm(Tensor([5, 0, 5],"float32"), p="fro", axis=list[0,1,], keepdim=True, )

[paddle error] paddle.linalg.norm(Tensor([5, 0, 5],"float32"), p="fro", axis=list[0,1,], keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:12:31.032764 test begin: paddle.linalg.norm(Tensor([5, 5, 0],"float32"), p="fro", axis=list[0,1,], keepdim=True, )

[paddle error] paddle.linalg.norm(Tensor([5, 5, 0],"float32"), p="fro", axis=list[0,1,], keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:12:32.933088 test begin: paddle.linalg.norm(x=Tensor([0, 3, 3],"float64"), axis=list[1,2,], p=1, )

[paddle error] paddle.linalg.norm(x=Tensor([0, 3, 3],"float64"), axis=list[1,2,], p=1, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:12:33.186775 test begin: paddle.linalg.norm(x=Tensor([0, 3, 3],"float64"), axis=list[1,2,], p=math.inf, )

[paddle error] paddle.linalg.norm(x=Tensor([0, 3, 3],"float64"), axis=list[1,2,], p=math.inf, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:12:40.171547 test begin: paddle.linalg.norm(x=Tensor([2, 3, 0],"float64"), p=-math.inf, axis=list[0,1,], keepdim=False, )

[paddle error] paddle.linalg.norm(x=Tensor([2, 3, 0],"float64"), p=-math.inf, axis=list[0,1,], keepdim=False, ) 
 (InvalidArgument) Zero-size tensor to reduction operation minimum which has no identity.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/reduce_min_kernel.cc:30)

2025-04-21 10:12:40.406896 test begin: paddle.linalg.norm(x=Tensor([2, 3, 0],"float64"), p=-math.inf, axis=list[0,1,], keepdim=True, )

[paddle error] paddle.linalg.norm(x=Tensor([2, 3, 0],"float64"), p=-math.inf, axis=list[0,1,], keepdim=True, ) 
 (InvalidArgument) Zero-size tensor to reduction operation minimum which has no identity.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/reduce_min_kernel.cc:30)

2025-04-21 10:12:40.590300 test begin: paddle.linalg.norm(x=Tensor([2, 3, 0],"float64"), p=1, axis=list[0,1,], keepdim=False, )

[paddle error] paddle.linalg.norm(x=Tensor([2, 3, 0],"float64"), p=1, axis=list[0,1,], keepdim=False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:12:40.790630 test begin: paddle.linalg.norm(x=Tensor([2, 3, 0],"float64"), p=1, axis=list[0,1,], keepdim=True, )

[paddle error] paddle.linalg.norm(x=Tensor([2, 3, 0],"float64"), p=1, axis=list[0,1,], keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:12:41.413775 test begin: paddle.linalg.norm(x=Tensor([2, 3, 0],"float64"), p=2, axis=list[0,1,], keepdim=False, )

[paddle error] paddle.linalg.norm(x=Tensor([2, 3, 0],"float64"), p=2, axis=list[0,1,], keepdim=False, ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:12:41.620362 test begin: paddle.linalg.norm(x=Tensor([2, 3, 0],"float64"), p=2, axis=list[0,1,], keepdim=True, )

[paddle error] paddle.linalg.norm(x=Tensor([2, 3, 0],"float64"), p=2, axis=list[0,1,], keepdim=True, ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:12:41.807246 test begin: paddle.linalg.norm(x=Tensor([2, 3, 0],"float64"), p=math.inf, axis=list[0,1,], keepdim=False, )

[paddle error] paddle.linalg.norm(x=Tensor([2, 3, 0],"float64"), p=math.inf, axis=list[0,1,], keepdim=False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:12:42.023684 test begin: paddle.linalg.norm(x=Tensor([2, 3, 0],"float64"), p=math.inf, axis=list[0,1,], keepdim=True, )

[paddle error] paddle.linalg.norm(x=Tensor([2, 3, 0],"float64"), p=math.inf, axis=list[0,1,], keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:12:42.609151 test begin: paddle.linalg.norm(x=Tensor([3, 0, 3],"float64"), axis=list[0,2,], p=1, )

[paddle error] paddle.linalg.norm(x=Tensor([3, 0, 3],"float64"), axis=list[0,2,], p=1, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:12:42.784251 test begin: paddle.linalg.norm(x=Tensor([3, 0, 3],"float64"), axis=list[0,2,], p=2, )

[paddle error] paddle.linalg.norm(x=Tensor([3, 0, 3],"float64"), axis=list[0,2,], p=2, ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:12:45.897492 test begin: paddle.linalg.pinv(Tensor([0, 200, 300],"float64"), rcond=1e-15, hermitian=False, )

[paddle error] paddle.linalg.pinv(Tensor([0, 200, 300],"float64"), rcond=1e-15, hermitian=False, ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:12:46.044616 test begin: paddle.linalg.pinv(Tensor([0, 4, 5],"float64"), rcond=1e-15, hermitian=False, )

[paddle error] paddle.linalg.pinv(Tensor([0, 4, 5],"float64"), rcond=1e-15, hermitian=False, ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:12:46.241815 test begin: paddle.linalg.pinv(Tensor([0, 4],"float64"), rcond=1e-15, hermitian=False, )

[paddle error] paddle.linalg.pinv(Tensor([0, 4],"float64"), rcond=1e-15, hermitian=False, ) 
 (InvalidArgument) The row of Input(X) should be greater than 0.
  [Hint: Expected 0 < m, but received 0:0 >= m:0.] (at ../paddle/phi/kernels/gpu/svd_kernel.cu:220)

2025-04-21 10:12:46.447472 test begin: paddle.linalg.pinv(Tensor([0, 5, 5],"float64"), rcond=1e-10, hermitian=False, )

[paddle error] paddle.linalg.pinv(Tensor([0, 5, 5],"float64"), rcond=1e-10, hermitian=False, ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:12:46.655016 test begin: paddle.linalg.pinv(Tensor([0, 5],"float64"), rcond=1e-15, hermitian=False, )

[paddle error] paddle.linalg.pinv(Tensor([0, 5],"float64"), rcond=1e-15, hermitian=False, ) 
 (InvalidArgument) The row of Input(X) should be greater than 0.
  [Hint: Expected 0 < m, but received 0:0 >= m:0.] (at ../paddle/phi/kernels/gpu/svd_kernel.cu:220)

2025-04-21 10:12:46.893841 test begin: paddle.linalg.pinv(Tensor([0, 6, 5, 4],"float64"), rcond=1e-15, hermitian=False, )

[paddle error] paddle.linalg.pinv(Tensor([0, 6, 5, 4],"float64"), rcond=1e-15, hermitian=False, ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:12:47.119923 test begin: paddle.linalg.pinv(Tensor([2, 0, 300],"float64"), rcond=1e-15, hermitian=False, )

[paddle error] paddle.linalg.pinv(Tensor([2, 0, 300],"float64"), rcond=1e-15, hermitian=False, ) 
 (InvalidArgument) The row of Input(X) should be greater than 0.
  [Hint: Expected 0 < m, but received 0:0 >= m:0.] (at ../paddle/phi/kernels/gpu/svd_kernel.cu:220)

2025-04-21 10:12:47.327092 test begin: paddle.linalg.pinv(Tensor([2, 200, 0],"float64"), rcond=1e-15, hermitian=False, )

[paddle error] paddle.linalg.pinv(Tensor([2, 200, 0],"float64"), rcond=1e-15, hermitian=False, ) 
 (InvalidArgument) The col of Input(X) should be greater than 0.
  [Hint: Expected 0 < n, but received 0:0 >= n:0.] (at ../paddle/phi/kernels/gpu/svd_kernel.cu:224)

2025-04-21 10:12:47.566394 test begin: paddle.linalg.pinv(Tensor([3, 0, 5, 4],"float64"), rcond=1e-15, hermitian=False, )

[paddle error] paddle.linalg.pinv(Tensor([3, 0, 5, 4],"float64"), rcond=1e-15, hermitian=False, ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:12:47.772550 test begin: paddle.linalg.pinv(Tensor([3, 0, 5],"float64"), rcond=1e-10, hermitian=False, )

[paddle error] paddle.linalg.pinv(Tensor([3, 0, 5],"float64"), rcond=1e-10, hermitian=False, ) 
 (InvalidArgument) The row of Input(X) should be greater than 0.
  [Hint: Expected 0 < m, but received 0:0 >= m:0.] (at ../paddle/phi/kernels/gpu/svd_kernel.cu:220)

2025-04-21 10:12:47.993292 test begin: paddle.linalg.pinv(Tensor([3, 0, 5],"float64"), rcond=1e-15, hermitian=False, )

[paddle error] paddle.linalg.pinv(Tensor([3, 0, 5],"float64"), rcond=1e-15, hermitian=False, ) 
 (InvalidArgument) The row of Input(X) should be greater than 0.
  [Hint: Expected 0 < m, but received 0:0 >= m:0.] (at ../paddle/phi/kernels/gpu/svd_kernel.cu:220)

2025-04-21 10:12:48.218817 test begin: paddle.linalg.pinv(Tensor([3, 4, 0],"float64"), rcond=1e-15, hermitian=False, )

[paddle error] paddle.linalg.pinv(Tensor([3, 4, 0],"float64"), rcond=1e-15, hermitian=False, ) 
 (InvalidArgument) The col of Input(X) should be greater than 0.
  [Hint: Expected 0 < n, but received 0:0 >= n:0.] (at ../paddle/phi/kernels/gpu/svd_kernel.cu:224)

2025-04-21 10:12:48.457299 test begin: paddle.linalg.pinv(Tensor([3, 5, 0],"float64"), rcond=1e-10, hermitian=False, )

[paddle error] paddle.linalg.pinv(Tensor([3, 5, 0],"float64"), rcond=1e-10, hermitian=False, ) 
 (InvalidArgument) The col of Input(X) should be greater than 0.
  [Hint: Expected 0 < n, but received 0:0 >= n:0.] (at ../paddle/phi/kernels/gpu/svd_kernel.cu:224)

2025-04-21 10:12:48.659725 test begin: paddle.linalg.pinv(Tensor([3, 6, 0, 4],"float64"), rcond=1e-15, hermitian=False, )

[paddle error] paddle.linalg.pinv(Tensor([3, 6, 0, 4],"float64"), rcond=1e-15, hermitian=False, ) 
 (InvalidArgument) The row of Input(X) should be greater than 0.
  [Hint: Expected 0 < m, but received 0:0 >= m:0.] (at ../paddle/phi/kernels/gpu/svd_kernel.cu:220)

2025-04-21 10:12:48.865489 test begin: paddle.linalg.pinv(Tensor([3, 6, 5, 0],"float64"), rcond=1e-15, hermitian=False, )

[paddle error] paddle.linalg.pinv(Tensor([3, 6, 5, 0],"float64"), rcond=1e-15, hermitian=False, ) 
 (InvalidArgument) The col of Input(X) should be greater than 0.
  [Hint: Expected 0 < n, but received 0:0 >= n:0.] (at ../paddle/phi/kernels/gpu/svd_kernel.cu:224)

2025-04-21 10:12:49.189004 test begin: paddle.linalg.pinv(Tensor([4, 0],"float64"), rcond=1e-15, hermitian=False, )

[paddle error] paddle.linalg.pinv(Tensor([4, 0],"float64"), rcond=1e-15, hermitian=False, ) 
 (InvalidArgument) The col of Input(X) should be greater than 0.
  [Hint: Expected 0 < n, but received 0:0 >= n:0.] (at ../paddle/phi/kernels/gpu/svd_kernel.cu:224)

2025-04-21 10:12:49.354819 test begin: paddle.linalg.pinv(Tensor([5, 0],"float64"), rcond=1e-15, hermitian=False, )

[paddle error] paddle.linalg.pinv(Tensor([5, 0],"float64"), rcond=1e-15, hermitian=False, ) 
 (InvalidArgument) The col of Input(X) should be greater than 0.
  [Hint: Expected 0 < n, but received 0:0 >= n:0.] (at ../paddle/phi/kernels/gpu/svd_kernel.cu:224)

2025-04-21 10:12:49.566038 test begin: paddle.linalg.pinv(x=Tensor([0, 2, 2],"float64"), rcond=5, hermitian=True, )

[paddle error] paddle.linalg.pinv(x=Tensor([0, 2, 2],"float64"), rcond=5, hermitian=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:12:49.770880 test begin: paddle.linalg.pinv(x=Tensor([0, 4, 40],"float64"), )

[paddle error] paddle.linalg.pinv(x=Tensor([0, 4, 40],"float64"), ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:12:50.005855 test begin: paddle.linalg.pinv(x=Tensor([0, 4, 40],"float64"), rcond=0.5, )

[paddle error] paddle.linalg.pinv(x=Tensor([0, 4, 40],"float64"), rcond=0.5, ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:12:50.252758 test begin: paddle.linalg.pinv(x=Tensor([0, 40],"float64"), )

[paddle error] paddle.linalg.pinv(x=Tensor([0, 40],"float64"), ) 
 (InvalidArgument) The row of Input(X) should be greater than 0.
  [Hint: Expected 0 < m, but received 0:0 >= m:0.] (at ../paddle/phi/kernels/gpu/svd_kernel.cu:220)

2025-04-21 10:12:50.461240 test begin: paddle.linalg.pinv(x=Tensor([0, 4],"float32"), )

[paddle error] paddle.linalg.pinv(x=Tensor([0, 4],"float32"), ) 
 (InvalidArgument) The row of Input(X) should be greater than 0.
  [Hint: Expected 0 < m, but received 0:0 >= m:0.] (at ../paddle/phi/kernels/gpu/svd_kernel.cu:220)

2025-04-21 10:12:50.613094 test begin: paddle.linalg.pinv(x=Tensor([2, 0, 40],"float64"), )

[paddle error] paddle.linalg.pinv(x=Tensor([2, 0, 40],"float64"), ) 
 (InvalidArgument) The row of Input(X) should be greater than 0.
  [Hint: Expected 0 < m, but received 0:0 >= m:0.] (at ../paddle/phi/kernels/gpu/svd_kernel.cu:220)

2025-04-21 10:12:50.779186 test begin: paddle.linalg.pinv(x=Tensor([2, 0, 40],"float64"), rcond=0.5, )

[paddle error] paddle.linalg.pinv(x=Tensor([2, 0, 40],"float64"), rcond=0.5, ) 
 (InvalidArgument) The row of Input(X) should be greater than 0.
  [Hint: Expected 0 < m, but received 0:0 >= m:0.] (at ../paddle/phi/kernels/gpu/svd_kernel.cu:220)

2025-04-21 10:12:51.002022 test begin: paddle.linalg.pinv(x=Tensor([2, 0],"float64"), )

[paddle error] paddle.linalg.pinv(x=Tensor([2, 0],"float64"), ) 
 (InvalidArgument) The col of Input(X) should be greater than 0.
  [Hint: Expected 0 < n, but received 0:0 >= n:0.] (at ../paddle/phi/kernels/gpu/svd_kernel.cu:224)

2025-04-21 10:12:51.223863 test begin: paddle.linalg.pinv(x=Tensor([2, 4, 0],"float64"), )

[paddle error] paddle.linalg.pinv(x=Tensor([2, 4, 0],"float64"), ) 
 (InvalidArgument) The col of Input(X) should be greater than 0.
  [Hint: Expected 0 < n, but received 0:0 >= n:0.] (at ../paddle/phi/kernels/gpu/svd_kernel.cu:224)

2025-04-21 10:12:51.415966 test begin: paddle.linalg.pinv(x=Tensor([2, 4, 0],"float64"), rcond=0.5, )

[paddle error] paddle.linalg.pinv(x=Tensor([2, 4, 0],"float64"), rcond=0.5, ) 
 (InvalidArgument) The col of Input(X) should be greater than 0.
  [Hint: Expected 0 < n, but received 0:0 >= n:0.] (at ../paddle/phi/kernels/gpu/svd_kernel.cu:224)

2025-04-21 10:12:51.592990 test begin: paddle.linalg.pinv(x=Tensor([3, 0],"float32"), )

[paddle error] paddle.linalg.pinv(x=Tensor([3, 0],"float32"), ) 
 (InvalidArgument) The col of Input(X) should be greater than 0.
  [Hint: Expected 0 < n, but received 0:0 >= n:0.] (at ../paddle/phi/kernels/gpu/svd_kernel.cu:224)

2025-04-21 10:12:51.737513 test begin: paddle.linalg.pinv(x=Tensor([4, 0, 2],"float64"), rcond=5, hermitian=True, )

[paddle error] paddle.linalg.pinv(x=Tensor([4, 0, 2],"float64"), rcond=5, hermitian=True, ) 
 (InvalidArgument) Eigh op is designed for square matrix, consequentlyinner-most 2 dimensions of Input(X) should be symmetric.But received X's shape[-2] = 0 and shape[-1] = 2.
  [Hint: Expected input_dim[rank - 2] == input_dim[rank - 1], but received input_dim[rank - 2]:0 != input_dim[rank - 1]:2.] (at ../paddle/phi/infermeta/unary.cc:1151)

2025-04-21 10:12:51.947186 test begin: paddle.linalg.pinv(x=Tensor([4, 2, 0],"float64"), rcond=5, hermitian=True, )

[paddle error] paddle.linalg.pinv(x=Tensor([4, 2, 0],"float64"), rcond=5, hermitian=True, ) 
 (InvalidArgument) Eigh op is designed for square matrix, consequentlyinner-most 2 dimensions of Input(X) should be symmetric.But received X's shape[-2] = 2 and shape[-1] = 0.
  [Hint: Expected input_dim[rank - 2] == input_dim[rank - 1], but received input_dim[rank - 2]:2 != input_dim[rank - 1]:0.] (at ../paddle/phi/infermeta/unary.cc:1151)

2025-04-21 10:12:52.161231 test begin: paddle.linalg.solve(Tensor([0, 3, 3],"float64"), Tensor([0, 3, 3],"float64"), )

W0421 10:12:52.352577 118549 backward.cc:437] While running Node (SolveGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.solve(Tensor([0, 3, 3],"float64"), Tensor([0, 3, 3],"float64"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:12:52.352990 test begin: paddle.linalg.solve(Tensor([0, 3, 3],"float64"), Tensor([0, 3, 3],"float64"), left=False, )

W0421 10:12:52.577221 118634 backward.cc:437] While running Node (SolveGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.solve(Tensor([0, 3, 3],"float64"), Tensor([0, 3, 3],"float64"), left=False, ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:12:52.577629 test begin: paddle.linalg.solve(Tensor([0, 3, 3],"float64"), Tensor([1, 3, 3],"float64"), )

W0421 10:12:52.821650 118720 backward.cc:437] While running Node (SolveGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.solve(Tensor([0, 3, 3],"float64"), Tensor([1, 3, 3],"float64"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:12:52.822057 test begin: paddle.linalg.solve(Tensor([0, 3, 3],"float64"), Tensor([1, 3, 3],"float64"), left=False, )

W0421 10:12:53.111814 118815 backward.cc:437] While running Node (SolveGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.solve(Tensor([0, 3, 3],"float64"), Tensor([1, 3, 3],"float64"), left=False, ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:12:53.122444 test begin: paddle.linalg.solve(Tensor([10, 10],"float32"), Tensor([0, 10],"float32"), left=False, )

W0421 10:12:53.385434 118918 backward.cc:437] While running Node (SolveGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.solve(Tensor([10, 10],"float32"), Tensor([0, 10],"float32"), left=False, ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:12:53.389147 test begin: paddle.linalg.solve(Tensor([10, 10],"float32"), Tensor([10, 0],"float32"), )

W0421 10:12:53.555011 119072 backward.cc:437] While running Node (SolveGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.solve(Tensor([10, 10],"float32"), Tensor([10, 0],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:12:53.557425 test begin: paddle.linalg.solve(Tensor([10, 10],"float64"), Tensor([0, 10],"float64"), left=False, )

W0421 10:12:53.765379 119241 backward.cc:437] While running Node (SolveGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.solve(Tensor([10, 10],"float64"), Tensor([0, 10],"float64"), left=False, ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:12:53.776367 test begin: paddle.linalg.solve(Tensor([10, 10],"float64"), Tensor([10, 0],"float64"), )

W0421 10:12:54.022765 119387 backward.cc:437] While running Node (SolveGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.solve(Tensor([10, 10],"float64"), Tensor([10, 0],"float64"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:12:54.036698 test begin: paddle.linalg.solve(Tensor([2, 3, 3],"float64"), Tensor([1, 0, 3],"float64"), left=False, )

W0421 10:12:54.245328 119479 backward.cc:437] While running Node (SolveGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.solve(Tensor([2, 3, 3],"float64"), Tensor([1, 0, 3],"float64"), left=False, ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:12:54.245780 test begin: paddle.linalg.solve(Tensor([2, 3, 3],"float64"), Tensor([1, 3, 0],"float64"), )

W0421 10:12:54.455317 119486 backward.cc:437] While running Node (SolveGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.solve(Tensor([2, 3, 3],"float64"), Tensor([1, 3, 0],"float64"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:12:54.460061 test begin: paddle.linalg.solve(x=Tensor([0, 14, 14],"float64"), y=Tensor([0, 14, 2],"float64"), )

W0421 10:12:54.655956 119497 backward.cc:437] While running Node (SolveGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.solve(x=Tensor([0, 14, 14],"float64"), y=Tensor([0, 14, 2],"float64"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:12:54.690391 test begin: paddle.linalg.solve(x=Tensor([14, 14],"float64"), y=Tensor([14, 0],"float64"), )

W0421 10:12:54.893924 119525 backward.cc:437] While running Node (SolveGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.solve(x=Tensor([14, 14],"float64"), y=Tensor([14, 0],"float64"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:12:54.902105 test begin: paddle.linalg.solve(x=Tensor([4, 14, 14],"float64"), y=Tensor([4, 14, 0],"float64"), )

W0421 10:12:55.123639 119549 backward.cc:437] While running Node (SolveGradNode) raises an EnforceNotMet exception
[paddle error] paddle.linalg.solve(x=Tensor([4, 14, 14],"float64"), y=Tensor([4, 14, 0],"float64"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:12:55.126320 test begin: paddle.linalg.svd_lowrank(Tensor([0, 17],"float64"), q=4, )

[paddle error] paddle.linalg.svd_lowrank(Tensor([0, 17],"float64"), q=4, ) 
 q(=4) must be non-negative integer and not greater than min(m, n)=0
2025-04-21 10:12:55.375713 test begin: paddle.linalg.svd_lowrank(Tensor([0, 4, 17],"float64"), q=4, )

[paddle error] paddle.linalg.svd_lowrank(Tensor([0, 4, 17],"float64"), q=4, ) 
 ([0, 4, 17, 4, 17, 4, 17, 4], 4)
2025-04-21 10:12:55.640329 test begin: paddle.linalg.svd_lowrank(Tensor([1, 0, 17],"float64"), q=4, )

[paddle error] paddle.linalg.svd_lowrank(Tensor([1, 0, 17],"float64"), q=4, ) 
 q(=4) must be non-negative integer and not greater than min(m, n)=0
2025-04-21 10:12:55.835244 test begin: paddle.linalg.svd_lowrank(Tensor([1, 4, 0],"float64"), q=4, )

[paddle error] paddle.linalg.svd_lowrank(Tensor([1, 4, 0],"float64"), q=4, ) 
 q(=4) must be non-negative integer and not greater than min(m, n)=0
2025-04-21 10:12:56.095151 test begin: paddle.linalg.svd_lowrank(Tensor([4, 0],"float64"), q=4, )

[paddle error] paddle.linalg.svd_lowrank(Tensor([4, 0],"float64"), q=4, ) 
 q(=4) must be non-negative integer and not greater than min(m, n)=0
2025-04-21 10:13:07.219277 test begin: paddle.linalg.vector_norm(x=Tensor([2, 3, 0],"float64"), p=math.inf, axis=list[0,1,], keepdim=False, )

[paddle error] paddle.linalg.vector_norm(x=Tensor([2, 3, 0],"float64"), p=math.inf, axis=list[0,1,], keepdim=False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:13:07.360058 test begin: paddle.linalg.vector_norm(x=Tensor([2, 3, 0],"float64"), p=math.inf, axis=list[0,1,], keepdim=True, )

[paddle error] paddle.linalg.vector_norm(x=Tensor([2, 3, 0],"float64"), p=math.inf, axis=list[0,1,], keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:13:30.468768 test begin: paddle.logcumsumexp(Tensor([0, 10, 10],"float32"), axis=-1, )

[cuda error] paddle.logcumsumexp(Tensor([0, 10, 10],"float32"), axis=-1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:30.670444 test begin: paddle.logcumsumexp(Tensor([0, 10, 10],"float32"), axis=0, )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_logcumsumexp(_object*, _object*, _object*)
1   logcumsumexp_ad_func(paddle::Tensor const&, int, bool, bool, bool)
2   paddle::experimental::logcumsumexp(paddle::Tensor const&, int, bool, bool, bool)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201610 (unix time) try "date -d @1745201610" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fa5e85b51ea) received by PID 154340 (TID 0x7fa4de7c3700) from PID 18446744073312883178 ***]

2025-04-21 10:14:11.733905 test begin: paddle.logcumsumexp(Tensor([0, 4],"float32"), axis=-1, )

W0421 10:14:17.561391 146614 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:14:17.562623 146614 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.logcumsumexp(Tensor([0, 4],"float32"), axis=-1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:14:17.563710 test begin: paddle.logcumsumexp(Tensor([0, 4],"float32"), axis=-2, )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_logcumsumexp(_object*, _object*, _object*)
1   logcumsumexp_ad_func(paddle::Tensor const&, int, bool, bool, bool)
2   paddle::experimental::logcumsumexp(paddle::Tensor const&, int, bool, bool, bool)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201657 (unix time) try "date -d @1745201657" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f87febdd1ea) received by PID 144966 (TID 0x7f86ed34a700) from PID 18446744073688437226 ***]

2025-04-21 10:14:35.687504 test begin: paddle.logcumsumexp(Tensor([0, 4],"float32"), axis=0, )

W0421 10:14:43.900722 154715 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:14:43.901829 154715 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_logcumsumexp(_object*, _object*, _object*)
1   logcumsumexp_ad_func(paddle::Tensor const&, int, bool, bool, bool)
2   paddle::experimental::logcumsumexp(paddle::Tensor const&, int, bool, bool, bool)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201683 (unix time) try "date -d @1745201683" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f5d98e261ea) received by PID 153210 (TID 0x7f5cd94f4700) from PID 18446744071979557354 ***]

2025-04-21 10:15:01.901782 test begin: paddle.logcumsumexp(Tensor([10, 0, 10],"float32"), axis=-1, )

W0421 10:15:07.642481   475 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:15:07.643640   475 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.logcumsumexp(Tensor([10, 0, 10],"float32"), axis=-1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:15:07.644777 test begin: paddle.logcumsumexp(Tensor([10, 0, 10],"float32"), axis=0, )

[cuda error] paddle.logcumsumexp(Tensor([10, 0, 10],"float32"), axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:15:07.758725 test begin: paddle.logcumsumexp(Tensor([10, 10, 0],"float32"), axis=-1, )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_logcumsumexp(_object*, _object*, _object*)
1   logcumsumexp_ad_func(paddle::Tensor const&, int, bool, bool, bool)
2   paddle::experimental::logcumsumexp(paddle::Tensor const&, int, bool, bool, bool)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201707 (unix time) try "date -d @1745201707" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7efba58285c4) received by PID 162542 (TID 0x7efaaff48700) from PID 18446744072191378884 ***]

2025-04-21 10:15:12.009492 test begin: paddle.logcumsumexp(Tensor([10, 10, 0],"float32"), axis=0, )

W0421 10:15:18.143231  4618 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:15:18.144330  4618 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.logcumsumexp(Tensor([10, 10, 0],"float32"), axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:15:18.145724 test begin: paddle.logcumsumexp(Tensor([3, 0],"float32"), axis=-1, )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_logcumsumexp(_object*, _object*, _object*)
1   logcumsumexp_ad_func(paddle::Tensor const&, int, bool, bool, bool)
2   paddle::experimental::logcumsumexp(paddle::Tensor const&, int, bool, bool, bool)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201718 (unix time) try "date -d @1745201718" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f95dfe005c4) received by PID 2435 (TID 0x7f94d1dc2700) from PID 18446744073170585028 ***]

2025-04-21 10:15:22.431210 test begin: paddle.logcumsumexp(Tensor([3, 0],"float32"), axis=-2, )

W0421 10:15:28.722491  8748 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:15:28.723593  8748 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.logcumsumexp(Tensor([3, 0],"float32"), axis=-2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:15:28.724785 test begin: paddle.logcumsumexp(Tensor([3, 0],"float32"), axis=0, )

[cuda error] paddle.logcumsumexp(Tensor([3, 0],"float32"), axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:15:55.943011 test begin: paddle.logsumexp(Tensor([0, 16, 4, 8],"float32"), axis=3, keepdim=True, )

[paddle error] paddle.logsumexp(Tensor([0, 16, 4, 8],"float32"), axis=3, keepdim=True, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:15:56.213399 test begin: paddle.logsumexp(Tensor([0, 200, 40],"float32"), axis=-1, keepdim=False, )

[paddle error] paddle.logsumexp(Tensor([0, 200, 40],"float32"), axis=-1, keepdim=False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:15:56.469270 test begin: paddle.logsumexp(Tensor([0, 200, 40],"float32"), axis=list[0,2,], keepdim=False, )

[paddle error] paddle.logsumexp(Tensor([0, 200, 40],"float32"), axis=list[0,2,], keepdim=False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:15:56.684842 test begin: paddle.logsumexp(Tensor([0, 3, 4, 5],"float16"), list[-1,], False, )

[paddle error] paddle.logsumexp(Tensor([0, 3, 4, 5],"float16"), list[-1,], False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:15:56.872716 test begin: paddle.logsumexp(Tensor([0, 3, 4, 5],"float32"), 2, False, )

[paddle error] paddle.logsumexp(Tensor([0, 3, 4, 5],"float32"), 2, False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:15:57.034324 test begin: paddle.logsumexp(Tensor([0, 3, 4, 5],"float32"), list[-1,], False, )

[paddle error] paddle.logsumexp(Tensor([0, 3, 4, 5],"float32"), list[-1,], False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:15:57.231049 test begin: paddle.logsumexp(Tensor([0, 3, 4, 5],"float32"), list[2,-3,], False, )

[paddle error] paddle.logsumexp(Tensor([0, 3, 4, 5],"float32"), list[2,-3,], False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:15:57.445423 test begin: paddle.logsumexp(Tensor([0, 3, 4, 5],"float32"), tuple(0,1,-1,), False, )

[paddle error] paddle.logsumexp(Tensor([0, 3, 4, 5],"float32"), tuple(0,1,-1,), False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:15:57.696836 test begin: paddle.logsumexp(Tensor([0, 3, 4, 5],"float64"), list[-1,], True, )

[paddle error] paddle.logsumexp(Tensor([0, 3, 4, 5],"float64"), list[-1,], True, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:15:57.854257 test begin: paddle.logsumexp(Tensor([0, 3, 4, 5],"float64"), list[0,-1,], False, )

[paddle error] paddle.logsumexp(Tensor([0, 3, 4, 5],"float64"), list[0,-1,], False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:15:58.043688 test begin: paddle.logsumexp(Tensor([0, 3, 4, 5],"float64"), list[0,1,2,3,], False, )

[paddle error] paddle.logsumexp(Tensor([0, 3, 4, 5],"float64"), list[0,1,2,3,], False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:15:58.186440 test begin: paddle.logsumexp(Tensor([0, 4, 16, 1],"float32"), axis=1, keepdim=True, )

[paddle error] paddle.logsumexp(Tensor([0, 4, 16, 1],"float32"), axis=1, keepdim=True, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:15:58.387861 test begin: paddle.logsumexp(Tensor([0, 5, 6],"float64"), list[-1,], False, )

[paddle error] paddle.logsumexp(Tensor([0, 5, 6],"float64"), list[-1,], False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:15:58.600463 test begin: paddle.logsumexp(Tensor([0, 60],"float32"), axis=1, )

[paddle error] paddle.logsumexp(Tensor([0, 60],"float32"), axis=1, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:15:58.794474 test begin: paddle.logsumexp(Tensor([0, 8, 4, 8],"float32"), axis=3, keepdim=True, )

[paddle error] paddle.logsumexp(Tensor([0, 8, 4, 8],"float32"), axis=3, keepdim=True, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:15:59.024460 test begin: paddle.logsumexp(Tensor([0],"float32"), axis=0, )

[paddle error] paddle.logsumexp(Tensor([0],"float32"), axis=0, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:15:59.243741 test begin: paddle.logsumexp(Tensor([10, 0],"float32"), axis=1, )

[paddle error] paddle.logsumexp(Tensor([10, 0],"float32"), axis=1, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:15:59.499074 test begin: paddle.logsumexp(Tensor([2, 0, 4, 5],"float16"), list[-1,], False, )

[paddle error] paddle.logsumexp(Tensor([2, 0, 4, 5],"float16"), list[-1,], False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:15:59.783913 test begin: paddle.logsumexp(Tensor([2, 0, 4, 5],"float32"), 2, False, )

[paddle error] paddle.logsumexp(Tensor([2, 0, 4, 5],"float32"), 2, False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:00.042836 test begin: paddle.logsumexp(Tensor([2, 0, 4, 5],"float32"), list[-1,], False, )

[paddle error] paddle.logsumexp(Tensor([2, 0, 4, 5],"float32"), list[-1,], False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:00.223538 test begin: paddle.logsumexp(Tensor([2, 0, 4, 5],"float32"), list[2,-3,], False, )

[paddle error] paddle.logsumexp(Tensor([2, 0, 4, 5],"float32"), list[2,-3,], False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:00.468156 test begin: paddle.logsumexp(Tensor([2, 0, 4, 5],"float32"), tuple(0,1,-1,), False, )

[paddle error] paddle.logsumexp(Tensor([2, 0, 4, 5],"float32"), tuple(0,1,-1,), False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:00.727674 test begin: paddle.logsumexp(Tensor([2, 0, 4, 5],"float64"), list[-1,], True, )

[paddle error] paddle.logsumexp(Tensor([2, 0, 4, 5],"float64"), list[-1,], True, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:00.893126 test begin: paddle.logsumexp(Tensor([2, 0, 4, 5],"float64"), list[0,-1,], False, )

[paddle error] paddle.logsumexp(Tensor([2, 0, 4, 5],"float64"), list[0,-1,], False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:01.062708 test begin: paddle.logsumexp(Tensor([2, 0, 4, 5],"float64"), list[0,1,2,3,], False, )

[paddle error] paddle.logsumexp(Tensor([2, 0, 4, 5],"float64"), list[0,1,2,3,], False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:01.212129 test begin: paddle.logsumexp(Tensor([2, 0],"float32"), axis=1, )

[paddle error] paddle.logsumexp(Tensor([2, 0],"float32"), axis=1, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:01.343482 test begin: paddle.logsumexp(Tensor([2, 3, 0, 5],"float16"), list[-1,], False, )

[paddle error] paddle.logsumexp(Tensor([2, 3, 0, 5],"float16"), list[-1,], False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:01.536381 test begin: paddle.logsumexp(Tensor([2, 3, 0, 5],"float32"), 2, False, )

[paddle error] paddle.logsumexp(Tensor([2, 3, 0, 5],"float32"), 2, False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:01.696953 test begin: paddle.logsumexp(Tensor([2, 3, 0, 5],"float32"), list[-1,], False, )

[paddle error] paddle.logsumexp(Tensor([2, 3, 0, 5],"float32"), list[-1,], False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:01.848022 test begin: paddle.logsumexp(Tensor([2, 3, 0, 5],"float32"), list[2,-3,], False, )

[paddle error] paddle.logsumexp(Tensor([2, 3, 0, 5],"float32"), list[2,-3,], False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:02.088408 test begin: paddle.logsumexp(Tensor([2, 3, 0, 5],"float32"), tuple(0,1,-1,), False, )

[paddle error] paddle.logsumexp(Tensor([2, 3, 0, 5],"float32"), tuple(0,1,-1,), False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:02.230383 test begin: paddle.logsumexp(Tensor([2, 3, 0, 5],"float64"), list[-1,], True, )

[paddle error] paddle.logsumexp(Tensor([2, 3, 0, 5],"float64"), list[-1,], True, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:02.384599 test begin: paddle.logsumexp(Tensor([2, 3, 0, 5],"float64"), list[0,-1,], False, )

[paddle error] paddle.logsumexp(Tensor([2, 3, 0, 5],"float64"), list[0,-1,], False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:02.521780 test begin: paddle.logsumexp(Tensor([2, 3, 0, 5],"float64"), list[0,1,2,3,], False, )

[paddle error] paddle.logsumexp(Tensor([2, 3, 0, 5],"float64"), list[0,1,2,3,], False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:02.654534 test begin: paddle.logsumexp(Tensor([2, 3, 4, 0],"float16"), list[-1,], False, )

[paddle error] paddle.logsumexp(Tensor([2, 3, 4, 0],"float16"), list[-1,], False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:02.805470 test begin: paddle.logsumexp(Tensor([2, 3, 4, 0],"float32"), 2, False, )

[paddle error] paddle.logsumexp(Tensor([2, 3, 4, 0],"float32"), 2, False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:02.988638 test begin: paddle.logsumexp(Tensor([2, 3, 4, 0],"float32"), list[-1,], False, )

[paddle error] paddle.logsumexp(Tensor([2, 3, 4, 0],"float32"), list[-1,], False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:03.164701 test begin: paddle.logsumexp(Tensor([2, 3, 4, 0],"float32"), list[2,-3,], False, )

[paddle error] paddle.logsumexp(Tensor([2, 3, 4, 0],"float32"), list[2,-3,], False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:03.335257 test begin: paddle.logsumexp(Tensor([2, 3, 4, 0],"float32"), tuple(0,1,-1,), False, )

[paddle error] paddle.logsumexp(Tensor([2, 3, 4, 0],"float32"), tuple(0,1,-1,), False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:03.586918 test begin: paddle.logsumexp(Tensor([2, 3, 4, 0],"float64"), list[-1,], True, )

[paddle error] paddle.logsumexp(Tensor([2, 3, 4, 0],"float64"), list[-1,], True, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:03.791179 test begin: paddle.logsumexp(Tensor([2, 3, 4, 0],"float64"), list[0,-1,], False, )

[paddle error] paddle.logsumexp(Tensor([2, 3, 4, 0],"float64"), list[0,-1,], False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:04.010660 test begin: paddle.logsumexp(Tensor([2, 3, 4, 0],"float64"), list[0,1,2,3,], False, )

[paddle error] paddle.logsumexp(Tensor([2, 3, 4, 0],"float64"), list[0,1,2,3,], False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:04.225062 test begin: paddle.logsumexp(Tensor([26, 0, 16, 1],"float32"), axis=1, keepdim=True, )

[paddle error] paddle.logsumexp(Tensor([26, 0, 16, 1],"float32"), axis=1, keepdim=True, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:04.522852 test begin: paddle.logsumexp(Tensor([26, 0, 4, 8],"float32"), axis=3, keepdim=True, )

[paddle error] paddle.logsumexp(Tensor([26, 0, 4, 8],"float32"), axis=3, keepdim=True, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:04.830649 test begin: paddle.logsumexp(Tensor([26, 16, 0, 8],"float32"), axis=3, keepdim=True, )

[paddle error] paddle.logsumexp(Tensor([26, 16, 0, 8],"float32"), axis=3, keepdim=True, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:05.089114 test begin: paddle.logsumexp(Tensor([26, 16, 4, 0],"float32"), axis=3, keepdim=True, )

[paddle error] paddle.logsumexp(Tensor([26, 16, 4, 0],"float32"), axis=3, keepdim=True, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:05.316001 test begin: paddle.logsumexp(Tensor([26, 4, 0, 1],"float32"), axis=1, keepdim=True, )

[paddle error] paddle.logsumexp(Tensor([26, 4, 0, 1],"float32"), axis=1, keepdim=True, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:05.541683 test begin: paddle.logsumexp(Tensor([26, 4, 16, 0],"float32"), axis=1, keepdim=True, )

[paddle error] paddle.logsumexp(Tensor([26, 4, 16, 0],"float32"), axis=1, keepdim=True, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:05.667188 test begin: paddle.logsumexp(Tensor([26, 8, 0, 8],"float32"), axis=3, keepdim=True, )

[paddle error] paddle.logsumexp(Tensor([26, 8, 0, 8],"float32"), axis=3, keepdim=True, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:05.796008 test begin: paddle.logsumexp(Tensor([26, 8, 4, 0],"float32"), axis=3, keepdim=True, )

[paddle error] paddle.logsumexp(Tensor([26, 8, 4, 0],"float32"), axis=3, keepdim=True, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:05.918549 test begin: paddle.logsumexp(Tensor([30, 0, 40],"float32"), axis=-1, keepdim=False, )

[paddle error] paddle.logsumexp(Tensor([30, 0, 40],"float32"), axis=-1, keepdim=False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:06.065740 test begin: paddle.logsumexp(Tensor([30, 0, 40],"float32"), axis=list[0,2,], keepdim=False, )

[paddle error] paddle.logsumexp(Tensor([30, 0, 40],"float32"), axis=list[0,2,], keepdim=False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:06.209305 test begin: paddle.logsumexp(Tensor([30, 200, 0],"float32"), axis=-1, keepdim=False, )

[paddle error] paddle.logsumexp(Tensor([30, 200, 0],"float32"), axis=-1, keepdim=False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:06.397949 test begin: paddle.logsumexp(Tensor([30, 200, 0],"float32"), axis=list[0,2,], keepdim=False, )

[paddle error] paddle.logsumexp(Tensor([30, 200, 0],"float32"), axis=list[0,2,], keepdim=False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:06.516409 test begin: paddle.logsumexp(Tensor([4, 0, 6],"float64"), list[-1,], False, )

[paddle error] paddle.logsumexp(Tensor([4, 0, 6],"float64"), list[-1,], False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:06.739463 test begin: paddle.logsumexp(Tensor([4, 5, 0],"float64"), list[-1,], False, )

[paddle error] paddle.logsumexp(Tensor([4, 5, 0],"float64"), list[-1,], False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:06.966844 test begin: paddle.logsumexp(x=Tensor([0, 3, 2],"float32"), axis=2, )

[paddle error] paddle.logsumexp(x=Tensor([0, 3, 2],"float32"), axis=2, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:07.156584 test begin: paddle.logsumexp(x=Tensor([0, 3, 2],"float64"), axis=0, keepdim=True, )

[paddle error] paddle.logsumexp(x=Tensor([0, 3, 2],"float64"), axis=0, keepdim=True, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:07.294849 test begin: paddle.logsumexp(x=Tensor([0, 3, 2],"float64"), axis=2, )

[paddle error] paddle.logsumexp(x=Tensor([0, 3, 2],"float64"), axis=2, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:07.455508 test begin: paddle.logsumexp(x=Tensor([0, 3, 2],"float64"), axis=list[0,1,], )

[paddle error] paddle.logsumexp(x=Tensor([0, 3, 2],"float64"), axis=list[0,1,], ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:07.637631 test begin: paddle.logsumexp(x=Tensor([0, 3, 2],"float64"), axis=list[0,1,], keepdim=True, )

[paddle error] paddle.logsumexp(x=Tensor([0, 3, 2],"float64"), axis=list[0,1,], keepdim=True, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:07.769890 test begin: paddle.logsumexp(x=Tensor([2, 0, 2],"float32"), axis=2, )

[paddle error] paddle.logsumexp(x=Tensor([2, 0, 2],"float32"), axis=2, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:07.958570 test begin: paddle.logsumexp(x=Tensor([2, 0, 2],"float64"), axis=0, keepdim=True, )

[paddle error] paddle.logsumexp(x=Tensor([2, 0, 2],"float64"), axis=0, keepdim=True, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:08.136973 test begin: paddle.logsumexp(x=Tensor([2, 0, 2],"float64"), axis=2, )

[paddle error] paddle.logsumexp(x=Tensor([2, 0, 2],"float64"), axis=2, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:08.292762 test begin: paddle.logsumexp(x=Tensor([2, 0, 2],"float64"), axis=list[0,1,], )

[paddle error] paddle.logsumexp(x=Tensor([2, 0, 2],"float64"), axis=list[0,1,], ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:08.482566 test begin: paddle.logsumexp(x=Tensor([2, 0, 2],"float64"), axis=list[0,1,], keepdim=True, )

[paddle error] paddle.logsumexp(x=Tensor([2, 0, 2],"float64"), axis=list[0,1,], keepdim=True, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:08.609106 test begin: paddle.logsumexp(x=Tensor([2, 3, 0],"float32"), axis=2, )

[paddle error] paddle.logsumexp(x=Tensor([2, 3, 0],"float32"), axis=2, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:08.798935 test begin: paddle.logsumexp(x=Tensor([2, 3, 0],"float64"), axis=0, keepdim=True, )

[paddle error] paddle.logsumexp(x=Tensor([2, 3, 0],"float64"), axis=0, keepdim=True, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:09.011447 test begin: paddle.logsumexp(x=Tensor([2, 3, 0],"float64"), axis=2, )

[paddle error] paddle.logsumexp(x=Tensor([2, 3, 0],"float64"), axis=2, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:09.286015 test begin: paddle.logsumexp(x=Tensor([2, 3, 0],"float64"), axis=list[0,1,], )

[paddle error] paddle.logsumexp(x=Tensor([2, 3, 0],"float64"), axis=list[0,1,], ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:16:09.534363 test begin: paddle.logsumexp(x=Tensor([2, 3, 0],"float64"), axis=list[0,1,], keepdim=True, )

[paddle error] paddle.logsumexp(x=Tensor([2, 3, 0],"float64"), axis=list[0,1,], keepdim=True, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < xdim[i], but received 0:0 >= xdim[i]:0.] (at ../paddle/phi/kernels/gpu/logsumexp_kernel.cu:99)

2025-04-21 10:07:48.525519 test begin: paddle.mm(Tensor([1, 10],"float32"), Tensor([10, 0],"float32"), )

W0421 10:07:49.992558 11424 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.mm(Tensor([1, 10],"float32"), Tensor([10, 0],"float32"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)

2025-04-21 10:07:49.995697 test begin: paddle.mm(Tensor([1, 10],"float64"), Tensor([10, 0],"float64"), )

W0421 10:07:50.249845 12153 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.mm(Tensor([1, 10],"float64"), Tensor([10, 0],"float64"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:07:50.531523 test begin: paddle.mm(input=Tensor([2, 3],"float32"), mat2=Tensor([3, 0],"float32"), )

W0421 10:07:50.738250 12201 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.mm(input=Tensor([2, 3],"float32"), mat2=Tensor([3, 0],"float32"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)

2025-04-21 10:07:50.741764 test begin: paddle.mm(input=Tensor([2, 3],"float64"), mat2=Tensor([3, 0],"float64"), )

W0421 10:07:50.929077 12284 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.mm(input=Tensor([2, 3],"float64"), mat2=Tensor([3, 0],"float64"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:07:58.833647 test begin: paddle.multigammaln(Tensor([0, 20],"float32"), 2, )

[cuda error] paddle.multigammaln(Tensor([0, 20],"float32"), 2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:59.116831 test begin: paddle.multigammaln(Tensor([0, 20],"float64"), 2, )

[cuda error] paddle.multigammaln(Tensor([0, 20],"float64"), 2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:59.319738 test begin: paddle.multigammaln(Tensor([10, 0],"float32"), 2, )

[cuda error] paddle.multigammaln(Tensor([10, 0],"float32"), 2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:59.529431 test begin: paddle.multigammaln(Tensor([10, 0],"float64"), 2, )

[cuda error] paddle.multigammaln(Tensor([10, 0],"float64"), 2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:59.767877 test begin: paddle.mv(Tensor([0, 12],"float32"), Tensor([12],"float32"), )

[cuda error] paddle.mv(Tensor([0, 12],"float32"), Tensor([12],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:59.907320 test begin: paddle.mv(Tensor([0, 18],"float32"), Tensor([18],"float32"), )

[cuda error] paddle.mv(Tensor([0, 18],"float32"), Tensor([18],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:00.110866 test begin: paddle.mv(x=Tensor([0, 1],"float64"), vec=Tensor([1],"float64"), )

[cuda error] paddle.mv(x=Tensor([0, 1],"float64"), vec=Tensor([1],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:00.319010 test begin: paddle.mv(x=Tensor([0, 2],"float64"), vec=Tensor([2],"float64"), )

[cuda error] paddle.mv(x=Tensor([0, 2],"float64"), vec=Tensor([2],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:00.594661 test begin: paddle.nan_to_num(Tensor([0, 1],"float64"), neginf=-2.220446049250313e-16, )

[cuda error] paddle.nan_to_num(Tensor([0, 1],"float64"), neginf=-2.220446049250313e-16, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:00.868098 test begin: paddle.nan_to_num(Tensor([0, 2],"float32"), neginf=-1.1920928955078125e-07, )

[cuda error] paddle.nan_to_num(Tensor([0, 2],"float32"), neginf=-1.1920928955078125e-07, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:01.131488 test begin: paddle.nan_to_num(Tensor([0, 3],"float32"), )

[cuda error] paddle.nan_to_num(Tensor([0, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:01.436548 test begin: paddle.nan_to_num(Tensor([0, 3],"float32"), 1.0, 100.0, -10.0, )

[cuda error] paddle.nan_to_num(Tensor([0, 3],"float32"), 1.0, 100.0, -10.0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:01.718269 test begin: paddle.nan_to_num(Tensor([0, 3],"float32"), 1.0, 2.0, None, )

[cuda error] paddle.nan_to_num(Tensor([0, 3],"float32"), 1.0, 2.0, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:01.904933 test begin: paddle.nan_to_num(Tensor([0, 3],"float32"), 1.0, None, -10.0, )

[cuda error] paddle.nan_to_num(Tensor([0, 3],"float32"), 1.0, None, -10.0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:02.170706 test begin: paddle.nan_to_num(Tensor([0, 3],"float32"), 1.0, None, None, )

[cuda error] paddle.nan_to_num(Tensor([0, 3],"float32"), 1.0, None, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:02.359314 test begin: paddle.nan_to_num(Tensor([0, 4],"float32"), )

[cuda error] paddle.nan_to_num(Tensor([0, 4],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:02.612761 test begin: paddle.nan_to_num(Tensor([0, 5, 3],"float32"), neginf=-1.1920928955078125e-07, )

[cuda error] paddle.nan_to_num(Tensor([0, 5, 3],"float32"), neginf=-1.1920928955078125e-07, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:02.873103 test begin: paddle.nan_to_num(Tensor([0],"float64"), neginf=-2.220446049250313e-16, )

[cuda error] paddle.nan_to_num(Tensor([0],"float64"), neginf=-2.220446049250313e-16, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:03.057082 test begin: paddle.nan_to_num(Tensor([114, 0],"float64"), neginf=-2.220446049250313e-16, )

[cuda error] paddle.nan_to_num(Tensor([114, 0],"float64"), neginf=-2.220446049250313e-16, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:03.234470 test begin: paddle.nan_to_num(Tensor([148, 0, 3],"float32"), neginf=-1.1920928955078125e-07, )

[cuda error] paddle.nan_to_num(Tensor([148, 0, 3],"float32"), neginf=-1.1920928955078125e-07, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:03.421286 test begin: paddle.nan_to_num(Tensor([148, 5, 0],"float32"), neginf=-1.1920928955078125e-07, )

[cuda error] paddle.nan_to_num(Tensor([148, 5, 0],"float32"), neginf=-1.1920928955078125e-07, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:03.627522 test begin: paddle.nan_to_num(Tensor([1948, 0],"float32"), neginf=-1.1920928955078125e-07, )

[cuda error] paddle.nan_to_num(Tensor([1948, 0],"float32"), neginf=-1.1920928955078125e-07, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:03.887405 test begin: paddle.nan_to_num(Tensor([2, 0],"float32"), )

[cuda error] paddle.nan_to_num(Tensor([2, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:04.091670 test begin: paddle.nan_to_num(Tensor([2, 0],"float32"), 1.0, 100.0, -10.0, )

[cuda error] paddle.nan_to_num(Tensor([2, 0],"float32"), 1.0, 100.0, -10.0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:04.298971 test begin: paddle.nan_to_num(Tensor([2, 0],"float32"), 1.0, 2.0, None, )

[cuda error] paddle.nan_to_num(Tensor([2, 0],"float32"), 1.0, 2.0, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:04.542089 test begin: paddle.nan_to_num(Tensor([2, 0],"float32"), 1.0, None, -10.0, )

[cuda error] paddle.nan_to_num(Tensor([2, 0],"float32"), 1.0, None, -10.0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:04.749145 test begin: paddle.nan_to_num(Tensor([2, 0],"float32"), 1.0, None, None, )

[cuda error] paddle.nan_to_num(Tensor([2, 0],"float32"), 1.0, None, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:05.013041 test begin: paddle.nan_to_num(Tensor([400, 0],"float64"), neginf=-2.220446049250313e-16, )

[cuda error] paddle.nan_to_num(Tensor([400, 0],"float64"), neginf=-2.220446049250313e-16, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:05.222057 test begin: paddle.nanmean(Tensor([0, 3, 4, 5],"float32"), -1, False, )

[cuda error] paddle.nanmean(Tensor([0, 3, 4, 5],"float32"), -1, False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:05.543593 test begin: paddle.nanmean(Tensor([0, 3, 4, 5],"float32"), 2, True, )

[cuda error] paddle.nanmean(Tensor([0, 3, 4, 5],"float32"), 2, True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:05.901142 test begin: paddle.nanmean(Tensor([0, 3, 4, 5],"float32"), None, False, )

 ** On entry to SGEMM  parameter number 8 had an illegal value
 ** On entry to DGEMM  parameter number 8 had an illegal value
 ** On entry to SGEMM  parameter number 8 had an illegal value
 ** On entry to DGEMM  parameter number 8 had an illegal value
[cuda error] paddle.nanmean(Tensor([0, 3, 4, 5],"float32"), None, False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:06.047554 test begin: paddle.nanmean(Tensor([0, 3, 4, 5],"float32"), None, True, )

[cuda error] paddle.nanmean(Tensor([0, 3, 4, 5],"float32"), None, True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:06.174985 test begin: paddle.nanmean(Tensor([0, 3, 4, 5],"float32"), list[0,1,2,3,], False, )

[cuda error] paddle.nanmean(Tensor([0, 3, 4, 5],"float32"), list[0,1,2,3,], False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:06.326147 test begin: paddle.nanmean(Tensor([0, 3, 4, 5],"float32"), list[0,2,], False, )

[cuda error] paddle.nanmean(Tensor([0, 3, 4, 5],"float32"), list[0,2,], False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:06.610244 test begin: paddle.nanmean(Tensor([0, 3, 4, 5],"float32"), list[], False, )

[cuda error] paddle.nanmean(Tensor([0, 3, 4, 5],"float32"), list[], False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:06.843787 test begin: paddle.nanmean(Tensor([0, 3, 4, 5],"float32"), tuple(0,2,), False, )

[cuda error] paddle.nanmean(Tensor([0, 3, 4, 5],"float32"), tuple(0,2,), False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:07.023632 test begin: paddle.nanmean(Tensor([0, 3],"float32"), -1, False, )

[cuda error] paddle.nanmean(Tensor([0, 3],"float32"), -1, False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:07.214408 test begin: paddle.nanmean(Tensor([0, 3],"float32"), 0, True, )

[cuda error] paddle.nanmean(Tensor([0, 3],"float32"), 0, True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:07.354215 test begin: paddle.nanmean(Tensor([0, 3],"float32"), 1, False, )

[cuda error] paddle.nanmean(Tensor([0, 3],"float32"), 1, False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:07.545579 test begin: paddle.nanmean(Tensor([0, 3],"float32"), None, False, )

[cuda error] paddle.nanmean(Tensor([0, 3],"float32"), None, False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:07.788781 test begin: paddle.nanmean(Tensor([0, 3],"float32"), None, True, )

[cuda error] paddle.nanmean(Tensor([0, 3],"float32"), None, True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:07.941509 test begin: paddle.nanmean(Tensor([0, 3],"float32"), tuple(0,1,), False, )

[cuda error] paddle.nanmean(Tensor([0, 3],"float32"), tuple(0,1,), False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:08.161230 test begin: paddle.nanmean(Tensor([0, 5],"float32"), axis=None, )

[cuda error] paddle.nanmean(Tensor([0, 5],"float32"), axis=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:08.368321 test begin: paddle.nanmean(Tensor([0, 5],"float32"), keepdim=True, )

[cuda error] paddle.nanmean(Tensor([0, 5],"float32"), keepdim=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:08.580754 test begin: paddle.nanmean(Tensor([0],"float32"), axis=0, )

[cuda error] paddle.nanmean(Tensor([0],"float32"), axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:08.770330 test begin: paddle.nanmean(Tensor([2, 0, 4, 5],"float32"), -1, False, )

[cuda error] paddle.nanmean(Tensor([2, 0, 4, 5],"float32"), -1, False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:08.987003 test begin: paddle.nanmean(Tensor([2, 0, 4, 5],"float32"), 2, True, )

[cuda error] paddle.nanmean(Tensor([2, 0, 4, 5],"float32"), 2, True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:09.237577 test begin: paddle.nanmean(Tensor([2, 0, 4, 5],"float32"), None, False, )

[cuda error] paddle.nanmean(Tensor([2, 0, 4, 5],"float32"), None, False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:09.379938 test begin: paddle.nanmean(Tensor([2, 0, 4, 5],"float32"), None, True, )

[cuda error] paddle.nanmean(Tensor([2, 0, 4, 5],"float32"), None, True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:09.575338 test begin: paddle.nanmean(Tensor([2, 0, 4, 5],"float32"), list[0,1,2,3,], False, )

[cuda error] paddle.nanmean(Tensor([2, 0, 4, 5],"float32"), list[0,1,2,3,], False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:09.768545 test begin: paddle.nanmean(Tensor([2, 0, 4, 5],"float32"), list[0,2,], False, )

[cuda error] paddle.nanmean(Tensor([2, 0, 4, 5],"float32"), list[0,2,], False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:09.982574 test begin: paddle.nanmean(Tensor([2, 0, 4, 5],"float32"), list[], False, )

[cuda error] paddle.nanmean(Tensor([2, 0, 4, 5],"float32"), list[], False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:10.180089 test begin: paddle.nanmean(Tensor([2, 0, 4, 5],"float32"), tuple(0,2,), False, )

[cuda error] paddle.nanmean(Tensor([2, 0, 4, 5],"float32"), tuple(0,2,), False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:10.365256 test begin: paddle.nanmean(Tensor([2, 0],"float32"), -1, False, )

[cuda error] paddle.nanmean(Tensor([2, 0],"float32"), -1, False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:10.581418 test begin: paddle.nanmean(Tensor([2, 0],"float32"), 0, True, )

[cuda error] paddle.nanmean(Tensor([2, 0],"float32"), 0, True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:10.796247 test begin: paddle.nanmean(Tensor([2, 0],"float32"), 1, False, )

[cuda error] paddle.nanmean(Tensor([2, 0],"float32"), 1, False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:10.979759 test begin: paddle.nanmean(Tensor([2, 0],"float32"), None, False, )

[cuda error] paddle.nanmean(Tensor([2, 0],"float32"), None, False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:11.160122 test begin: paddle.nanmean(Tensor([2, 0],"float32"), None, True, )

[cuda error] paddle.nanmean(Tensor([2, 0],"float32"), None, True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:11.427743 test begin: paddle.nanmean(Tensor([2, 0],"float32"), tuple(0,1,), False, )

[cuda error] paddle.nanmean(Tensor([2, 0],"float32"), tuple(0,1,), False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:11.635078 test begin: paddle.nanmean(Tensor([2, 3, 0, 5],"float32"), -1, False, )

[cuda error] paddle.nanmean(Tensor([2, 3, 0, 5],"float32"), -1, False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:11.836229 test begin: paddle.nanmean(Tensor([2, 3, 0, 5],"float32"), 2, True, )

[cuda error] paddle.nanmean(Tensor([2, 3, 0, 5],"float32"), 2, True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:12.078713 test begin: paddle.nanmean(Tensor([2, 3, 0, 5],"float32"), None, False, )

[cuda error] paddle.nanmean(Tensor([2, 3, 0, 5],"float32"), None, False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:12.278240 test begin: paddle.nanmean(Tensor([2, 3, 0, 5],"float32"), None, True, )

[cuda error] paddle.nanmean(Tensor([2, 3, 0, 5],"float32"), None, True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:12.483973 test begin: paddle.nanmean(Tensor([2, 3, 0, 5],"float32"), list[0,1,2,3,], False, )

[cuda error] paddle.nanmean(Tensor([2, 3, 0, 5],"float32"), list[0,1,2,3,], False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:12.662082 test begin: paddle.nanmean(Tensor([2, 3, 0, 5],"float32"), list[0,2,], False, )

[cuda error] paddle.nanmean(Tensor([2, 3, 0, 5],"float32"), list[0,2,], False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:12.919177 test begin: paddle.nanmean(Tensor([2, 3, 0, 5],"float32"), list[], False, )

[cuda error] paddle.nanmean(Tensor([2, 3, 0, 5],"float32"), list[], False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:13.039302 test begin: paddle.nanmean(Tensor([2, 3, 0, 5],"float32"), tuple(0,2,), False, )

[cuda error] paddle.nanmean(Tensor([2, 3, 0, 5],"float32"), tuple(0,2,), False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:13.129106 test begin: paddle.nanmean(Tensor([2, 3, 4, 0],"float32"), -1, False, )

[cuda error] paddle.nanmean(Tensor([2, 3, 4, 0],"float32"), -1, False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:13.318155 test begin: paddle.nanmean(Tensor([2, 3, 4, 0],"float32"), 2, True, )

[cuda error] paddle.nanmean(Tensor([2, 3, 4, 0],"float32"), 2, True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:13.641255 test begin: paddle.nanmean(Tensor([2, 3, 4, 0],"float32"), None, False, )

[cuda error] paddle.nanmean(Tensor([2, 3, 4, 0],"float32"), None, False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:13.869154 test begin: paddle.nanmean(Tensor([2, 3, 4, 0],"float32"), None, True, )

[cuda error] paddle.nanmean(Tensor([2, 3, 4, 0],"float32"), None, True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:14.137997 test begin: paddle.nanmean(Tensor([2, 3, 4, 0],"float32"), list[0,1,2,3,], False, )

[cuda error] paddle.nanmean(Tensor([2, 3, 4, 0],"float32"), list[0,1,2,3,], False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:14.352604 test begin: paddle.nanmean(Tensor([2, 3, 4, 0],"float32"), list[0,2,], False, )

[cuda error] paddle.nanmean(Tensor([2, 3, 4, 0],"float32"), list[0,2,], False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:14.626975 test begin: paddle.nanmean(Tensor([2, 3, 4, 0],"float32"), list[], False, )

[cuda error] paddle.nanmean(Tensor([2, 3, 4, 0],"float32"), list[], False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:14.912877 test begin: paddle.nanmean(Tensor([2, 3, 4, 0],"float32"), tuple(0,2,), False, )

[cuda error] paddle.nanmean(Tensor([2, 3, 4, 0],"float32"), tuple(0,2,), False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:15.185766 test begin: paddle.nanmean(Tensor([3, 0],"float32"), axis=None, )

[cuda error] paddle.nanmean(Tensor([3, 0],"float32"), axis=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:15.431098 test begin: paddle.nanmean(Tensor([3, 0],"float32"), keepdim=True, )

[cuda error] paddle.nanmean(Tensor([3, 0],"float32"), keepdim=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:15.677152 test begin: paddle.nansum(Tensor([0, 3, 4, 5],"float32"), axis=None, keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([0, 3, 4, 5],"float32"), axis=None, keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:15.880883 test begin: paddle.nansum(Tensor([0, 3, 4, 5],"float32"), axis=None, keepdim=True, name=None, )

[cuda error] paddle.nansum(Tensor([0, 3, 4, 5],"float32"), axis=None, keepdim=True, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:16.086959 test begin: paddle.nansum(Tensor([0, 3, 4, 5],"float32"), axis=list[-1,], keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([0, 3, 4, 5],"float32"), axis=list[-1,], keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:16.252218 test begin: paddle.nansum(Tensor([0, 3, 4, 5],"float32"), axis=list[0,1,2,3,], keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([0, 3, 4, 5],"float32"), axis=list[0,1,2,3,], keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:16.499774 test begin: paddle.nansum(Tensor([0, 3, 4, 5],"float32"), axis=list[0,2,], keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([0, 3, 4, 5],"float32"), axis=list[0,2,], keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:16.640888 test begin: paddle.nansum(Tensor([0, 3, 4, 5],"float32"), axis=list[2,], keepdim=True, name=None, )

[cuda error] paddle.nansum(Tensor([0, 3, 4, 5],"float32"), axis=list[2,], keepdim=True, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:16.832384 test begin: paddle.nansum(Tensor([0, 3, 4, 5],"float32"), axis=list[], keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([0, 3, 4, 5],"float32"), axis=list[], keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:17.099181 test begin: paddle.nansum(Tensor([0, 3, 4, 5],"float32"), axis=tuple(0,2,), keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([0, 3, 4, 5],"float32"), axis=tuple(0,2,), keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:17.299637 test begin: paddle.nansum(Tensor([0, 3],"float32"), axis=None, keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([0, 3],"float32"), axis=None, keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:17.506268 test begin: paddle.nansum(Tensor([0, 3],"float32"), axis=None, keepdim=True, name=None, )

[cuda error] paddle.nansum(Tensor([0, 3],"float32"), axis=None, keepdim=True, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:17.724774 test begin: paddle.nansum(Tensor([0, 3],"float32"), axis=list[-1,], keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([0, 3],"float32"), axis=list[-1,], keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:17.935327 test begin: paddle.nansum(Tensor([0, 3],"float32"), axis=list[0,], keepdim=True, name=None, )

[cuda error] paddle.nansum(Tensor([0, 3],"float32"), axis=list[0,], keepdim=True, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:18.168490 test begin: paddle.nansum(Tensor([0, 3],"float32"), axis=list[1,], keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([0, 3],"float32"), axis=list[1,], keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:18.366714 test begin: paddle.nansum(Tensor([0, 3],"float32"), axis=tuple(0,1,), keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([0, 3],"float32"), axis=tuple(0,1,), keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:18.582484 test begin: paddle.nansum(Tensor([0, 4],"float32"), )

[cuda error] paddle.nansum(Tensor([0, 4],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:18.795609 test begin: paddle.nansum(Tensor([0, 5],"float32"), axis=None, )

[cuda error] paddle.nansum(Tensor([0, 5],"float32"), axis=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:19.011589 test begin: paddle.nansum(Tensor([0, 5],"float32"), axis=None, keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([0, 5],"float32"), axis=None, keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:19.219501 test begin: paddle.nansum(Tensor([0, 5],"float32"), axis=None, keepdim=True, name=None, )

[cuda error] paddle.nansum(Tensor([0, 5],"float32"), axis=None, keepdim=True, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:19.422172 test begin: paddle.nansum(Tensor([0, 5],"float32"), keepdim=True, )

[cuda error] paddle.nansum(Tensor([0, 5],"float32"), keepdim=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:19.700306 test begin: paddle.nansum(Tensor([0],"float32"), axis=0, )

[cuda error] paddle.nansum(Tensor([0],"float32"), axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:19.916029 test begin: paddle.nansum(Tensor([0],"float32"), axis=list[0,], keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([0],"float32"), axis=list[0,], keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:20.159218 test begin: paddle.nansum(Tensor([2, 0, 4, 5],"float32"), axis=None, keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([2, 0, 4, 5],"float32"), axis=None, keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:20.287342 test begin: paddle.nansum(Tensor([2, 0, 4, 5],"float32"), axis=None, keepdim=True, name=None, )

[cuda error] paddle.nansum(Tensor([2, 0, 4, 5],"float32"), axis=None, keepdim=True, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:20.407417 test begin: paddle.nansum(Tensor([2, 0, 4, 5],"float32"), axis=list[-1,], keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([2, 0, 4, 5],"float32"), axis=list[-1,], keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:20.593289 test begin: paddle.nansum(Tensor([2, 0, 4, 5],"float32"), axis=list[0,1,2,3,], keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([2, 0, 4, 5],"float32"), axis=list[0,1,2,3,], keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:20.779948 test begin: paddle.nansum(Tensor([2, 0, 4, 5],"float32"), axis=list[0,2,], keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([2, 0, 4, 5],"float32"), axis=list[0,2,], keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:20.914973 test begin: paddle.nansum(Tensor([2, 0, 4, 5],"float32"), axis=list[2,], keepdim=True, name=None, )

[cuda error] paddle.nansum(Tensor([2, 0, 4, 5],"float32"), axis=list[2,], keepdim=True, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:21.154844 test begin: paddle.nansum(Tensor([2, 0, 4, 5],"float32"), axis=list[], keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([2, 0, 4, 5],"float32"), axis=list[], keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:21.364773 test begin: paddle.nansum(Tensor([2, 0, 4, 5],"float32"), axis=tuple(0,2,), keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([2, 0, 4, 5],"float32"), axis=tuple(0,2,), keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:21.570052 test begin: paddle.nansum(Tensor([2, 0],"float32"), )

[cuda error] paddle.nansum(Tensor([2, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:21.756565 test begin: paddle.nansum(Tensor([2, 0],"float32"), axis=None, keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([2, 0],"float32"), axis=None, keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:21.944446 test begin: paddle.nansum(Tensor([2, 0],"float32"), axis=None, keepdim=True, name=None, )

[cuda error] paddle.nansum(Tensor([2, 0],"float32"), axis=None, keepdim=True, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:22.212614 test begin: paddle.nansum(Tensor([2, 0],"float32"), axis=list[-1,], keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([2, 0],"float32"), axis=list[-1,], keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:22.423152 test begin: paddle.nansum(Tensor([2, 0],"float32"), axis=list[0,], keepdim=True, name=None, )

[cuda error] paddle.nansum(Tensor([2, 0],"float32"), axis=list[0,], keepdim=True, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:22.659098 test begin: paddle.nansum(Tensor([2, 0],"float32"), axis=list[1,], keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([2, 0],"float32"), axis=list[1,], keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:22.873506 test begin: paddle.nansum(Tensor([2, 0],"float32"), axis=tuple(0,1,), keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([2, 0],"float32"), axis=tuple(0,1,), keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:23.080732 test begin: paddle.nansum(Tensor([2, 3, 0, 5],"float32"), axis=None, keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([2, 3, 0, 5],"float32"), axis=None, keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:23.305495 test begin: paddle.nansum(Tensor([2, 3, 0, 5],"float32"), axis=None, keepdim=True, name=None, )

[cuda error] paddle.nansum(Tensor([2, 3, 0, 5],"float32"), axis=None, keepdim=True, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:23.514524 test begin: paddle.nansum(Tensor([2, 3, 0, 5],"float32"), axis=list[-1,], keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([2, 3, 0, 5],"float32"), axis=list[-1,], keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:23.785083 test begin: paddle.nansum(Tensor([2, 3, 0, 5],"float32"), axis=list[0,1,2,3,], keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([2, 3, 0, 5],"float32"), axis=list[0,1,2,3,], keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:24.047615 test begin: paddle.nansum(Tensor([2, 3, 0, 5],"float32"), axis=list[0,2,], keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([2, 3, 0, 5],"float32"), axis=list[0,2,], keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:24.261610 test begin: paddle.nansum(Tensor([2, 3, 0, 5],"float32"), axis=list[2,], keepdim=True, name=None, )

[cuda error] paddle.nansum(Tensor([2, 3, 0, 5],"float32"), axis=list[2,], keepdim=True, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:24.501595 test begin: paddle.nansum(Tensor([2, 3, 0, 5],"float32"), axis=list[], keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([2, 3, 0, 5],"float32"), axis=list[], keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:24.652715 test begin: paddle.nansum(Tensor([2, 3, 0, 5],"float32"), axis=tuple(0,2,), keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([2, 3, 0, 5],"float32"), axis=tuple(0,2,), keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:24.777834 test begin: paddle.nansum(Tensor([2, 3, 4, 0],"float32"), axis=None, keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([2, 3, 4, 0],"float32"), axis=None, keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:24.905367 test begin: paddle.nansum(Tensor([2, 3, 4, 0],"float32"), axis=None, keepdim=True, name=None, )

[cuda error] paddle.nansum(Tensor([2, 3, 4, 0],"float32"), axis=None, keepdim=True, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:25.070087 test begin: paddle.nansum(Tensor([2, 3, 4, 0],"float32"), axis=list[-1,], keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([2, 3, 4, 0],"float32"), axis=list[-1,], keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:25.280717 test begin: paddle.nansum(Tensor([2, 3, 4, 0],"float32"), axis=list[0,1,2,3,], keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([2, 3, 4, 0],"float32"), axis=list[0,1,2,3,], keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:25.535913 test begin: paddle.nansum(Tensor([2, 3, 4, 0],"float32"), axis=list[0,2,], keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([2, 3, 4, 0],"float32"), axis=list[0,2,], keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:25.669206 test begin: paddle.nansum(Tensor([2, 3, 4, 0],"float32"), axis=list[2,], keepdim=True, name=None, )

[cuda error] paddle.nansum(Tensor([2, 3, 4, 0],"float32"), axis=list[2,], keepdim=True, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:25.828386 test begin: paddle.nansum(Tensor([2, 3, 4, 0],"float32"), axis=list[], keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([2, 3, 4, 0],"float32"), axis=list[], keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:26.120548 test begin: paddle.nansum(Tensor([2, 3, 4, 0],"float32"), axis=tuple(0,2,), keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([2, 3, 4, 0],"float32"), axis=tuple(0,2,), keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:26.325836 test begin: paddle.nansum(Tensor([3, 0],"float32"), axis=None, )

[cuda error] paddle.nansum(Tensor([3, 0],"float32"), axis=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:26.588382 test begin: paddle.nansum(Tensor([3, 0],"float32"), axis=None, keepdim=False, name=None, )

[cuda error] paddle.nansum(Tensor([3, 0],"float32"), axis=None, keepdim=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:26.882328 test begin: paddle.nansum(Tensor([3, 0],"float32"), axis=None, keepdim=True, name=None, )

[cuda error] paddle.nansum(Tensor([3, 0],"float32"), axis=None, keepdim=True, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:27.096978 test begin: paddle.nansum(Tensor([3, 0],"float32"), keepdim=True, )

[cuda error] paddle.nansum(Tensor([3, 0],"float32"), keepdim=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:27.311419 test begin: paddle.nansum(x=Tensor([0, 2, 3, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, )

[cuda error] paddle.nansum(x=Tensor([0, 2, 3, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:27.538268 test begin: paddle.nansum(x=Tensor([0, 3, 3],"float64"), )

[cuda error] paddle.nansum(x=Tensor([0, 3, 3],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:27.680084 test begin: paddle.nansum(x=Tensor([0, 3, 3],"float64"), axis=-1, )

[cuda error] paddle.nansum(x=Tensor([0, 3, 3],"float64"), axis=-1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:27.819421 test begin: paddle.nansum(x=Tensor([0, 3, 3],"float64"), axis=0, )

[cuda error] paddle.nansum(x=Tensor([0, 3, 3],"float64"), axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:27.983103 test begin: paddle.nansum(x=Tensor([0, 3, 3],"float64"), axis=0, keepdim=True, )

[cuda error] paddle.nansum(x=Tensor([0, 3, 3],"float64"), axis=0, keepdim=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:28.155227 test begin: paddle.nansum(x=Tensor([3, 0, 3, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, )

[cuda error] paddle.nansum(x=Tensor([3, 0, 3, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:28.283480 test begin: paddle.nansum(x=Tensor([3, 0, 3],"float64"), )

[cuda error] paddle.nansum(x=Tensor([3, 0, 3],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:28.417028 test begin: paddle.nansum(x=Tensor([3, 0, 3],"float64"), axis=-1, )

[cuda error] paddle.nansum(x=Tensor([3, 0, 3],"float64"), axis=-1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:28.544720 test begin: paddle.nansum(x=Tensor([3, 0, 3],"float64"), axis=0, )

[cuda error] paddle.nansum(x=Tensor([3, 0, 3],"float64"), axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:28.679764 test begin: paddle.nansum(x=Tensor([3, 0, 3],"float64"), axis=0, keepdim=True, )

[cuda error] paddle.nansum(x=Tensor([3, 0, 3],"float64"), axis=0, keepdim=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:28.824039 test begin: paddle.nansum(x=Tensor([3, 2, 0, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, )

[cuda error] paddle.nansum(x=Tensor([3, 2, 0, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:28.988737 test begin: paddle.nansum(x=Tensor([3, 2, 3, 0, 5, 1, 2],"float64"), axis=3, keepdim=True, )

[cuda error] paddle.nansum(x=Tensor([3, 2, 3, 0, 5, 1, 2],"float64"), axis=3, keepdim=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:29.278433 test begin: paddle.nansum(x=Tensor([3, 2, 3, 4, 0, 1, 2],"float64"), axis=3, keepdim=True, )

[cuda error] paddle.nansum(x=Tensor([3, 2, 3, 4, 0, 1, 2],"float64"), axis=3, keepdim=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:29.455891 test begin: paddle.nansum(x=Tensor([3, 2, 3, 4, 5, 0, 2],"float64"), axis=3, keepdim=True, )

[cuda error] paddle.nansum(x=Tensor([3, 2, 3, 4, 5, 0, 2],"float64"), axis=3, keepdim=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:29.672238 test begin: paddle.nansum(x=Tensor([3, 2, 3, 4, 5, 1, 0],"float64"), axis=3, keepdim=True, )

[cuda error] paddle.nansum(x=Tensor([3, 2, 3, 4, 5, 1, 0],"float64"), axis=3, keepdim=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:29.813186 test begin: paddle.nansum(x=Tensor([3, 3, 0],"float64"), )

[cuda error] paddle.nansum(x=Tensor([3, 3, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:29.984443 test begin: paddle.nansum(x=Tensor([3, 3, 0],"float64"), axis=-1, )

[cuda error] paddle.nansum(x=Tensor([3, 3, 0],"float64"), axis=-1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:30.168214 test begin: paddle.nansum(x=Tensor([3, 3, 0],"float64"), axis=0, )

[cuda error] paddle.nansum(x=Tensor([3, 3, 0],"float64"), axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:30.333619 test begin: paddle.nansum(x=Tensor([3, 3, 0],"float64"), axis=0, keepdim=True, )

[cuda error] paddle.nansum(x=Tensor([3, 3, 0],"float64"), axis=0, keepdim=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:33.906438 test begin: paddle.nextafter(Tensor([0, 3, 2],"float32"), Tensor([0, 3, 2],"float64"), )

W0421 10:08:34.128290 26276 dygraph_functions.cc:54801] got different data type, run type promotion automatically, this may cause data type been changed.
[cuda error] paddle.nextafter(Tensor([0, 3, 2],"float32"), Tensor([0, 3, 2],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:34.136309 test begin: paddle.nextafter(Tensor([0, 3, 2],"float64"), Tensor([0, 3, 2],"float32"), )

[cuda error] paddle.nextafter(Tensor([0, 3, 2],"float64"), Tensor([0, 3, 2],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:34.419263 test begin: paddle.nextafter(Tensor([0, 3, 4, 5],"float32"), Tensor([0, 3, 4, 5],"float32"), )

[cuda error] paddle.nextafter(Tensor([0, 3, 4, 5],"float32"), Tensor([0, 3, 4, 5],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:34.627817 test begin: paddle.nextafter(Tensor([2, 0, 4, 5],"float32"), Tensor([2, 0, 4, 5],"float32"), )

[cuda error] paddle.nextafter(Tensor([2, 0, 4, 5],"float32"), Tensor([2, 0, 4, 5],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:34.845107 test begin: paddle.nextafter(Tensor([2, 3, 0, 5],"float32"), Tensor([2, 3, 0, 5],"float32"), )

[cuda error] paddle.nextafter(Tensor([2, 3, 0, 5],"float32"), Tensor([2, 3, 0, 5],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:35.054462 test begin: paddle.nextafter(Tensor([2, 3, 4, 0],"float32"), Tensor([2, 3, 4, 0],"float32"), )

[cuda error] paddle.nextafter(Tensor([2, 3, 4, 0],"float32"), Tensor([2, 3, 4, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:35.287996 test begin: paddle.nextafter(Tensor([4, 0, 2],"float32"), Tensor([4, 0, 2],"float64"), )

[cuda error] paddle.nextafter(Tensor([4, 0, 2],"float32"), Tensor([4, 0, 2],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:35.537028 test begin: paddle.nextafter(Tensor([4, 0, 2],"float64"), Tensor([4, 0, 2],"float32"), )

[cuda error] paddle.nextafter(Tensor([4, 0, 2],"float64"), Tensor([4, 0, 2],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:35.795027 test begin: paddle.nextafter(Tensor([4, 3, 0],"float32"), Tensor([4, 3, 0],"float64"), )

[cuda error] paddle.nextafter(Tensor([4, 3, 0],"float32"), Tensor([4, 3, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:35.935320 test begin: paddle.nextafter(Tensor([4, 3, 0],"float64"), Tensor([4, 3, 0],"float32"), )

[cuda error] paddle.nextafter(Tensor([4, 3, 0],"float64"), Tensor([4, 3, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:36.101594 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([0, 1024, 49],"float32"), 1, None, )

[paddle error] paddle.nn.functional.adaptive_avg_pool1d(Tensor([0, 1024, 49],"float32"), 1, None, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:08:36.317676 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([0, 256, 16],"float32"), 1, None, )

[paddle error] paddle.nn.functional.adaptive_avg_pool1d(Tensor([0, 256, 16],"float32"), 1, None, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:08:36.531845 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([0, 3, 32],"float32"), 16, )

[cuda error] paddle.nn.functional.adaptive_avg_pool1d(Tensor([0, 3, 32],"float32"), 16, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:36.740661 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([0, 3, 32],"float32"), 16, None, )

[cuda error] paddle.nn.functional.adaptive_avg_pool1d(Tensor([0, 3, 32],"float32"), 16, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:36.945442 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([0, 3, 32],"float32"), output_size=16, )

[cuda error] paddle.nn.functional.adaptive_avg_pool1d(Tensor([0, 3, 32],"float32"), output_size=16, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:37.149157 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([0, 3, 8],"float32"), 2, None, )

[cuda error] paddle.nn.functional.adaptive_avg_pool1d(Tensor([0, 3, 8],"float32"), 2, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:37.400486 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([0, 3, 8],"float64"), 2, None, )

[cuda error] paddle.nn.functional.adaptive_avg_pool1d(Tensor([0, 3, 8],"float64"), 2, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:37.604653 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([0, 3, 8],"float64"), 4, None, )

[cuda error] paddle.nn.functional.adaptive_avg_pool1d(Tensor([0, 3, 8],"float64"), 4, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:37.804690 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([0, 3, 8],"float64"), 8, None, )

[cuda error] paddle.nn.functional.adaptive_avg_pool1d(Tensor([0, 3, 8],"float64"), 8, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:38.007088 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([12, 0, 16],"float32"), 1, None, )

[paddle error] paddle.nn.functional.adaptive_avg_pool1d(Tensor([12, 0, 16],"float32"), 1, None, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:08:38.216789 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([12, 0, 49],"float32"), 1, None, )

[paddle error] paddle.nn.functional.adaptive_avg_pool1d(Tensor([12, 0, 49],"float32"), 1, None, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:08:38.505816 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([12, 1024, 0],"float32"), 1, None, )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   Pool2dGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::pool2d_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, bool, std::string const&, std::string const&, bool, bool, std::string const&, paddle::Tensor*)
4   void phi::Pool2dGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, bool, std::string const&, std::string const&, bool, bool, std::string const&, phi::DenseTensor*)
5   void phi::PoolGradRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, std::string const&, std::string const&, bool, bool, std::string const&, float, phi::DenseTensor*)
6   phi::funcs::Pool2dGradFunctor<phi::GPUContext, phi::funcs::AvgPoolGrad<float>, float>::operator()(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string, bool, bool, phi::DenseTensor*, phi::funcs::AvgPoolGrad<float>)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201318 (unix time) try "date -d @1745201318" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7feb012826dd) received by PID 155430 (TID 0x7fe92109a700) from PID 19408605 ***]

2025-04-21 10:08:59.294352 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([12, 256, 0],"float32"), 1, None, )

W0421 10:09:05.222508 34815 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:09:05.223716 34815 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   Pool2dGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::pool2d_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, bool, std::string const&, std::string const&, bool, bool, std::string const&, paddle::Tensor*)
4   void phi::Pool2dGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, bool, std::string const&, std::string const&, bool, bool, std::string const&, phi::DenseTensor*)
5   void phi::PoolGradRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, std::string const&, std::string const&, bool, bool, std::string const&, float, phi::DenseTensor*)
6   phi::funcs::Pool2dGradFunctor<phi::GPUContext, phi::funcs::AvgPoolGrad<float>, float>::operator()(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string, bool, bool, phi::DenseTensor*, phi::funcs::AvgPoolGrad<float>)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201345 (unix time) try "date -d @1745201345" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f1aeae456dd) received by PID 33476 (TID 0x7f1a4bd0b700) from PID 18446744073355417309 ***]

2025-04-21 10:09:22.245180 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([2, 0, 32],"float32"), 16, )

W0421 10:09:27.888996 43526 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:09:27.890149 43526 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_pool2d(_object*, _object*, _object*)
1   pool2d_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, std::vector<int, std::allocator<int> >, std::vector<int, std::allocator<int> >, bool, bool, std::string, std::string, bool, bool, std::string)
2   paddle::experimental::pool2d(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, bool, std::string const&, std::string const&, bool, bool, std::string const&)
3   void phi::Pool2dKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, bool, std::string const&, std::string const&, bool, bool, std::string const&, phi::DenseTensor*)
4   void phi::PoolRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, std::string const&, std::string const&, bool, bool, std::string const&, float, phi::DenseTensor*)
5   phi::funcs::Pool2dFunctor<phi::GPUContext, phi::funcs::AvgPool<float>, float>::operator()(phi::GPUContext const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string, bool, bool, phi::DenseTensor*, phi::funcs::AvgPool<float>)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201367 (unix time) try "date -d @1745201367" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7ff090ea260e) received by PID 42198 (TID 0x7ff005dc2700) from PID 18446744071845848590 ***]

2025-04-21 10:09:46.462676 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([2, 0, 32],"float32"), 16, None, )

W0421 10:09:52.346410 51857 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:09:52.348059 51857 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_pool2d(_object*, _object*, _object*)
1   pool2d_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, std::vector<int, std::allocator<int> >, std::vector<int, std::allocator<int> >, bool, bool, std::string, std::string, bool, bool, std::string)
2   paddle::experimental::pool2d(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, bool, std::string const&, std::string const&, bool, bool, std::string const&)
3   void phi::Pool2dKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, bool, std::string const&, std::string const&, bool, bool, std::string const&, phi::DenseTensor*)
4   void phi::PoolRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, std::string const&, std::string const&, bool, bool, std::string const&, float, phi::DenseTensor*)
5   phi::funcs::Pool2dFunctor<phi::GPUContext, phi::funcs::AvgPool<float>, float>::operator()(phi::GPUContext const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string, bool, bool, phi::DenseTensor*, phi::funcs::AvgPool<float>)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201392 (unix time) try "date -d @1745201392" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f65781bf60e) received by PID 50664 (TID 0x7f64fc949700) from PID 2015098382 ***]

2025-04-21 10:10:09.254091 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([2, 0, 32],"float32"), output_size=16, )

W0421 10:10:15.063699 59738 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:10:15.064843 59738 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_pool2d(_object*, _object*, _object*)
1   pool2d_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, std::vector<int, std::allocator<int> >, std::vector<int, std::allocator<int> >, bool, bool, std::string, std::string, bool, bool, std::string)
2   paddle::experimental::pool2d(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, bool, std::string const&, std::string const&, bool, bool, std::string const&)
3   void phi::Pool2dKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, bool, std::string const&, std::string const&, bool, bool, std::string const&, phi::DenseTensor*)
4   void phi::PoolRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, std::string const&, std::string const&, bool, bool, std::string const&, float, phi::DenseTensor*)
5   phi::funcs::Pool2dFunctor<phi::GPUContext, phi::funcs::AvgPool<float>, float>::operator()(phi::GPUContext const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string, bool, bool, phi::DenseTensor*, phi::funcs::AvgPool<float>)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201415 (unix time) try "date -d @1745201415" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fa79f66760e) received by PID 58590 (TID 0x7fa6e9f48700) from PID 18446744072088876558 ***]

2025-04-21 10:10:32.221794 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([2, 0, 8],"float32"), 2, None, )

W0421 10:10:38.341629 68721 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:10:38.342762 68721 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_pool2d(_object*, _object*, _object*)
1   pool2d_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, std::vector<int, std::allocator<int> >, std::vector<int, std::allocator<int> >, bool, bool, std::string, std::string, bool, bool, std::string)
2   paddle::experimental::pool2d(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, bool, std::string const&, std::string const&, bool, bool, std::string const&)
3   void phi::Pool2dKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, bool, std::string const&, std::string const&, bool, bool, std::string const&, phi::DenseTensor*)
4   void phi::PoolRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, std::string const&, std::string const&, bool, bool, std::string const&, float, phi::DenseTensor*)
5   phi::funcs::Pool2dFunctor<phi::GPUContext, phi::funcs::AvgPool<float>, float>::operator()(phi::GPUContext const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string, bool, bool, phi::DenseTensor*, phi::funcs::AvgPool<float>)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201438 (unix time) try "date -d @1745201438" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f8a8e28360e) received by PID 66861 (TID 0x7f89f52b7700) from PID 18446744071799584270 ***]

2025-04-21 10:10:42.422015 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([2, 0, 8],"float64"), 2, None, )

W0421 10:10:48.717535 72186 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:10:48.718657 72186 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_pool2d(_object*, _object*, _object*)
1   pool2d_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, std::vector<int, std::allocator<int> >, std::vector<int, std::allocator<int> >, bool, bool, std::string, std::string, bool, bool, std::string)
2   paddle::experimental::pool2d(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, bool, std::string const&, std::string const&, bool, bool, std::string const&)
3   void phi::Pool2dKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, bool, std::string const&, std::string const&, bool, bool, std::string const&, phi::DenseTensor*)
4   void phi::PoolRawKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, std::string const&, std::string const&, bool, bool, std::string const&, float, phi::DenseTensor*)
5   phi::funcs::Pool2dFunctor<phi::GPUContext, phi::funcs::AvgPool<double>, double>::operator()(phi::GPUContext const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string, bool, bool, phi::DenseTensor*, phi::funcs::AvgPool<double>)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201448 (unix time) try "date -d @1745201448" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fc7c236619e) received by PID 71036 (TID 0x7fc7252b7700) from PID 18446744072672928158 ***]

2025-04-21 10:10:52.634465 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([2, 0, 8],"float64"), 4, None, )

W0421 10:10:58.536893 76136 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:10:58.538340 76136 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_pool2d(_object*, _object*, _object*)
1   pool2d_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, std::vector<int, std::allocator<int> >, std::vector<int, std::allocator<int> >, bool, bool, std::string, std::string, bool, bool, std::string)
2   paddle::experimental::pool2d(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, bool, std::string const&, std::string const&, bool, bool, std::string const&)
3   void phi::Pool2dKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, bool, std::string const&, std::string const&, bool, bool, std::string const&, phi::DenseTensor*)
4   void phi::PoolRawKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, std::string const&, std::string const&, bool, bool, std::string const&, float, phi::DenseTensor*)
5   phi::funcs::Pool2dFunctor<phi::GPUContext, phi::funcs::AvgPool<double>, double>::operator()(phi::GPUContext const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string, bool, bool, phi::DenseTensor*, phi::funcs::AvgPool<double>)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201458 (unix time) try "date -d @1745201458" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fdce4cb619e) received by PID 74820 (TID 0x7fdc6bb85700) from PID 18446744073253118366 ***]

2025-04-21 10:11:15.547858 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([2, 0, 8],"float64"), 8, None, )

W0421 10:11:21.485023 84703 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:11:21.486153 84703 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_pool2d(_object*, _object*, _object*)
1   pool2d_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, std::vector<int, std::allocator<int> >, std::vector<int, std::allocator<int> >, bool, bool, std::string, std::string, bool, bool, std::string)
2   paddle::experimental::pool2d(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, bool, std::string const&, std::string const&, bool, bool, std::string const&)
3   void phi::Pool2dKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, bool, std::string const&, std::string const&, bool, bool, std::string const&, phi::DenseTensor*)
4   void phi::PoolRawKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, std::string const&, std::string const&, bool, bool, std::string const&, float, phi::DenseTensor*)
5   phi::funcs::Pool2dFunctor<phi::GPUContext, phi::funcs::AvgPool<double>, double>::operator()(phi::GPUContext const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string, bool, bool, phi::DenseTensor*, phi::funcs::AvgPool<double>)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201481 (unix time) try "date -d @1745201481" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f26ade0419e) received by PID 83403 (TID 0x7f25fc7c3700) from PID 18446744072331739550 ***]

2025-04-21 10:11:45.304768 test begin: paddle.nn.functional.adaptive_avg_pool1d(x=Tensor([0, 3, 8],"float32"), output_size=2, )

[cuda error] paddle.nn.functional.adaptive_avg_pool1d(x=Tensor([0, 3, 8],"float32"), output_size=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:45.963229 test begin: paddle.nn.functional.adaptive_avg_pool1d(x=Tensor([0, 3, 8],"float64"), output_size=2, )

[cuda error] paddle.nn.functional.adaptive_avg_pool1d(x=Tensor([0, 3, 8],"float64"), output_size=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:46.176907 test begin: paddle.nn.functional.adaptive_avg_pool1d(x=Tensor([0, 3, 8],"float64"), output_size=4, )

[cuda error] paddle.nn.functional.adaptive_avg_pool1d(x=Tensor([0, 3, 8],"float64"), output_size=4, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:46.411529 test begin: paddle.nn.functional.adaptive_avg_pool1d(x=Tensor([0, 3, 8],"float64"), output_size=8, )

[cuda error] paddle.nn.functional.adaptive_avg_pool1d(x=Tensor([0, 3, 8],"float64"), output_size=8, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:46.648702 test begin: paddle.nn.functional.adaptive_avg_pool1d(x=Tensor([2, 0, 8],"float32"), output_size=2, )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_pool2d(_object*, _object*, _object*)
1   pool2d_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, std::vector<int, std::allocator<int> >, std::vector<int, std::allocator<int> >, bool, bool, std::string, std::string, bool, bool, std::string)
2   paddle::experimental::pool2d(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, bool, std::string const&, std::string const&, bool, bool, std::string const&)
3   void phi::Pool2dKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, bool, std::string const&, std::string const&, bool, bool, std::string const&, phi::DenseTensor*)
4   void phi::PoolRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, std::string const&, std::string const&, bool, bool, std::string const&, float, phi::DenseTensor*)
5   phi::funcs::Pool2dFunctor<phi::GPUContext, phi::funcs::AvgPool<float>, float>::operator()(phi::GPUContext const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string, bool, bool, phi::DenseTensor*, phi::funcs::AvgPool<float>)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201506 (unix time) try "date -d @1745201506" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fbf5d6ed60e) received by PID 91574 (TID 0x7fbd6e8be700) from PID 1567544846 ***]

2025-04-21 10:11:51.092078 test begin: paddle.nn.functional.adaptive_avg_pool1d(x=Tensor([2, 0, 8],"float64"), output_size=2, )

W0421 10:11:57.161078 97061 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:11:57.162425 97061 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_pool2d(_object*, _object*, _object*)
1   pool2d_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, std::vector<int, std::allocator<int> >, std::vector<int, std::allocator<int> >, bool, bool, std::string, std::string, bool, bool, std::string)
2   paddle::experimental::pool2d(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, bool, std::string const&, std::string const&, bool, bool, std::string const&)
3   void phi::Pool2dKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, bool, std::string const&, std::string const&, bool, bool, std::string const&, phi::DenseTensor*)
4   void phi::PoolRawKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, std::string const&, std::string const&, bool, bool, std::string const&, float, phi::DenseTensor*)
5   phi::funcs::Pool2dFunctor<phi::GPUContext, phi::funcs::AvgPool<double>, double>::operator()(phi::GPUContext const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string, bool, bool, phi::DenseTensor*, phi::funcs::AvgPool<double>)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201517 (unix time) try "date -d @1745201517" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f1911d5419e) received by PID 95827 (TID 0x7f18a67c3700) from PID 299188638 ***]

2025-04-21 10:12:01.080854 test begin: paddle.nn.functional.adaptive_avg_pool1d(x=Tensor([2, 0, 8],"float64"), output_size=4, )

W0421 10:12:06.932421 100532 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:12:06.933483 100532 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_pool2d(_object*, _object*, _object*)
1   pool2d_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, std::vector<int, std::allocator<int> >, std::vector<int, std::allocator<int> >, bool, bool, std::string, std::string, bool, bool, std::string)
2   paddle::experimental::pool2d(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, bool, std::string const&, std::string const&, bool, bool, std::string const&)
3   void phi::Pool2dKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, bool, std::string const&, std::string const&, bool, bool, std::string const&, phi::DenseTensor*)
4   void phi::PoolRawKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, std::string const&, std::string const&, bool, bool, std::string const&, float, phi::DenseTensor*)
5   phi::funcs::Pool2dFunctor<phi::GPUContext, phi::funcs::AvgPool<double>, double>::operator()(phi::GPUContext const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string, bool, bool, phi::DenseTensor*, phi::funcs::AvgPool<double>)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201526 (unix time) try "date -d @1745201526" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f380a3ef19e) received by PID 99132 (TID 0x7f37872b7700) from PID 171897246 ***]

2025-04-21 10:12:10.883216 test begin: paddle.nn.functional.adaptive_avg_pool1d(x=Tensor([2, 0, 8],"float64"), output_size=8, )

W0421 10:12:17.144466 103881 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:12:17.145574 103881 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_pool2d(_object*, _object*, _object*)
1   pool2d_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>, std::vector<int, std::allocator<int> >, std::vector<int, std::allocator<int> >, bool, bool, std::string, std::string, bool, bool, std::string)
2   paddle::experimental::pool2d(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, bool, std::string const&, std::string const&, bool, bool, std::string const&)
3   void phi::Pool2dKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, bool, std::string const&, std::string const&, bool, bool, std::string const&, phi::DenseTensor*)
4   void phi::PoolRawKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, bool, std::string const&, std::string const&, bool, bool, std::string const&, float, phi::DenseTensor*)
5   phi::funcs::Pool2dFunctor<phi::GPUContext, phi::funcs::AvgPool<double>, double>::operator()(phi::GPUContext const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string, bool, bool, phi::DenseTensor*, phi::funcs::AvgPool<double>)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201537 (unix time) try "date -d @1745201537" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f903f64019e) received by PID 102754 (TID 0x7f8fd5f48700) from PID 1063518622 ***]

2025-04-21 10:12:26.528453 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([0, 1, 4],"float64"), 4, True, None, )

element 1 of tensors does not require grad and does not have a grad_fn
[cuda error] paddle.nn.functional.adaptive_max_pool1d(Tensor([0, 1, 4],"float64"), 4, True, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:27.068738 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([0, 3, 32],"float32"), 16, False, None, )

[cuda error] paddle.nn.functional.adaptive_max_pool1d(Tensor([0, 3, 32],"float32"), 16, False, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:27.304748 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([0, 3, 32],"float32"), output_size=16, )

[cuda error] paddle.nn.functional.adaptive_max_pool1d(Tensor([0, 3, 32],"float32"), output_size=16, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:27.516662 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([0, 3, 32],"float64"), 8, False, None, )

[cuda error] paddle.nn.functional.adaptive_max_pool1d(Tensor([0, 3, 32],"float64"), 8, False, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:27.731862 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([0, 3, 8],"float32"), 4, False, None, )

[cuda error] paddle.nn.functional.adaptive_max_pool1d(Tensor([0, 3, 8],"float32"), 4, False, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:27.941846 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([0, 3, 8],"float64"), 4, False, None, )

[cuda error] paddle.nn.functional.adaptive_max_pool1d(Tensor([0, 3, 8],"float64"), 4, False, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:28.155188 test begin: paddle.nn.functional.adaptive_max_pool1d(x=Tensor([0, 1, 4],"float64"), output_size=4, return_mask=True, )

element 1 of tensors does not require grad and does not have a grad_fn
[cuda error] paddle.nn.functional.adaptive_max_pool1d(x=Tensor([0, 1, 4],"float64"), output_size=4, return_mask=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:28.416842 test begin: paddle.nn.functional.adaptive_max_pool1d(x=Tensor([0, 3, 32],"float64"), output_size=8, )

[cuda error] paddle.nn.functional.adaptive_max_pool1d(x=Tensor([0, 3, 32],"float64"), output_size=8, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:28.617314 test begin: paddle.nn.functional.adaptive_max_pool1d(x=Tensor([0, 3, 8],"float32"), output_size=4, )

[cuda error] paddle.nn.functional.adaptive_max_pool1d(x=Tensor([0, 3, 8],"float32"), output_size=4, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:28.899241 test begin: paddle.nn.functional.adaptive_max_pool1d(x=Tensor([0, 3, 8],"float64"), output_size=4, )

[cuda error] paddle.nn.functional.adaptive_max_pool1d(x=Tensor([0, 3, 8],"float64"), output_size=4, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:29.118272 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([0, 1, 5, 5],"float32"), output_size=3, return_mask=True, name=None, )

element 1 of tensors does not require grad and does not have a grad_fn
[cuda error] paddle.nn.functional.adaptive_max_pool2d(Tensor([0, 1, 5, 5],"float32"), output_size=3, return_mask=True, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:29.253888 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([0, 1, 5, 5],"float64"), output_size=3, return_mask=True, name=None, )

element 1 of tensors does not require grad and does not have a grad_fn
[cuda error] paddle.nn.functional.adaptive_max_pool2d(Tensor([0, 1, 5, 5],"float64"), output_size=3, return_mask=True, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:29.477739 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([0, 1, 8, 8],"float32"), output_size=3, return_mask=False, name=None, )

[cuda error] paddle.nn.functional.adaptive_max_pool2d(Tensor([0, 1, 8, 8],"float32"), output_size=3, return_mask=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:29.711929 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([0, 1, 8, 8],"float64"), output_size=3, return_mask=False, name=None, )

[cuda error] paddle.nn.functional.adaptive_max_pool2d(Tensor([0, 1, 8, 8],"float64"), output_size=3, return_mask=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:29.967440 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([0, 2, 8, 8],"float64"), output_size=4, return_mask=False, name=None, )

[cuda error] paddle.nn.functional.adaptive_max_pool2d(Tensor([0, 2, 8, 8],"float64"), output_size=4, return_mask=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:30.160766 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([0, 3, 7, 7],"float32"), output_size=5, return_mask=False, name=None, )

[cuda error] paddle.nn.functional.adaptive_max_pool2d(Tensor([0, 3, 7, 7],"float32"), output_size=5, return_mask=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:30.378444 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([0, 3, 7, 7],"float32"), output_size=list[2,5,], return_mask=False, name=None, )

[cuda error] paddle.nn.functional.adaptive_max_pool2d(Tensor([0, 3, 7, 7],"float32"), output_size=list[2,5,], return_mask=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:30.581914 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([0, 3, 7, 7],"float32"), output_size=list[3,3,], return_mask=False, name=None, )

[cuda error] paddle.nn.functional.adaptive_max_pool2d(Tensor([0, 3, 7, 7],"float32"), output_size=list[3,3,], return_mask=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:30.789154 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([0, 3, 7, 7],"float32"), output_size=list[None,3,], return_mask=False, name=None, )

[cuda error] paddle.nn.functional.adaptive_max_pool2d(Tensor([0, 3, 7, 7],"float32"), output_size=list[None,3,], return_mask=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:31.021320 test begin: paddle.nn.functional.adaptive_max_pool2d(x=Tensor([0, 1, 5, 5],"float32"), output_size=3, return_mask=True, )

element 1 of tensors does not require grad and does not have a grad_fn
[cuda error] paddle.nn.functional.adaptive_max_pool2d(x=Tensor([0, 1, 5, 5],"float32"), output_size=3, return_mask=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:31.233889 test begin: paddle.nn.functional.adaptive_max_pool2d(x=Tensor([0, 1, 5, 5],"float64"), output_size=3, return_mask=True, )

element 1 of tensors does not require grad and does not have a grad_fn
[cuda error] paddle.nn.functional.adaptive_max_pool2d(x=Tensor([0, 1, 5, 5],"float64"), output_size=3, return_mask=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:31.537657 test begin: paddle.nn.functional.adaptive_max_pool2d(x=Tensor([0, 1, 8, 8],"float32"), output_size=3, )

[cuda error] paddle.nn.functional.adaptive_max_pool2d(x=Tensor([0, 1, 8, 8],"float32"), output_size=3, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:31.809294 test begin: paddle.nn.functional.adaptive_max_pool2d(x=Tensor([0, 1, 8, 8],"float64"), output_size=3, )

[cuda error] paddle.nn.functional.adaptive_max_pool2d(x=Tensor([0, 1, 8, 8],"float64"), output_size=3, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:32.021226 test begin: paddle.nn.functional.adaptive_max_pool2d(x=Tensor([0, 2, 8, 8],"float64"), output_size=4, )

[cuda error] paddle.nn.functional.adaptive_max_pool2d(x=Tensor([0, 2, 8, 8],"float64"), output_size=4, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:32.249221 test begin: paddle.nn.functional.adaptive_max_pool2d(x=Tensor([0, 3, 7, 7],"float32"), output_size=5, )

[cuda error] paddle.nn.functional.adaptive_max_pool2d(x=Tensor([0, 3, 7, 7],"float32"), output_size=5, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:32.490010 test begin: paddle.nn.functional.adaptive_max_pool2d(x=Tensor([0, 3, 7, 7],"float32"), output_size=list[2,5,], )

[cuda error] paddle.nn.functional.adaptive_max_pool2d(x=Tensor([0, 3, 7, 7],"float32"), output_size=list[2,5,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:32.695011 test begin: paddle.nn.functional.adaptive_max_pool2d(x=Tensor([0, 3, 7, 7],"float32"), output_size=list[None,3,], )

[cuda error] paddle.nn.functional.adaptive_max_pool2d(x=Tensor([0, 3, 7, 7],"float32"), output_size=list[None,3,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:32.951253 test begin: paddle.nn.functional.adaptive_max_pool2d(x=Tensor([0, 3, 7, 7],"float32"), return_mask=False, output_size=list[3,3,], )

[cuda error] paddle.nn.functional.adaptive_max_pool2d(x=Tensor([0, 3, 7, 7],"float32"), return_mask=False, output_size=list[3,3,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:33.245755 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([0, 1, 5, 5, 5],"float32"), output_size=3, return_mask=True, name=None, )

element 1 of tensors does not require grad and does not have a grad_fn
[cuda error] paddle.nn.functional.adaptive_max_pool3d(Tensor([0, 1, 5, 5, 5],"float32"), output_size=3, return_mask=True, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:33.459873 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([0, 1, 5, 5, 5],"float64"), output_size=3, return_mask=True, name=None, )

element 1 of tensors does not require grad and does not have a grad_fn
[cuda error] paddle.nn.functional.adaptive_max_pool3d(Tensor([0, 1, 5, 5, 5],"float64"), output_size=3, return_mask=True, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:33.676916 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([0, 2, 8, 8, 8],"float32"), output_size=4, return_mask=False, name=None, )

[cuda error] paddle.nn.functional.adaptive_max_pool3d(Tensor([0, 2, 8, 8, 8],"float32"), output_size=4, return_mask=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:33.923911 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([0, 2, 8, 8, 8],"float64"), output_size=4, return_mask=False, name=None, )

[cuda error] paddle.nn.functional.adaptive_max_pool3d(Tensor([0, 2, 8, 8, 8],"float64"), output_size=4, return_mask=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:34.125723 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([0, 3, 5, 7, 7],"float32"), output_size=5, return_mask=False, name=None, )

[cuda error] paddle.nn.functional.adaptive_max_pool3d(Tensor([0, 3, 5, 7, 7],"float32"), output_size=5, return_mask=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:34.363306 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([0, 3, 5, 7, 7],"float32"), output_size=list[2,3,5,], return_mask=False, name=None, )

[cuda error] paddle.nn.functional.adaptive_max_pool3d(Tensor([0, 3, 5, 7, 7],"float32"), output_size=list[2,3,5,], return_mask=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:34.608019 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([0, 3, 5, 7, 7],"float32"), output_size=list[3,3,3,], return_mask=False, name=None, )

[cuda error] paddle.nn.functional.adaptive_max_pool3d(Tensor([0, 3, 5, 7, 7],"float32"), output_size=list[3,3,3,], return_mask=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:34.834155 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([0, 3, 5, 7, 7],"float32"), output_size=list[None,3,None,], return_mask=False, name=None, )

[cuda error] paddle.nn.functional.adaptive_max_pool3d(Tensor([0, 3, 5, 7, 7],"float32"), output_size=list[None,3,None,], return_mask=False, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:35.107759 test begin: paddle.nn.functional.adaptive_max_pool3d(x=Tensor([0, 1, 5, 5, 5],"float32"), output_size=3, return_mask=True, )

element 1 of tensors does not require grad and does not have a grad_fn
[cuda error] paddle.nn.functional.adaptive_max_pool3d(x=Tensor([0, 1, 5, 5, 5],"float32"), output_size=3, return_mask=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:35.301991 test begin: paddle.nn.functional.adaptive_max_pool3d(x=Tensor([0, 1, 5, 5, 5],"float64"), output_size=3, return_mask=True, )

element 1 of tensors does not require grad and does not have a grad_fn
[cuda error] paddle.nn.functional.adaptive_max_pool3d(x=Tensor([0, 1, 5, 5, 5],"float64"), output_size=3, return_mask=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:35.526491 test begin: paddle.nn.functional.adaptive_max_pool3d(x=Tensor([0, 2, 8, 8, 8],"float32"), output_size=4, )

[cuda error] paddle.nn.functional.adaptive_max_pool3d(x=Tensor([0, 2, 8, 8, 8],"float32"), output_size=4, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:35.737056 test begin: paddle.nn.functional.adaptive_max_pool3d(x=Tensor([0, 2, 8, 8, 8],"float64"), output_size=4, )

[cuda error] paddle.nn.functional.adaptive_max_pool3d(x=Tensor([0, 2, 8, 8, 8],"float64"), output_size=4, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:36.106949 test begin: paddle.nn.functional.adaptive_max_pool3d(x=Tensor([0, 3, 5, 7, 7],"float32"), output_size=5, )

[cuda error] paddle.nn.functional.adaptive_max_pool3d(x=Tensor([0, 3, 5, 7, 7],"float32"), output_size=5, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:36.323513 test begin: paddle.nn.functional.adaptive_max_pool3d(x=Tensor([0, 3, 5, 7, 7],"float32"), output_size=list[2,3,5,], )

[cuda error] paddle.nn.functional.adaptive_max_pool3d(x=Tensor([0, 3, 5, 7, 7],"float32"), output_size=list[2,3,5,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:36.534440 test begin: paddle.nn.functional.adaptive_max_pool3d(x=Tensor([0, 3, 5, 7, 7],"float32"), output_size=list[3,3,3,], )

[cuda error] paddle.nn.functional.adaptive_max_pool3d(x=Tensor([0, 3, 5, 7, 7],"float32"), output_size=list[3,3,3,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:36.732477 test begin: paddle.nn.functional.adaptive_max_pool3d(x=Tensor([0, 3, 5, 7, 7],"float32"), output_size=list[None,3,None,], )

[cuda error] paddle.nn.functional.adaptive_max_pool3d(x=Tensor([0, 3, 5, 7, 7],"float32"), output_size=list[None,3,None,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:42.164970 test begin: paddle.nn.functional.cosine_similarity(Tensor([0, 12, 10],"float32"), Tensor([0, 1, 10],"float32"), axis=2, eps=1e-06, )

[accuracy error] backward  paddle.nn.functional.cosine_similarity(Tensor([0, 12, 10],"float32"), Tensor([0, 1, 10],"float32"), axis=2, eps=1e-06, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (0, 10), (0, 1, 10) mismatch)
 x: array([], shape=(0, 10), dtype=float32)
 y: array([], shape=(0, 1, 10), dtype=float32)
2025-04-21 10:12:44.214911 test begin: paddle.nn.functional.cosine_similarity(Tensor([0, 5, 2],"float64"), Tensor([1, 5, 2],"float64"), axis=-1, eps=1e-08, )

[accuracy error] backward  paddle.nn.functional.cosine_similarity(Tensor([0, 5, 2],"float64"), Tensor([1, 5, 2],"float64"), axis=-1, eps=1e-08, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (5, 2), (1, 5, 2) mismatch)
 x: array([[0., 0.],
       [0., 0.],
       [0., 0.],...
 y: array([[[0., 0.],
        [0., 0.],
        [0., 0.],...
2025-04-21 10:12:44.437354 test begin: paddle.nn.functional.cosine_similarity(Tensor([0, 5, 2],"float64"), Tensor([1, 5, 2],"float64"), axis=-2, eps=1e-08, )

[accuracy error] backward  paddle.nn.functional.cosine_similarity(Tensor([0, 5, 2],"float64"), Tensor([1, 5, 2],"float64"), axis=-2, eps=1e-08, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (5, 2), (1, 5, 2) mismatch)
 x: array([[0., 0.],
       [0., 0.],
       [0., 0.],...
 y: array([[[0., 0.],
        [0., 0.],
        [0., 0.],...
2025-04-21 10:12:44.665770 test begin: paddle.nn.functional.cosine_similarity(Tensor([0, 5, 2],"float64"), Tensor([1, 5, 2],"float64"), axis=0, eps=1e-08, )

[accuracy error] backward  paddle.nn.functional.cosine_similarity(Tensor([0, 5, 2],"float64"), Tensor([1, 5, 2],"float64"), axis=0, eps=1e-08, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (5, 2), (1, 5, 2) mismatch)
 x: array([[0., 0.],
       [0., 0.],
       [0., 0.],...
 y: array([[[0., 0.],
        [0., 0.],
        [0., 0.],...
2025-04-21 10:12:44.897054 test begin: paddle.nn.functional.cosine_similarity(Tensor([0, 5, 2],"float64"), Tensor([1, 5, 2],"float64"), axis=1, eps=1e-08, )

[accuracy error] backward  paddle.nn.functional.cosine_similarity(Tensor([0, 5, 2],"float64"), Tensor([1, 5, 2],"float64"), axis=1, eps=1e-08, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (5, 2), (1, 5, 2) mismatch)
 x: array([[0., 0.],
       [0., 0.],
       [0., 0.],...
 y: array([[[0., 0.],
        [0., 0.],
        [0., 0.],...
2025-04-21 10:12:45.108923 test begin: paddle.nn.functional.cosine_similarity(Tensor([0, 5, 2],"float64"), Tensor([1, 5, 2],"float64"), axis=2, eps=1e-08, )

[accuracy error] backward  paddle.nn.functional.cosine_similarity(Tensor([0, 5, 2],"float64"), Tensor([1, 5, 2],"float64"), axis=2, eps=1e-08, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (5, 2), (1, 5, 2) mismatch)
 x: array([[0., 0.],
       [0., 0.],
       [0., 0.],...
 y: array([[[0., 0.],
        [0., 0.],
        [0., 0.],...
2025-04-21 10:12:46.123367 test begin: paddle.nn.functional.cosine_similarity(Tensor([0, 5],"float64"), Tensor([1, 5],"float64"), axis=1, eps=1e-06, )

[accuracy error] backward  paddle.nn.functional.cosine_similarity(Tensor([0, 5],"float64"), Tensor([1, 5],"float64"), axis=1, eps=1e-06, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (5,), (1, 5) mismatch)
 x: array([0., 0., 0., 0., 0.])
 y: array([[0., 0., 0., 0., 0.]])
2025-04-21 10:12:46.318557 test begin: paddle.nn.functional.cosine_similarity(Tensor([0, 5],"float64"), Tensor([1, 5],"float64"), axis=1, eps=1e-07, )

[accuracy error] backward  paddle.nn.functional.cosine_similarity(Tensor([0, 5],"float64"), Tensor([1, 5],"float64"), axis=1, eps=1e-07, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (5,), (1, 5) mismatch)
 x: array([0., 0., 0., 0., 0.])
 y: array([[0., 0., 0., 0., 0.]])
2025-04-21 10:12:46.463324 test begin: paddle.nn.functional.cosine_similarity(Tensor([0, 5],"float64"), Tensor([1, 5],"float64"), axis=1, eps=1e-08, )

[accuracy error] backward  paddle.nn.functional.cosine_similarity(Tensor([0, 5],"float64"), Tensor([1, 5],"float64"), axis=1, eps=1e-08, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (5,), (1, 5) mismatch)
 x: array([0., 0., 0., 0., 0.])
 y: array([[0., 0., 0., 0., 0.]])
2025-04-21 10:12:46.592924 test begin: paddle.nn.functional.cosine_similarity(Tensor([0, 5],"float64"), Tensor([1, 5],"float64"), axis=1, eps=1e-09, )

[accuracy error] backward  paddle.nn.functional.cosine_similarity(Tensor([0, 5],"float64"), Tensor([1, 5],"float64"), axis=1, eps=1e-09, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (5,), (1, 5) mismatch)
 x: array([0., 0., 0., 0., 0.])
 y: array([[0., 0., 0., 0., 0.]])
2025-04-21 10:12:49.866797 test begin: paddle.nn.functional.cosine_similarity(Tensor([1, 5, 2],"float64"), Tensor([0, 5, 2],"float64"), axis=-1, eps=1e-08, )

[accuracy error] backward  paddle.nn.functional.cosine_similarity(Tensor([1, 5, 2],"float64"), Tensor([0, 5, 2],"float64"), axis=-1, eps=1e-08, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (5, 2), (1, 5, 2) mismatch)
 x: array([[0., 0.],
       [0., 0.],
       [0., 0.],...
 y: array([[[0., 0.],
        [0., 0.],
        [0., 0.],...
2025-04-21 10:12:50.089533 test begin: paddle.nn.functional.cosine_similarity(Tensor([1, 5, 2],"float64"), Tensor([0, 5, 2],"float64"), axis=-2, eps=1e-08, )

[accuracy error] backward  paddle.nn.functional.cosine_similarity(Tensor([1, 5, 2],"float64"), Tensor([0, 5, 2],"float64"), axis=-2, eps=1e-08, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (5, 2), (1, 5, 2) mismatch)
 x: array([[0., 0.],
       [0., 0.],
       [0., 0.],...
 y: array([[[0., 0.],
        [0., 0.],
        [0., 0.],...
2025-04-21 10:12:50.242482 test begin: paddle.nn.functional.cosine_similarity(Tensor([1, 5, 2],"float64"), Tensor([0, 5, 2],"float64"), axis=0, eps=1e-08, )

[accuracy error] backward  paddle.nn.functional.cosine_similarity(Tensor([1, 5, 2],"float64"), Tensor([0, 5, 2],"float64"), axis=0, eps=1e-08, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (5, 2), (1, 5, 2) mismatch)
 x: array([[0., 0.],
       [0., 0.],
       [0., 0.],...
 y: array([[[0., 0.],
        [0., 0.],
        [0., 0.],...
2025-04-21 10:12:50.458318 test begin: paddle.nn.functional.cosine_similarity(Tensor([1, 5, 2],"float64"), Tensor([0, 5, 2],"float64"), axis=1, eps=1e-08, )

[accuracy error] backward  paddle.nn.functional.cosine_similarity(Tensor([1, 5, 2],"float64"), Tensor([0, 5, 2],"float64"), axis=1, eps=1e-08, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (5, 2), (1, 5, 2) mismatch)
 x: array([[0., 0.],
       [0., 0.],
       [0., 0.],...
 y: array([[[0., 0.],
        [0., 0.],
        [0., 0.],...
2025-04-21 10:12:50.623018 test begin: paddle.nn.functional.cosine_similarity(Tensor([1, 5, 2],"float64"), Tensor([0, 5, 2],"float64"), axis=2, eps=1e-08, )

[accuracy error] backward  paddle.nn.functional.cosine_similarity(Tensor([1, 5, 2],"float64"), Tensor([0, 5, 2],"float64"), axis=2, eps=1e-08, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (5, 2), (1, 5, 2) mismatch)
 x: array([[0., 0.],
       [0., 0.],
       [0., 0.],...
 y: array([[[0., 0.],
        [0., 0.],
        [0., 0.],...
2025-04-21 10:12:50.888123 test begin: paddle.nn.functional.cosine_similarity(Tensor([1, 5],"float64"), Tensor([0, 5],"float64"), axis=1, eps=1e-06, )

[accuracy error] backward  paddle.nn.functional.cosine_similarity(Tensor([1, 5],"float64"), Tensor([0, 5],"float64"), axis=1, eps=1e-06, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (5,), (1, 5) mismatch)
 x: array([0., 0., 0., 0., 0.])
 y: array([[0., 0., 0., 0., 0.]])
2025-04-21 10:12:51.015042 test begin: paddle.nn.functional.cosine_similarity(Tensor([1, 5],"float64"), Tensor([0, 5],"float64"), axis=1, eps=1e-07, )

[accuracy error] backward  paddle.nn.functional.cosine_similarity(Tensor([1, 5],"float64"), Tensor([0, 5],"float64"), axis=1, eps=1e-07, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (5,), (1, 5) mismatch)
 x: array([0., 0., 0., 0., 0.])
 y: array([[0., 0., 0., 0., 0.]])
2025-04-21 10:12:51.229096 test begin: paddle.nn.functional.cosine_similarity(Tensor([1, 5],"float64"), Tensor([0, 5],"float64"), axis=1, eps=1e-08, )

[accuracy error] backward  paddle.nn.functional.cosine_similarity(Tensor([1, 5],"float64"), Tensor([0, 5],"float64"), axis=1, eps=1e-08, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (5,), (1, 5) mismatch)
 x: array([0., 0., 0., 0., 0.])
 y: array([[0., 0., 0., 0., 0.]])
2025-04-21 10:12:51.412702 test begin: paddle.nn.functional.cosine_similarity(Tensor([1, 5],"float64"), Tensor([0, 5],"float64"), axis=1, eps=1e-09, )

[accuracy error] backward  paddle.nn.functional.cosine_similarity(Tensor([1, 5],"float64"), Tensor([0, 5],"float64"), axis=1, eps=1e-09, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (5,), (1, 5) mismatch)
 x: array([0., 0., 0., 0., 0.])
 y: array([[0., 0., 0., 0., 0.]])
2025-04-21 10:12:51.767694 test begin: paddle.nn.functional.cosine_similarity(Tensor([10, 0, 10],"float32"), Tensor([10, 1, 10],"float32"), axis=2, eps=1e-06, )

[accuracy error] backward  paddle.nn.functional.cosine_similarity(Tensor([10, 0, 10],"float32"), Tensor([10, 1, 10],"float32"), axis=2, eps=1e-06, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (10, 10), (10, 1, 10) mismatch)
 x: array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],...
 y: array([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],

       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],...
2025-04-21 10:12:52.042400 test begin: paddle.nn.functional.cosine_similarity(Tensor([10, 12, 0],"float32"), Tensor([10, 1, 0],"float32"), axis=2, eps=1e-06, )

[accuracy error] backward  paddle.nn.functional.cosine_similarity(Tensor([10, 12, 0],"float32"), Tensor([10, 1, 0],"float32"), axis=2, eps=1e-06, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (10, 0), (10, 1, 0) mismatch)
 x: array([], shape=(10, 0), dtype=float32)
 y: array([], shape=(10, 1, 0), dtype=float32)
2025-04-21 10:12:54.776714 test begin: paddle.nn.functional.cosine_similarity(x1=Tensor([2, 0, 4],"float64"), x2=Tensor([2, 0, 4],"float64"), axis=1, eps=0, )

[accuracy error] paddle.nn.functional.cosine_similarity(x1=Tensor([2, 0, 4],"float64"), x2=Tensor([2, 0, 4],"float64"), axis=1, eps=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[nan, nan, nan, nan],
       [nan, nan, nan, nan]])
 y: array([[0., 0., 0., 0.],
       [0., 0., 0., 0.]])
2025-04-21 10:13:16.161466 test begin: paddle.nn.functional.grid_sample(Tensor([0, 1, 176, 176],"float32"), Tensor([0, 1, 12544, 2],"float32"), align_corners=False, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([0, 1, 176, 176],"float32"), Tensor([0, 1, 12544, 2],"float32"), align_corners=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:16.355067 test begin: paddle.nn.functional.grid_sample(Tensor([0, 1, 176, 176],"float32"), Tensor([0, 1, 37632, 2],"float32"), align_corners=False, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([0, 1, 176, 176],"float32"), Tensor([0, 1, 37632, 2],"float32"), align_corners=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:16.574735 test begin: paddle.nn.functional.grid_sample(Tensor([0, 2, 3, 3],"float64"), Tensor([0, 3, 3, 2],"float64"), mode="bilinear", padding_mode="reflection", align_corners=True, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([0, 2, 3, 3],"float64"), Tensor([0, 3, 3, 2],"float64"), mode="bilinear", padding_mode="reflection", align_corners=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:16.793768 test begin: paddle.nn.functional.grid_sample(Tensor([0, 256, 64, 64],"float32"), Tensor([0, 64, 64, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=True, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([0, 256, 64, 64],"float32"), Tensor([0, 64, 64, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:16.944427 test begin: paddle.nn.functional.grid_sample(Tensor([0, 3, 16, 16],"float32"), Tensor([0, 16, 16, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([0, 3, 16, 16],"float32"), Tensor([0, 16, 16, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:17.086871 test begin: paddle.nn.functional.grid_sample(Tensor([0, 3, 2, 2],"float32"), Tensor([0, 2, 2, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([0, 3, 2, 2],"float32"), Tensor([0, 2, 2, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:17.304411 test begin: paddle.nn.functional.grid_sample(Tensor([0, 3, 256, 256],"float32"), Tensor([0, 256, 256, 2],"float32"), mode="bilinear", padding_mode="reflection", align_corners=True, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([0, 3, 256, 256],"float32"), Tensor([0, 256, 256, 2],"float32"), mode="bilinear", padding_mode="reflection", align_corners=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:17.518348 test begin: paddle.nn.functional.grid_sample(Tensor([0, 3, 256, 256],"float32"), Tensor([0, 256, 256, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=True, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([0, 3, 256, 256],"float32"), Tensor([0, 256, 256, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:17.763429 test begin: paddle.nn.functional.grid_sample(Tensor([0, 32, 20, 20],"float32"), Tensor([0, 476, 4, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([0, 32, 20, 20],"float32"), Tensor([0, 476, 4, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:18.102418 test begin: paddle.nn.functional.grid_sample(Tensor([0, 4, 28, 28],"float32"), Tensor([0, 28, 28, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([0, 4, 28, 28],"float32"), Tensor([0, 28, 28, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:18.311996 test begin: paddle.nn.functional.grid_sample(Tensor([0, 4, 28, 28],"float32"), Tensor([0, 34, 34, 2],"float32"), mode="nearest", padding_mode="zeros", align_corners=False, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([0, 4, 28, 28],"float32"), Tensor([0, 34, 34, 2],"float32"), mode="nearest", padding_mode="zeros", align_corners=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:18.527084 test begin: paddle.nn.functional.grid_sample(Tensor([0, 4, 280, 350],"float32"), Tensor([0, 280, 350, 2],"float32"), mode="nearest", padding_mode="zeros", align_corners=False, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([0, 4, 280, 350],"float32"), Tensor([0, 280, 350, 2],"float32"), mode="nearest", padding_mode="zeros", align_corners=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:18.729899 test begin: paddle.nn.functional.grid_sample(Tensor([1, 0, 176, 176],"float32"), Tensor([1, 0, 12544, 2],"float32"), align_corners=False, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([1, 0, 176, 176],"float32"), Tensor([1, 0, 12544, 2],"float32"), align_corners=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:18.943809 test begin: paddle.nn.functional.grid_sample(Tensor([1, 0, 176, 176],"float32"), Tensor([1, 0, 37632, 2],"float32"), align_corners=False, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([1, 0, 176, 176],"float32"), Tensor([1, 0, 37632, 2],"float32"), align_corners=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:19.541727 test begin: paddle.nn.functional.grid_sample(Tensor([1, 0, 28, 28],"float32"), Tensor([1, 0, 28, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([1, 0, 28, 28],"float32"), Tensor([1, 0, 28, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:19.835200 test begin: paddle.nn.functional.grid_sample(Tensor([1, 0, 28, 28],"float32"), Tensor([1, 0, 34, 2],"float32"), mode="nearest", padding_mode="zeros", align_corners=False, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([1, 0, 28, 28],"float32"), Tensor([1, 0, 34, 2],"float32"), mode="nearest", padding_mode="zeros", align_corners=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:20.579501 test begin: paddle.nn.functional.grid_sample(Tensor([1, 0, 280, 350],"float32"), Tensor([1, 0, 350, 2],"float32"), mode="nearest", padding_mode="zeros", align_corners=False, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([1, 0, 280, 350],"float32"), Tensor([1, 0, 350, 2],"float32"), mode="nearest", padding_mode="zeros", align_corners=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:21.085090 test begin: paddle.nn.functional.grid_sample(Tensor([1, 1, 176, 176],"float32"), Tensor([1, 0, 12544, 2],"float32"), align_corners=False, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([1, 1, 176, 176],"float32"), Tensor([1, 0, 12544, 2],"float32"), align_corners=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:21.275806 test begin: paddle.nn.functional.grid_sample(Tensor([1, 1, 176, 176],"float32"), Tensor([1, 0, 37632, 2],"float32"), align_corners=False, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([1, 1, 176, 176],"float32"), Tensor([1, 0, 37632, 2],"float32"), align_corners=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:21.506047 test begin: paddle.nn.functional.grid_sample(Tensor([1, 1, 176, 176],"float32"), Tensor([1, 1, 0, 2],"float32"), align_corners=False, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([1, 1, 176, 176],"float32"), Tensor([1, 1, 0, 2],"float32"), align_corners=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:21.780056 test begin: paddle.nn.functional.grid_sample(Tensor([1, 4, 28, 28],"float32"), Tensor([1, 0, 28, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([1, 4, 28, 28],"float32"), Tensor([1, 0, 28, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:21.973361 test begin: paddle.nn.functional.grid_sample(Tensor([1, 4, 28, 28],"float32"), Tensor([1, 0, 34, 2],"float32"), mode="nearest", padding_mode="zeros", align_corners=False, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([1, 4, 28, 28],"float32"), Tensor([1, 0, 34, 2],"float32"), mode="nearest", padding_mode="zeros", align_corners=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:22.362642 test begin: paddle.nn.functional.grid_sample(Tensor([1, 4, 28, 28],"float32"), Tensor([1, 28, 0, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([1, 4, 28, 28],"float32"), Tensor([1, 28, 0, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:22.613402 test begin: paddle.nn.functional.grid_sample(Tensor([1, 4, 28, 28],"float32"), Tensor([1, 34, 0, 2],"float32"), mode="nearest", padding_mode="zeros", align_corners=False, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([1, 4, 28, 28],"float32"), Tensor([1, 34, 0, 2],"float32"), mode="nearest", padding_mode="zeros", align_corners=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:22.763952 test begin: paddle.nn.functional.grid_sample(Tensor([1, 4, 280, 350],"float32"), Tensor([1, 0, 350, 2],"float32"), mode="nearest", padding_mode="zeros", align_corners=False, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([1, 4, 280, 350],"float32"), Tensor([1, 0, 350, 2],"float32"), mode="nearest", padding_mode="zeros", align_corners=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:22.983394 test begin: paddle.nn.functional.grid_sample(Tensor([1, 4, 280, 350],"float32"), Tensor([1, 280, 0, 2],"float32"), mode="nearest", padding_mode="zeros", align_corners=False, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([1, 4, 280, 350],"float32"), Tensor([1, 280, 0, 2],"float32"), mode="nearest", padding_mode="zeros", align_corners=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:23.245741 test begin: paddle.nn.functional.grid_sample(Tensor([128, 0, 20, 20],"float32"), Tensor([128, 0, 4, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([128, 0, 20, 20],"float32"), Tensor([128, 0, 4, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:23.566435 test begin: paddle.nn.functional.grid_sample(Tensor([128, 32, 20, 20],"float32"), Tensor([128, 0, 4, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([128, 32, 20, 20],"float32"), Tensor([128, 0, 4, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:23.707698 test begin: paddle.nn.functional.grid_sample(Tensor([128, 32, 20, 20],"float32"), Tensor([128, 476, 0, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([128, 32, 20, 20],"float32"), Tensor([128, 476, 0, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:23.878531 test begin: paddle.nn.functional.grid_sample(Tensor([16, 0, 256, 256],"float32"), Tensor([16, 0, 256, 2],"float32"), mode="bilinear", padding_mode="reflection", align_corners=True, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([16, 0, 256, 256],"float32"), Tensor([16, 0, 256, 2],"float32"), mode="bilinear", padding_mode="reflection", align_corners=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:24.060482 test begin: paddle.nn.functional.grid_sample(Tensor([16, 0, 256, 256],"float32"), Tensor([16, 0, 256, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=True, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([16, 0, 256, 256],"float32"), Tensor([16, 0, 256, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:24.856190 test begin: paddle.nn.functional.grid_sample(Tensor([16, 0, 64, 64],"float32"), Tensor([16, 0, 64, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=True, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([16, 0, 64, 64],"float32"), Tensor([16, 0, 64, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:25.540944 test begin: paddle.nn.functional.grid_sample(Tensor([16, 256, 64, 64],"float32"), Tensor([16, 0, 64, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=True, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([16, 256, 64, 64],"float32"), Tensor([16, 0, 64, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:26.078736 test begin: paddle.nn.functional.grid_sample(Tensor([16, 256, 64, 64],"float32"), Tensor([16, 64, 0, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=True, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([16, 256, 64, 64],"float32"), Tensor([16, 64, 0, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:27.340759 test begin: paddle.nn.functional.grid_sample(Tensor([16, 3, 256, 256],"float32"), Tensor([16, 0, 256, 2],"float32"), mode="bilinear", padding_mode="reflection", align_corners=True, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([16, 3, 256, 256],"float32"), Tensor([16, 0, 256, 2],"float32"), mode="bilinear", padding_mode="reflection", align_corners=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:27.617131 test begin: paddle.nn.functional.grid_sample(Tensor([16, 3, 256, 256],"float32"), Tensor([16, 0, 256, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=True, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([16, 3, 256, 256],"float32"), Tensor([16, 0, 256, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:27.855841 test begin: paddle.nn.functional.grid_sample(Tensor([16, 3, 256, 256],"float32"), Tensor([16, 256, 0, 2],"float32"), mode="bilinear", padding_mode="reflection", align_corners=True, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([16, 3, 256, 256],"float32"), Tensor([16, 256, 0, 2],"float32"), mode="bilinear", padding_mode="reflection", align_corners=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:28.169112 test begin: paddle.nn.functional.grid_sample(Tensor([16, 3, 256, 256],"float32"), Tensor([16, 256, 0, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=True, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([16, 3, 256, 256],"float32"), Tensor([16, 256, 0, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:28.532738 test begin: paddle.nn.functional.grid_sample(Tensor([2, 0, 3, 3],"float64"), Tensor([2, 0, 3, 2],"float64"), mode="bilinear", padding_mode="reflection", align_corners=True, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([2, 0, 3, 3],"float64"), Tensor([2, 0, 3, 2],"float64"), mode="bilinear", padding_mode="reflection", align_corners=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:28.941778 test begin: paddle.nn.functional.grid_sample(Tensor([2, 2, 3, 3],"float64"), Tensor([2, 0, 3, 2],"float64"), mode="bilinear", padding_mode="reflection", align_corners=True, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([2, 2, 3, 3],"float64"), Tensor([2, 0, 3, 2],"float64"), mode="bilinear", padding_mode="reflection", align_corners=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:29.057876 test begin: paddle.nn.functional.grid_sample(Tensor([2, 2, 3, 3],"float64"), Tensor([2, 3, 0, 2],"float64"), mode="bilinear", padding_mode="reflection", align_corners=True, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([2, 2, 3, 3],"float64"), Tensor([2, 3, 0, 2],"float64"), mode="bilinear", padding_mode="reflection", align_corners=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:29.182905 test begin: paddle.nn.functional.grid_sample(Tensor([56, 0, 16, 16],"float32"), Tensor([56, 0, 16, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([56, 0, 16, 16],"float32"), Tensor([56, 0, 16, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:29.638694 test begin: paddle.nn.functional.grid_sample(Tensor([56, 0, 2, 2],"float32"), Tensor([56, 0, 2, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([56, 0, 2, 2],"float32"), Tensor([56, 0, 2, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:30.080171 test begin: paddle.nn.functional.grid_sample(Tensor([56, 3, 16, 16],"float32"), Tensor([56, 0, 16, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([56, 3, 16, 16],"float32"), Tensor([56, 0, 16, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:30.278897 test begin: paddle.nn.functional.grid_sample(Tensor([56, 3, 16, 16],"float32"), Tensor([56, 16, 0, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([56, 3, 16, 16],"float32"), Tensor([56, 16, 0, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:30.598427 test begin: paddle.nn.functional.grid_sample(Tensor([56, 3, 2, 2],"float32"), Tensor([56, 0, 2, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([56, 3, 2, 2],"float32"), Tensor([56, 0, 2, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:30.790814 test begin: paddle.nn.functional.grid_sample(Tensor([56, 3, 2, 2],"float32"), Tensor([56, 2, 0, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, )

[cuda error] paddle.nn.functional.grid_sample(Tensor([56, 3, 2, 2],"float32"), Tensor([56, 2, 0, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:30.988724 test begin: paddle.nn.functional.grid_sample(x=Tensor([0, 64, 80, 94, 311],"float32"), grid=Tensor([0, 280, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )

[cuda error] paddle.nn.functional.grid_sample(x=Tensor([0, 64, 80, 94, 311],"float32"), grid=Tensor([0, 280, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:31.640160 test begin: paddle.nn.functional.grid_sample(x=Tensor([4, 0, 80, 94, 311],"float32"), grid=Tensor([4, 0, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )

[cuda error] paddle.nn.functional.grid_sample(x=Tensor([4, 0, 80, 94, 311],"float32"), grid=Tensor([4, 0, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:45.437183 test begin: paddle.nn.functional.grid_sample(x=Tensor([4, 64, 80, 94, 311],"float32"), grid=Tensor([4, 0, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )

[cuda error] paddle.nn.functional.grid_sample(x=Tensor([4, 64, 80, 94, 311],"float32"), grid=Tensor([4, 0, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:58.060886 test begin: paddle.nn.functional.grid_sample(x=Tensor([4, 64, 80, 94, 311],"float32"), grid=Tensor([4, 280, 0, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )

[cuda error] paddle.nn.functional.grid_sample(x=Tensor([4, 64, 80, 94, 311],"float32"), grid=Tensor([4, 280, 0, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:14:09.229458 test begin: paddle.nn.functional.grid_sample(x=Tensor([4, 64, 80, 94, 311],"float32"), grid=Tensor([4, 280, 376, 0, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )

[cuda error] paddle.nn.functional.grid_sample(x=Tensor([4, 64, 80, 94, 311],"float32"), grid=Tensor([4, 280, 376, 0, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:14:56.076477 test begin: paddle.nn.functional.layer_norm(Tensor([0, 10, 4, 4],"float32"), list[10,4,4,], )

[cuda error] paddle.nn.functional.layer_norm(Tensor([0, 10, 4, 4],"float32"), list[10,4,4,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:14:56.204144 test begin: paddle.nn.functional.layer_norm(Tensor([0, 10, 4, 4],"float32"), tuple(10,4,4,), )

[cuda error] paddle.nn.functional.layer_norm(Tensor([0, 10, 4, 4],"float32"), tuple(10,4,4,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:14:56.315136 test begin: paddle.nn.functional.layer_norm(Tensor([0, 10, 60, 30],"float32"), list[10,60,30,], weight=None, bias=None, epsilon=1e-05, )

[cuda error] paddle.nn.functional.layer_norm(Tensor([0, 10, 60, 30],"float32"), list[10,60,30,], weight=None, bias=None, epsilon=1e-05, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:14:56.460232 test begin: paddle.nn.functional.layer_norm(Tensor([0, 10, 60, 70],"float32"), list[10,60,70,], weight=None, bias=None, epsilon=1e-05, )

[cuda error] paddle.nn.functional.layer_norm(Tensor([0, 10, 60, 70],"float32"), list[10,60,70,], weight=None, bias=None, epsilon=1e-05, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:14:56.576018 test begin: paddle.nn.functional.layer_norm(Tensor([0, 100],"float32"), list[100,], weight=Tensor([100],"float32"), bias=Tensor([100],"float32"), epsilon=1e-05, )

[cuda error] paddle.nn.functional.layer_norm(Tensor([0, 100],"float32"), list[100,], weight=Tensor([100],"float32"), bias=Tensor([100],"float32"), epsilon=1e-05, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:14:56.819631 test begin: paddle.nn.functional.layer_norm(Tensor([0, 128, 256],"float32"), list[256,], weight=Tensor([256],"float32"), bias=Tensor([256],"float32"), epsilon=1e-05, )

[cuda error] paddle.nn.functional.layer_norm(Tensor([0, 128, 256],"float32"), list[256,], weight=Tensor([256],"float32"), bias=Tensor([256],"float32"), epsilon=1e-05, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:14:56.948756 test begin: paddle.nn.functional.layer_norm(Tensor([0, 129],"float32"), list[129,], Tensor([129],"float32"), None, )

[cuda error] paddle.nn.functional.layer_norm(Tensor([0, 129],"float32"), list[129,], Tensor([129],"float32"), None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:14:57.103733 test begin: paddle.nn.functional.layer_norm(Tensor([0, 20],"float16"), list[20,], Tensor([20],"float16"), Tensor([20],"float16"), )

[cuda error] paddle.nn.functional.layer_norm(Tensor([0, 20],"float16"), list[20,], Tensor([20],"float16"), Tensor([20],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:14:57.324360 test begin: paddle.nn.functional.layer_norm(Tensor([0, 32, 128],"float32"), list[32,128,], )

[cuda error] paddle.nn.functional.layer_norm(Tensor([0, 32, 128],"float32"), list[32,128,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:14:57.449483 test begin: paddle.nn.functional.layer_norm(Tensor([0, 4],"float32"), list[4,], None, None, )

[cuda error] paddle.nn.functional.layer_norm(Tensor([0, 4],"float32"), list[4,], None, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:14:57.558169 test begin: paddle.nn.functional.layer_norm(Tensor([0, 4],"float32"), list[4,], Tensor([4],"float32"), Tensor([4],"float32"), )

[cuda error] paddle.nn.functional.layer_norm(Tensor([0, 4],"float32"), list[4,], Tensor([4],"float32"), Tensor([4],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:14:57.699989 test begin: paddle.nn.functional.layer_norm(Tensor([0, 512],"float32"), list[512,], None, None, )

[cuda error] paddle.nn.functional.layer_norm(Tensor([0, 512],"float32"), list[512,], None, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:14:57.836748 test begin: paddle.nn.functional.layer_norm(Tensor([0, 6, 6, 3],"float32"), list[6,6,3,], weight=None, bias=None, epsilon=1e-05, )

[cuda error] paddle.nn.functional.layer_norm(Tensor([0, 6, 6, 3],"float32"), list[6,6,3,], weight=None, bias=None, epsilon=1e-05, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:14:58.051054 test begin: paddle.nn.functional.layer_norm(Tensor([0, 6, 6, 3],"float64"), list[6,6,3,], weight=None, bias=None, epsilon=1e-05, )

[cuda error] paddle.nn.functional.layer_norm(Tensor([0, 6, 6, 3],"float64"), list[6,6,3,], weight=None, bias=None, epsilon=1e-05, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:14:58.195233 test begin: paddle.nn.functional.layer_norm(Tensor([0, 64, 128],"float32"), list[64,128,], None, None, )

[cuda error] paddle.nn.functional.layer_norm(Tensor([0, 64, 128],"float32"), list[64,128,], None, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:14:58.385217 test begin: paddle.nn.functional.layer_norm(Tensor([0, 64, 64],"float32"), list[64,], None, None, )

[cuda error] paddle.nn.functional.layer_norm(Tensor([0, 64, 64],"float32"), list[64,], None, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:14:58.596020 test begin: paddle.nn.functional.layer_norm(Tensor([0, 64, 64],"float32"), list[64,], Tensor([64],"float32"), Tensor([64],"float32"), )

[cuda error] paddle.nn.functional.layer_norm(Tensor([0, 64, 64],"float32"), list[64,], Tensor([64],"float32"), Tensor([64],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:14:58.800556 test begin: paddle.nn.functional.layer_norm(Tensor([0, 768],"float32"), list[768,], None, None, )

[cuda error] paddle.nn.functional.layer_norm(Tensor([0, 768],"float32"), list[768,], None, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:14:59.017653 test begin: paddle.nn.functional.layer_norm(Tensor([128, 0, 64],"float32"), list[64,], None, None, )

[cuda error] paddle.nn.functional.layer_norm(Tensor([128, 0, 64],"float32"), list[64,], None, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:14:59.275365 test begin: paddle.nn.functional.layer_norm(Tensor([128, 0, 64],"float32"), list[64,], Tensor([64],"float32"), Tensor([64],"float32"), )

[cuda error] paddle.nn.functional.layer_norm(Tensor([128, 0, 64],"float32"), list[64,], Tensor([64],"float32"), Tensor([64],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:14:59.544534 test begin: paddle.nn.functional.layer_norm(Tensor([8, 0, 256],"float32"), list[256,], weight=Tensor([256],"float32"), bias=Tensor([256],"float32"), epsilon=1e-05, )

[cuda error] paddle.nn.functional.layer_norm(Tensor([8, 0, 256],"float32"), list[256,], weight=Tensor([256],"float32"), bias=Tensor([256],"float32"), epsilon=1e-05, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:14:59.805950 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([0, 2, 2, 3],"float32"), )

[cuda error] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([0, 2, 2, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:15:00.052122 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([0, 2, 2, 3],"float32"), epsilon=1e-05, )

[cuda error] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([0, 2, 2, 3],"float32"), epsilon=1e-05, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:15:00.233880 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([0, 2, 2, 3],"float32"), epsilon=1e-05, weight=None, bias=None, )

[cuda error] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([0, 2, 2, 3],"float32"), epsilon=1e-05, weight=None, bias=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:15:00.425502 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([0, 2, 2, 3],"float64"), )

[cuda error] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([0, 2, 2, 3],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:15:00.655345 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([0, 2, 2, 3],"float64"), epsilon=1e-05, )

[cuda error] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([0, 2, 2, 3],"float64"), epsilon=1e-05, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:15:00.922271 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([0, 2, 2, 3],"float64"), epsilon=1e-05, weight=None, bias=None, )

[cuda error] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([0, 2, 2, 3],"float64"), epsilon=1e-05, weight=None, bias=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:15:01.188843 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,3,), x=Tensor([0, 2, 3],"float32"), )

[cuda error] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,3,), x=Tensor([0, 2, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:15:01.403770 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,3,), x=Tensor([0, 2, 3],"float32"), epsilon=1e-05, )

[cuda error] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,3,), x=Tensor([0, 2, 3],"float32"), epsilon=1e-05, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:15:01.607912 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,3,), x=Tensor([0, 2, 3],"float64"), )

[cuda error] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,3,), x=Tensor([0, 2, 3],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:15:01.842636 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,3,), x=Tensor([0, 2, 3],"float64"), epsilon=1e-05, )

[cuda error] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,3,), x=Tensor([0, 2, 3],"float64"), epsilon=1e-05, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:15:02.025655 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(3,), x=Tensor([0, 3],"float32"), )

[cuda error] paddle.nn.functional.layer_norm(normalized_shape=tuple(3,), x=Tensor([0, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:15:02.132684 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(3,), x=Tensor([0, 3],"float32"), epsilon=1e-05, )

[cuda error] paddle.nn.functional.layer_norm(normalized_shape=tuple(3,), x=Tensor([0, 3],"float32"), epsilon=1e-05, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:15:02.240141 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(3,), x=Tensor([0, 3],"float64"), )

[cuda error] paddle.nn.functional.layer_norm(normalized_shape=tuple(3,), x=Tensor([0, 3],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:15:02.347962 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(3,), x=Tensor([0, 3],"float64"), epsilon=1e-05, )

[cuda error] paddle.nn.functional.layer_norm(normalized_shape=tuple(3,), x=Tensor([0, 3],"float64"), epsilon=1e-05, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:15:28.901251 test begin: paddle.nn.functional.normalize(Tensor([0, 10],"float32"), axis=0, )

W0421 10:15:28.997613 10771 backward.cc:437] While running Node (MaximumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.nn.functional.normalize(Tensor([0, 10],"float32"), axis=0, ) 
 (InvalidArgument) ReduceOp: invalid axis, when x_dims is 1, axis[i] should less than x_dims, but got 1.
  [Hint: Expected e < dim_size, but received e:1 >= dim_size:1.] (at ../paddle/phi/kernels/funcs/reduce_function.h:112)

2025-04-21 10:15:30.426624 test begin: paddle.nn.functional.normalize(Tensor([0, 5],"float32"), axis=0, )

W0421 10:15:30.674103 11325 backward.cc:437] While running Node (MaximumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.nn.functional.normalize(Tensor([0, 5],"float32"), axis=0, ) 
 (InvalidArgument) ReduceOp: invalid axis, when x_dims is 1, axis[i] should less than x_dims, but got 1.
  [Hint: Expected e < dim_size, but received e:1 >= dim_size:1.] (at ../paddle/phi/kernels/funcs/reduce_function.h:112)

2025-04-21 10:15:32.049030 test begin: paddle.nn.functional.normalize(Tensor([1, 0, 16, 16],"float32"), axis=1, )

W0421 10:15:32.343653 12129 backward.cc:437] While running Node (MaximumGradNode) raises an EnforceNotMet exception
free(): invalid pointer


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   std::_Hashtable<egr::GradNodeBase*, std::pair<egr::GradNodeBase* const, std::unique_ptr<egr::GradTensorHolder, std::default_delete<egr::GradTensorHolder> > >, std::allocator<std::pair<egr::GradNodeBase* const, std::unique_ptr<egr::GradTensorHolder, std::default_delete<egr::GradTensorHolder> > > >, std::__detail::_Select1st, std::equal_to<egr::GradNodeBase*>, std::hash<egr::GradNodeBase*>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<false, false, true> >::~_Hashtable()

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201732 (unix time) try "date -d @1745201732" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x19fca) received by PID 106442 (TID 0x7f7dfb5fe700) from PID 106442 ***]

2025-04-21 10:15:37.970559 test begin: paddle.nn.functional.normalize(Tensor([1, 0, 32, 32],"float32"), axis=1, )

W0421 10:15:45.075743 13859 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:15:45.077368 13859 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
W0421 10:15:45.084862 13859 backward.cc:437] While running Node (MaximumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.nn.functional.normalize(Tensor([1, 0, 32, 32],"float32"), axis=1, ) 
 (InvalidArgument) ReduceOp: invalid axis, when x_dims is 3, axis[i] should less than x_dims, but got 3.
  [Hint: Expected e < dim_size, but received e:3 >= dim_size:3.] (at ../paddle/phi/kernels/funcs/reduce_function.h:112)

2025-04-21 10:15:45.085636 test begin: paddle.nn.functional.normalize(Tensor([1, 0],"float32"), axis=1, )

free(): invalid next size (fast)


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201745 (unix time) try "date -d @1745201745" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x3039) received by PID 12345 (TID 0x7fae79f48700) from PID 12345 ***]

2025-04-21 10:15:56.830691 test begin: paddle.nn.functional.normalize(Tensor([10, 0],"float16"), )

W0421 10:15:57.024744 19635 backward.cc:437] While running Node (MaximumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.nn.functional.normalize(Tensor([10, 0],"float16"), ) 
 (InvalidArgument) ReduceOp: invalid axis, when x_dims is 1, axis[i] should less than x_dims, but got 1.
  [Hint: Expected e < dim_size, but received e:1 >= dim_size:1.] (at ../paddle/phi/kernels/funcs/reduce_function.h:112)

2025-04-21 10:15:57.025309 test begin: paddle.nn.functional.normalize(Tensor([10, 0],"float32"), )

W0421 10:15:57.318048 19654 backward.cc:437] While running Node (MaximumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.nn.functional.normalize(Tensor([10, 0],"float32"), ) 
 (InvalidArgument) ReduceOp: invalid axis, when x_dims is 1, axis[i] should less than x_dims, but got 1.
  [Hint: Expected e < dim_size, but received e:1 >= dim_size:1.] (at ../paddle/phi/kernels/funcs/reduce_function.h:112)

2025-04-21 10:15:57.526122 test begin: paddle.nn.functional.normalize(Tensor([10, 0],"float32"), p=1.5, )

W0421 10:15:57.760295 19760 backward.cc:437] While running Node (MaximumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.nn.functional.normalize(Tensor([10, 0],"float32"), p=1.5, ) 
 (InvalidArgument) ReduceOp: invalid axis, when x_dims is 1, axis[i] should less than x_dims, but got 1.
  [Hint: Expected e < dim_size, but received e:1 >= dim_size:1.] (at ../paddle/phi/kernels/funcs/reduce_function.h:112)

2025-04-21 10:15:57.760732 test begin: paddle.nn.functional.normalize(Tensor([12, 0],"float32"), axis=-1, )

W0421 10:15:58.036846 19906 backward.cc:437] While running Node (MaximumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.nn.functional.normalize(Tensor([12, 0],"float32"), axis=-1, ) 
 (InvalidArgument) ReduceOp: invalid axis, when x_dims is 1, axis[i] should less than x_dims, but got 1.
  [Hint: Expected e < dim_size, but received e:1 >= dim_size:1.] (at ../paddle/phi/kernels/funcs/reduce_function.h:112)

2025-04-21 10:15:58.037553 test begin: paddle.nn.functional.normalize(Tensor([2, 0],"float16"), p=2, axis=-1, )

W0421 10:15:58.253569 19993 backward.cc:437] While running Node (MaximumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.nn.functional.normalize(Tensor([2, 0],"float16"), p=2, axis=-1, ) 
 (InvalidArgument) ReduceOp: invalid axis, when x_dims is 1, axis[i] should less than x_dims, but got 1.
  [Hint: Expected e < dim_size, but received e:1 >= dim_size:1.] (at ../paddle/phi/kernels/funcs/reduce_function.h:112)

2025-04-21 10:15:58.253962 test begin: paddle.nn.functional.normalize(Tensor([80, 0],"float32"), axis=-1, )

W0421 10:15:58.533504 20076 backward.cc:437] While running Node (MaximumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.nn.functional.normalize(Tensor([80, 0],"float32"), axis=-1, ) 
 (InvalidArgument) ReduceOp: invalid axis, when x_dims is 1, axis[i] should less than x_dims, but got 1.
  [Hint: Expected e < dim_size, but received e:1 >= dim_size:1.] (at ../paddle/phi/kernels/funcs/reduce_function.h:112)

2025-04-21 10:16:01.215911 test begin: paddle.nn.functional.normalize(x=Tensor([1, 0],"float32"), axis=-1, )

W0421 10:16:01.424510 21036 backward.cc:437] While running Node (MaximumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.nn.functional.normalize(x=Tensor([1, 0],"float32"), axis=-1, ) 
 (InvalidArgument) ReduceOp: invalid axis, when x_dims is 1, axis[i] should less than x_dims, but got 1.
  [Hint: Expected e < dim_size, but received e:1 >= dim_size:1.] (at ../paddle/phi/kernels/funcs/reduce_function.h:112)

2025-04-21 10:16:01.424860 test begin: paddle.nn.functional.normalize(x=Tensor([2, 0],"float32"), )

W0421 10:16:01.673600 21054 backward.cc:437] While running Node (MaximumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.nn.functional.normalize(x=Tensor([2, 0],"float32"), ) 
 (InvalidArgument) ReduceOp: invalid axis, when x_dims is 1, axis[i] should less than x_dims, but got 1.
  [Hint: Expected e < dim_size, but received e:1 >= dim_size:1.] (at ../paddle/phi/kernels/funcs/reduce_function.h:112)

2025-04-21 10:16:01.674079 test begin: paddle.nn.functional.normalize(x=Tensor([2, 0],"float64"), )

W0421 10:16:01.877291 21063 backward.cc:437] While running Node (MaximumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.nn.functional.normalize(x=Tensor([2, 0],"float64"), ) 
 (InvalidArgument) ReduceOp: invalid axis, when x_dims is 1, axis[i] should less than x_dims, but got 1.
  [Hint: Expected e < dim_size, but received e:1 >= dim_size:1.] (at ../paddle/phi/kernels/funcs/reduce_function.h:112)

2025-04-21 10:16:01.877977 test begin: paddle.nn.functional.normalize(x=Tensor([4, 0, 6, 7],"float64"), )

W0421 10:16:02.207880 21139 backward.cc:437] While running Node (MaximumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.nn.functional.normalize(x=Tensor([4, 0, 6, 7],"float64"), ) 
 (InvalidArgument) The 2-th dimension of input tensor is expected to be equal with the 2-th dimension of output tensor 7 or 1, but received 6.
  [Hint: Expected in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1 == true, but received in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1:0 != true:1.] (at ../paddle/phi/kernels/funcs/dims_simplifier.h:134)

2025-04-21 10:16:02.208471 test begin: paddle.nn.functional.normalize(x=Tensor([4, 0, 6, 7],"float64"), p=1, )

W0421 10:16:02.421588 21308 backward.cc:437] While running Node (MaximumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.nn.functional.normalize(x=Tensor([4, 0, 6, 7],"float64"), p=1, ) 
 (InvalidArgument) The 2-th dimension of input tensor is expected to be equal with the 2-th dimension of output tensor 7 or 1, but received 6.
  [Hint: Expected in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1 == true, but received in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1:0 != true:1.] (at ../paddle/phi/kernels/funcs/dims_simplifier.h:134)

2025-04-21 10:16:02.421985 test begin: paddle.nn.functional.normalize(x=Tensor([4, 0, 6, 7],"float64"), p=4, )

W0421 10:16:02.591733 21323 backward.cc:437] While running Node (MaximumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.nn.functional.normalize(x=Tensor([4, 0, 6, 7],"float64"), p=4, ) 
 (InvalidArgument) The 2-th dimension of input tensor is expected to be equal with the 2-th dimension of output tensor 7 or 1, but received 6.
  [Hint: Expected in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1 == true, but received in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1:0 != true:1.] (at ../paddle/phi/kernels/funcs/dims_simplifier.h:134)

2025-04-21 10:16:02.789144 test begin: paddle.nn.functional.normalize(x=Tensor([4, 0, 6],"float64"), )

W0421 10:16:03.075960 21415 backward.cc:437] While running Node (MaximumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.nn.functional.normalize(x=Tensor([4, 0, 6],"float64"), ) 
 (InvalidArgument) ReduceOp: invalid axis, when x_dims is 2, axis[i] should less than x_dims, but got 2.
  [Hint: Expected e < dim_size, but received e:2 >= dim_size:2.] (at ../paddle/phi/kernels/funcs/reduce_function.h:112)

2025-04-21 10:16:03.076431 test begin: paddle.nn.functional.normalize(x=Tensor([4, 0],"float64"), p=1.2, )

W0421 10:16:03.278537 21571 backward.cc:437] While running Node (MaximumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.nn.functional.normalize(x=Tensor([4, 0],"float64"), p=1.2, ) 
 (InvalidArgument) ReduceOp: invalid axis, when x_dims is 1, axis[i] should less than x_dims, but got 1.
  [Hint: Expected e < dim_size, but received e:1 >= dim_size:1.] (at ../paddle/phi/kernels/funcs/reduce_function.h:112)

2025-04-21 10:16:05.103840 test begin: paddle.nn.functional.normalize(x=Tensor([4, 5, 6, 0],"float64"), p=4, axis=3, )

free(): invalid next size (fast)


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   MaximumGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::maximum_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor*, paddle::Tensor*)
4   void phi::MaximumGradKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201765 (unix time) try "date -d @1745201765" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x3ed0) received by PID 16080 (TID 0x7fa9a9935700) from PID 16080 ***]

2025-04-21 10:16:09.416785 test begin: paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float32"), Tensor([0, 100],"float32"), -1, 1e-06, False, None, )

W0421 10:16:15.242743 23854 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:16:15.244289 23854 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float32"), Tensor([0, 100],"float32"), -1, 1e-06, False, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:15.254520 test begin: paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float32"), Tensor([0, 100],"float32"), -1, 1e-06, True, None, )

[cuda error] paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float32"), Tensor([0, 100],"float32"), -1, 1e-06, True, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:15.386289 test begin: paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float32"), Tensor([0, 100],"float32"), -math.inf, 1e-06, False, None, )

[cuda error] paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float32"), Tensor([0, 100],"float32"), -math.inf, 1e-06, False, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:15.603747 test begin: paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float32"), Tensor([0, 100],"float32"), -math.inf, 1e-06, True, None, )

[cuda error] paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float32"), Tensor([0, 100],"float32"), -math.inf, 1e-06, True, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:16.248119 test begin: paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float32"), Tensor([0, 100],"float32"), 1, 1e-06, False, None, )

[cuda error] paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float32"), Tensor([0, 100],"float32"), 1, 1e-06, False, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:16.444316 test begin: paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float32"), Tensor([0, 100],"float32"), 1, 1e-06, True, None, )

[cuda error] paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float32"), Tensor([0, 100],"float32"), 1, 1e-06, True, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:16.651763 test begin: paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float32"), Tensor([0, 100],"float32"), 2, 1e-06, False, None, )

[cuda error] paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float32"), Tensor([0, 100],"float32"), 2, 1e-06, False, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:16.856768 test begin: paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float32"), Tensor([0, 100],"float32"), 2, 1e-06, True, None, )

[cuda error] paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float32"), Tensor([0, 100],"float32"), 2, 1e-06, True, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:17.060080 test begin: paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float32"), Tensor([0, 100],"float32"), math.inf, 1e-06, False, None, )

[cuda error] paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float32"), Tensor([0, 100],"float32"), math.inf, 1e-06, False, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:17.268707 test begin: paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float32"), Tensor([0, 100],"float32"), math.inf, 1e-06, True, None, )

[cuda error] paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float32"), Tensor([0, 100],"float32"), math.inf, 1e-06, True, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:17.402276 test begin: paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float32"), Tensor([0, 1],"float32"), 2.0, 1e-06, False, None, )

W0421 10:16:17.523674 26833 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float32"), Tensor([0, 1],"float32"), 2.0, 1e-06, False, None, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:16:17.540323 test begin: paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float32"), Tensor([100],"float32"), 2.0, 1e-06, False, None, )

W0421 10:16:17.668953 26922 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float32"), Tensor([100],"float32"), 2.0, 1e-06, False, None, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:16:17.669512 test begin: paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float64"), Tensor([0, 100],"float64"), -1, 1e-06, False, None, )

[cuda error] paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float64"), Tensor([0, 100],"float64"), -1, 1e-06, False, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:17.862795 test begin: paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float64"), Tensor([0, 100],"float64"), -1, 1e-06, True, None, )

[cuda error] paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float64"), Tensor([0, 100],"float64"), -1, 1e-06, True, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:18.081156 test begin: paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float64"), Tensor([0, 100],"float64"), -math.inf, 1e-06, False, None, )

[cuda error] paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float64"), Tensor([0, 100],"float64"), -math.inf, 1e-06, False, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:18.283560 test begin: paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float64"), Tensor([0, 100],"float64"), -math.inf, 1e-06, True, None, )

[cuda error] paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float64"), Tensor([0, 100],"float64"), -math.inf, 1e-06, True, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:18.810175 test begin: paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float64"), Tensor([0, 100],"float64"), 1, 1e-06, False, None, )

[cuda error] paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float64"), Tensor([0, 100],"float64"), 1, 1e-06, False, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:18.952566 test begin: paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float64"), Tensor([0, 100],"float64"), 1, 1e-06, True, None, )

[cuda error] paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float64"), Tensor([0, 100],"float64"), 1, 1e-06, True, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:19.168908 test begin: paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float64"), Tensor([0, 100],"float64"), 2, 1e-06, False, None, )

[cuda error] paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float64"), Tensor([0, 100],"float64"), 2, 1e-06, False, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:19.376044 test begin: paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float64"), Tensor([0, 100],"float64"), 2, 1e-06, True, None, )

[cuda error] paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float64"), Tensor([0, 100],"float64"), 2, 1e-06, True, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:19.535336 test begin: paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float64"), Tensor([0, 100],"float64"), math.inf, 1e-06, False, None, )

[cuda error] paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float64"), Tensor([0, 100],"float64"), math.inf, 1e-06, False, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:19.671397 test begin: paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float64"), Tensor([0, 100],"float64"), math.inf, 1e-06, True, None, )

[cuda error] paddle.nn.functional.pairwise_distance(Tensor([0, 100],"float64"), Tensor([0, 100],"float64"), math.inf, 1e-06, True, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:19.873762 test begin: paddle.nn.functional.pairwise_distance(Tensor([0],"float32"), Tensor([0],"float32"), )

[cuda error] paddle.nn.functional.pairwise_distance(Tensor([0],"float32"), Tensor([0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:20.514318 test begin: paddle.nn.functional.pairwise_distance(Tensor([0],"float32"), Tensor([0],"float32"), 1, 1e-06, False, None, )

[cuda error] paddle.nn.functional.pairwise_distance(Tensor([0],"float32"), Tensor([0],"float32"), 1, 1e-06, False, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:20.660097 test begin: paddle.nn.functional.pairwise_distance(Tensor([0],"float32"), Tensor([0],"float32"), 1, 1e-06, True, None, )

[cuda error] paddle.nn.functional.pairwise_distance(Tensor([0],"float32"), Tensor([0],"float32"), 1, 1e-06, True, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:20.901065 test begin: paddle.nn.functional.pairwise_distance(Tensor([0],"float32"), Tensor([0],"float32"), 2, 1e-06, False, None, )

[cuda error] paddle.nn.functional.pairwise_distance(Tensor([0],"float32"), Tensor([0],"float32"), 2, 1e-06, False, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:21.128439 test begin: paddle.nn.functional.pairwise_distance(Tensor([0],"float32"), Tensor([0],"float32"), 2, 1e-06, True, None, )

[cuda error] paddle.nn.functional.pairwise_distance(Tensor([0],"float32"), Tensor([0],"float32"), 2, 1e-06, True, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:21.904138 test begin: paddle.nn.functional.pairwise_distance(Tensor([0],"float64"), Tensor([0],"float64"), 1, 1e-06, False, None, )

[cuda error] paddle.nn.functional.pairwise_distance(Tensor([0],"float64"), Tensor([0],"float64"), 1, 1e-06, False, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:22.113527 test begin: paddle.nn.functional.pairwise_distance(Tensor([0],"float64"), Tensor([0],"float64"), 1, 1e-06, True, None, )

[cuda error] paddle.nn.functional.pairwise_distance(Tensor([0],"float64"), Tensor([0],"float64"), 1, 1e-06, True, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:22.388971 test begin: paddle.nn.functional.pairwise_distance(Tensor([0],"float64"), Tensor([0],"float64"), 2, 1e-06, False, None, )

[cuda error] paddle.nn.functional.pairwise_distance(Tensor([0],"float64"), Tensor([0],"float64"), 2, 1e-06, False, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:22.687555 test begin: paddle.nn.functional.pairwise_distance(Tensor([0],"float64"), Tensor([0],"float64"), 2, 1e-06, True, None, )

[cuda error] paddle.nn.functional.pairwise_distance(Tensor([0],"float64"), Tensor([0],"float64"), 2, 1e-06, True, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:23.648019 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 0],"float32"), Tensor([100, 0],"float32"), 1, 1e-06, False, None, )

[cuda error] paddle.nn.functional.pairwise_distance(Tensor([100, 0],"float32"), Tensor([100, 0],"float32"), 1, 1e-06, False, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:23.874516 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 0],"float32"), Tensor([100, 0],"float32"), 1, 1e-06, True, None, )

[cuda error] paddle.nn.functional.pairwise_distance(Tensor([100, 0],"float32"), Tensor([100, 0],"float32"), 1, 1e-06, True, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:24.200717 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 0],"float32"), Tensor([100, 0],"float32"), 2, 1e-06, False, None, )

[cuda error] paddle.nn.functional.pairwise_distance(Tensor([100, 0],"float32"), Tensor([100, 0],"float32"), 2, 1e-06, False, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:24.516384 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 0],"float32"), Tensor([100, 0],"float32"), 2, 1e-06, True, None, )

[cuda error] paddle.nn.functional.pairwise_distance(Tensor([100, 0],"float32"), Tensor([100, 0],"float32"), 2, 1e-06, True, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:24.659759 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 0],"float32"), Tensor([100, 0],"float32"), 2.0, 1e-06, False, None, )

[cuda error] paddle.nn.functional.pairwise_distance(Tensor([100, 0],"float32"), Tensor([100, 0],"float32"), 2.0, 1e-06, False, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:24.819608 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 0],"float32"), Tensor([100, 1],"float32"), 2.0, 1e-06, False, None, )

W0421 10:16:25.099388 29514 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.nn.functional.pairwise_distance(Tensor([100, 0],"float32"), Tensor([100, 1],"float32"), 2.0, 1e-06, False, None, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:16:25.646703 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 0],"float64"), Tensor([100, 0],"float64"), 1, 1e-06, False, None, )

[cuda error] paddle.nn.functional.pairwise_distance(Tensor([100, 0],"float64"), Tensor([100, 0],"float64"), 1, 1e-06, False, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:25.873444 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 0],"float64"), Tensor([100, 0],"float64"), 1, 1e-06, True, None, )

[cuda error] paddle.nn.functional.pairwise_distance(Tensor([100, 0],"float64"), Tensor([100, 0],"float64"), 1, 1e-06, True, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:26.172146 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 0],"float64"), Tensor([100, 0],"float64"), 2, 1e-06, False, None, )

[cuda error] paddle.nn.functional.pairwise_distance(Tensor([100, 0],"float64"), Tensor([100, 0],"float64"), 2, 1e-06, False, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:26.456611 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 0],"float64"), Tensor([100, 0],"float64"), 2, 1e-06, True, None, )

[cuda error] paddle.nn.functional.pairwise_distance(Tensor([100, 0],"float64"), Tensor([100, 0],"float64"), 2, 1e-06, True, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:26.825144 test begin: paddle.nn.functional.pairwise_distance(x=Tensor([0, 100],"float32"), y=Tensor([0, 1],"float32"), p=2.0, epsilon=1e-06, keepdim=False, )

W0421 10:16:27.001286 30721 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.nn.functional.pairwise_distance(x=Tensor([0, 100],"float32"), y=Tensor([0, 1],"float32"), p=2.0, epsilon=1e-06, keepdim=False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:16:27.004562 test begin: paddle.nn.functional.pairwise_distance(x=Tensor([0, 100],"float32"), y=Tensor([100],"float32"), p=2.0, epsilon=1e-06, keepdim=False, )

W0421 10:16:27.258625 30799 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.nn.functional.pairwise_distance(x=Tensor([0, 100],"float32"), y=Tensor([100],"float32"), p=2.0, epsilon=1e-06, keepdim=False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:16:27.260639 test begin: paddle.nn.functional.pairwise_distance(x=Tensor([100, 0],"float32"), y=Tensor([100, 0],"float32"), p=2.0, epsilon=1e-06, keepdim=False, )

[cuda error] paddle.nn.functional.pairwise_distance(x=Tensor([100, 0],"float32"), y=Tensor([100, 0],"float32"), p=2.0, epsilon=1e-06, keepdim=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:27.487033 test begin: paddle.nn.functional.pairwise_distance(x=Tensor([100, 0],"float32"), y=Tensor([100, 1],"float32"), p=2.0, epsilon=1e-06, keepdim=False, )

W0421 10:16:27.784668 30853 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.nn.functional.pairwise_distance(x=Tensor([100, 0],"float32"), y=Tensor([100, 1],"float32"), p=2.0, epsilon=1e-06, keepdim=False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:16:49.745902 test begin: paddle.nn.functional.rrelu(Tensor([0, 2, 3, 4],"float64"), 0.05, 0.25, training=False, )

[cuda error] paddle.nn.functional.rrelu(Tensor([0, 2, 3, 4],"float64"), 0.05, 0.25, training=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:49.958031 test begin: paddle.nn.functional.rrelu(Tensor([0, 2, 3, 4],"float64"), 0.1, 0.33, training=False, )

[cuda error] paddle.nn.functional.rrelu(Tensor([0, 2, 3, 4],"float64"), 0.1, 0.33, training=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:50.236311 test begin: paddle.nn.functional.rrelu(Tensor([0, 3, 4, 5],"float16"), 0.1, 0.3, training=False, )

[cuda error] paddle.nn.functional.rrelu(Tensor([0, 3, 4, 5],"float16"), 0.1, 0.3, training=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:50.473189 test begin: paddle.nn.functional.rrelu(Tensor([0, 3, 4, 5],"float16"), 0.3, 0.300000009, training=True, )

[cuda error] paddle.nn.functional.rrelu(Tensor([0, 3, 4, 5],"float16"), 0.3, 0.300000009, training=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:50.759752 test begin: paddle.nn.functional.rrelu(Tensor([0, 3, 4, 5],"float32"), 0.1, 0.3, training=False, )

[cuda error] paddle.nn.functional.rrelu(Tensor([0, 3, 4, 5],"float32"), 0.1, 0.3, training=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:51.096283 test begin: paddle.nn.functional.rrelu(Tensor([0, 3, 4, 5],"float32"), 0.3, 0.300000009, training=True, )

[cuda error] paddle.nn.functional.rrelu(Tensor([0, 3, 4, 5],"float32"), 0.3, 0.300000009, training=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:51.374535 test begin: paddle.nn.functional.rrelu(Tensor([1, 0, 3, 4],"float64"), 0.05, 0.25, training=False, )

[cuda error] paddle.nn.functional.rrelu(Tensor([1, 0, 3, 4],"float64"), 0.05, 0.25, training=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:51.545181 test begin: paddle.nn.functional.rrelu(Tensor([1, 0, 3, 4],"float64"), 0.1, 0.33, training=False, )

[cuda error] paddle.nn.functional.rrelu(Tensor([1, 0, 3, 4],"float64"), 0.1, 0.33, training=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:51.793232 test begin: paddle.nn.functional.rrelu(Tensor([1, 2, 0, 4],"float64"), 0.05, 0.25, training=False, )

[cuda error] paddle.nn.functional.rrelu(Tensor([1, 2, 0, 4],"float64"), 0.05, 0.25, training=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:52.033143 test begin: paddle.nn.functional.rrelu(Tensor([1, 2, 0, 4],"float64"), 0.1, 0.33, training=False, )

[cuda error] paddle.nn.functional.rrelu(Tensor([1, 2, 0, 4],"float64"), 0.1, 0.33, training=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:52.270897 test begin: paddle.nn.functional.rrelu(Tensor([1, 2, 3, 0],"float64"), 0.05, 0.25, training=False, )

[cuda error] paddle.nn.functional.rrelu(Tensor([1, 2, 3, 0],"float64"), 0.05, 0.25, training=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:52.486003 test begin: paddle.nn.functional.rrelu(Tensor([1, 2, 3, 0],"float64"), 0.1, 0.33, training=False, )

[cuda error] paddle.nn.functional.rrelu(Tensor([1, 2, 3, 0],"float64"), 0.1, 0.33, training=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:52.796265 test begin: paddle.nn.functional.rrelu(Tensor([2, 0, 4, 5],"float16"), 0.1, 0.3, training=False, )

[cuda error] paddle.nn.functional.rrelu(Tensor([2, 0, 4, 5],"float16"), 0.1, 0.3, training=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:53.112790 test begin: paddle.nn.functional.rrelu(Tensor([2, 0, 4, 5],"float16"), 0.3, 0.300000009, training=True, )

[cuda error] paddle.nn.functional.rrelu(Tensor([2, 0, 4, 5],"float16"), 0.3, 0.300000009, training=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:53.291449 test begin: paddle.nn.functional.rrelu(Tensor([2, 0, 4, 5],"float32"), 0.1, 0.3, training=False, )

[cuda error] paddle.nn.functional.rrelu(Tensor([2, 0, 4, 5],"float32"), 0.1, 0.3, training=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:53.555716 test begin: paddle.nn.functional.rrelu(Tensor([2, 0, 4, 5],"float32"), 0.3, 0.300000009, training=True, )

[cuda error] paddle.nn.functional.rrelu(Tensor([2, 0, 4, 5],"float32"), 0.3, 0.300000009, training=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:53.777076 test begin: paddle.nn.functional.rrelu(Tensor([2, 3, 0, 5],"float16"), 0.1, 0.3, training=False, )

[cuda error] paddle.nn.functional.rrelu(Tensor([2, 3, 0, 5],"float16"), 0.1, 0.3, training=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:53.963402 test begin: paddle.nn.functional.rrelu(Tensor([2, 3, 0, 5],"float16"), 0.3, 0.300000009, training=True, )

[cuda error] paddle.nn.functional.rrelu(Tensor([2, 3, 0, 5],"float16"), 0.3, 0.300000009, training=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:54.103578 test begin: paddle.nn.functional.rrelu(Tensor([2, 3, 0, 5],"float32"), 0.1, 0.3, training=False, )

[cuda error] paddle.nn.functional.rrelu(Tensor([2, 3, 0, 5],"float32"), 0.1, 0.3, training=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:54.279596 test begin: paddle.nn.functional.rrelu(Tensor([2, 3, 0, 5],"float32"), 0.3, 0.300000009, training=True, )

[cuda error] paddle.nn.functional.rrelu(Tensor([2, 3, 0, 5],"float32"), 0.3, 0.300000009, training=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:54.403019 test begin: paddle.nn.functional.rrelu(Tensor([2, 3, 4, 0],"float16"), 0.1, 0.3, training=False, )

[cuda error] paddle.nn.functional.rrelu(Tensor([2, 3, 4, 0],"float16"), 0.1, 0.3, training=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:54.655329 test begin: paddle.nn.functional.rrelu(Tensor([2, 3, 4, 0],"float16"), 0.3, 0.300000009, training=True, )

[cuda error] paddle.nn.functional.rrelu(Tensor([2, 3, 4, 0],"float16"), 0.3, 0.300000009, training=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:54.911019 test begin: paddle.nn.functional.rrelu(Tensor([2, 3, 4, 0],"float32"), 0.1, 0.3, training=False, )

[cuda error] paddle.nn.functional.rrelu(Tensor([2, 3, 4, 0],"float32"), 0.1, 0.3, training=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:16:55.186331 test begin: paddle.nn.functional.rrelu(Tensor([2, 3, 4, 0],"float32"), 0.3, 0.300000009, training=True, )

[cuda error] paddle.nn.functional.rrelu(Tensor([2, 3, 4, 0],"float32"), 0.3, 0.300000009, training=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:17:42.343156 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([0, 5],"float64"), Tensor([0, 5],"float64"), Tensor([0, 5],"float64"), margin=0.3, swap=False, reduction="mean", name=None, )

[cuda error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([0, 5],"float64"), Tensor([0, 5],"float64"), Tensor([0, 5],"float64"), margin=0.3, swap=False, reduction="mean", name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:17:42.543713 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([0, 5],"float64"), Tensor([0, 5],"float64"), Tensor([0, 5],"float64"), margin=0.3, swap=False, reduction="none", name=None, )

[cuda error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([0, 5],"float64"), Tensor([0, 5],"float64"), Tensor([0, 5],"float64"), margin=0.3, swap=False, reduction="none", name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:17:42.793391 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([0, 5],"float64"), Tensor([0, 5],"float64"), Tensor([0, 5],"float64"), margin=0.3, swap=False, reduction="sum", name=None, )

[cuda error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([0, 5],"float64"), Tensor([0, 5],"float64"), Tensor([0, 5],"float64"), margin=0.3, swap=False, reduction="sum", name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:17:42.941976 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([0, 5],"float64"), Tensor([0, 5],"float64"), Tensor([0, 5],"float64"), margin=0.3, swap=True, reduction="mean", name=None, )

[cuda error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([0, 5],"float64"), Tensor([0, 5],"float64"), Tensor([0, 5],"float64"), margin=0.3, swap=True, reduction="mean", name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:17:43.178369 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 0],"float64"), Tensor([5, 0],"float64"), Tensor([5, 0],"float64"), margin=0.3, swap=False, reduction="mean", name=None, )

[paddle error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 0],"float64"), Tensor([5, 0],"float64"), Tensor([5, 0],"float64"), margin=0.3, swap=False, reduction="mean", name=None, ) 
 The positive distance or negative distance should be greater than 0, The distance functions should be checked.
2025-04-21 10:17:43.457670 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 0],"float64"), Tensor([5, 0],"float64"), Tensor([5, 0],"float64"), margin=0.3, swap=False, reduction="none", name=None, )

[paddle error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 0],"float64"), Tensor([5, 0],"float64"), Tensor([5, 0],"float64"), margin=0.3, swap=False, reduction="none", name=None, ) 
 The positive distance or negative distance should be greater than 0, The distance functions should be checked.
2025-04-21 10:17:43.748861 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 0],"float64"), Tensor([5, 0],"float64"), Tensor([5, 0],"float64"), margin=0.3, swap=False, reduction="sum", name=None, )

[paddle error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 0],"float64"), Tensor([5, 0],"float64"), Tensor([5, 0],"float64"), margin=0.3, swap=False, reduction="sum", name=None, ) 
 The positive distance or negative distance should be greater than 0, The distance functions should be checked.
2025-04-21 10:17:43.962623 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 0],"float64"), Tensor([5, 0],"float64"), Tensor([5, 0],"float64"), margin=0.3, swap=True, reduction="mean", name=None, )

[paddle error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 0],"float64"), Tensor([5, 0],"float64"), Tensor([5, 0],"float64"), margin=0.3, swap=True, reduction="mean", name=None, ) 
 The positive distance or negative distance should be greater than 0, The distance functions should be checked.
2025-04-21 10:17:44.232947 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([0, 5],"float64"), positive=Tensor([0, 5],"float64"), negative=Tensor([0, 5],"float64"), distance_function=None, margin=0.3, swap=False, reduction="mean", )

[cuda error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([0, 5],"float64"), positive=Tensor([0, 5],"float64"), negative=Tensor([0, 5],"float64"), distance_function=None, margin=0.3, swap=False, reduction="mean", ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:17:44.583681 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([0, 5],"float64"), positive=Tensor([0, 5],"float64"), negative=Tensor([0, 5],"float64"), distance_function=None, margin=0.3, swap=False, reduction="none", )

[cuda error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([0, 5],"float64"), positive=Tensor([0, 5],"float64"), negative=Tensor([0, 5],"float64"), distance_function=None, margin=0.3, swap=False, reduction="none", ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:17:44.967063 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([0, 5],"float64"), positive=Tensor([0, 5],"float64"), negative=Tensor([0, 5],"float64"), distance_function=None, margin=0.3, swap=False, reduction="sum", )

[cuda error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([0, 5],"float64"), positive=Tensor([0, 5],"float64"), negative=Tensor([0, 5],"float64"), distance_function=None, margin=0.3, swap=False, reduction="sum", ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:17:45.201236 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([0, 5],"float64"), positive=Tensor([0, 5],"float64"), negative=Tensor([0, 5],"float64"), distance_function=None, margin=0.3, swap=True, reduction="mean", )

[cuda error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([0, 5],"float64"), positive=Tensor([0, 5],"float64"), negative=Tensor([0, 5],"float64"), distance_function=None, margin=0.3, swap=True, reduction="mean", ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:17:45.535624 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 0],"float64"), positive=Tensor([5, 0],"float64"), negative=Tensor([5, 0],"float64"), distance_function=None, margin=0.3, swap=False, reduction="mean", )

[paddle error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 0],"float64"), positive=Tensor([5, 0],"float64"), negative=Tensor([5, 0],"float64"), distance_function=None, margin=0.3, swap=False, reduction="mean", ) 
 The positive distance or negative distance should be greater than 0, The distance functions should be checked.
2025-04-21 10:17:45.720610 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 0],"float64"), positive=Tensor([5, 0],"float64"), negative=Tensor([5, 0],"float64"), distance_function=None, margin=0.3, swap=False, reduction="none", )

[paddle error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 0],"float64"), positive=Tensor([5, 0],"float64"), negative=Tensor([5, 0],"float64"), distance_function=None, margin=0.3, swap=False, reduction="none", ) 
 The positive distance or negative distance should be greater than 0, The distance functions should be checked.
2025-04-21 10:17:45.875727 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 0],"float64"), positive=Tensor([5, 0],"float64"), negative=Tensor([5, 0],"float64"), distance_function=None, margin=0.3, swap=False, reduction="sum", )

[paddle error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 0],"float64"), positive=Tensor([5, 0],"float64"), negative=Tensor([5, 0],"float64"), distance_function=None, margin=0.3, swap=False, reduction="sum", ) 
 The positive distance or negative distance should be greater than 0, The distance functions should be checked.
2025-04-21 10:17:46.079888 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 0],"float64"), positive=Tensor([5, 0],"float64"), negative=Tensor([5, 0],"float64"), distance_function=None, margin=0.3, swap=True, reduction="mean", )

[paddle error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 0],"float64"), positive=Tensor([5, 0],"float64"), negative=Tensor([5, 0],"float64"), distance_function=None, margin=0.3, swap=True, reduction="mean", ) 
 The positive distance or negative distance should be greater than 0, The distance functions should be checked.
2025-04-21 10:19:30.690651 test begin: paddle.nn.utils.vector_to_parameters(Tensor([165],"float32"), list[Tensor([0, 15],"float32"),Tensor([15],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([165],"float32"), list[Tensor([0, 15],"float32"),Tensor([15],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [0, 15], input(X)'s shape = [165], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:15 != input_axis_dim:165.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:19:30.891440 test begin: paddle.nn.utils.vector_to_parameters(Tensor([165],"float32"), list[Tensor([10, 0],"float32"),Tensor([15],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([165],"float32"), list[Tensor([10, 0],"float32"),Tensor([15],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [0, 15], input(X)'s shape = [165], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:15 != input_axis_dim:165.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:19:31.099575 test begin: paddle.nn.utils.vector_to_parameters(Tensor([165],"float32"), list[Tensor([10, 15],"float32"),Tensor([0],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([165],"float32"), list[Tensor([10, 15],"float32"),Tensor([0],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [150, 0], input(X)'s shape = [165], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:150 != input_axis_dim:165.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:19:31.328687 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:31.687516 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:31.973671 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:32.269325 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:32.591555 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:32.983794 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:33.291347 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:33.665321 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:33.982699 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:34.309524 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:34.623174 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:34.922194 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:35.148652 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:35.353611 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:35.636893 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([0],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([0],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:35.951706 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:36.248694 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:36.467725 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:36.752635 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:37.047235 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:37.376491 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:37.811053 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:38.062895 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:38.292042 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:38.561943 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:38.789402 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:40.205489 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:40.547953 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:40.992543 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:42.560236 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:42.992405 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:43.506895 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:43.821122 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:44.034269 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:44.381919 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:44.598321 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:44.810761 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([0],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([0],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:45.097629 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:45.403535 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:45.685055 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:45.885858 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:46.254386 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:46.599411 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:47.014008 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:47.457257 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:47.805871 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:48.197094 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:48.595837 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:48.809844 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:49.014487 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:49.307046 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:49.630235 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:49.940203 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:50.168354 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:50.444725 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:50.681708 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:50.969810 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:51.405951 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:51.649326 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([0],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([0],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:52.106767 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:52.438332 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:52.829741 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:53.141238 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:53.467266 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:53.756600 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:54.067385 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:54.367765 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:54.651003 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:54.947892 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:55.151525 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:55.533978 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:55.836137 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:56.143253 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:56.434929 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:56.739202 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:56.987767 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:57.293281 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:57.676845 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:58.089685 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:58.388896 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:58.666561 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([0],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([0],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:58.955148 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:59.215191 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:59.563022 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:19:59.924210 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:00.230236 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:00.574450 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:00.888751 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:01.215983 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:01.523490 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:01.829852 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:02.212858 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:02.457267 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:02.739104 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:03.026601 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:03.307496 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:03.521975 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:03.819832 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:04.134542 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:04.455859 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:04.758349 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:05.145249 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:05.593993 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:05.917302 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:06.304904 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:06.584849 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:06.827498 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:07.154102 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:07.482241 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:07.822644 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:08.139046 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:08.520814 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:08.746318 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:09.048657 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:09.351438 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:09.847330 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:10.226243 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([0],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([0],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:10.554072 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:10.960764 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:11.322444 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:11.645707 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:12.154268 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:12.539548 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:12.895648 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:13.309912 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:13.712117 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:14.119593 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:14.491343 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:14.810331 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:15.175487 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:15.491758 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:15.830577 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:16.165897 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:16.468603 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:16.755024 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:17.133241 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:17.454436 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:17.749676 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:18.068988 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:18.517468 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:18.852522 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:19.233754 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:19.532950 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:19.930189 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:20.159344 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:20.503630 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:20.873129 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:21.182861 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:21.505200 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:21.806644 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:22.165683 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:22.450486 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:22.852521 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([0],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([0],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:23.163762 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:23.494609 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:23.791337 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:24.012163 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:24.325739 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:24.611706 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:24.926949 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:25.231970 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:25.600211 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:25.881607 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:26.237042 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:26.556159 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:26.856086 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:27.213925 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:27.535245 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:27.880404 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:28.213670 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:28.575205 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:28.920865 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:29.215005 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:29.527222 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:29.919524 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:30.275720 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:30.599246 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:30.829571 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:31.178157 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:31.512482 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:31.947804 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:32.257837 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:32.619298 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:32.858824 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:33.260521 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:33.643523 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:34.042649 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:34.397153 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:34.744976 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([0],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([0],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:35.005923 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:35.303002 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:35.638174 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:35.946665 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:36.294428 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:36.643089 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:36.970066 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:37.352503 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:37.939816 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:38.205746 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:38.451738 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:38.763256 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:39.063680 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:39.388531 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:39.685757 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:40.089280 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:40.384209 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:40.613047 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:40.900194 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:41.226361 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:41.508613 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:41.916137 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:42.416341 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:42.732513 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:43.063246 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:43.419185 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:43.727578 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:44.032247 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:44.370889 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:44.695959 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:45.127130 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:45.425848 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:45.832409 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([0],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:46.126491 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([0, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:46.522676 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 0],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:46.814931 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([0],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([0],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:47.129109 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([0, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:47.427189 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:47.776841 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:48.151910 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:48.479659 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:48.785773 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:49.104666 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:49.408279 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:49.784083 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:50.150209 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),Tensor([128],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:50.504645 test begin: paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([1851904],"float32"), list[Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 128],"float32"),Tensor([128],"float32"),Tensor([128, 512],"float32"),Tensor([512],"float32"),Tensor([512, 128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([128],"float32"),Tensor([0],"float32"),], ) 
 Unimplemented error. Invalid dimension to be accessed. Now only supports access to dimension 0 to 9, but received dimension is 172.
  [../paddle/common/ddim.h:61]
2025-04-21 10:20:50.832714 test begin: paddle.nn.utils.vector_to_parameters(Tensor([27],"float32"), list[Tensor([0, 2, 4],"float32"),Tensor([3],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([27],"float32"), list[Tensor([0, 2, 4],"float32"),Tensor([3],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [0, 3], input(X)'s shape = [27], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:3 != input_axis_dim:27.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:51.073044 test begin: paddle.nn.utils.vector_to_parameters(Tensor([27],"float32"), list[Tensor([3, 0, 4],"float32"),Tensor([3],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([27],"float32"), list[Tensor([3, 0, 4],"float32"),Tensor([3],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [0, 3], input(X)'s shape = [27], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:3 != input_axis_dim:27.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:51.341051 test begin: paddle.nn.utils.vector_to_parameters(Tensor([27],"float32"), list[Tensor([3, 2, 0],"float32"),Tensor([3],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([27],"float32"), list[Tensor([3, 2, 0],"float32"),Tensor([3],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [0, 3], input(X)'s shape = [27], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:3 != input_axis_dim:27.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:51.623392 test begin: paddle.nn.utils.vector_to_parameters(Tensor([27],"float32"), list[Tensor([3, 2, 4],"float32"),Tensor([0],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([27],"float32"), list[Tensor([3, 2, 4],"float32"),Tensor([0],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [24, 0], input(X)'s shape = [27], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:24 != input_axis_dim:27.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:51.934381 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([0, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([0, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [0, 768, 48, 48, 768, 768, 48, 48], input(X)'s shape = [2880], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:2496 != input_axis_dim:2880.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:52.063602 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 0],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 0],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [0, 768, 48, 48, 768, 768, 48, 48], input(X)'s shape = [2880], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:2496 != input_axis_dim:2880.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:52.261008 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([0, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([0, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [384, 0, 48, 48, 768, 768, 48, 48], input(X)'s shape = [2880], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:2112 != input_axis_dim:2880.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:52.472331 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 0],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 0],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [384, 0, 48, 48, 768, 768, 48, 48], input(X)'s shape = [2880], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:2112 != input_axis_dim:2880.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:52.696256 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([0],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([0],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [384, 768, 0, 48, 768, 768, 48, 48], input(X)'s shape = [2880], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:2832 != input_axis_dim:2880.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:52.906504 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([0],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([0],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [384, 768, 48, 0, 768, 768, 48, 48], input(X)'s shape = [2880], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:2832 != input_axis_dim:2880.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:53.113126 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([0, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([0, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [384, 768, 48, 48, 0, 768, 48, 48], input(X)'s shape = [2880], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:2112 != input_axis_dim:2880.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:53.370919 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 0],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 0],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [384, 768, 48, 48, 0, 768, 48, 48], input(X)'s shape = [2880], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:2112 != input_axis_dim:2880.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:53.574871 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([0, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([0, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [384, 768, 48, 48, 768, 0, 48, 48], input(X)'s shape = [2880], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:2112 != input_axis_dim:2880.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:53.819214 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 0],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 0],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [384, 768, 48, 48, 768, 0, 48, 48], input(X)'s shape = [2880], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:2112 != input_axis_dim:2880.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:54.020824 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([0],"float32"),Tensor([48],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([0],"float32"),Tensor([48],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [384, 768, 48, 48, 768, 768, 0, 48], input(X)'s shape = [2880], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:2832 != input_axis_dim:2880.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:54.144691 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([0],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([0],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [384, 768, 48, 48, 768, 768, 48, 0], input(X)'s shape = [2880], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:2832 != input_axis_dim:2880.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:54.255342 test begin: paddle.nn.utils.vector_to_parameters(Tensor([30],"float32"), list[Tensor([0, 3],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([30],"float32"), list[Tensor([0, 3],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [0, 0], input(X)'s shape = [30], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:0 != input_axis_dim:30.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:54.411179 test begin: paddle.nn.utils.vector_to_parameters(Tensor([30],"float32"), list[Tensor([10, 0],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([30],"float32"), list[Tensor([10, 0],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [0, 0], input(X)'s shape = [30], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:0 != input_axis_dim:30.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:54.639460 test begin: paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([0, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([0, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [0, 1024, 64, 64, 1024, 1024, 64, 64], input(X)'s shape = [3840], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:3328 != input_axis_dim:3840.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:54.779482 test begin: paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 0],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 0],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [0, 1024, 64, 64, 1024, 1024, 64, 64], input(X)'s shape = [3840], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:3328 != input_axis_dim:3840.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:55.015262 test begin: paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([0, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([0, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [512, 0, 64, 64, 1024, 1024, 64, 64], input(X)'s shape = [3840], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:2816 != input_axis_dim:3840.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:55.220943 test begin: paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 0],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 0],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [512, 0, 64, 64, 1024, 1024, 64, 64], input(X)'s shape = [3840], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:2816 != input_axis_dim:3840.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:55.455879 test begin: paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([0],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([0],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [512, 1024, 0, 64, 1024, 1024, 64, 64], input(X)'s shape = [3840], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:3776 != input_axis_dim:3840.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:55.667516 test begin: paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([0],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([0],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [512, 1024, 64, 0, 1024, 1024, 64, 64], input(X)'s shape = [3840], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:3776 != input_axis_dim:3840.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:55.794733 test begin: paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([0, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([0, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [512, 1024, 64, 64, 0, 1024, 64, 64], input(X)'s shape = [3840], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:2816 != input_axis_dim:3840.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:56.015906 test begin: paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 0],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 0],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [512, 1024, 64, 64, 0, 1024, 64, 64], input(X)'s shape = [3840], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:2816 != input_axis_dim:3840.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:56.240261 test begin: paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([0, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([0, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [512, 1024, 64, 64, 1024, 0, 64, 64], input(X)'s shape = [3840], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:2816 != input_axis_dim:3840.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:56.446018 test begin: paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 0],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 0],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [512, 1024, 64, 64, 1024, 0, 64, 64], input(X)'s shape = [3840], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:2816 != input_axis_dim:3840.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:56.609864 test begin: paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([0],"float32"),Tensor([64],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([0],"float32"),Tensor([64],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [512, 1024, 64, 64, 1024, 1024, 0, 64], input(X)'s shape = [3840], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:3776 != input_axis_dim:3840.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:56.813417 test begin: paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([0],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([0],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [512, 1024, 64, 64, 1024, 1024, 64, 0], input(X)'s shape = [3840], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:3776 != input_axis_dim:3840.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:57.072729 test begin: paddle.nn.utils.vector_to_parameters(Tensor([387],"float32"), list[Tensor([0, 2, 4, 4, 4],"float32"),Tensor([3],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([387],"float32"), list[Tensor([0, 2, 4, 4, 4],"float32"),Tensor([3],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [0, 3], input(X)'s shape = [387], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:3 != input_axis_dim:387.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:57.277161 test begin: paddle.nn.utils.vector_to_parameters(Tensor([387],"float32"), list[Tensor([3, 0, 4, 4, 4],"float32"),Tensor([3],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([387],"float32"), list[Tensor([3, 0, 4, 4, 4],"float32"),Tensor([3],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [0, 3], input(X)'s shape = [387], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:3 != input_axis_dim:387.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:57.439164 test begin: paddle.nn.utils.vector_to_parameters(Tensor([387],"float32"), list[Tensor([3, 2, 0, 4, 4],"float32"),Tensor([3],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([387],"float32"), list[Tensor([3, 2, 0, 4, 4],"float32"),Tensor([3],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [0, 3], input(X)'s shape = [387], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:3 != input_axis_dim:387.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:57.702021 test begin: paddle.nn.utils.vector_to_parameters(Tensor([387],"float32"), list[Tensor([3, 2, 4, 0, 4],"float32"),Tensor([3],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([387],"float32"), list[Tensor([3, 2, 4, 0, 4],"float32"),Tensor([3],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [0, 3], input(X)'s shape = [387], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:3 != input_axis_dim:387.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:57.939431 test begin: paddle.nn.utils.vector_to_parameters(Tensor([387],"float32"), list[Tensor([3, 2, 4, 4, 0],"float32"),Tensor([3],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([387],"float32"), list[Tensor([3, 2, 4, 4, 0],"float32"),Tensor([3],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [0, 3], input(X)'s shape = [387], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:3 != input_axis_dim:387.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:58.159768 test begin: paddle.nn.utils.vector_to_parameters(Tensor([387],"float32"), list[Tensor([3, 2, 4, 4, 4],"float32"),Tensor([0],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([387],"float32"), list[Tensor([3, 2, 4, 4, 4],"float32"),Tensor([0],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [384, 0], input(X)'s shape = [387], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:384 != input_axis_dim:387.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:58.371442 test begin: paddle.nn.utils.vector_to_parameters(Tensor([6],"float32"), list[Tensor([0, 2],"float32"),Tensor([2],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([6],"float32"), list[Tensor([0, 2],"float32"),Tensor([2],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [0, 2], input(X)'s shape = [6], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:2 != input_axis_dim:6.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:58.581128 test begin: paddle.nn.utils.vector_to_parameters(Tensor([6],"float32"), list[Tensor([2, 0],"float32"),Tensor([2],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([6],"float32"), list[Tensor([2, 0],"float32"),Tensor([2],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [0, 2], input(X)'s shape = [6], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:2 != input_axis_dim:6.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:58.823863 test begin: paddle.nn.utils.vector_to_parameters(Tensor([6],"float32"), list[Tensor([2, 2],"float32"),Tensor([0],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([6],"float32"), list[Tensor([2, 2],"float32"),Tensor([0],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [4, 0], input(X)'s shape = [6], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:4 != input_axis_dim:6.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:59.046874 test begin: paddle.nn.utils.vector_to_parameters(Tensor([99],"float32"), list[Tensor([0, 2, 4, 4],"float32"),Tensor([3],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([99],"float32"), list[Tensor([0, 2, 4, 4],"float32"),Tensor([3],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [0, 3], input(X)'s shape = [99], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:3 != input_axis_dim:99.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:59.288969 test begin: paddle.nn.utils.vector_to_parameters(Tensor([99],"float32"), list[Tensor([3, 0, 4, 4],"float32"),Tensor([3],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([99],"float32"), list[Tensor([3, 0, 4, 4],"float32"),Tensor([3],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [0, 3], input(X)'s shape = [99], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:3 != input_axis_dim:99.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:59.511701 test begin: paddle.nn.utils.vector_to_parameters(Tensor([99],"float32"), list[Tensor([3, 2, 0, 4],"float32"),Tensor([3],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([99],"float32"), list[Tensor([3, 2, 0, 4],"float32"),Tensor([3],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [0, 3], input(X)'s shape = [99], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:3 != input_axis_dim:99.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:59.734795 test begin: paddle.nn.utils.vector_to_parameters(Tensor([99],"float32"), list[Tensor([3, 2, 4, 0],"float32"),Tensor([3],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([99],"float32"), list[Tensor([3, 2, 4, 0],"float32"),Tensor([3],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [0, 3], input(X)'s shape = [99], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:3 != input_axis_dim:99.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:20:59.950032 test begin: paddle.nn.utils.vector_to_parameters(Tensor([99],"float32"), list[Tensor([3, 2, 4, 4],"float32"),Tensor([0],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([99],"float32"), list[Tensor([3, 2, 4, 4],"float32"),Tensor([0],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [96, 0], input(X)'s shape = [99], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:96 != input_axis_dim:99.] (at ../paddle/phi/infermeta/unary.cc:4446)

2025-04-21 10:21:09.015669 test begin: paddle.outer(Tensor([10],"float32"), Tensor([0],"float32"), )

 ** On entry to SGEMM  parameter number 8 had an illegal value
W0421 10:21:10.460188 136927 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.outer(Tensor([10],"float32"), Tensor([0],"float32"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)

2025-04-21 10:21:10.904201 test begin: paddle.outer(x=Tensor([4],"float64"), y=Tensor([0],"float64"), )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:21:11.115406 137974 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.outer(x=Tensor([4],"float64"), y=Tensor([0],"float64"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:21:11.115696 test begin: paddle.pdist(Tensor([0, 20],"float32"), 0, )

[cuda error] paddle.pdist(Tensor([0, 20],"float32"), 0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:21:11.315644 test begin: paddle.pdist(Tensor([0, 20],"float32"), 1.0, )

[cuda error] paddle.pdist(Tensor([0, 20],"float32"), 1.0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:21:11.516381 test begin: paddle.pdist(Tensor([0, 20],"float32"), 1.5, )

[cuda error] paddle.pdist(Tensor([0, 20],"float32"), 1.5, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:21:11.718563 test begin: paddle.pdist(Tensor([0, 20],"float32"), 2.0, )

[cuda error] paddle.pdist(Tensor([0, 20],"float32"), 2.0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:21:11.921875 test begin: paddle.pdist(Tensor([0, 20],"float32"), 2.5, )

[cuda error] paddle.pdist(Tensor([0, 20],"float32"), 2.5, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:21:12.151115 test begin: paddle.pdist(Tensor([0, 20],"float32"), 3.0, )

[cuda error] paddle.pdist(Tensor([0, 20],"float32"), 3.0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:21:12.284410 test begin: paddle.pdist(Tensor([0, 20],"float32"), math.inf, )

[cuda error] paddle.pdist(Tensor([0, 20],"float32"), math.inf, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:21:12.470107 test begin: paddle.pdist(Tensor([0, 20],"float64"), 2.0, )

[cuda error] paddle.pdist(Tensor([0, 20],"float64"), 2.0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:21:12.683127 test begin: paddle.pdist(Tensor([10, 0],"float32"), 0, )

W0421 10:21:12.888307 138982 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.pdist(Tensor([10, 0],"float32"), 0, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:21:12.888718 test begin: paddle.pdist(Tensor([10, 0],"float32"), 1.0, )

W0421 10:21:13.105427 139021 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.pdist(Tensor([10, 0],"float32"), 1.0, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:21:13.105868 test begin: paddle.pdist(Tensor([10, 0],"float32"), 1.5, )

W0421 10:21:13.312333 139160 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.pdist(Tensor([10, 0],"float32"), 1.5, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:21:13.312813 test begin: paddle.pdist(Tensor([10, 0],"float32"), 2.0, )

W0421 10:21:13.517987 139249 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.pdist(Tensor([10, 0],"float32"), 2.0, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:21:13.518381 test begin: paddle.pdist(Tensor([10, 0],"float32"), 2.5, )

W0421 10:21:13.719846 139325 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.pdist(Tensor([10, 0],"float32"), 2.5, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:21:13.720253 test begin: paddle.pdist(Tensor([10, 0],"float32"), 3.0, )

W0421 10:21:13.921999 139337 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.pdist(Tensor([10, 0],"float32"), 3.0, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:21:13.922364 test begin: paddle.pdist(Tensor([10, 0],"float32"), math.inf, )

W0421 10:21:14.128365 139421 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.pdist(Tensor([10, 0],"float32"), math.inf, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:21:14.128831 test begin: paddle.pdist(Tensor([50, 0],"float64"), 2.0, )

W0421 10:21:14.331295 139495 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.pdist(Tensor([50, 0],"float64"), 2.0, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:21:21.200807 test begin: paddle.real(Tensor([0, 10, 10, 20],"complex64"), )

W0421 10:21:21.410990 142286 backward.cc:437] While running Node (RealGradNode) raises an EnforceNotMet exception
[paddle error] paddle.real(Tensor([0, 10, 10, 20],"complex64"), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:21:21.411424 test begin: paddle.real(Tensor([0, 20, 2, 3],"complex128"), )

W0421 10:21:21.627050 142299 backward.cc:437] While running Node (RealGradNode) raises an EnforceNotMet exception
[paddle error] paddle.real(Tensor([0, 20, 2, 3],"complex128"), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:21:21.627572 test begin: paddle.real(Tensor([10, 0, 10, 20],"complex64"), )

W0421 10:21:21.874879 142441 backward.cc:437] While running Node (RealGradNode) raises an EnforceNotMet exception
[paddle error] paddle.real(Tensor([10, 0, 10, 20],"complex64"), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:21:21.875182 test begin: paddle.real(Tensor([10, 10, 0, 20],"complex64"), )

W0421 10:21:22.079335 142523 backward.cc:437] While running Node (RealGradNode) raises an EnforceNotMet exception
[paddle error] paddle.real(Tensor([10, 10, 0, 20],"complex64"), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:21:22.079705 test begin: paddle.real(Tensor([10, 10, 10, 0],"complex64"), )

W0421 10:21:22.280496 142545 backward.cc:437] While running Node (RealGradNode) raises an EnforceNotMet exception
[paddle error] paddle.real(Tensor([10, 10, 10, 0],"complex64"), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:21:22.280803 test begin: paddle.real(Tensor([2, 0, 2, 3],"complex128"), )

W0421 10:21:22.481948 142560 backward.cc:437] While running Node (RealGradNode) raises an EnforceNotMet exception
[paddle error] paddle.real(Tensor([2, 0, 2, 3],"complex128"), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:21:22.482264 test begin: paddle.real(Tensor([2, 20, 0, 3],"complex128"), )

W0421 10:21:22.715956 142645 backward.cc:437] While running Node (RealGradNode) raises an EnforceNotMet exception
[paddle error] paddle.real(Tensor([2, 20, 0, 3],"complex128"), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:21:22.716590 test begin: paddle.real(Tensor([2, 20, 2, 0],"complex128"), )

W0421 10:21:22.921687 142778 backward.cc:437] While running Node (RealGradNode) raises an EnforceNotMet exception
[paddle error] paddle.real(Tensor([2, 20, 2, 0],"complex128"), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:21:22.922024 test begin: paddle.real(x=Tensor([0, 10],"complex128"), )

W0421 10:21:23.231173 142893 backward.cc:437] While running Node (RealGradNode) raises an EnforceNotMet exception
[paddle error] paddle.real(x=Tensor([0, 10],"complex128"), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:21:23.231552 test begin: paddle.real(x=Tensor([0, 10],"complex64"), )

W0421 10:21:23.409240 143104 backward.cc:437] While running Node (RealGradNode) raises an EnforceNotMet exception
[paddle error] paddle.real(x=Tensor([0, 10],"complex64"), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:21:23.409560 test begin: paddle.real(x=Tensor([1, 0],"complex64"), )

W0421 10:21:23.530138 143113 backward.cc:437] While running Node (RealGradNode) raises an EnforceNotMet exception
[paddle error] paddle.real(x=Tensor([1, 0],"complex64"), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:21:23.530519 test begin: paddle.real(x=Tensor([20, 0],"complex128"), )

W0421 10:21:23.722198 143184 backward.cc:437] While running Node (RealGradNode) raises an EnforceNotMet exception
[paddle error] paddle.real(x=Tensor([20, 0],"complex128"), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:21:27.559405 test begin: paddle.renorm(Tensor([0, 2, 3],"float32"), 1.0, -1, 2.05, )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_renorm(_object*, _object*, _object*)
1   renorm_ad_func(paddle::Tensor const&, float, int, float)
2   paddle::experimental::renorm(paddle::Tensor const&, float, int, float)
3   void phi::RenormKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, float, int, float, phi::DenseTensor*)
4   void phi::funcs::RenormFunc<float>(phi::GPUContext const&, float const*, float*, float, int, float, long, common::DDim const&, long)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202087 (unix time) try "date -d @1745202087" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f1bf756ebd9) received by PID 22478 (TID 0x7f19b0ada700) from PID 18446744073564253145 ***]

2025-04-21 10:21:32.235193 test begin: paddle.renorm(Tensor([0, 2, 3],"float32"), 1.0, 2, 2.05, )

W0421 10:21:38.989045 146668 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:21:38.990227 146668 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_renorm(_object*, _object*, _object*)
1   renorm_ad_func(paddle::Tensor const&, float, int, float)
2   paddle::experimental::renorm(paddle::Tensor const&, float, int, float)
3   void phi::RenormKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, float, int, float, phi::DenseTensor*)
4   void phi::funcs::RenormFunc<float>(phi::GPUContext const&, float const*, float*, float, int, float, long, common::DDim const&, long)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202099 (unix time) try "date -d @1745202099" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f2681d51bd9) received by PID 145021 (TID 0x7f25ae7c3700) from PID 18446744071592811481 ***]

2025-04-21 10:21:56.906641 test begin: paddle.renorm(Tensor([0, 20, 1],"float32"), 1.0, -1, 2.05, )

W0421 10:22:02.942898 156106 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:22:02.944072 156106 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_renorm(_object*, _object*, _object*)
1   renorm_ad_func(paddle::Tensor const&, float, int, float)
2   paddle::experimental::renorm(paddle::Tensor const&, float, int, float)
3   void phi::RenormKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, float, int, float, phi::DenseTensor*)
4   void phi::funcs::RenormFunc<float>(phi::GPUContext const&, float const*, float*, float, int, float, long, common::DDim const&, long)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202122 (unix time) try "date -d @1745202122" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f69bdcfbbd9) received by PID 154769 (TID 0x7f68d87c3700) from PID 18446744072599092185 ***]

2025-04-21 10:22:07.027535 test begin: paddle.renorm(Tensor([10, 0, 1],"float32"), 1.0, -1, 2.05, )

W0421 10:22:12.775810 159954 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:22:12.777073 159954 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_renorm(_object*, _object*, _object*)
1   renorm_ad_func(paddle::Tensor const&, float, int, float)
2   paddle::experimental::renorm(paddle::Tensor const&, float, int, float)
3   void phi::RenormKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, float, int, float, phi::DenseTensor*)
4   void phi::funcs::RenormFunc<float>(phi::GPUContext const&, float const*, float*, float, int, float, long, common::DDim const&, long)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202132 (unix time) try "date -d @1745202132" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f7a825a6bd9) received by PID 158545 (TID 0x7f79af2b7700) from PID 18446744071601548249 ***]

2025-04-21 10:22:17.037337 test begin: paddle.renorm(Tensor([10, 20, 0],"float32"), 1.0, -1, 2.05, )

W0421 10:22:23.153045 163609 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:22:23.154808 163609 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_renorm(_object*, _object*, _object*)
1   renorm_ad_func(paddle::Tensor const&, float, int, float)
2   paddle::experimental::renorm(paddle::Tensor const&, float, int, float)
3   void phi::RenormKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, float, int, float, phi::DenseTensor*)
4   void phi::funcs::RenormFunc<float>(phi::GPUContext const&, float const*, float*, float, int, float, long, common::DDim const&, long)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202143 (unix time) try "date -d @1745202143" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f73f9f27bd9) received by PID 161922 (TID 0x7f72de7c3700) from PID 18446744073608002521 ***]

2025-04-21 10:22:41.307186 test begin: paddle.renorm(Tensor([2, 0, 3],"float32"), 1.0, -1, 2.05, )

W0421 10:22:47.622452  9481 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:22:47.624770  9481 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_renorm(_object*, _object*, _object*)
1   renorm_ad_func(paddle::Tensor const&, float, int, float)
2   paddle::experimental::renorm(paddle::Tensor const&, float, int, float)
3   void phi::RenormKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, float, int, float, phi::DenseTensor*)
4   void phi::funcs::RenormFunc<float>(phi::GPUContext const&, float const*, float*, float, int, float, long, common::DDim const&, long)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202167 (unix time) try "date -d @1745202167" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7faff03acbd9) received by PID 8321 (TID 0x7faeda949700) from PID 18446744073444969433 ***]

2025-04-21 10:22:51.779249 test begin: paddle.renorm(Tensor([2, 0, 3],"float32"), 1.0, 2, 2.05, )

W0421 10:22:58.899283 13879 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:22:58.900799 13879 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_renorm(_object*, _object*, _object*)
1   renorm_ad_func(paddle::Tensor const&, float, int, float)
2   paddle::experimental::renorm(paddle::Tensor const&, float, int, float)
3   void phi::RenormKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, float, int, float, phi::DenseTensor*)
4   void phi::funcs::RenormFunc<float>(phi::GPUContext const&, float const*, float*, float, int, float, long, common::DDim const&, long)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202178 (unix time) try "date -d @1745202178" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fd19845abd9) received by PID 12357 (TID 0x7fd0a8949700) from PID 18446744071969287129 ***]

2025-04-21 10:23:02.977309 test begin: paddle.renorm(Tensor([2, 2, 0],"float32"), 1.0, -1, 2.05, )

W0421 10:23:09.018846 17956 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:23:09.020136 17956 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_renorm(_object*, _object*, _object*)
1   renorm_ad_func(paddle::Tensor const&, float, int, float)
2   paddle::experimental::renorm(paddle::Tensor const&, float, int, float)
3   void phi::RenormKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, float, int, float, phi::DenseTensor*)
4   void phi::funcs::RenormFunc<float>(phi::GPUContext const&, float const*, float*, float, int, float, long, common::DDim const&, long)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202189 (unix time) try "date -d @1745202189" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f757b84ebd9) received by PID 16813 (TID 0x7f7491f48700) from PID 2072308697 ***]

2025-04-21 10:23:26.419160 test begin: paddle.renorm(Tensor([2, 2, 0],"float32"), 1.0, 2, 2.05, )

W0421 10:23:32.247361 27708 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:23:32.249042 27708 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_renorm(_object*, _object*, _object*)
1   renorm_ad_func(paddle::Tensor const&, float, int, float)
2   paddle::experimental::renorm(paddle::Tensor const&, float, int, float)
3   void phi::RenormKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, float, int, float, phi::DenseTensor*)
4   void phi::funcs::RenormFunc<float>(phi::GPUContext const&, float const*, float*, float, int, float, long, common::DDim const&, long)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202212 (unix time) try "date -d @1745202212" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f739906fbd9) received by PID 26333 (TID 0x7f72abdc2700) from PID 18446744071981956057 ***]

2025-04-21 10:23:36.930520 test begin: paddle.renorm(x=Tensor([0, 2, 3],"float32"), p=1, axis=0, max_norm=5, )

W0421 10:23:43.467413 31646 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:23:43.468578 31646 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_renorm(_object*, _object*, _object*)
1   renorm_ad_func(paddle::Tensor const&, float, int, float)
2   paddle::experimental::renorm(paddle::Tensor const&, float, int, float)
3   void phi::RenormKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, float, int, float, phi::DenseTensor*)
4   void phi::funcs::RenormFunc<float>(phi::GPUContext const&, float const*, float*, float, int, float, long, common::DDim const&, long)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202223 (unix time) try "date -d @1745202223" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fb20b9d4bd9) received by PID 29987 (TID 0x7fb113f48700) from PID 194857945 ***]

2025-04-21 10:23:47.734863 test begin: paddle.renorm(x=Tensor([0, 2, 3],"float64"), p=1, axis=0, max_norm=5, )

W0421 10:23:53.836222 35884 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:23:53.837474 35884 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_renorm(_object*, _object*, _object*)
1   renorm_ad_func(paddle::Tensor const&, float, int, float)
2   paddle::experimental::renorm(paddle::Tensor const&, float, int, float)
3   void phi::RenormKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, float, int, float, phi::DenseTensor*)
4   void phi::funcs::RenormFunc<double>(phi::GPUContext const&, double const*, double*, float, int, float, long, common::DDim const&, long)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202233 (unix time) try "date -d @1745202233" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f0bb3de46e9) received by PID 34342 (TID 0x7f0ac4949700) from PID 18446744072432273129 ***]

2025-04-21 10:24:11.877013 test begin: paddle.renorm(x=Tensor([0, 2, 3],"float64"), p=1.2, axis=2, max_norm=6.5, )

W0421 10:24:18.160904 44590 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:24:18.162416 44590 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_renorm(_object*, _object*, _object*)
1   renorm_ad_func(paddle::Tensor const&, float, int, float)
2   paddle::experimental::renorm(paddle::Tensor const&, float, int, float)
3   void phi::RenormKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, float, int, float, phi::DenseTensor*)
4   void phi::funcs::RenormFunc<double>(phi::GPUContext const&, double const*, double*, float, int, float, long, common::DDim const&, long)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202258 (unix time) try "date -d @1745202258" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f02b5fb66e9) received by PID 43405 (TID 0x7f01ec7c3700) from PID 18446744072467736297 ***]

2025-04-21 10:24:22.301700 test begin: paddle.renorm(x=Tensor([0, 2, 3],"float64"), p=1.5, axis=2, max_norm=20, )

W0421 10:24:29.249274 48817 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:24:29.250430 48817 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_renorm(_object*, _object*, _object*)
1   renorm_ad_func(paddle::Tensor const&, float, int, float)
2   paddle::experimental::renorm(paddle::Tensor const&, float, int, float)
3   void phi::RenormKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, float, int, float, phi::DenseTensor*)
4   void phi::funcs::RenormFunc<double>(phi::GPUContext const&, double const*, double*, float, int, float, long, common::DDim const&, long)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202269 (unix time) try "date -d @1745202269" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f4a8de886e9) received by PID 47360 (TID 0x7f49aa7c3700) from PID 18446744071795410665 ***]

2025-04-21 10:24:33.439735 test begin: paddle.renorm(x=Tensor([0, 2, 3],"float64"), p=2, axis=1, max_norm=20, )

W0421 10:24:40.995151 52585 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:24:40.996428 52585 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_renorm(_object*, _object*, _object*)
1   renorm_ad_func(paddle::Tensor const&, float, int, float)
2   paddle::experimental::renorm(paddle::Tensor const&, float, int, float)
3   void phi::RenormKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, float, int, float, phi::DenseTensor*)
4   void phi::funcs::RenormFunc<double>(phi::GPUContext const&, double const*, double*, float, int, float, long, common::DDim const&, long)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202281 (unix time) try "date -d @1745202281" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f3470c5b6e9) received by PID 51230 (TID 0x7f335f935700) from PID 1892005609 ***]

2025-04-21 10:24:58.537382 test begin: paddle.renorm(x=Tensor([0, 2, 3],"float64"), p=2, axis=1, max_norm=40, )

W0421 10:25:04.778081 62250 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:25:04.779170 62250 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_renorm(_object*, _object*, _object*)
1   renorm_ad_func(paddle::Tensor const&, float, int, float)
2   paddle::experimental::renorm(paddle::Tensor const&, float, int, float)
3   void phi::RenormKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, float, int, float, phi::DenseTensor*)
4   void phi::funcs::RenormFunc<double>(phi::GPUContext const&, double const*, double*, float, int, float, long, common::DDim const&, long)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202304 (unix time) try "date -d @1745202304" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f68584656e9) received by PID 61061 (TID 0x7f6784949700) from PID 1481004777 ***]

2025-04-21 10:25:08.736437 test begin: paddle.renorm(x=Tensor([0, 2, 3],"float64"), p=2, axis=1, max_norm=50, )

W0421 10:25:14.596377 66490 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:25:14.597539 66490 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_renorm(_object*, _object*, _object*)
1   renorm_ad_func(paddle::Tensor const&, float, int, float)
2   paddle::experimental::renorm(paddle::Tensor const&, float, int, float)
3   void phi::RenormKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, float, int, float, phi::DenseTensor*)
4   void phi::funcs::RenormFunc<double>(phi::GPUContext const&, double const*, double*, float, int, float, long, common::DDim const&, long)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202314 (unix time) try "date -d @1745202314" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f346d8fc6e9) received by PID 64942 (TID 0x7f3391dc2700) from PID 1838139113 ***]

2025-04-21 10:25:18.902565 test begin: paddle.renorm(x=Tensor([3, 0, 3],"float32"), p=1, axis=0, max_norm=5, )

W0421 10:25:25.319337 70948 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:25:25.320720 70948 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_renorm(_object*, _object*, _object*)
1   renorm_ad_func(paddle::Tensor const&, float, int, float)
2   paddle::experimental::renorm(paddle::Tensor const&, float, int, float)
3   void phi::RenormKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, float, int, float, phi::DenseTensor*)
4   void phi::funcs::RenormFunc<float>(phi::GPUContext const&, float const*, float*, float, int, float, long, common::DDim const&, long)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202325 (unix time) try "date -d @1745202325" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f47788aabd9) received by PID 69252 (TID 0x7f46496f8700) from PID 2022353881 ***]

2025-04-21 10:25:43.167164 test begin: paddle.renorm(x=Tensor([3, 0, 3],"float64"), p=1, axis=0, max_norm=5, )

W0421 10:25:49.110740 79982 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:25:49.111824 79982 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_renorm(_object*, _object*, _object*)
1   renorm_ad_func(paddle::Tensor const&, float, int, float)
2   paddle::experimental::renorm(paddle::Tensor const&, float, int, float)
3   void phi::RenormKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, float, int, float, phi::DenseTensor*)
4   void phi::funcs::RenormFunc<double>(phi::GPUContext const&, double const*, double*, float, int, float, long, common::DDim const&, long)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202349 (unix time) try "date -d @1745202349" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fd86886f6e9) received by PID 78691 (TID 0x7fd79d34a700) from PID 1753675497 ***]

2025-04-21 10:25:53.398336 test begin: paddle.renorm(x=Tensor([3, 0, 3],"float64"), p=1.2, axis=2, max_norm=6.5, )

W0421 10:25:59.857496 83951 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:25:59.858700 83951 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_renorm(_object*, _object*, _object*)
1   renorm_ad_func(paddle::Tensor const&, float, int, float)
2   paddle::experimental::renorm(paddle::Tensor const&, float, int, float)
3   void phi::RenormKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, float, int, float, phi::DenseTensor*)
4   void phi::funcs::RenormFunc<double>(phi::GPUContext const&, double const*, double*, float, int, float, long, common::DDim const&, long)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202359 (unix time) try "date -d @1745202359" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fd32590e6e9) received by PID 82443 (TID 0x7fd23bdc2700) from PID 630253289 ***]

2025-04-21 10:26:03.996481 test begin: paddle.renorm(x=Tensor([3, 0, 3],"float64"), p=1.5, axis=2, max_norm=20, )

W0421 10:26:10.360997 88114 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:26:10.362056 88114 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_renorm(_object*, _object*, _object*)
1   renorm_ad_func(paddle::Tensor const&, float, int, float)
2   paddle::experimental::renorm(paddle::Tensor const&, float, int, float)
3   void phi::RenormKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, float, int, float, phi::DenseTensor*)
4   void phi::funcs::RenormFunc<double>(phi::GPUContext const&, double const*, double*, float, int, float, long, common::DDim const&, long)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202370 (unix time) try "date -d @1745202370" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f1dd687c6e9) received by PID 86794 (TID 0x7f1cf14f4700) from PID 18446744073013806825 ***]

2025-04-21 10:26:14.265960 test begin: paddle.renorm(x=Tensor([3, 0, 3],"float64"), p=2, axis=1, max_norm=20, )

W0421 10:26:20.182958 92143 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:26:20.184140 92143 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_renorm(_object*, _object*, _object*)
1   renorm_ad_func(paddle::Tensor const&, float, int, float)
2   paddle::experimental::renorm(paddle::Tensor const&, float, int, float)
3   void phi::RenormKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, float, int, float, phi::DenseTensor*)
4   void phi::funcs::RenormFunc<double>(phi::GPUContext const&, double const*, double*, float, int, float, long, common::DDim const&, long)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202380 (unix time) try "date -d @1745202380" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f6d2bbe66e9) received by PID 90771 (TID 0x7f6c56949700) from PID 733898473 ***]

2025-04-21 10:26:24.099560 test begin: paddle.renorm(x=Tensor([3, 0, 3],"float64"), p=2, axis=1, max_norm=40, )

W0421 10:26:30.364347 96047 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:26:30.365545 96047 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_renorm(_object*, _object*, _object*)
1   renorm_ad_func(paddle::Tensor const&, float, int, float)
2   paddle::experimental::renorm(paddle::Tensor const&, float, int, float)
3   void phi::RenormKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, float, int, float, phi::DenseTensor*)
4   void phi::funcs::RenormFunc<double>(phi::GPUContext const&, double const*, double*, float, int, float, long, common::DDim const&, long)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202390 (unix time) try "date -d @1745202390" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fcf0159e6e9) received by PID 94679 (TID 0x7fce19dc2700) from PID 22669033 ***]

2025-04-21 10:26:48.336923 test begin: paddle.renorm(x=Tensor([3, 0, 3],"float64"), p=2, axis=1, max_norm=50, )

W0421 10:26:54.410225 105646 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:26:54.411418 105646 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_renorm(_object*, _object*, _object*)
1   renorm_ad_func(paddle::Tensor const&, float, int, float)
2   paddle::experimental::renorm(paddle::Tensor const&, float, int, float)
3   void phi::RenormKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, float, int, float, phi::DenseTensor*)
4   void phi::funcs::RenormFunc<double>(phi::GPUContext const&, double const*, double*, float, int, float, long, common::DDim const&, long)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202414 (unix time) try "date -d @1745202414" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fa9d68ee6e9) received by PID 104461 (TID 0x7fa8f1744700) from PID 18446744073014273769 ***]

2025-04-21 10:26:58.395347 test begin: paddle.renorm(x=Tensor([3, 2, 0],"float32"), p=1, axis=0, max_norm=5, )

W0421 10:27:04.352377 109792 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:27:04.353824 109792 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_renorm(_object*, _object*, _object*)
1   renorm_ad_func(paddle::Tensor const&, float, int, float)
2   paddle::experimental::renorm(paddle::Tensor const&, float, int, float)
3   void phi::RenormKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, float, int, float, phi::DenseTensor*)
4   void phi::funcs::RenormFunc<float>(phi::GPUContext const&, float const*, float*, float, int, float, long, common::DDim const&, long)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202424 (unix time) try "date -d @1745202424" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fd673a44bd9) received by PID 108079 (TID 0x7fd5a3f48700) from PID 1940147161 ***]

2025-04-21 10:27:08.318952 test begin: paddle.renorm(x=Tensor([3, 2, 0],"float64"), p=1, axis=0, max_norm=5, )

W0421 10:27:14.607913 113153 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:27:14.609397 113153 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_renorm(_object*, _object*, _object*)
1   renorm_ad_func(paddle::Tensor const&, float, int, float)
2   paddle::experimental::renorm(paddle::Tensor const&, float, int, float)
3   void phi::RenormKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, float, int, float, phi::DenseTensor*)
4   void phi::funcs::RenormFunc<double>(phi::GPUContext const&, double const*, double*, float, int, float, long, common::DDim const&, long)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202434 (unix time) try "date -d @1745202434" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f405e05d6e9) received by PID 111934 (TID 0x7f3f267c3700) from PID 1577441001 ***]

2025-04-21 10:27:32.661555 test begin: paddle.renorm(x=Tensor([3, 2, 0],"float64"), p=1.2, axis=2, max_norm=6.5, )

W0421 10:27:39.573191 123557 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:27:39.575098 123557 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_renorm(_object*, _object*, _object*)
1   renorm_ad_func(paddle::Tensor const&, float, int, float)
2   paddle::experimental::renorm(paddle::Tensor const&, float, int, float)
3   void phi::RenormKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, float, int, float, phi::DenseTensor*)
4   void phi::funcs::RenormFunc<double>(phi::GPUContext const&, double const*, double*, float, int, float, long, common::DDim const&, long)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202459 (unix time) try "date -d @1745202459" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7febf431e6e9) received by PID 121943 (TID 0x7feb0e949700) from PID 18446744073511495401 ***]

2025-04-21 10:27:43.803685 test begin: paddle.renorm(x=Tensor([3, 2, 0],"float64"), p=1.5, axis=2, max_norm=20, )

W0421 10:27:51.380120 127237 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:27:51.382396 127237 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_renorm(_object*, _object*, _object*)
1   renorm_ad_func(paddle::Tensor const&, float, int, float)
2   paddle::experimental::renorm(paddle::Tensor const&, float, int, float)
3   void phi::RenormKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, float, int, float, phi::DenseTensor*)
4   void phi::funcs::RenormFunc<double>(phi::GPUContext const&, double const*, double*, float, int, float, long, common::DDim const&, long)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202471 (unix time) try "date -d @1745202471" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f70ceb646e9) received by PID 125950 (TID 0x7f6fe987e700) from PID 18446744072882636521 ***]

2025-04-21 10:27:55.877168 test begin: paddle.renorm(x=Tensor([3, 2, 0],"float64"), p=2, axis=1, max_norm=20, )

W0421 10:28:02.172171 131540 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:28:02.173655 131540 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_renorm(_object*, _object*, _object*)
1   renorm_ad_func(paddle::Tensor const&, float, int, float)
2   paddle::experimental::renorm(paddle::Tensor const&, float, int, float)
3   void phi::RenormKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, float, int, float, phi::DenseTensor*)
4   void phi::funcs::RenormFunc<double>(phi::GPUContext const&, double const*, double*, float, int, float, long, common::DDim const&, long)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202482 (unix time) try "date -d @1745202482" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f3b23c816e9) received by PID 129961 (TID 0x7f3a58949700) from PID 600315625 ***]

2025-04-21 10:28:20.466333 test begin: paddle.renorm(x=Tensor([3, 2, 0],"float64"), p=2, axis=1, max_norm=40, )

W0421 10:28:26.589700 140630 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:28:26.590719 140630 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_renorm(_object*, _object*, _object*)
1   renorm_ad_func(paddle::Tensor const&, float, int, float)
2   paddle::experimental::renorm(paddle::Tensor const&, float, int, float)
3   void phi::RenormKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, float, int, float, phi::DenseTensor*)
4   void phi::funcs::RenormFunc<double>(phi::GPUContext const&, double const*, double*, float, int, float, long, common::DDim const&, long)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202506 (unix time) try "date -d @1745202506" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f42922ee6e9) received by PID 139413 (TID 0x7f41c1137700) from PID 18446744071867131625 ***]

2025-04-21 10:28:31.015077 test begin: paddle.renorm(x=Tensor([3, 2, 0],"float64"), p=2, axis=1, max_norm=50, )

W0421 10:28:38.597702 144535 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:28:38.598868 144535 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_renorm(_object*, _object*, _object*)
1   renorm_ad_func(paddle::Tensor const&, float, int, float)
2   paddle::experimental::renorm(paddle::Tensor const&, float, int, float)
3   void phi::RenormKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, float, int, float, phi::DenseTensor*)
4   void phi::funcs::RenormFunc<double>(phi::GPUContext const&, double const*, double*, float, int, float, long, common::DDim const&, long)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202518 (unix time) try "date -d @1745202518" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f48d3a2f6e9) received by PID 143315 (TID 0x7f47fdf48700) from PID 18446744072965256937 ***]

2025-04-21 10:28:42.672210 test begin: paddle.repeat_interleave(Tensor([0, 1, 384, 384],"float32"), repeats=3, axis=1, )

W0421 10:28:49.620890 148802 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:28:49.622087 148802 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.repeat_interleave(Tensor([0, 1, 384, 384],"float32"), repeats=3, axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:49.624079 test begin: paddle.repeat_interleave(Tensor([0, 1, 768, 768],"float32"), repeats=3, axis=1, )

[cuda error] paddle.repeat_interleave(Tensor([0, 1, 768, 768],"float32"), repeats=3, axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:49.788619 test begin: paddle.repeat_interleave(Tensor([0, 128],"float32"), 128, 0, )

[cuda error] paddle.repeat_interleave(Tensor([0, 128],"float32"), 128, 0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:49.948014 test begin: paddle.repeat_interleave(Tensor([0, 1500, 1024],"float32"), 5, axis=0, )

[cuda error] paddle.repeat_interleave(Tensor([0, 1500, 1024],"float32"), 5, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:50.107451 test begin: paddle.repeat_interleave(Tensor([0, 1500, 1280],"float32"), 5, axis=0, )

[cuda error] paddle.repeat_interleave(Tensor([0, 1500, 1280],"float32"), 5, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:50.272299 test begin: paddle.repeat_interleave(Tensor([0, 2],"float32"), 2, axis=1, )

[cuda error] paddle.repeat_interleave(Tensor([0, 2],"float32"), 2, axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:50.559660 test begin: paddle.repeat_interleave(Tensor([0, 2],"int64"), 1, axis=0, )

[cuda error] paddle.repeat_interleave(Tensor([0, 2],"int64"), 1, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:50.716755 test begin: paddle.repeat_interleave(Tensor([0, 2],"int64"), 2, axis=0, )

[cuda error] paddle.repeat_interleave(Tensor([0, 2],"int64"), 2, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:50.889474 test begin: paddle.repeat_interleave(Tensor([0, 384, 1],"float32"), 1, 2, )

[cuda error] paddle.repeat_interleave(Tensor([0, 384, 1],"float32"), 1, 2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:51.102936 test begin: paddle.repeat_interleave(Tensor([0, 3],"bfloat16"), 2, None, )

[cuda error] paddle.repeat_interleave(Tensor([0, 3],"bfloat16"), 2, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:51.293041 test begin: paddle.repeat_interleave(Tensor([0, 3],"int32"), 2, None, )

[cuda error] paddle.repeat_interleave(Tensor([0, 3],"int32"), 2, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:51.526896 test begin: paddle.repeat_interleave(Tensor([0, 3],"int64"), 5, axis=0, )

[cuda error] paddle.repeat_interleave(Tensor([0, 3],"int64"), 5, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:51.696728 test begin: paddle.repeat_interleave(Tensor([0, 70],"int64"), 3, 1, )

[cuda error] paddle.repeat_interleave(Tensor([0, 70],"int64"), 3, 1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:51.853268 test begin: paddle.repeat_interleave(Tensor([1, 0, 1024],"float32"), 5, axis=0, )

[cuda error] paddle.repeat_interleave(Tensor([1, 0, 1024],"float32"), 5, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:52.132461 test begin: paddle.repeat_interleave(Tensor([1, 0, 1280],"float32"), 5, axis=0, )

[cuda error] paddle.repeat_interleave(Tensor([1, 0, 1280],"float32"), 5, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:52.318466 test begin: paddle.repeat_interleave(Tensor([1, 0],"float32"), 128, 0, )

[cuda error] paddle.repeat_interleave(Tensor([1, 0],"float32"), 128, 0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:52.478475 test begin: paddle.repeat_interleave(Tensor([1, 0],"float32"), 2, axis=1, )

[cuda error] paddle.repeat_interleave(Tensor([1, 0],"float32"), 2, axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:52.651138 test begin: paddle.repeat_interleave(Tensor([1, 0],"int64"), 1, axis=0, )

[cuda error] paddle.repeat_interleave(Tensor([1, 0],"int64"), 1, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:52.863584 test begin: paddle.repeat_interleave(Tensor([1, 0],"int64"), 2, axis=0, )

[cuda error] paddle.repeat_interleave(Tensor([1, 0],"int64"), 2, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:53.167726 test begin: paddle.repeat_interleave(Tensor([1, 0],"int64"), 3, 1, )

[cuda error] paddle.repeat_interleave(Tensor([1, 0],"int64"), 3, 1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:53.388083 test begin: paddle.repeat_interleave(Tensor([1, 0],"int64"), 5, axis=0, )

[cuda error] paddle.repeat_interleave(Tensor([1, 0],"int64"), 5, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:53.653728 test begin: paddle.repeat_interleave(Tensor([1, 1500, 0],"float32"), 5, axis=0, )

[cuda error] paddle.repeat_interleave(Tensor([1, 1500, 0],"float32"), 5, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:53.921356 test begin: paddle.repeat_interleave(Tensor([10, 0],"float32"), 2, axis=1, )

[cuda error] paddle.repeat_interleave(Tensor([10, 0],"float32"), 2, axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:54.181718 test begin: paddle.repeat_interleave(Tensor([13, 0, 1],"float32"), 1, 2, )

[cuda error] paddle.repeat_interleave(Tensor([13, 0, 1],"float32"), 1, 2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:54.397605 test begin: paddle.repeat_interleave(Tensor([13, 384, 0],"float32"), 1, 2, )

[cuda error] paddle.repeat_interleave(Tensor([13, 384, 0],"float32"), 1, 2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:54.700243 test begin: paddle.repeat_interleave(Tensor([14, 0, 384, 384],"float32"), repeats=3, axis=1, )

[cuda error] paddle.repeat_interleave(Tensor([14, 0, 384, 384],"float32"), repeats=3, axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:54.922264 test begin: paddle.repeat_interleave(Tensor([14, 1, 0, 384],"float32"), repeats=3, axis=1, )

[cuda error] paddle.repeat_interleave(Tensor([14, 1, 0, 384],"float32"), repeats=3, axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:55.178120 test begin: paddle.repeat_interleave(Tensor([14, 1, 384, 0],"float32"), repeats=3, axis=1, )

[cuda error] paddle.repeat_interleave(Tensor([14, 1, 384, 0],"float32"), repeats=3, axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:55.480084 test begin: paddle.repeat_interleave(Tensor([16, 0, 1],"float32"), 1, 2, )

[cuda error] paddle.repeat_interleave(Tensor([16, 0, 1],"float32"), 1, 2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:55.743317 test begin: paddle.repeat_interleave(Tensor([16, 384, 0],"float32"), 1, 2, )

[cuda error] paddle.repeat_interleave(Tensor([16, 384, 0],"float32"), 1, 2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:55.929568 test begin: paddle.repeat_interleave(Tensor([2, 0],"bfloat16"), 2, None, )

[cuda error] paddle.repeat_interleave(Tensor([2, 0],"bfloat16"), 2, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:56.163153 test begin: paddle.repeat_interleave(Tensor([2, 0],"int32"), 2, None, )

[cuda error] paddle.repeat_interleave(Tensor([2, 0],"int32"), 2, None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:56.335981 test begin: paddle.repeat_interleave(Tensor([5, 0, 768, 768],"float32"), repeats=3, axis=1, )

[cuda error] paddle.repeat_interleave(Tensor([5, 0, 768, 768],"float32"), repeats=3, axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:56.583814 test begin: paddle.repeat_interleave(Tensor([5, 1, 0, 768],"float32"), repeats=3, axis=1, )

[cuda error] paddle.repeat_interleave(Tensor([5, 1, 0, 768],"float32"), repeats=3, axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:56.809632 test begin: paddle.repeat_interleave(Tensor([5, 1, 768, 0],"float32"), repeats=3, axis=1, )

[cuda error] paddle.repeat_interleave(Tensor([5, 1, 768, 0],"float32"), repeats=3, axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:57.042943 test begin: paddle.repeat_interleave(x=Tensor([0, 2, 4, 4, 5],"float64"), repeats=2, )

[cuda error] paddle.repeat_interleave(x=Tensor([0, 2, 4, 4, 5],"float64"), repeats=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:57.317553 test begin: paddle.repeat_interleave(x=Tensor([0, 2, 4, 4, 5],"float64"), repeats=2, axis=1, )

[cuda error] paddle.repeat_interleave(x=Tensor([0, 2, 4, 4, 5],"float64"), repeats=2, axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:57.505752 test begin: paddle.repeat_interleave(x=Tensor([0, 2, 4, 4, 5],"int32"), repeats=2, axis=3, )

element 0 of tensors does not require grad and does not have a grad_fn
[cuda error] paddle.repeat_interleave(x=Tensor([0, 2, 4, 4, 5],"int32"), repeats=2, axis=3, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:57.725365 test begin: paddle.repeat_interleave(x=Tensor([0, 2, 4, 5],"float64"), repeats=2, )

[cuda error] paddle.repeat_interleave(x=Tensor([0, 2, 4, 5],"float64"), repeats=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:57.979666 test begin: paddle.repeat_interleave(x=Tensor([0, 2, 4],"float64"), repeats=2, )

[cuda error] paddle.repeat_interleave(x=Tensor([0, 2, 4],"float64"), repeats=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:58.199335 test begin: paddle.repeat_interleave(x=Tensor([0, 2],"float64"), repeats=2, )

[cuda error] paddle.repeat_interleave(x=Tensor([0, 2],"float64"), repeats=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:58.388767 test begin: paddle.repeat_interleave(x=Tensor([0],"float32"), repeats=3, )

[cuda error] paddle.repeat_interleave(x=Tensor([0],"float32"), repeats=3, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:58.598202 test begin: paddle.repeat_interleave(x=Tensor([0],"float64"), repeats=3, )

[cuda error] paddle.repeat_interleave(x=Tensor([0],"float64"), repeats=3, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:58.806551 test begin: paddle.repeat_interleave(x=Tensor([4, 0, 4, 4, 5],"float64"), repeats=2, )

[cuda error] paddle.repeat_interleave(x=Tensor([4, 0, 4, 4, 5],"float64"), repeats=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:58.977448 test begin: paddle.repeat_interleave(x=Tensor([4, 0, 4, 4, 5],"float64"), repeats=2, axis=1, )

[cuda error] paddle.repeat_interleave(x=Tensor([4, 0, 4, 4, 5],"float64"), repeats=2, axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:59.192760 test begin: paddle.repeat_interleave(x=Tensor([4, 0, 4, 4, 5],"int32"), repeats=2, axis=3, )

element 0 of tensors does not require grad and does not have a grad_fn
[cuda error] paddle.repeat_interleave(x=Tensor([4, 0, 4, 4, 5],"int32"), repeats=2, axis=3, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:59.385031 test begin: paddle.repeat_interleave(x=Tensor([4, 0, 4, 5],"float64"), repeats=2, )

[cuda error] paddle.repeat_interleave(x=Tensor([4, 0, 4, 5],"float64"), repeats=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:59.597005 test begin: paddle.repeat_interleave(x=Tensor([4, 0, 4],"float64"), repeats=2, )

[cuda error] paddle.repeat_interleave(x=Tensor([4, 0, 4],"float64"), repeats=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:28:59.881547 test begin: paddle.repeat_interleave(x=Tensor([4, 0],"float64"), repeats=2, )

[cuda error] paddle.repeat_interleave(x=Tensor([4, 0],"float64"), repeats=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:29:00.186452 test begin: paddle.repeat_interleave(x=Tensor([4, 2, 0, 4, 5],"float64"), repeats=2, )

[cuda error] paddle.repeat_interleave(x=Tensor([4, 2, 0, 4, 5],"float64"), repeats=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:29:00.386216 test begin: paddle.repeat_interleave(x=Tensor([4, 2, 0, 4, 5],"float64"), repeats=2, axis=1, )

[cuda error] paddle.repeat_interleave(x=Tensor([4, 2, 0, 4, 5],"float64"), repeats=2, axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:29:00.677900 test begin: paddle.repeat_interleave(x=Tensor([4, 2, 0, 4, 5],"int32"), repeats=2, axis=3, )

element 0 of tensors does not require grad and does not have a grad_fn
[cuda error] paddle.repeat_interleave(x=Tensor([4, 2, 0, 4, 5],"int32"), repeats=2, axis=3, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:29:00.882536 test begin: paddle.repeat_interleave(x=Tensor([4, 2, 0, 5],"float64"), repeats=2, )

[cuda error] paddle.repeat_interleave(x=Tensor([4, 2, 0, 5],"float64"), repeats=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:29:01.107142 test begin: paddle.repeat_interleave(x=Tensor([4, 2, 0],"float64"), repeats=2, )

[cuda error] paddle.repeat_interleave(x=Tensor([4, 2, 0],"float64"), repeats=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:29:01.374787 test begin: paddle.repeat_interleave(x=Tensor([4, 2, 4, 0, 5],"float64"), repeats=2, )

[cuda error] paddle.repeat_interleave(x=Tensor([4, 2, 4, 0, 5],"float64"), repeats=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:29:01.633947 test begin: paddle.repeat_interleave(x=Tensor([4, 2, 4, 0, 5],"float64"), repeats=2, axis=1, )

[cuda error] paddle.repeat_interleave(x=Tensor([4, 2, 4, 0, 5],"float64"), repeats=2, axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:29:01.922039 test begin: paddle.repeat_interleave(x=Tensor([4, 2, 4, 0, 5],"int32"), repeats=2, axis=3, )

element 0 of tensors does not require grad and does not have a grad_fn
[cuda error] paddle.repeat_interleave(x=Tensor([4, 2, 4, 0, 5],"int32"), repeats=2, axis=3, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:29:02.253037 test begin: paddle.repeat_interleave(x=Tensor([4, 2, 4, 0],"float64"), repeats=2, )

[cuda error] paddle.repeat_interleave(x=Tensor([4, 2, 4, 0],"float64"), repeats=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:29:02.439601 test begin: paddle.repeat_interleave(x=Tensor([4, 2, 4, 4, 0],"float64"), repeats=2, )

[cuda error] paddle.repeat_interleave(x=Tensor([4, 2, 4, 4, 0],"float64"), repeats=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:29:02.599114 test begin: paddle.repeat_interleave(x=Tensor([4, 2, 4, 4, 0],"float64"), repeats=2, axis=1, )

[cuda error] paddle.repeat_interleave(x=Tensor([4, 2, 4, 4, 0],"float64"), repeats=2, axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:29:02.812825 test begin: paddle.repeat_interleave(x=Tensor([4, 2, 4, 4, 0],"int32"), repeats=2, axis=3, )

element 0 of tensors does not require grad and does not have a grad_fn
[cuda error] paddle.repeat_interleave(x=Tensor([4, 2, 4, 4, 0],"int32"), repeats=2, axis=3, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:32.154110 test begin: paddle.roll(Tensor([0, 16, 14, 14, 384],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([0, 16, 14, 14, 384],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:32.316626 test begin: paddle.roll(Tensor([0, 16, 14, 14, 384],"float16"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([0, 16, 14, 14, 384],"float16"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:32.526886 test begin: paddle.roll(Tensor([0, 16, 14, 14, 384],"float32"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([0, 16, 14, 14, 384],"float32"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:32.736101 test begin: paddle.roll(Tensor([0, 16, 14, 14, 384],"float32"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([0, 16, 14, 14, 384],"float32"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:32.937467 test begin: paddle.roll(Tensor([0, 16, 14, 7, 768],"float16"), shifts=tuple(-4,-3,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([0, 16, 14, 7, 768],"float16"), shifts=tuple(-4,-3,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:33.139288 test begin: paddle.roll(Tensor([0, 16, 14, 7, 768],"float16"), shifts=tuple(4,3,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([0, 16, 14, 7, 768],"float16"), shifts=tuple(4,3,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:33.377292 test begin: paddle.roll(Tensor([0, 16, 14, 7, 768],"float32"), shifts=tuple(-4,-3,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([0, 16, 14, 7, 768],"float32"), shifts=tuple(-4,-3,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:33.567585 test begin: paddle.roll(Tensor([0, 16, 14, 7, 768],"float32"), shifts=tuple(4,3,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([0, 16, 14, 7, 768],"float32"), shifts=tuple(4,3,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:33.779811 test begin: paddle.roll(Tensor([0, 16, 16, 64],"float32"), shifts=tuple(-2,-2,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([0, 16, 16, 64],"float32"), shifts=tuple(-2,-2,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:34.049709 test begin: paddle.roll(Tensor([0, 16, 16, 64],"float32"), shifts=tuple(2,2,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([0, 16, 16, 64],"float32"), shifts=tuple(2,2,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:34.233994 test begin: paddle.roll(Tensor([0, 16, 7, 14, 768],"float16"), shifts=tuple(-4,0,-3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([0, 16, 7, 14, 768],"float16"), shifts=tuple(-4,0,-3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:34.447707 test begin: paddle.roll(Tensor([0, 16, 7, 14, 768],"float16"), shifts=tuple(4,0,3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([0, 16, 7, 14, 768],"float16"), shifts=tuple(4,0,3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:34.599985 test begin: paddle.roll(Tensor([0, 16, 7, 14, 768],"float32"), shifts=tuple(-4,0,-3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([0, 16, 7, 14, 768],"float32"), shifts=tuple(-4,0,-3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:34.774671 test begin: paddle.roll(Tensor([0, 16, 7, 14, 768],"float32"), shifts=tuple(4,0,3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([0, 16, 7, 14, 768],"float32"), shifts=tuple(4,0,3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:34.911984 test begin: paddle.roll(Tensor([0, 16, 7, 7, 768],"float16"), shifts=tuple(-4,0,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([0, 16, 7, 7, 768],"float16"), shifts=tuple(-4,0,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:35.109133 test begin: paddle.roll(Tensor([0, 16, 7, 7, 768],"float16"), shifts=tuple(4,0,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([0, 16, 7, 7, 768],"float16"), shifts=tuple(4,0,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:35.259154 test begin: paddle.roll(Tensor([0, 16, 7, 7, 768],"float32"), shifts=tuple(-4,0,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([0, 16, 7, 7, 768],"float32"), shifts=tuple(-4,0,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:35.471639 test begin: paddle.roll(Tensor([0, 16, 7, 7, 768],"float32"), shifts=tuple(4,0,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([0, 16, 7, 7, 768],"float32"), shifts=tuple(4,0,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:35.621719 test begin: paddle.roll(Tensor([0, 161, 126, 96],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([0, 161, 126, 96],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:35.850490 test begin: paddle.roll(Tensor([0, 161, 126, 96],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([0, 161, 126, 96],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:36.001431 test begin: paddle.roll(Tensor([0, 192, 144, 192],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([0, 192, 144, 192],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:36.240587 test begin: paddle.roll(Tensor([0, 192, 144, 192],"float32"), shifts=tuple(6,6,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([0, 192, 144, 192],"float32"), shifts=tuple(6,6,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:36.457519 test begin: paddle.roll(Tensor([0, 21, 21, 768],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([0, 21, 21, 768],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:36.607138 test begin: paddle.roll(Tensor([0, 21, 21, 768],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([0, 21, 21, 768],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:36.747181 test begin: paddle.roll(Tensor([0, 24, 24, 1536],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([0, 24, 24, 1536],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:36.945719 test begin: paddle.roll(Tensor([0, 24, 24, 1536],"float32"), shifts=tuple(6,6,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([0, 24, 24, 1536],"float32"), shifts=tuple(6,6,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:37.156488 test begin: paddle.roll(Tensor([0, 32, 32, 32],"float32"), shifts=tuple(-2,-2,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([0, 32, 32, 32],"float32"), shifts=tuple(-2,-2,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:37.372184 test begin: paddle.roll(Tensor([0, 32, 32, 32],"float32"), shifts=tuple(2,2,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([0, 32, 32, 32],"float32"), shifts=tuple(2,2,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:10:37.582333 test begin: paddle.roll(Tensor([0, 3],"float64"), shifts=1, )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   RollGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::roll_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, std::vector<long, std::allocator<long> > const&, paddle::Tensor*)
4   void phi::RollGradKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, std::vector<long, std::allocator<long> > const&, phi::DenseTensor*)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201437 (unix time) try "date -d @1745201437" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f89808ed5f3) received by PID 158657 (TID 0x7f8735fff700) from PID 18446744071571428851 ***]

2025-04-21 10:10:55.256895 test begin: paddle.roll(Tensor([0, 3],"float64"), shifts=1, axis=0, )

W0421 10:11:01.055132 76951 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:11:01.056222 76951 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.roll(Tensor([0, 3],"float64"), shifts=1, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:01.064634 test begin: paddle.roll(Tensor([0],"float32"), -2, name=None, )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   RollGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::roll_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, std::vector<long, std::allocator<long> > const&, paddle::Tensor*)
4   void phi::RollGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, std::vector<long, std::allocator<long> > const&, phi::DenseTensor*)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201461 (unix time) try "date -d @1745201461" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f0e649b76e3) received by PID 75870 (TID 0x7f0d856f8700) from PID 1687910115 ***]

2025-04-21 10:11:05.248237 test begin: paddle.roll(Tensor([0],"float32"), -5, name=None, )

W0421 10:11:11.215742 80568 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:11:11.216987 80568 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   RollGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::roll_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, std::vector<long, std::allocator<long> > const&, paddle::Tensor*)
4   void phi::RollGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, std::vector<long, std::allocator<long> > const&, phi::DenseTensor*)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201471 (unix time) try "date -d @1745201471" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f4c2dcaa6e3) received by PID 79175 (TID 0x7f4b147c3700) from PID 768255715 ***]

2025-04-21 10:11:15.055912 test begin: paddle.roll(Tensor([1, 0, 126, 96],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), )

W0421 10:11:21.332676 84593 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:11:21.333966 84593 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.roll(Tensor([1, 0, 126, 96],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:21.353962 test begin: paddle.roll(Tensor([1, 0, 126, 96],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([1, 0, 126, 96],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:21.551487 test begin: paddle.roll(Tensor([1, 0, 14, 14, 384],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 0, 14, 14, 384],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:21.747537 test begin: paddle.roll(Tensor([1, 0, 14, 14, 384],"float16"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 0, 14, 14, 384],"float16"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:21.949941 test begin: paddle.roll(Tensor([1, 0, 14, 14, 384],"float32"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 0, 14, 14, 384],"float32"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:22.144337 test begin: paddle.roll(Tensor([1, 0, 14, 14, 384],"float32"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 0, 14, 14, 384],"float32"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:22.348875 test begin: paddle.roll(Tensor([1, 0, 14, 7, 768],"float16"), shifts=tuple(-4,-3,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 0, 14, 7, 768],"float16"), shifts=tuple(-4,-3,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:22.576904 test begin: paddle.roll(Tensor([1, 0, 14, 7, 768],"float16"), shifts=tuple(4,3,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 0, 14, 7, 768],"float16"), shifts=tuple(4,3,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:22.770706 test begin: paddle.roll(Tensor([1, 0, 14, 7, 768],"float32"), shifts=tuple(-4,-3,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 0, 14, 7, 768],"float32"), shifts=tuple(-4,-3,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:22.907010 test begin: paddle.roll(Tensor([1, 0, 14, 7, 768],"float32"), shifts=tuple(4,3,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 0, 14, 7, 768],"float32"), shifts=tuple(4,3,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:23.103537 test begin: paddle.roll(Tensor([1, 0, 144, 192],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([1, 0, 144, 192],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:23.311213 test begin: paddle.roll(Tensor([1, 0, 144, 192],"float32"), shifts=tuple(6,6,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([1, 0, 144, 192],"float32"), shifts=tuple(6,6,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:23.519315 test begin: paddle.roll(Tensor([1, 0, 21, 768],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([1, 0, 21, 768],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:23.706830 test begin: paddle.roll(Tensor([1, 0, 21, 768],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([1, 0, 21, 768],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:23.937773 test begin: paddle.roll(Tensor([1, 0, 24, 1536],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([1, 0, 24, 1536],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:24.155928 test begin: paddle.roll(Tensor([1, 0, 24, 1536],"float32"), shifts=tuple(6,6,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([1, 0, 24, 1536],"float32"), shifts=tuple(6,6,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:24.343638 test begin: paddle.roll(Tensor([1, 0, 7, 14, 768],"float16"), shifts=tuple(-4,0,-3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 0, 7, 14, 768],"float16"), shifts=tuple(-4,0,-3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:24.484825 test begin: paddle.roll(Tensor([1, 0, 7, 14, 768],"float16"), shifts=tuple(4,0,3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 0, 7, 14, 768],"float16"), shifts=tuple(4,0,3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:24.648543 test begin: paddle.roll(Tensor([1, 0, 7, 14, 768],"float32"), shifts=tuple(-4,0,-3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 0, 7, 14, 768],"float32"), shifts=tuple(-4,0,-3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:24.845670 test begin: paddle.roll(Tensor([1, 0, 7, 14, 768],"float32"), shifts=tuple(4,0,3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 0, 7, 14, 768],"float32"), shifts=tuple(4,0,3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:25.183571 test begin: paddle.roll(Tensor([1, 0, 7, 7, 768],"float16"), shifts=tuple(-4,0,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 0, 7, 7, 768],"float16"), shifts=tuple(-4,0,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:25.394702 test begin: paddle.roll(Tensor([1, 0, 7, 7, 768],"float16"), shifts=tuple(4,0,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 0, 7, 7, 768],"float16"), shifts=tuple(4,0,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:25.596695 test begin: paddle.roll(Tensor([1, 0, 7, 7, 768],"float32"), shifts=tuple(-4,0,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 0, 7, 7, 768],"float32"), shifts=tuple(-4,0,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:25.796389 test begin: paddle.roll(Tensor([1, 0, 7, 7, 768],"float32"), shifts=tuple(4,0,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 0, 7, 7, 768],"float32"), shifts=tuple(4,0,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:26.011286 test begin: paddle.roll(Tensor([1, 16, 0, 14, 384],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 0, 14, 384],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:26.206421 test begin: paddle.roll(Tensor([1, 16, 0, 14, 384],"float16"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 0, 14, 384],"float16"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:26.429422 test begin: paddle.roll(Tensor([1, 16, 0, 14, 384],"float32"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 0, 14, 384],"float32"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:26.558638 test begin: paddle.roll(Tensor([1, 16, 0, 14, 384],"float32"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 0, 14, 384],"float32"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:26.673653 test begin: paddle.roll(Tensor([1, 16, 0, 14, 768],"float16"), shifts=tuple(-4,0,-3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 0, 14, 768],"float16"), shifts=tuple(-4,0,-3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:26.820222 test begin: paddle.roll(Tensor([1, 16, 0, 14, 768],"float16"), shifts=tuple(4,0,3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 0, 14, 768],"float16"), shifts=tuple(4,0,3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:26.937876 test begin: paddle.roll(Tensor([1, 16, 0, 14, 768],"float32"), shifts=tuple(-4,0,-3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 0, 14, 768],"float32"), shifts=tuple(-4,0,-3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:27.149905 test begin: paddle.roll(Tensor([1, 16, 0, 14, 768],"float32"), shifts=tuple(4,0,3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 0, 14, 768],"float32"), shifts=tuple(4,0,3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:27.342378 test begin: paddle.roll(Tensor([1, 16, 0, 7, 768],"float16"), shifts=tuple(-4,-3,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 0, 7, 768],"float16"), shifts=tuple(-4,-3,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:27.550049 test begin: paddle.roll(Tensor([1, 16, 0, 7, 768],"float16"), shifts=tuple(-4,0,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 0, 7, 768],"float16"), shifts=tuple(-4,0,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:27.717931 test begin: paddle.roll(Tensor([1, 16, 0, 7, 768],"float16"), shifts=tuple(4,0,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 0, 7, 768],"float16"), shifts=tuple(4,0,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:27.917946 test begin: paddle.roll(Tensor([1, 16, 0, 7, 768],"float16"), shifts=tuple(4,3,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 0, 7, 768],"float16"), shifts=tuple(4,3,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:28.127546 test begin: paddle.roll(Tensor([1, 16, 0, 7, 768],"float32"), shifts=tuple(-4,-3,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 0, 7, 768],"float32"), shifts=tuple(-4,-3,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:28.371094 test begin: paddle.roll(Tensor([1, 16, 0, 7, 768],"float32"), shifts=tuple(-4,0,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 0, 7, 768],"float32"), shifts=tuple(-4,0,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:28.611690 test begin: paddle.roll(Tensor([1, 16, 0, 7, 768],"float32"), shifts=tuple(4,0,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 0, 7, 768],"float32"), shifts=tuple(4,0,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:28.860943 test begin: paddle.roll(Tensor([1, 16, 0, 7, 768],"float32"), shifts=tuple(4,3,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 0, 7, 768],"float32"), shifts=tuple(4,3,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:29.118356 test begin: paddle.roll(Tensor([1, 16, 14, 0, 384],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 14, 0, 384],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:29.290399 test begin: paddle.roll(Tensor([1, 16, 14, 0, 384],"float16"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 14, 0, 384],"float16"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:29.486356 test begin: paddle.roll(Tensor([1, 16, 14, 0, 384],"float32"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 14, 0, 384],"float32"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:29.635893 test begin: paddle.roll(Tensor([1, 16, 14, 0, 384],"float32"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 14, 0, 384],"float32"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:29.823256 test begin: paddle.roll(Tensor([1, 16, 14, 0, 768],"float16"), shifts=tuple(-4,-3,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 14, 0, 768],"float16"), shifts=tuple(-4,-3,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:29.973901 test begin: paddle.roll(Tensor([1, 16, 14, 0, 768],"float16"), shifts=tuple(4,3,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 14, 0, 768],"float16"), shifts=tuple(4,3,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:30.164773 test begin: paddle.roll(Tensor([1, 16, 14, 0, 768],"float32"), shifts=tuple(-4,-3,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 14, 0, 768],"float32"), shifts=tuple(-4,-3,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:30.369907 test begin: paddle.roll(Tensor([1, 16, 14, 0, 768],"float32"), shifts=tuple(4,3,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 14, 0, 768],"float32"), shifts=tuple(4,3,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:30.572069 test begin: paddle.roll(Tensor([1, 16, 14, 14, 0],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 14, 14, 0],"float16"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:30.811525 test begin: paddle.roll(Tensor([1, 16, 14, 14, 0],"float16"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 14, 14, 0],"float16"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:31.009525 test begin: paddle.roll(Tensor([1, 16, 14, 14, 0],"float32"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 14, 14, 0],"float32"), shifts=tuple(-4,-3,-3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:31.274118 test begin: paddle.roll(Tensor([1, 16, 14, 14, 0],"float32"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 14, 14, 0],"float32"), shifts=tuple(4,3,3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:31.411913 test begin: paddle.roll(Tensor([1, 16, 14, 7, 0],"float16"), shifts=tuple(-4,-3,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 14, 7, 0],"float16"), shifts=tuple(-4,-3,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:31.618189 test begin: paddle.roll(Tensor([1, 16, 14, 7, 0],"float16"), shifts=tuple(4,3,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 14, 7, 0],"float16"), shifts=tuple(4,3,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:31.822533 test begin: paddle.roll(Tensor([1, 16, 14, 7, 0],"float32"), shifts=tuple(-4,-3,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 14, 7, 0],"float32"), shifts=tuple(-4,-3,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:32.030344 test begin: paddle.roll(Tensor([1, 16, 14, 7, 0],"float32"), shifts=tuple(4,3,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 14, 7, 0],"float32"), shifts=tuple(4,3,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:32.234034 test begin: paddle.roll(Tensor([1, 16, 7, 0, 768],"float16"), shifts=tuple(-4,0,-3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 7, 0, 768],"float16"), shifts=tuple(-4,0,-3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:32.434338 test begin: paddle.roll(Tensor([1, 16, 7, 0, 768],"float16"), shifts=tuple(-4,0,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 7, 0, 768],"float16"), shifts=tuple(-4,0,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:32.635114 test begin: paddle.roll(Tensor([1, 16, 7, 0, 768],"float16"), shifts=tuple(4,0,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 7, 0, 768],"float16"), shifts=tuple(4,0,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:32.976797 test begin: paddle.roll(Tensor([1, 16, 7, 0, 768],"float16"), shifts=tuple(4,0,3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 7, 0, 768],"float16"), shifts=tuple(4,0,3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:33.104891 test begin: paddle.roll(Tensor([1, 16, 7, 0, 768],"float32"), shifts=tuple(-4,0,-3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 7, 0, 768],"float32"), shifts=tuple(-4,0,-3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:33.217506 test begin: paddle.roll(Tensor([1, 16, 7, 0, 768],"float32"), shifts=tuple(-4,0,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 7, 0, 768],"float32"), shifts=tuple(-4,0,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:33.356915 test begin: paddle.roll(Tensor([1, 16, 7, 0, 768],"float32"), shifts=tuple(4,0,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 7, 0, 768],"float32"), shifts=tuple(4,0,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:33.475242 test begin: paddle.roll(Tensor([1, 16, 7, 0, 768],"float32"), shifts=tuple(4,0,3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 7, 0, 768],"float32"), shifts=tuple(4,0,3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:33.604137 test begin: paddle.roll(Tensor([1, 16, 7, 14, 0],"float16"), shifts=tuple(-4,0,-3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 7, 14, 0],"float16"), shifts=tuple(-4,0,-3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:33.792104 test begin: paddle.roll(Tensor([1, 16, 7, 14, 0],"float16"), shifts=tuple(4,0,3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 7, 14, 0],"float16"), shifts=tuple(4,0,3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:34.002995 test begin: paddle.roll(Tensor([1, 16, 7, 14, 0],"float32"), shifts=tuple(-4,0,-3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 7, 14, 0],"float32"), shifts=tuple(-4,0,-3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:34.209818 test begin: paddle.roll(Tensor([1, 16, 7, 14, 0],"float32"), shifts=tuple(4,0,3,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 7, 14, 0],"float32"), shifts=tuple(4,0,3,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:34.526516 test begin: paddle.roll(Tensor([1, 16, 7, 7, 0],"float16"), shifts=tuple(-4,0,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 7, 7, 0],"float16"), shifts=tuple(-4,0,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:34.862917 test begin: paddle.roll(Tensor([1, 16, 7, 7, 0],"float16"), shifts=tuple(4,0,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 7, 7, 0],"float16"), shifts=tuple(4,0,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:35.061432 test begin: paddle.roll(Tensor([1, 16, 7, 7, 0],"float32"), shifts=tuple(-4,0,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 7, 7, 0],"float32"), shifts=tuple(-4,0,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:35.213464 test begin: paddle.roll(Tensor([1, 16, 7, 7, 0],"float32"), shifts=tuple(4,0,0,), axis=tuple(1,2,3,), )

[cuda error] paddle.roll(Tensor([1, 16, 7, 7, 0],"float32"), shifts=tuple(4,0,0,), axis=tuple(1,2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:35.403337 test begin: paddle.roll(Tensor([1, 161, 0, 96],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([1, 161, 0, 96],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:35.689326 test begin: paddle.roll(Tensor([1, 161, 0, 96],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([1, 161, 0, 96],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:35.927278 test begin: paddle.roll(Tensor([1, 161, 126, 0],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([1, 161, 126, 0],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:36.146725 test begin: paddle.roll(Tensor([1, 161, 126, 0],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([1, 161, 126, 0],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:36.419807 test begin: paddle.roll(Tensor([1, 192, 0, 192],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([1, 192, 0, 192],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:36.634391 test begin: paddle.roll(Tensor([1, 192, 0, 192],"float32"), shifts=tuple(6,6,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([1, 192, 0, 192],"float32"), shifts=tuple(6,6,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:36.977544 test begin: paddle.roll(Tensor([1, 192, 144, 0],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([1, 192, 144, 0],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:37.212958 test begin: paddle.roll(Tensor([1, 192, 144, 0],"float32"), shifts=tuple(6,6,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([1, 192, 144, 0],"float32"), shifts=tuple(6,6,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:37.440085 test begin: paddle.roll(Tensor([1, 21, 0, 768],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([1, 21, 0, 768],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:37.653456 test begin: paddle.roll(Tensor([1, 21, 0, 768],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([1, 21, 0, 768],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:37.814167 test begin: paddle.roll(Tensor([1, 21, 21, 0],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([1, 21, 21, 0],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:38.041266 test begin: paddle.roll(Tensor([1, 21, 21, 0],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([1, 21, 21, 0],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:38.267972 test begin: paddle.roll(Tensor([1, 24, 0, 1536],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([1, 24, 0, 1536],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:38.493867 test begin: paddle.roll(Tensor([1, 24, 0, 1536],"float32"), shifts=tuple(6,6,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([1, 24, 0, 1536],"float32"), shifts=tuple(6,6,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:38.719164 test begin: paddle.roll(Tensor([1, 24, 24, 0],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([1, 24, 24, 0],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:38.933605 test begin: paddle.roll(Tensor([1, 24, 24, 0],"float32"), shifts=tuple(6,6,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([1, 24, 24, 0],"float32"), shifts=tuple(6,6,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:39.153037 test begin: paddle.roll(Tensor([12, 0, 16, 64],"float32"), shifts=tuple(-2,-2,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([12, 0, 16, 64],"float32"), shifts=tuple(-2,-2,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:39.306266 test begin: paddle.roll(Tensor([12, 0, 16, 64],"float32"), shifts=tuple(2,2,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([12, 0, 16, 64],"float32"), shifts=tuple(2,2,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:39.499696 test begin: paddle.roll(Tensor([12, 0, 32, 32],"float32"), shifts=tuple(-2,-2,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([12, 0, 32, 32],"float32"), shifts=tuple(-2,-2,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:39.726099 test begin: paddle.roll(Tensor([12, 0, 32, 32],"float32"), shifts=tuple(2,2,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([12, 0, 32, 32],"float32"), shifts=tuple(2,2,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:39.866744 test begin: paddle.roll(Tensor([12, 16, 0, 64],"float32"), shifts=tuple(-2,-2,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([12, 16, 0, 64],"float32"), shifts=tuple(-2,-2,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:40.075454 test begin: paddle.roll(Tensor([12, 16, 0, 64],"float32"), shifts=tuple(2,2,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([12, 16, 0, 64],"float32"), shifts=tuple(2,2,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:40.213710 test begin: paddle.roll(Tensor([12, 16, 16, 0],"float32"), shifts=tuple(-2,-2,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([12, 16, 16, 0],"float32"), shifts=tuple(-2,-2,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:40.410093 test begin: paddle.roll(Tensor([12, 16, 16, 0],"float32"), shifts=tuple(2,2,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([12, 16, 16, 0],"float32"), shifts=tuple(2,2,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:40.556731 test begin: paddle.roll(Tensor([12, 32, 0, 32],"float32"), shifts=tuple(-2,-2,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([12, 32, 0, 32],"float32"), shifts=tuple(-2,-2,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:40.681251 test begin: paddle.roll(Tensor([12, 32, 0, 32],"float32"), shifts=tuple(2,2,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([12, 32, 0, 32],"float32"), shifts=tuple(2,2,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:40.922214 test begin: paddle.roll(Tensor([12, 32, 32, 0],"float32"), shifts=tuple(-2,-2,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([12, 32, 32, 0],"float32"), shifts=tuple(-2,-2,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:41.065769 test begin: paddle.roll(Tensor([12, 32, 32, 0],"float32"), shifts=tuple(2,2,), axis=tuple(1,2,), )

[cuda error] paddle.roll(Tensor([12, 32, 32, 0],"float32"), shifts=tuple(2,2,), axis=tuple(1,2,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:41.188387 test begin: paddle.roll(Tensor([3, 0],"float64"), shifts=1, )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   RollGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::roll_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, std::vector<long, std::allocator<long> > const&, paddle::Tensor*)
4   void phi::RollGradKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, std::vector<long, std::allocator<long> > const&, phi::DenseTensor*)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201501 (unix time) try "date -d @1745201501" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f4a8c51a5f3) received by PID 83230 (TID 0x7f4988949700) from PID 18446744071768745459 ***]

2025-04-21 10:11:58.232268 test begin: paddle.roll(Tensor([3, 0],"float64"), shifts=1, axis=0, )

W0421 10:12:04.054351 99520 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:12:04.055567 99520 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.roll(Tensor([3, 0],"float64"), shifts=1, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:04.069009 test begin: paddle.roll(x=Tensor([0, 3],"float32"), shifts=0, axis=None, )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   RollGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::roll_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, std::vector<long, std::allocator<long> > const&, paddle::Tensor*)
4   void phi::RollGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, std::vector<long, std::allocator<long> > const&, phi::DenseTensor*)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201524 (unix time) try "date -d @1745201524" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fac1f8a46e3) received by PID 98291 (TID 0x7fab2bf48700) from PID 529155811 ***]

2025-04-21 10:12:08.534063 test begin: paddle.roll(x=Tensor([0, 3],"float64"), shifts=-1, axis=0, )

W0421 10:12:14.639999 103229 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:12:14.641117 103229 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.roll(x=Tensor([0, 3],"float64"), shifts=-1, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:14.653991 test begin: paddle.roll(x=Tensor([0, 3],"float64"), shifts=0, axis=None, )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   RollGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::roll_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, std::vector<long, std::allocator<long> > const&, paddle::Tensor*)
4   void phi::RollGradKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, std::vector<long, std::allocator<long> > const&, phi::DenseTensor*)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201534 (unix time) try "date -d @1745201534" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fa02ad935f3) received by PID 101891 (TID 0x7f9f1babb700) from PID 718878195 ***]

2025-04-21 10:12:18.834123 test begin: paddle.roll(x=Tensor([0, 3],"float64"), shifts=1, axis=None, )

W0421 10:12:25.089599 106633 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:12:25.090684 106633 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   RollGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::roll_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, std::vector<long, std::allocator<long> > const&, paddle::Tensor*)
4   void phi::RollGradKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, std::vector<long, std::allocator<long> > const&, phi::DenseTensor*)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201545 (unix time) try "date -d @1745201545" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f83214535f3) received by PID 105290 (TID 0x7f8245dc2700) from PID 558183923 ***]

2025-04-21 10:12:42.654741 test begin: paddle.roll(x=Tensor([0, 3],"float64"), shifts=list[-1,1,], axis=list[0,1,], )

W0421 10:12:48.540048 114840 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:12:48.541217 114840 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.roll(x=Tensor([0, 3],"float64"), shifts=list[-1,1,], axis=list[0,1,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:48.548824 test begin: paddle.roll(x=Tensor([0, 3],"float64"), shifts=tuple(-1,1,), axis=tuple(0,1,), )

[cuda error] paddle.roll(x=Tensor([0, 3],"float64"), shifts=tuple(-1,1,), axis=tuple(0,1,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:48.739217 test begin: paddle.roll(x=Tensor([3, 0],"float32"), shifts=0, axis=None, )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   RollGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::roll_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, std::vector<long, std::allocator<long> > const&, paddle::Tensor*)
4   void phi::RollGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, std::vector<long, std::allocator<long> > const&, phi::DenseTensor*)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201568 (unix time) try "date -d @1745201568" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fae7e53f6e3) received by PID 113675 (TID 0x7fadbb2b7700) from PID 2119431907 ***]

2025-04-21 10:12:52.585353 test begin: paddle.roll(x=Tensor([3, 0],"float64"), shifts=-1, axis=0, )

W0421 10:12:58.949689 118723 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:12:58.951048 118723 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.roll(x=Tensor([3, 0],"float64"), shifts=-1, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:58.962285 test begin: paddle.roll(x=Tensor([3, 0],"float64"), shifts=0, axis=None, )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   RollGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::roll_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, std::vector<long, std::allocator<long> > const&, paddle::Tensor*)
4   void phi::RollGradKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, std::vector<long, std::allocator<long> > const&, phi::DenseTensor*)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201579 (unix time) try "date -d @1745201579" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f93dcf045f3) received by PID 117706 (TID 0x7f92e5b85700) from PID 18446744073121318387 ***]

2025-04-21 10:13:03.295726 test begin: paddle.roll(x=Tensor([3, 0],"float64"), shifts=1, axis=None, )

W0421 10:13:09.265746 122460 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:13:09.266870 122460 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   RollGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::roll_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, std::vector<long, std::allocator<long> > const&, paddle::Tensor*)
4   void phi::RollGradKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, std::vector<long, std::allocator<long> > const&, phi::DenseTensor*)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201589 (unix time) try "date -d @1745201589" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f44d4d645f3) received by PID 121200 (TID 0x7f43e5935700) from PID 18446744072985396723 ***]

2025-04-21 10:13:26.226655 test begin: paddle.roll(x=Tensor([3, 0],"float64"), shifts=list[-1,1,], axis=list[0,1,], )

W0421 10:13:32.400769 131165 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:13:32.401978 131165 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.roll(x=Tensor([3, 0],"float64"), shifts=list[-1,1,], axis=list[0,1,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:32.411329 test begin: paddle.roll(x=Tensor([3, 0],"float64"), shifts=tuple(-1,1,), axis=tuple(0,1,), )

[cuda error] paddle.roll(x=Tensor([3, 0],"float64"), shifts=tuple(-1,1,), axis=tuple(0,1,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:32.619506 test begin: paddle.rot90(Tensor([0, 3],"float32"), k=1, axes=list[0,1,], )

[cuda error] paddle.rot90(Tensor([0, 3],"float32"), k=1, axes=list[0,1,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:32.850902 test begin: paddle.rot90(Tensor([2, 0],"float32"), k=1, axes=list[0,1,], )

[cuda error] paddle.rot90(Tensor([2, 0],"float32"), k=1, axes=list[0,1,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:33.253223 test begin: paddle.rot90(x=Tensor([0, 4, 4, 4],"float64"), )

[cuda error] paddle.rot90(x=Tensor([0, 4, 4, 4],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:33.449723 test begin: paddle.rot90(x=Tensor([0, 4, 4, 4],"float64"), k=-1, axes=list[1,2,], )

[cuda error] paddle.rot90(x=Tensor([0, 4, 4, 4],"float64"), k=-1, axes=list[1,2,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:33.661904 test begin: paddle.rot90(x=Tensor([0, 4, 4, 4],"float64"), k=-1, axes=tuple(2,3,), )

[cuda error] paddle.rot90(x=Tensor([0, 4, 4, 4],"float64"), k=-1, axes=tuple(2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:33.861711 test begin: paddle.rot90(x=Tensor([0, 4, 4],"float64"), )

[cuda error] paddle.rot90(x=Tensor([0, 4, 4],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:34.111183 test begin: paddle.rot90(x=Tensor([0, 4],"float32"), )

[cuda error] paddle.rot90(x=Tensor([0, 4],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:34.279258 test begin: paddle.rot90(x=Tensor([0, 4],"float64"), )

[cuda error] paddle.rot90(x=Tensor([0, 4],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:34.402076 test begin: paddle.rot90(x=Tensor([0, 4],"float64"), k=-1, )

[cuda error] paddle.rot90(x=Tensor([0, 4],"float64"), k=-1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:35.077596 test begin: paddle.rot90(x=Tensor([3, 0],"float64"), )

[cuda error] paddle.rot90(x=Tensor([3, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:35.260571 test begin: paddle.rot90(x=Tensor([4, 0, 4, 4],"float64"), )

[cuda error] paddle.rot90(x=Tensor([4, 0, 4, 4],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:35.451808 test begin: paddle.rot90(x=Tensor([4, 0, 4, 4],"float64"), k=-1, axes=list[1,2,], )

[cuda error] paddle.rot90(x=Tensor([4, 0, 4, 4],"float64"), k=-1, axes=list[1,2,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:35.706945 test begin: paddle.rot90(x=Tensor([4, 0, 4, 4],"float64"), k=-1, axes=tuple(2,3,), )

[cuda error] paddle.rot90(x=Tensor([4, 0, 4, 4],"float64"), k=-1, axes=tuple(2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:35.923628 test begin: paddle.rot90(x=Tensor([4, 0, 4],"float64"), )

[cuda error] paddle.rot90(x=Tensor([4, 0, 4],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:36.092593 test begin: paddle.rot90(x=Tensor([4, 0],"float32"), )

[cuda error] paddle.rot90(x=Tensor([4, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:36.309668 test begin: paddle.rot90(x=Tensor([4, 0],"float64"), k=-1, )

[cuda error] paddle.rot90(x=Tensor([4, 0],"float64"), k=-1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:36.731996 test begin: paddle.rot90(x=Tensor([4, 4, 0, 4],"float64"), )

[cuda error] paddle.rot90(x=Tensor([4, 4, 0, 4],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:36.920960 test begin: paddle.rot90(x=Tensor([4, 4, 0, 4],"float64"), k=-1, axes=list[1,2,], )

[cuda error] paddle.rot90(x=Tensor([4, 4, 0, 4],"float64"), k=-1, axes=list[1,2,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:37.097267 test begin: paddle.rot90(x=Tensor([4, 4, 0, 4],"float64"), k=-1, axes=tuple(2,3,), )

[cuda error] paddle.rot90(x=Tensor([4, 4, 0, 4],"float64"), k=-1, axes=tuple(2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:37.221924 test begin: paddle.rot90(x=Tensor([4, 4, 0],"float64"), )

[cuda error] paddle.rot90(x=Tensor([4, 4, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:37.345398 test begin: paddle.rot90(x=Tensor([4, 4, 4, 0],"float64"), )

[cuda error] paddle.rot90(x=Tensor([4, 4, 4, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:37.533036 test begin: paddle.rot90(x=Tensor([4, 4, 4, 0],"float64"), k=-1, axes=list[1,2,], )

[cuda error] paddle.rot90(x=Tensor([4, 4, 4, 0],"float64"), k=-1, axes=list[1,2,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:37.672592 test begin: paddle.rot90(x=Tensor([4, 4, 4, 0],"float64"), k=-1, axes=tuple(2,3,), )

[cuda error] paddle.rot90(x=Tensor([4, 4, 4, 0],"float64"), k=-1, axes=tuple(2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:13:41.468989 test begin: paddle.row_stack(list[Tensor([0],"float64"),Tensor([0],"float64"),Tensor([0],"float64"),], )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   UnsqueezeGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::unsqueeze_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   paddle::Tensor::type() const

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201621 (unix time) try "date -d @1745201621" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x0) received by PID 129997 (TID 0x7f3ced0b7700) from PID 0 ***]

2025-04-21 10:13:51.867251 test begin: paddle.row_stack(list[Tensor([0],"float64"),], )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   UnsqueezeGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::unsqueeze_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   paddle::Tensor::type() const

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201632 (unix time) try "date -d @1745201632" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x0) received by PID 136602 (TID 0x7f0abf87e700) from PID 0 ***]

2025-04-21 10:14:29.092182 test begin: paddle.sgn(Tensor([0, 4],"complex128"), )

[paddle error] paddle.sgn(Tensor([0, 4],"complex128"), ) 
 (InvalidArgument) Expected the stride of last dimension of input(X) to be 1.But received 2. This means that the last dimension of theTensor(x) is not continuous and cannot be as_complex directly.You can call x.contiguous() to make the Tensor(x) contiguous first.
  [Hint: Expected x.strides()[x.strides().size() - 1] == 1, but received x.strides()[x.strides().size() - 1]:2 != 1:1.] (at ../paddle/phi/kernels/stride/as_complex_kernel.cc:35)

2025-04-21 10:14:29.287022 test begin: paddle.sgn(Tensor([0, 4],"complex64"), )

[paddle error] paddle.sgn(Tensor([0, 4],"complex64"), ) 
 (InvalidArgument) Expected the stride of last dimension of input(X) to be 1.But received 2. This means that the last dimension of theTensor(x) is not continuous and cannot be as_complex directly.You can call x.contiguous() to make the Tensor(x) contiguous first.
  [Hint: Expected x.strides()[x.strides().size() - 1] == 1, but received x.strides()[x.strides().size() - 1]:2 != 1:1.] (at ../paddle/phi/kernels/stride/as_complex_kernel.cc:35)

2025-04-21 10:14:30.237351 test begin: paddle.sgn(Tensor([2, 0],"complex128"), )

[paddle error] paddle.sgn(Tensor([2, 0],"complex128"), ) 
 (InvalidArgument) Expected the stride of last dimension of input(X) to be 1.But received 2. This means that the last dimension of theTensor(x) is not continuous and cannot be as_complex directly.You can call x.contiguous() to make the Tensor(x) contiguous first.
  [Hint: Expected x.strides()[x.strides().size() - 1] == 1, but received x.strides()[x.strides().size() - 1]:2 != 1:1.] (at ../paddle/phi/kernels/stride/as_complex_kernel.cc:35)

2025-04-21 10:14:30.397823 test begin: paddle.sgn(Tensor([2, 0],"complex64"), )

[paddle error] paddle.sgn(Tensor([2, 0],"complex64"), ) 
 (InvalidArgument) Expected the stride of last dimension of input(X) to be 1.But received 2. This means that the last dimension of theTensor(x) is not continuous and cannot be as_complex directly.You can call x.contiguous() to make the Tensor(x) contiguous first.
  [Hint: Expected x.strides()[x.strides().size() - 1] == 1, but received x.strides()[x.strides().size() - 1]:2 != 1:1.] (at ../paddle/phi/kernels/stride/as_complex_kernel.cc:35)

2025-04-21 10:14:48.258801 test begin: paddle.sinc(Tensor([0, 64],"float32"), )

[cuda error] paddle.sinc(Tensor([0, 64],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:14:48.466735 test begin: paddle.sinc(Tensor([0, 64],"float64"), )

[cuda error] paddle.sinc(Tensor([0, 64],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:14:48.656522 test begin: paddle.sinc(Tensor([0],"float32"), )

[cuda error] paddle.sinc(Tensor([0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:14:48.837606 test begin: paddle.sinc(Tensor([0],"float64"), )

[cuda error] paddle.sinc(Tensor([0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:14:49.038052 test begin: paddle.sinc(Tensor([16, 0],"float32"), )

[cuda error] paddle.sinc(Tensor([16, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:14:49.172164 test begin: paddle.sinc(Tensor([16, 0],"float64"), )

[cuda error] paddle.sinc(Tensor([16, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:17.417722 test begin: paddle.Tensor.amax(Tensor([0, 2, 4, 5],"float32"), axis=-1, keepdim=True, )

W0421 10:07:23.314576 161413 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:07:23.315795 161413 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.Tensor.amax(Tensor([0, 2, 4, 5],"float32"), axis=-1, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:07:23.317074 test begin: paddle.Tensor.amax(Tensor([0, 2, 5, 4],"float32"), axis=2, keepdim=True, )

[paddle error] paddle.Tensor.amax(Tensor([0, 2, 5, 4],"float32"), axis=2, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:07:23.563285 test begin: paddle.Tensor.amax(Tensor([0, 2, 5, 4],"float32"), axis=tuple(1,2,), keepdim=True, )

[paddle error] paddle.Tensor.amax(Tensor([0, 2, 5, 4],"float32"), axis=tuple(1,2,), keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:07:23.788199 test begin: paddle.Tensor.amax(Tensor([3, 0, 4, 5],"float32"), axis=-1, keepdim=True, )

[paddle error] paddle.Tensor.amax(Tensor([3, 0, 4, 5],"float32"), axis=-1, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:07:23.987881 test begin: paddle.Tensor.amax(Tensor([3, 0, 5, 4],"float32"), axis=2, keepdim=True, )

[paddle error] paddle.Tensor.amax(Tensor([3, 0, 5, 4],"float32"), axis=2, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:07:24.204942 test begin: paddle.Tensor.amax(Tensor([3, 2, 0, 5],"float32"), axis=-1, keepdim=True, )

[paddle error] paddle.Tensor.amax(Tensor([3, 2, 0, 5],"float32"), axis=-1, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:07:24.466657 test begin: paddle.Tensor.amax(Tensor([3, 2, 5, 0],"float32"), axis=2, keepdim=True, )

[paddle error] paddle.Tensor.amax(Tensor([3, 2, 5, 0],"float32"), axis=2, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:07:24.700478 test begin: paddle.Tensor.amax(Tensor([3, 2, 5, 0],"float32"), axis=tuple(1,2,), keepdim=True, )

[paddle error] paddle.Tensor.amax(Tensor([3, 2, 5, 0],"float32"), axis=tuple(1,2,), keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:07:24.941294 test begin: paddle.Tensor.amin(Tensor([0, 2, 4, 5],"float32"), axis=-1, keepdim=True, )

[paddle error] paddle.Tensor.amin(Tensor([0, 2, 4, 5],"float32"), axis=-1, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:07:25.151471 test begin: paddle.Tensor.amin(Tensor([0, 2, 5, 4],"float32"), axis=2, keepdim=True, )

[paddle error] paddle.Tensor.amin(Tensor([0, 2, 5, 4],"float32"), axis=2, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:07:25.377137 test begin: paddle.Tensor.amin(Tensor([0, 2, 5, 4],"float32"), axis=tuple(1,2,), keepdim=True, )

[paddle error] paddle.Tensor.amin(Tensor([0, 2, 5, 4],"float32"), axis=tuple(1,2,), keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:07:25.578422 test begin: paddle.Tensor.amin(Tensor([3, 0, 4, 5],"float32"), axis=-1, keepdim=True, )

[paddle error] paddle.Tensor.amin(Tensor([3, 0, 4, 5],"float32"), axis=-1, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:07:25.765525 test begin: paddle.Tensor.amin(Tensor([3, 0, 5, 4],"float32"), axis=2, keepdim=True, )

[paddle error] paddle.Tensor.amin(Tensor([3, 0, 5, 4],"float32"), axis=2, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:07:25.913449 test begin: paddle.Tensor.amin(Tensor([3, 2, 0, 5],"float32"), axis=-1, keepdim=True, )

[paddle error] paddle.Tensor.amin(Tensor([3, 2, 0, 5],"float32"), axis=-1, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:07:26.052706 test begin: paddle.Tensor.amin(Tensor([3, 2, 5, 0],"float32"), axis=2, keepdim=True, )

[paddle error] paddle.Tensor.amin(Tensor([3, 2, 5, 0],"float32"), axis=2, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:07:26.303993 test begin: paddle.Tensor.amin(Tensor([3, 2, 5, 0],"float32"), axis=tuple(1,2,), keepdim=True, )

[paddle error] paddle.Tensor.amin(Tensor([3, 2, 5, 0],"float32"), axis=tuple(1,2,), keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:07:26.458744 test begin: paddle.Tensor.argmax(Tensor([0, 1, 10285],"float32"), axis=-2, )

[paddle error] paddle.Tensor.argmax(Tensor([0, 1, 10285],"float32"), axis=-2, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:07:26.654054 test begin: paddle.Tensor.argmax(Tensor([0, 1, 24276],"float32"), axis=-2, )

[paddle error] paddle.Tensor.argmax(Tensor([0, 1, 24276],"float32"), axis=-2, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:07:26.907498 test begin: paddle.Tensor.argmax(Tensor([0, 100, 8000],"float32"), axis=2, )

[paddle error] paddle.Tensor.argmax(Tensor([0, 100, 8000],"float32"), axis=2, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:07:27.253025 test begin: paddle.Tensor.argmax(Tensor([0, 101, 8000],"float32"), axis=2, )

[paddle error] paddle.Tensor.argmax(Tensor([0, 101, 8000],"float32"), axis=2, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:07:27.557850 test begin: paddle.Tensor.argmax(Tensor([0, 10],"float32"), axis=1, )

[paddle error] paddle.Tensor.argmax(Tensor([0, 10],"float32"), axis=1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:07:27.745780 test begin: paddle.Tensor.argmax(Tensor([0, 157920, 2],"float32"), axis=-1, )

[paddle error] paddle.Tensor.argmax(Tensor([0, 157920, 2],"float32"), axis=-1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:07:27.954395 test begin: paddle.Tensor.argmax(Tensor([0, 3, 3],"float32"), 1, )

[paddle error] paddle.Tensor.argmax(Tensor([0, 3, 3],"float32"), 1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:07:28.221548 test begin: paddle.Tensor.argmax(Tensor([0, 3],"float32"), 1, )

[paddle error] paddle.Tensor.argmax(Tensor([0, 3],"float32"), 1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:07:28.485746 test begin: paddle.Tensor.argmax(Tensor([0, 4],"float32"), axis=-1, )

[paddle error] paddle.Tensor.argmax(Tensor([0, 4],"float32"), axis=-1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:07:28.633159 test begin: paddle.Tensor.argmax(Tensor([0, 77],"int64"), axis=-1, )

[paddle error] paddle.Tensor.argmax(Tensor([0, 77],"int64"), axis=-1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:07:28.866145 test begin: paddle.Tensor.argmax(Tensor([0, 7],"int32"), -1, )

[paddle error] paddle.Tensor.argmax(Tensor([0, 7],"int32"), -1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:07:29.056977 test begin: paddle.Tensor.argmax(Tensor([0, 90, 22400],"float32"), axis=1, )

[paddle error] paddle.Tensor.argmax(Tensor([0, 90, 22400],"float32"), axis=1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:07:29.264412 test begin: paddle.Tensor.argmax(Tensor([1, 1, 0],"float32"), axis=-2, )

[paddle error] paddle.Tensor.argmax(Tensor([1, 1, 0],"float32"), axis=-2, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:07:29.408848 test begin: paddle.Tensor.argmax(Tensor([13, 3, 0],"float32"), 1, )

[paddle error] paddle.Tensor.argmax(Tensor([13, 3, 0],"float32"), 1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:07:29.563227 test begin: paddle.Tensor.argmax(Tensor([30, 0, 8000],"float32"), axis=2, )

[paddle error] paddle.Tensor.argmax(Tensor([30, 0, 8000],"float32"), axis=2, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:07:29.739295 test begin: paddle.Tensor.argmax(Tensor([4, 0, 2],"float32"), axis=-1, )

[paddle error] paddle.Tensor.argmax(Tensor([4, 0, 2],"float32"), axis=-1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:07:29.979149 test begin: paddle.Tensor.argmax(Tensor([4, 90, 0],"float32"), axis=1, )

[paddle error] paddle.Tensor.argmax(Tensor([4, 90, 0],"float32"), axis=1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:226)

2025-04-21 10:07:31.229144 test begin: paddle.Tensor.bmm(Tensor([0, 108472, 3],"float32"), Tensor([0, 3, 2],"float32"), )

W0421 10:07:31.548985  5250 backward.cc:437] While running Node (BmmGradNode) raises an EnforceNotMet exception
[paddle error] paddle.Tensor.bmm(Tensor([0, 108472, 3],"float32"), Tensor([0, 3, 2],"float32"), ) 
 (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 24, memory's size is 0.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):24 > memory_size():0.] (at ../paddle/phi/core/dense_tensor_impl.cc:48)

2025-04-21 10:07:31.557345 test begin: paddle.Tensor.bmm(Tensor([0, 1156, 3],"float32"), Tensor([0, 3, 2],"float32"), )

W0421 10:07:31.749962  5562 backward.cc:437] While running Node (BmmGradNode) raises an EnforceNotMet exception
[paddle error] paddle.Tensor.bmm(Tensor([0, 1156, 3],"float32"), Tensor([0, 3, 2],"float32"), ) 
 (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 24, memory's size is 0.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):24 > memory_size():0.] (at ../paddle/phi/core/dense_tensor_impl.cc:48)

2025-04-21 10:07:33.329565 test begin: paddle.Tensor.bmm(Tensor([1, 108472, 3],"float32"), Tensor([1, 3, 0],"float32"), )

W0421 10:07:33.566874  6068 backward.cc:437] While running Node (BmmGradNode) raises an EnforceNotMet exception
[paddle error] paddle.Tensor.bmm(Tensor([1, 108472, 3],"float32"), Tensor([1, 3, 0],"float32"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:76)

2025-04-21 10:07:33.572680 test begin: paddle.Tensor.bmm(Tensor([1, 1156, 3],"float32"), Tensor([1, 3, 0],"float32"), )

W0421 10:07:33.778937  6152 backward.cc:437] While running Node (BmmGradNode) raises an EnforceNotMet exception
[paddle error] paddle.Tensor.bmm(Tensor([1, 1156, 3],"float32"), Tensor([1, 3, 0],"float32"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:76)

2025-04-21 10:07:33.786399 test begin: paddle.Tensor.cholesky_solve(x=Tensor([0, 2, 4, 3],"float64"), y=Tensor([0, 2, 4, 4],"float64"), )

[paddle error] paddle.Tensor.cholesky_solve(x=Tensor([0, 2, 4, 3],"float64"), y=Tensor([0, 2, 4, 4],"float64"), ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:07:36.577897 test begin: paddle.Tensor.cholesky_solve(x=Tensor([0, 2, 4, 3],"float64"), y=Tensor([0, 2, 4, 4],"float64"), upper=True, )

/usr/local/lib/python3.9/dist-packages/torch/utils/_device.py:106: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:135.)
  return func(*args, **kwargs)
[paddle error] paddle.Tensor.cholesky_solve(x=Tensor([0, 2, 4, 3],"float64"), y=Tensor([0, 2, 4, 4],"float64"), upper=True, ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:07:36.789352 test begin: paddle.Tensor.cholesky_solve(x=Tensor([0, 4, 3],"float64"), y=Tensor([0, 4, 4],"float64"), )

[paddle error] paddle.Tensor.cholesky_solve(x=Tensor([0, 4, 3],"float64"), y=Tensor([0, 4, 4],"float64"), ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:07:37.017619 test begin: paddle.Tensor.cholesky_solve(x=Tensor([4, 0],"float32"), y=Tensor([4, 4],"float32"), )

[paddle error] paddle.Tensor.cholesky_solve(x=Tensor([4, 0],"float32"), y=Tensor([4, 4],"float32"), ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:07:37.818055 test begin: paddle.Tensor.cholesky_solve(x=Tensor([4, 0],"float64"), y=Tensor([4, 4],"float64"), )

[paddle error] paddle.Tensor.cholesky_solve(x=Tensor([4, 0],"float64"), y=Tensor([4, 4],"float64"), ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:07:38.103296 test begin: paddle.Tensor.cholesky_solve(x=Tensor([5, 0, 4, 3],"float64"), y=Tensor([5, 0, 4, 4],"float64"), )

[paddle error] paddle.Tensor.cholesky_solve(x=Tensor([5, 0, 4, 3],"float64"), y=Tensor([5, 0, 4, 4],"float64"), ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:07:38.243531 test begin: paddle.Tensor.cholesky_solve(x=Tensor([5, 0, 4, 3],"float64"), y=Tensor([5, 0, 4, 4],"float64"), upper=True, )

[paddle error] paddle.Tensor.cholesky_solve(x=Tensor([5, 0, 4, 3],"float64"), y=Tensor([5, 0, 4, 4],"float64"), upper=True, ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:07:38.459870 test begin: paddle.Tensor.cholesky_solve(x=Tensor([5, 2, 4, 0],"float64"), y=Tensor([5, 2, 4, 4],"float64"), )

[paddle error] paddle.Tensor.cholesky_solve(x=Tensor([5, 2, 4, 0],"float64"), y=Tensor([5, 2, 4, 4],"float64"), ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:07:38.742418 test begin: paddle.Tensor.cholesky_solve(x=Tensor([5, 2, 4, 0],"float64"), y=Tensor([5, 2, 4, 4],"float64"), upper=True, )

[paddle error] paddle.Tensor.cholesky_solve(x=Tensor([5, 2, 4, 0],"float64"), y=Tensor([5, 2, 4, 4],"float64"), upper=True, ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:07:38.966658 test begin: paddle.Tensor.cholesky_solve(x=Tensor([5, 4, 0],"float64"), y=Tensor([5, 4, 4],"float64"), )

[paddle error] paddle.Tensor.cholesky_solve(x=Tensor([5, 4, 0],"float64"), y=Tensor([5, 4, 4],"float64"), ) 
 (PreconditionNotMet) Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:43)

2025-04-21 10:08:34.503196 test begin: paddle.Tensor.cumsum(Tensor([0, 10, 8],"float32"), 1, )

[cuda error] paddle.Tensor.cumsum(Tensor([0, 10, 8],"float32"), 1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:34.709006 test begin: paddle.Tensor.cumsum(Tensor([0, 10, 8],"float32"), 2, )

[cuda error] paddle.Tensor.cumsum(Tensor([0, 10, 8],"float32"), 2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:35.008838 test begin: paddle.Tensor.cumsum(Tensor([0, 100],"int64"), axis=1, )

[cuda error] paddle.Tensor.cumsum(Tensor([0, 100],"int64"), axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:35.214419 test begin: paddle.Tensor.cumsum(Tensor([0, 10],"int64"), axis=1, )

[cuda error] paddle.Tensor.cumsum(Tensor([0, 10],"int64"), axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:35.406921 test begin: paddle.Tensor.cumsum(Tensor([0, 12, 9],"float32"), 1, )

[cuda error] paddle.Tensor.cumsum(Tensor([0, 12, 9],"float32"), 1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:35.607518 test begin: paddle.Tensor.cumsum(Tensor([0, 12, 9],"float32"), 2, )

[cuda error] paddle.Tensor.cumsum(Tensor([0, 12, 9],"float32"), 2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:35.859982 test begin: paddle.Tensor.cumsum(Tensor([0, 14],"int32"), -1, )

[cuda error] paddle.Tensor.cumsum(Tensor([0, 14],"int32"), -1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:36.064563 test begin: paddle.Tensor.cumsum(Tensor([0, 1],"float32"), axis=-1, )

[cuda error] paddle.Tensor.cumsum(Tensor([0, 1],"float32"), axis=-1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:36.270316 test begin: paddle.Tensor.cumsum(Tensor([0, 2],"float32"), axis=-1, )

[cuda error] paddle.Tensor.cumsum(Tensor([0, 2],"float32"), axis=-1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:36.684868 test begin: paddle.Tensor.cumsum(Tensor([0, 4, 2],"int64"), axis=1, )

[cuda error] paddle.Tensor.cumsum(Tensor([0, 4, 2],"int64"), axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:36.870600 test begin: paddle.Tensor.cumsum(Tensor([0, 4, 2],"int64"), axis=2, )

[cuda error] paddle.Tensor.cumsum(Tensor([0, 4, 2],"int64"), axis=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:38.140366 test begin: paddle.Tensor.cumsum(Tensor([1, 0, 8],"float32"), 2, )

[cuda error] paddle.Tensor.cumsum(Tensor([1, 0, 8],"float32"), 2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:38.724407 test begin: paddle.Tensor.cumsum(Tensor([1, 0, 9],"float32"), 2, )

[cuda error] paddle.Tensor.cumsum(Tensor([1, 0, 9],"float32"), 2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:39.181050 test begin: paddle.Tensor.cumsum(Tensor([1, 10, 0],"float32"), 1, )

[cuda error] paddle.Tensor.cumsum(Tensor([1, 10, 0],"float32"), 1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:39.662380 test begin: paddle.Tensor.cumsum(Tensor([1, 12, 0],"float32"), 1, )

[cuda error] paddle.Tensor.cumsum(Tensor([1, 12, 0],"float32"), 1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:40.600003 test begin: paddle.Tensor.cumsum(Tensor([3, 0, 2],"int64"), axis=0, )

[cuda error] paddle.Tensor.cumsum(Tensor([3, 0, 2],"int64"), axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:41.058557 test begin: paddle.Tensor.cumsum(Tensor([3, 0, 2],"int64"), axis=2, )

[cuda error] paddle.Tensor.cumsum(Tensor([3, 0, 2],"int64"), axis=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:41.246111 test begin: paddle.Tensor.cumsum(Tensor([3, 0],"int64"), axis=0, )

[cuda error] paddle.Tensor.cumsum(Tensor([3, 0],"int64"), axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:41.790483 test begin: paddle.Tensor.cumsum(Tensor([3, 4, 0],"int64"), axis=0, )

[cuda error] paddle.Tensor.cumsum(Tensor([3, 4, 0],"int64"), axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:41.982840 test begin: paddle.Tensor.cumsum(Tensor([3, 4, 0],"int64"), axis=1, )

[cuda error] paddle.Tensor.cumsum(Tensor([3, 4, 0],"int64"), axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:42.395367 test begin: paddle.Tensor.cumsum(Tensor([5, 0],"int64"), axis=0, )

[cuda error] paddle.Tensor.cumsum(Tensor([5, 0],"int64"), axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:42.660711 test begin: paddle.Tensor.diag_embed(Tensor([0, 1, 2],"float32"), )

[cuda error] paddle.Tensor.diag_embed(Tensor([0, 1, 2],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:42.849484 test begin: paddle.Tensor.diag_embed(Tensor([1, 0, 2],"float32"), )

[cuda error] paddle.Tensor.diag_embed(Tensor([1, 0, 2],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:43.035712 test begin: paddle.Tensor.diag_embed(Tensor([1, 1, 0],"float32"), )

[cuda error] paddle.Tensor.diag_embed(Tensor([1, 1, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:43.242596 test begin: paddle.Tensor.diagonal(Tensor([0, 2],"float32"), axis1=-2, axis2=-1, )

W0421 10:08:43.451542 29547 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.Tensor.diagonal(Tensor([0, 2],"float32"), axis1=-2, axis2=-1, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:08:43.451958 test begin: paddle.Tensor.diagonal(Tensor([0, 3],"float64"), axis1=-2, axis2=-1, )

W0421 10:08:43.655913 29562 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.Tensor.diagonal(Tensor([0, 3],"float64"), axis1=-2, axis2=-1, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:08:43.656383 test begin: paddle.Tensor.diagonal(Tensor([2, 0],"float32"), axis1=-2, axis2=-1, )

W0421 10:08:43.862679 29583 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.Tensor.diagonal(Tensor([2, 0],"float32"), axis1=-2, axis2=-1, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:08:43.863117 test begin: paddle.Tensor.diagonal(Tensor([3, 0],"float64"), axis1=-2, axis2=-1, )

W0421 10:08:44.068769 29601 backward.cc:437] While running Node (DiagonalGradNode) raises an EnforceNotMet exception
[paddle error] paddle.Tensor.diagonal(Tensor([3, 0],"float64"), axis1=-2, axis2=-1, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1296)

2025-04-21 10:08:52.789700 test begin: paddle.Tensor.expand_as(Tensor([0, 1, 1, 1],"float32"), Tensor([0, 3, 28, 28],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[paddle error] paddle.Tensor.expand_as(Tensor([0, 1, 1, 1],"float32"), Tensor([0, 3, 28, 28],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:08:53.025606 test begin: paddle.Tensor.expand_as(Tensor([0, 1, 1, 1],"float32"), Tensor([0, 3, 280, 350],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[paddle error] paddle.Tensor.expand_as(Tensor([0, 1, 1, 1],"float32"), Tensor([0, 3, 280, 350],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:08:53.242478 test begin: paddle.Tensor.expand_as(Tensor([0, 1, 32],"float32"), Tensor([0, 4, 32],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[paddle error] paddle.Tensor.expand_as(Tensor([0, 1, 32],"float32"), Tensor([0, 4, 32],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:08:53.479449 test begin: paddle.Tensor.expand_as(Tensor([0, 128],"int32"), Tensor([0, 128],"int64"), )

[paddle error] paddle.Tensor.expand_as(Tensor([0, 128],"int32"), Tensor([0, 128],"int64"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:08:53.734293 test begin: paddle.Tensor.expand_as(Tensor([0, 1],"int32"), Tensor([0, 1],"int64"), )

[paddle error] paddle.Tensor.expand_as(Tensor([0, 1],"int32"), Tensor([0, 1],"int64"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:08:53.926534 test begin: paddle.Tensor.expand_as(Tensor([1, 0, 1, 1],"float32"), Tensor([1, 0, 28, 28],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[paddle error] paddle.Tensor.expand_as(Tensor([1, 0, 1, 1],"float32"), Tensor([1, 0, 28, 28],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:08:54.164918 test begin: paddle.Tensor.expand_as(Tensor([1, 0, 1, 1],"float32"), Tensor([1, 0, 280, 350],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[paddle error] paddle.Tensor.expand_as(Tensor([1, 0, 1, 1],"float32"), Tensor([1, 0, 280, 350],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:08:54.388202 test begin: paddle.Tensor.expand_as(Tensor([1, 0],"int32"), Tensor([5, 0],"int64"), )

[paddle error] paddle.Tensor.expand_as(Tensor([1, 0],"int32"), Tensor([5, 0],"int64"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:08:54.595846 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 0, 1],"float32"), Tensor([1, 3, 0, 28],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[paddle error] paddle.Tensor.expand_as(Tensor([1, 1, 0, 1],"float32"), Tensor([1, 3, 0, 28],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:08:54.800622 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 0, 1],"float32"), Tensor([1, 3, 0, 350],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[paddle error] paddle.Tensor.expand_as(Tensor([1, 1, 0, 1],"float32"), Tensor([1, 3, 0, 350],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:08:55.014605 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 0],"float32"), Tensor([1, 3, 28, 0],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[paddle error] paddle.Tensor.expand_as(Tensor([1, 1, 1, 0],"float32"), Tensor([1, 3, 28, 0],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:08:55.205462 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 0],"float32"), Tensor([1, 3, 280, 0],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[paddle error] paddle.Tensor.expand_as(Tensor([1, 1, 1, 0],"float32"), Tensor([1, 3, 280, 0],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:08:55.516119 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([0, 3, 28, 28],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[paddle error] paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([0, 3, 28, 28],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:08:55.777217 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([0, 3, 280, 350],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[paddle error] paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([0, 3, 280, 350],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:08:55.960927 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 0, 28, 28],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[paddle error] paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 0, 28, 28],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:08:56.230640 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 0, 280, 350],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[paddle error] paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 0, 280, 350],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:08:56.462382 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 0, 28],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[paddle error] paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 0, 28],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:08:56.668050 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 0, 350],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[paddle error] paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 0, 350],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:08:56.980229 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 28, 0],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[paddle error] paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 28, 0],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:08:57.162762 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 280, 0],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[paddle error] paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 280, 0],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:08:57.364274 test begin: paddle.Tensor.expand_as(Tensor([1, 128],"int32"), Tensor([0, 128],"int64"), )

[paddle error] paddle.Tensor.expand_as(Tensor([1, 128],"int32"), Tensor([0, 128],"int64"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:08:57.548981 test begin: paddle.Tensor.expand_as(Tensor([1, 1],"int32"), Tensor([0, 1],"int64"), )

[paddle error] paddle.Tensor.expand_as(Tensor([1, 1],"int32"), Tensor([0, 1],"int64"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:08:57.780041 test begin: paddle.Tensor.expand_as(Tensor([1, 1],"int32"), Tensor([5, 0],"int64"), )

[paddle error] paddle.Tensor.expand_as(Tensor([1, 1],"int32"), Tensor([5, 0],"int64"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:08:57.920854 test begin: paddle.Tensor.expand_as(Tensor([2, 0, 32],"float32"), Tensor([2, 0, 32],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[paddle error] paddle.Tensor.expand_as(Tensor([2, 0, 32],"float32"), Tensor([2, 0, 32],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:08:58.165720 test begin: paddle.Tensor.expand_as(Tensor([2, 1, 0],"float32"), Tensor([2, 4, 0],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[paddle error] paddle.Tensor.expand_as(Tensor([2, 1, 0],"float32"), Tensor([2, 4, 0],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:08:58.371996 test begin: paddle.Tensor.expand_as(Tensor([2, 1, 32],"float32"), Tensor([2, 0, 32],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[paddle error] paddle.Tensor.expand_as(Tensor([2, 1, 32],"float32"), Tensor([2, 0, 32],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:46)

2025-04-21 10:09:03.660817 test begin: paddle.Tensor.fill_diagonal_(Tensor([0, 128],"float32"), 0, wrap=False, )

[cuda error] paddle.Tensor.fill_diagonal_(Tensor([0, 128],"float32"), 0, wrap=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:03.870396 test begin: paddle.Tensor.fill_diagonal_(Tensor([128, 0],"float32"), 0, wrap=False, )

[cuda error] paddle.Tensor.fill_diagonal_(Tensor([128, 0],"float32"), 0, wrap=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:41.253476 test begin: paddle.Tensor.flip(Tensor([0, 14],"int32"), list[-1,], )

[cuda error] paddle.Tensor.flip(Tensor([0, 14],"int32"), list[-1,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:41.438741 test begin: paddle.Tensor.flip(Tensor([0, 224, 224],"float32"), axis=list[-1,], )

[cuda error] paddle.Tensor.flip(Tensor([0, 224, 224],"float32"), axis=list[-1,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:41.653526 test begin: paddle.Tensor.flip(Tensor([0, 280, 350],"float32"), axis=list[-1,], )

[cuda error] paddle.Tensor.flip(Tensor([0, 280, 350],"float32"), axis=list[-1,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:41.838772 test begin: paddle.Tensor.flip(Tensor([0, 280, 350],"float32"), axis=list[-2,], )

[cuda error] paddle.Tensor.flip(Tensor([0, 280, 350],"float32"), axis=list[-2,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:42.016479 test begin: paddle.Tensor.flip(Tensor([0, 2],"int64"), list[1,], )

[cuda error] paddle.Tensor.flip(Tensor([0, 2],"int64"), list[1,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:42.202736 test begin: paddle.Tensor.flip(Tensor([0, 400, 300],"float32"), axis=list[-2,], )

[cuda error] paddle.Tensor.flip(Tensor([0, 400, 300],"float32"), axis=list[-2,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:42.470753 test begin: paddle.Tensor.flip(Tensor([13, 0],"int32"), list[-1,], )

[cuda error] paddle.Tensor.flip(Tensor([13, 0],"int32"), list[-1,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:42.666226 test begin: paddle.Tensor.flip(Tensor([3, 0, 224],"float32"), axis=list[-1,], )

[cuda error] paddle.Tensor.flip(Tensor([3, 0, 224],"float32"), axis=list[-1,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:42.869568 test begin: paddle.Tensor.flip(Tensor([3, 0, 300],"float32"), axis=list[-2,], )

[cuda error] paddle.Tensor.flip(Tensor([3, 0, 300],"float32"), axis=list[-2,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:43.049084 test begin: paddle.Tensor.flip(Tensor([3, 0, 350],"float32"), axis=list[-1,], )

[cuda error] paddle.Tensor.flip(Tensor([3, 0, 350],"float32"), axis=list[-1,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:43.231339 test begin: paddle.Tensor.flip(Tensor([3, 0, 350],"float32"), axis=list[-2,], )

[cuda error] paddle.Tensor.flip(Tensor([3, 0, 350],"float32"), axis=list[-2,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:43.440742 test begin: paddle.Tensor.flip(Tensor([3, 224, 0],"float32"), axis=list[-1,], )

[cuda error] paddle.Tensor.flip(Tensor([3, 224, 0],"float32"), axis=list[-1,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:43.636992 test begin: paddle.Tensor.flip(Tensor([3, 280, 0],"float32"), axis=list[-1,], )

[cuda error] paddle.Tensor.flip(Tensor([3, 280, 0],"float32"), axis=list[-1,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:43.830551 test begin: paddle.Tensor.flip(Tensor([3, 280, 0],"float32"), axis=list[-2,], )

[cuda error] paddle.Tensor.flip(Tensor([3, 280, 0],"float32"), axis=list[-2,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:44.012910 test begin: paddle.Tensor.flip(Tensor([3, 400, 0],"float32"), axis=list[-2,], )

[cuda error] paddle.Tensor.flip(Tensor([3, 400, 0],"float32"), axis=list[-2,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:44.218674 test begin: paddle.Tensor.flip(Tensor([4, 0],"int64"), list[1,], )

[cuda error] paddle.Tensor.flip(Tensor([4, 0],"int64"), list[1,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:44.427873 test begin: paddle.Tensor.flip(Tensor([5, 0],"int64"), list[1,], )

[cuda error] paddle.Tensor.flip(Tensor([5, 0],"int64"), list[1,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:50.433854 test begin: paddle.Tensor.inner(x=Tensor([0],"float64"), y=Tensor([0],"float64"), )

[paddle error] paddle.Tensor.inner(x=Tensor([0],"float64"), y=Tensor([0],"float64"), ) 
 (InvalidArgument) can not reshape 0 to -1, 0, because the unspecified dimension 0 can be any number and is ambiguous
  [Hint: Expected unk_dim_idx == -1, but received unk_dim_idx:0 != -1:-1.] (at ../paddle/phi/infermeta/unary.cc:2209)

2025-04-21 10:09:51.220525 test begin: paddle.Tensor.inner(x=Tensor([2, 5, 3, 0],"float64"), y=Tensor([3, 2, 5, 0],"float64"), )

[paddle error] paddle.Tensor.inner(x=Tensor([2, 5, 3, 0],"float64"), y=Tensor([3, 2, 5, 0],"float64"), ) 
 (InvalidArgument) can not reshape 2, 5, 3, 0 to -1, 0, because the unspecified dimension 0 can be any number and is ambiguous
  [Hint: Expected unk_dim_idx == -1, but received unk_dim_idx:0 != -1:-1.] (at ../paddle/phi/infermeta/unary.cc:2209)

2025-04-21 10:09:51.401169 test begin: paddle.Tensor.inner(x=Tensor([2, 5, 3, 4],"float64"), y=Tensor([0, 2, 5, 4],"float64"), )

W0421 10:09:51.587611 53394 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.Tensor.inner(x=Tensor([2, 5, 3, 4],"float64"), y=Tensor([0, 2, 5, 4],"float64"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:51.587918 test begin: paddle.Tensor.inner(x=Tensor([2, 5, 3, 4],"float64"), y=Tensor([3, 0, 5, 4],"float64"), )

W0421 10:09:51.791491 53414 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.Tensor.inner(x=Tensor([2, 5, 3, 4],"float64"), y=Tensor([3, 0, 5, 4],"float64"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:51.791813 test begin: paddle.Tensor.inner(x=Tensor([2, 5, 3, 4],"float64"), y=Tensor([3, 2, 0, 4],"float64"), )

W0421 10:09:52.012923 53509 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.Tensor.inner(x=Tensor([2, 5, 3, 4],"float64"), y=Tensor([3, 2, 0, 4],"float64"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:52.016867 test begin: paddle.Tensor.inner(x=Tensor([3, 0],"float64"), y=Tensor([5, 0],"float64"), )

[paddle error] paddle.Tensor.inner(x=Tensor([3, 0],"float64"), y=Tensor([5, 0],"float64"), ) 
 (InvalidArgument) can not reshape 3, 0 to -1, 0, because the unspecified dimension 0 can be any number and is ambiguous
  [Hint: Expected unk_dim_idx == -1, but received unk_dim_idx:0 != -1:-1.] (at ../paddle/phi/infermeta/unary.cc:2209)

2025-04-21 10:09:52.243381 test begin: paddle.Tensor.inner(x=Tensor([3, 4],"float64"), y=Tensor([0, 2, 4],"float64"), )

W0421 10:09:52.435729 53824 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.Tensor.inner(x=Tensor([3, 4],"float64"), y=Tensor([0, 2, 4],"float64"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:52.436055 test begin: paddle.Tensor.inner(x=Tensor([3, 4],"float64"), y=Tensor([0, 2, 5, 4],"float64"), )

W0421 10:09:52.618294 53901 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.Tensor.inner(x=Tensor([3, 4],"float64"), y=Tensor([0, 2, 5, 4],"float64"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:52.618626 test begin: paddle.Tensor.inner(x=Tensor([3, 4],"float64"), y=Tensor([0, 4],"float64"), )

W0421 10:09:52.799505 53917 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.Tensor.inner(x=Tensor([3, 4],"float64"), y=Tensor([0, 4],"float64"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:52.799908 test begin: paddle.Tensor.inner(x=Tensor([3, 4],"float64"), y=Tensor([3, 0, 4],"float64"), )

W0421 10:09:52.986912 53938 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.Tensor.inner(x=Tensor([3, 4],"float64"), y=Tensor([3, 0, 4],"float64"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:52.987246 test begin: paddle.Tensor.inner(x=Tensor([3, 4],"float64"), y=Tensor([3, 0, 5, 4],"float64"), )

W0421 10:09:53.177309 53951 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.Tensor.inner(x=Tensor([3, 4],"float64"), y=Tensor([3, 0, 5, 4],"float64"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:53.177654 test begin: paddle.Tensor.inner(x=Tensor([3, 4],"float64"), y=Tensor([3, 2, 0, 4],"float64"), )

W0421 10:09:53.366027 54037 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.Tensor.inner(x=Tensor([3, 4],"float64"), y=Tensor([3, 2, 0, 4],"float64"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:53.379529 test begin: paddle.Tensor.inner(x=Tensor([4, 0],"float32"), y=Tensor([4, 0],"float32"), )

[paddle error] paddle.Tensor.inner(x=Tensor([4, 0],"float32"), y=Tensor([4, 0],"float32"), ) 
 (InvalidArgument) can not reshape 4, 0 to -1, 0, because the unspecified dimension 0 can be any number and is ambiguous
  [Hint: Expected unk_dim_idx == -1, but received unk_dim_idx:0 != -1:-1.] (at ../paddle/phi/infermeta/unary.cc:2209)

2025-04-21 10:09:53.520230 test begin: paddle.Tensor.inner(x=Tensor([4, 4],"float32"), y=Tensor([0, 4],"float32"), )

W0421 10:09:53.675951 54210 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.Tensor.inner(x=Tensor([4, 4],"float32"), y=Tensor([0, 4],"float32"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)

2025-04-21 10:09:54.138666 test begin: paddle.Tensor.inner(x=Tensor([5, 3, 0],"float64"), y=Tensor([2, 5, 0],"float64"), )

[paddle error] paddle.Tensor.inner(x=Tensor([5, 3, 0],"float64"), y=Tensor([2, 5, 0],"float64"), ) 
 (InvalidArgument) can not reshape 5, 3, 0 to -1, 0, because the unspecified dimension 0 can be any number and is ambiguous
  [Hint: Expected unk_dim_idx == -1, but received unk_dim_idx:0 != -1:-1.] (at ../paddle/phi/infermeta/unary.cc:2209)

2025-04-21 10:09:54.313700 test begin: paddle.Tensor.inner(x=Tensor([5, 3, 4],"float64"), y=Tensor([0, 5, 4],"float64"), )

W0421 10:09:54.440398 54618 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.Tensor.inner(x=Tensor([5, 3, 4],"float64"), y=Tensor([0, 5, 4],"float64"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:54.440719 test begin: paddle.Tensor.inner(x=Tensor([5, 3, 4],"float64"), y=Tensor([2, 0, 4],"float64"), )

W0421 10:09:54.666966 54628 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.Tensor.inner(x=Tensor([5, 3, 4],"float64"), y=Tensor([2, 0, 4],"float64"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:54.669232 test begin: paddle.Tensor.isclose(x=Tensor([0, 4, 5],"float64"), y=Tensor([0, 4, 5],"float64"), )

[cuda error] paddle.Tensor.isclose(x=Tensor([0, 4, 5],"float64"), y=Tensor([0, 4, 5],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:54.879134 test begin: paddle.Tensor.isclose(x=Tensor([0],"float32"), y=Tensor([0],"float32"), )

[cuda error] paddle.Tensor.isclose(x=Tensor([0],"float32"), y=Tensor([0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:55.022839 test begin: paddle.Tensor.isclose(x=Tensor([0],"float64"), y=Tensor([0],"float64"), )

[cuda error] paddle.Tensor.isclose(x=Tensor([0],"float64"), y=Tensor([0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:55.205466 test begin: paddle.Tensor.isclose(x=Tensor([3, 0, 5],"float64"), y=Tensor([3, 0, 5],"float64"), )

[cuda error] paddle.Tensor.isclose(x=Tensor([3, 0, 5],"float64"), y=Tensor([3, 0, 5],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:55.412022 test begin: paddle.Tensor.isclose(x=Tensor([3, 4, 0],"float64"), y=Tensor([3, 4, 0],"float64"), )

[cuda error] paddle.Tensor.isclose(x=Tensor([3, 4, 0],"float64"), y=Tensor([3, 4, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:55.624389 test begin: paddle.Tensor.kthvalue(Tensor([0, 200, 10],"float32"), k=200, axis=1, )

element 1 of tensors does not require grad and does not have a grad_fn
[cuda error] paddle.Tensor.kthvalue(Tensor([0, 200, 10],"float32"), k=200, axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:09:55.767308 test begin: paddle.Tensor.kthvalue(Tensor([2, 200, 0],"float32"), k=200, axis=1, )

 ** On entry to SgemmStridedBatched parameter number 8 had an illegal value
 ** On entry to SgemmStridedBatched parameter number 8 had an illegal value
 ** On entry to DGEMM  parameter number 8 had an illegal value
 ** On entry to DGEMM  parameter number 8 had an illegal value
 ** On entry to DGEMM  parameter number 8 had an illegal value
 ** On entry to DGEMM  parameter number 8 had an illegal value
 ** On entry to DGEMM  parameter number 8 had an illegal value
 ** On entry to DGEMM  parameter number 8 had an illegal value
 ** On entry to DGEMM  parameter number 8 had an illegal value
 ** On entry to DGEMM  parameter number 8 had an illegal value
 ** On entry to DGEMM  parameter number 8 had an illegal value
 ** On entry to SGEMM  parameter number 8 had an illegal value
 ** On entry to DGEMM  parameter number 8 had an illegal value
 ** On entry to DGEMM  parameter number 8 had an illegal value


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_kthvalue(_object*, _object*, _object*)
1   kthvalue_ad_func(paddle::Tensor const&, int, int, bool)
2   paddle::experimental::kthvalue(paddle::Tensor const&, int, int, bool)
3   void phi::KthvalueKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, int, bool, phi::DenseTensor*, phi::DenseTensor*)
4   phi::funcs::Transpose<phi::GPUContext, float, 3>::operator()(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, std::vector<int, std::allocator<int> > const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201395 (unix time) try "date -d @1745201395" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f8aa6f1d6ca) received by PID 160052 (TID 0x7f88cd8a4700) from PID 18446744072215451338 ***]

2025-04-21 10:10:00.174218 test begin: paddle.Tensor.lerp(x=Tensor([0, 5, 4, 3],"float64"), y=Tensor([0, 5, 4, 3],"float64"), weight=0.0, )

W0421 10:10:06.539477 56340 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:10:06.541488 56340 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.Tensor.lerp(x=Tensor([0, 5, 4, 3],"float64"), y=Tensor([0, 5, 4, 3],"float64"), weight=0.0, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:06.544860 test begin: paddle.Tensor.lerp(x=Tensor([0, 5, 4, 3],"float64"), y=Tensor([0, 5, 4, 3],"float64"), weight=0.5, )

[paddle error] paddle.Tensor.lerp(x=Tensor([0, 5, 4, 3],"float64"), y=Tensor([0, 5, 4, 3],"float64"), weight=0.5, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:06.792152 test begin: paddle.Tensor.lerp(x=Tensor([0, 5, 4, 3],"float64"), y=Tensor([0, 5, 4, 3],"float64"), weight=1.0, )

[paddle error] paddle.Tensor.lerp(x=Tensor([0, 5, 4, 3],"float64"), y=Tensor([0, 5, 4, 3],"float64"), weight=1.0, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:07.010102 test begin: paddle.Tensor.lerp(x=Tensor([0, 5, 4],"float64"), y=Tensor([0, 5, 4],"float64"), weight=0.5, )

[paddle error] paddle.Tensor.lerp(x=Tensor([0, 5, 4],"float64"), y=Tensor([0, 5, 4],"float64"), weight=0.5, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:07.267914 test begin: paddle.Tensor.lerp(x=Tensor([0, 5],"float64"), y=Tensor([0, 5],"float64"), weight=0.5, )

[paddle error] paddle.Tensor.lerp(x=Tensor([0, 5],"float64"), y=Tensor([0, 5],"float64"), weight=0.5, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:07.491563 test begin: paddle.Tensor.lerp(x=Tensor([0, 5],"float64"), y=Tensor([1],"float64"), weight=0.2, )

[paddle error] paddle.Tensor.lerp(x=Tensor([0, 5],"float64"), y=Tensor([1],"float64"), weight=0.2, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:07.714633 test begin: paddle.Tensor.lerp(x=Tensor([0],"float32"), y=Tensor([0],"float32"), weight=0.5, )

[paddle error] paddle.Tensor.lerp(x=Tensor([0],"float32"), y=Tensor([0],"float32"), weight=0.5, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:07.866131 test begin: paddle.Tensor.lerp(x=Tensor([0],"float64"), y=Tensor([0],"float64"), weight=0.5, )

[paddle error] paddle.Tensor.lerp(x=Tensor([0],"float64"), y=Tensor([0],"float64"), weight=0.5, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:07.989042 test begin: paddle.Tensor.lerp(x=Tensor([4, 0, 4, 3],"float64"), y=Tensor([4, 0, 4, 3],"float64"), weight=0.0, )

[paddle error] paddle.Tensor.lerp(x=Tensor([4, 0, 4, 3],"float64"), y=Tensor([4, 0, 4, 3],"float64"), weight=0.0, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:08.188052 test begin: paddle.Tensor.lerp(x=Tensor([4, 0, 4, 3],"float64"), y=Tensor([4, 0, 4, 3],"float64"), weight=0.5, )

[paddle error] paddle.Tensor.lerp(x=Tensor([4, 0, 4, 3],"float64"), y=Tensor([4, 0, 4, 3],"float64"), weight=0.5, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:08.406515 test begin: paddle.Tensor.lerp(x=Tensor([4, 0, 4, 3],"float64"), y=Tensor([4, 0, 4, 3],"float64"), weight=1.0, )

[paddle error] paddle.Tensor.lerp(x=Tensor([4, 0, 4, 3],"float64"), y=Tensor([4, 0, 4, 3],"float64"), weight=1.0, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:08.642607 test begin: paddle.Tensor.lerp(x=Tensor([4, 0, 4],"float64"), y=Tensor([4, 0, 4],"float64"), weight=0.5, )

[paddle error] paddle.Tensor.lerp(x=Tensor([4, 0, 4],"float64"), y=Tensor([4, 0, 4],"float64"), weight=0.5, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:08.789226 test begin: paddle.Tensor.lerp(x=Tensor([4, 0],"float64"), y=Tensor([1],"float64"), weight=0.2, )

[paddle error] paddle.Tensor.lerp(x=Tensor([4, 0],"float64"), y=Tensor([1],"float64"), weight=0.2, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:08.991063 test begin: paddle.Tensor.lerp(x=Tensor([4, 0],"float64"), y=Tensor([4, 0],"float64"), weight=0.5, )

[paddle error] paddle.Tensor.lerp(x=Tensor([4, 0],"float64"), y=Tensor([4, 0],"float64"), weight=0.5, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:09.218914 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 0, 3],"float64"), y=Tensor([4, 5, 0, 3],"float64"), weight=0.0, )

[paddle error] paddle.Tensor.lerp(x=Tensor([4, 5, 0, 3],"float64"), y=Tensor([4, 5, 0, 3],"float64"), weight=0.0, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:09.407798 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 0, 3],"float64"), y=Tensor([4, 5, 0, 3],"float64"), weight=0.5, )

[paddle error] paddle.Tensor.lerp(x=Tensor([4, 5, 0, 3],"float64"), y=Tensor([4, 5, 0, 3],"float64"), weight=0.5, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:09.615055 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 0, 3],"float64"), y=Tensor([4, 5, 0, 3],"float64"), weight=1.0, )

[paddle error] paddle.Tensor.lerp(x=Tensor([4, 5, 0, 3],"float64"), y=Tensor([4, 5, 0, 3],"float64"), weight=1.0, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:09.880529 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 0],"float64"), y=Tensor([4, 5, 0],"float64"), weight=0.5, )

[paddle error] paddle.Tensor.lerp(x=Tensor([4, 5, 0],"float64"), y=Tensor([4, 5, 0],"float64"), weight=0.5, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:10.093753 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 4, 0],"float64"), y=Tensor([4, 5, 4, 0],"float64"), weight=0.0, )

[paddle error] paddle.Tensor.lerp(x=Tensor([4, 5, 4, 0],"float64"), y=Tensor([4, 5, 4, 0],"float64"), weight=0.0, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:10.228823 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 4, 0],"float64"), y=Tensor([4, 5, 4, 0],"float64"), weight=0.5, )

[paddle error] paddle.Tensor.lerp(x=Tensor([4, 5, 4, 0],"float64"), y=Tensor([4, 5, 4, 0],"float64"), weight=0.5, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:10.453723 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 4, 0],"float64"), y=Tensor([4, 5, 4, 0],"float64"), weight=1.0, )

[paddle error] paddle.Tensor.lerp(x=Tensor([4, 5, 4, 0],"float64"), y=Tensor([4, 5, 4, 0],"float64"), weight=1.0, ) 
 (InvalidArgument) LerpKernel's input x must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/lerp_kernel.cu:50)

2025-04-21 10:10:14.635458 test begin: paddle.Tensor.lu(Tensor([0, 3, 2, 2],"float64"), )

/usr/local/lib/python3.9/dist-packages/torch/_tensor.py:857: UserWarning: torch.lu is deprecated in favor of torch.linalg.lu_factor / torch.linalg.lu_factor_ex and will be removed in a future PyTorch release.
LU, pivots = torch.lu(A, compute_pivots)
should be replaced with
LU, pivots = torch.linalg.lu_factor(A, compute_pivots)
and
LU, pivots, info = torch.lu(A, compute_pivots, get_infos=True)
should be replaced with
LU, pivots, info = torch.linalg.lu_factor_ex(A, compute_pivots) (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:1990.)
  LU, pivots, infos = torch._lu_with_info(
element 1 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.lu(Tensor([0, 3, 2, 2],"float64"), ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < x_dim[i], but received 0:0 >= x_dim[i]:0.] (at ../paddle/phi/kernels/impl/lu_kernel_impl.h:532)

2025-04-21 10:10:14.827196 test begin: paddle.Tensor.lu(Tensor([0, 3, 3],"float64"), )

element 1 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.lu(Tensor([0, 3, 3],"float64"), ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < x_dim[i], but received 0:0 >= x_dim[i]:0.] (at ../paddle/phi/kernels/impl/lu_kernel_impl.h:532)

2025-04-21 10:10:15.006084 test begin: paddle.Tensor.lu(Tensor([0, 3],"float32"), )

element 1 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.lu(Tensor([0, 3],"float32"), ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < x_dim[i], but received 0:0 >= x_dim[i]:0.] (at ../paddle/phi/kernels/impl/lu_kernel_impl.h:532)

2025-04-21 10:10:15.228407 test begin: paddle.Tensor.lu(Tensor([0, 3],"float64"), )

element 1 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.lu(Tensor([0, 3],"float64"), ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < x_dim[i], but received 0:0 >= x_dim[i]:0.] (at ../paddle/phi/kernels/impl/lu_kernel_impl.h:532)

2025-04-21 10:10:15.443688 test begin: paddle.Tensor.lu(Tensor([3, 0, 3],"float64"), )

element 1 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.lu(Tensor([3, 0, 3],"float64"), ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < x_dim[i], but received 0:0 >= x_dim[i]:0.] (at ../paddle/phi/kernels/impl/lu_kernel_impl.h:532)

2025-04-21 10:10:15.645994 test begin: paddle.Tensor.lu(Tensor([3, 0],"float32"), )

element 1 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.lu(Tensor([3, 0],"float32"), ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < x_dim[i], but received 0:0 >= x_dim[i]:0.] (at ../paddle/phi/kernels/impl/lu_kernel_impl.h:532)

2025-04-21 10:10:15.840805 test begin: paddle.Tensor.lu(Tensor([3, 0],"float64"), )

element 1 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.lu(Tensor([3, 0],"float64"), ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < x_dim[i], but received 0:0 >= x_dim[i]:0.] (at ../paddle/phi/kernels/impl/lu_kernel_impl.h:532)

2025-04-21 10:10:16.033258 test begin: paddle.Tensor.lu(Tensor([3, 3, 0],"float64"), )

element 1 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.lu(Tensor([3, 3, 0],"float64"), ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < x_dim[i], but received 0:0 >= x_dim[i]:0.] (at ../paddle/phi/kernels/impl/lu_kernel_impl.h:532)

2025-04-21 10:10:16.259734 test begin: paddle.Tensor.lu(Tensor([4, 0, 2, 2],"float64"), )

element 1 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.lu(Tensor([4, 0, 2, 2],"float64"), ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < x_dim[i], but received 0:0 >= x_dim[i]:0.] (at ../paddle/phi/kernels/impl/lu_kernel_impl.h:532)

2025-04-21 10:10:16.465748 test begin: paddle.Tensor.lu(Tensor([4, 3, 0, 2],"float64"), )

element 1 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.lu(Tensor([4, 3, 0, 2],"float64"), ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < x_dim[i], but received 0:0 >= x_dim[i]:0.] (at ../paddle/phi/kernels/impl/lu_kernel_impl.h:532)

2025-04-21 10:10:16.672406 test begin: paddle.Tensor.lu(Tensor([4, 3, 2, 0],"float64"), )

element 1 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.lu(Tensor([4, 3, 2, 0],"float64"), ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < x_dim[i], but received 0:0 >= x_dim[i]:0.] (at ../paddle/phi/kernels/impl/lu_kernel_impl.h:532)

2025-04-21 10:10:31.881660 test begin: paddle.Tensor.matmul(Tensor([0, 100, 1],"float64"), Tensor([0, 1, 40],"float64"), )

[accuracy error] paddle.Tensor.matmul(Tensor([0, 100, 1],"float64"), Tensor([0, 1, 40],"float64"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (0, 100, 1, 40), (0, 100, 40) mismatch)
 x: array([], shape=(0, 100, 1, 40), dtype=float64)
 y: array([], shape=(0, 100, 40), dtype=float64)
2025-04-21 10:10:32.128276 test begin: paddle.Tensor.matmul(Tensor([0, 100, 1],"float64"), Tensor([0, 1, 4],"float64"), )

[accuracy error] paddle.Tensor.matmul(Tensor([0, 100, 1],"float64"), Tensor([0, 1, 4],"float64"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (0, 100, 1, 4), (0, 100, 4) mismatch)
 x: array([], shape=(0, 100, 1, 4), dtype=float64)
 y: array([], shape=(0, 100, 4), dtype=float64)
2025-04-21 10:10:32.355074 test begin: paddle.Tensor.matmul(Tensor([0, 100, 1],"float64"), Tensor([1, 1, 40],"float64"), )

[accuracy error] paddle.Tensor.matmul(Tensor([0, 100, 1],"float64"), Tensor([1, 1, 40],"float64"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (0, 100, 1, 40), (0, 100, 40) mismatch)
 x: array([], shape=(0, 100, 1, 40), dtype=float64)
 y: array([], shape=(0, 100, 40), dtype=float64)
2025-04-21 10:10:32.583819 test begin: paddle.Tensor.matmul(Tensor([0, 100, 1],"float64"), Tensor([1, 1, 4],"float64"), )

[accuracy error] paddle.Tensor.matmul(Tensor([0, 100, 1],"float64"), Tensor([1, 1, 4],"float64"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (0, 100, 1, 4), (0, 100, 4) mismatch)
 x: array([], shape=(0, 100, 1, 4), dtype=float64)
 y: array([], shape=(0, 100, 4), dtype=float64)
2025-04-21 10:10:32.794352 test begin: paddle.Tensor.matmul(Tensor([0, 12, 197, 197],"float16"), Tensor([0, 12, 197, 64],"float16"), )

[accuracy error] paddle.Tensor.matmul(Tensor([0, 12, 197, 197],"float16"), Tensor([0, 12, 197, 64],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (0, 12, 197, 12, 197, 64), (0, 12, 197, 64) mismatch)
 x: array([], shape=(0, 12, 197, 12, 197, 64), dtype=float16)
 y: array([], shape=(0, 12, 197, 64), dtype=float16)
2025-04-21 10:10:33.322002 test begin: paddle.Tensor.matmul(Tensor([0, 12, 197, 197],"float32"), Tensor([0, 12, 197, 64],"float32"), )

[accuracy error] paddle.Tensor.matmul(Tensor([0, 12, 197, 197],"float32"), Tensor([0, 12, 197, 64],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (0, 12, 197, 12, 197, 64), (0, 12, 197, 64) mismatch)
 x: array([], shape=(0, 12, 197, 12, 197, 64), dtype=float32)
 y: array([], shape=(0, 12, 197, 64), dtype=float32)
2025-04-21 10:10:35.970377 test begin: paddle.Tensor.matmul(Tensor([1, 0, 1],"float64"), Tensor([1, 1, 40],"float64"), )

[accuracy error] paddle.Tensor.matmul(Tensor([1, 0, 1],"float64"), Tensor([1, 1, 40],"float64"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (1, 0, 1, 40), (1, 0, 40) mismatch)
 x: array([], shape=(1, 0, 1, 40), dtype=float64)
 y: array([], shape=(1, 0, 40), dtype=float64)
2025-04-21 10:10:36.224545 test begin: paddle.Tensor.matmul(Tensor([1, 0, 1],"float64"), Tensor([1, 1, 4],"float64"), )

[accuracy error] paddle.Tensor.matmul(Tensor([1, 0, 1],"float64"), Tensor([1, 1, 4],"float64"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (1, 0, 1, 4), (1, 0, 4) mismatch)
 x: array([], shape=(1, 0, 1, 4), dtype=float64)
 y: array([], shape=(1, 0, 4), dtype=float64)
2025-04-21 10:10:36.374033 test begin: paddle.Tensor.matmul(Tensor([1, 100, 1],"float64"), Tensor([0, 1, 40],"float64"), )

[accuracy error] paddle.Tensor.matmul(Tensor([1, 100, 1],"float64"), Tensor([0, 1, 40],"float64"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (1, 100, 1, 40), (0, 100, 40) mismatch)
 x: array([[[[0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.]],...
 y: array([], shape=(0, 100, 40), dtype=float64)
2025-04-21 10:10:36.574919 test begin: paddle.Tensor.matmul(Tensor([1, 100, 1],"float64"), Tensor([0, 1, 4],"float64"), )

[accuracy error] paddle.Tensor.matmul(Tensor([1, 100, 1],"float64"), Tensor([0, 1, 4],"float64"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (1, 100, 1, 4), (0, 100, 4) mismatch)
 x: array([[[[0., 0., 0., 0.]],

        [[0., 0., 0., 0.]],...
 y: array([], shape=(0, 100, 4), dtype=float64)
2025-04-21 10:10:36.800074 test begin: paddle.Tensor.matmul(Tensor([1, 100, 1],"float64"), Tensor([1, 1, 0],"float64"), )

[accuracy error] paddle.Tensor.matmul(Tensor([1, 100, 1],"float64"), Tensor([1, 1, 0],"float64"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (1, 100, 1, 0), (1, 100, 0) mismatch)
 x: array([], shape=(1, 100, 1, 0), dtype=float64)
 y: array([], shape=(1, 100, 0), dtype=float64)
2025-04-21 10:10:37.012718 test begin: paddle.Tensor.matmul(Tensor([1, 1],"float64"), Tensor([1, 0],"float64"), )

W0421 10:10:38.458518 70346 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.Tensor.matmul(Tensor([1, 1],"float64"), Tensor([1, 0],"float64"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:38.744252 test begin: paddle.Tensor.matmul(Tensor([112, 0, 197, 197],"float16"), Tensor([112, 0, 197, 64],"float16"), )

[accuracy error] paddle.Tensor.matmul(Tensor([112, 0, 197, 197],"float16"), Tensor([112, 0, 197, 64],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (112, 0, 197, 0, 197, 64), (112, 0, 197, 64) mismatch)
 x: array([], shape=(112, 0, 197, 0, 197, 64), dtype=float16)
 y: array([], shape=(112, 0, 197, 64), dtype=float16)
2025-04-21 10:10:39.247139 test begin: paddle.Tensor.matmul(Tensor([112, 0, 197, 197],"float32"), Tensor([112, 0, 197, 64],"float32"), )

[accuracy error] paddle.Tensor.matmul(Tensor([112, 0, 197, 197],"float32"), Tensor([112, 0, 197, 64],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (112, 0, 197, 0, 197, 64), (112, 0, 197, 64) mismatch)
 x: array([], shape=(112, 0, 197, 0, 197, 64), dtype=float32)
 y: array([], shape=(112, 0, 197, 64), dtype=float32)
2025-04-21 10:10:39.863940 test begin: paddle.Tensor.matmul(Tensor([112, 12, 0, 197],"float16"), Tensor([112, 12, 197, 64],"float16"), )

[accuracy error] paddle.Tensor.matmul(Tensor([112, 12, 0, 197],"float16"), Tensor([112, 12, 197, 64],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (112, 12, 0, 12, 197, 64), (112, 12, 0, 64) mismatch)
 x: array([], shape=(112, 12, 0, 12, 197, 64), dtype=float16)
 y: array([], shape=(112, 12, 0, 64), dtype=float16)
2025-04-21 10:10:40.500084 test begin: paddle.Tensor.matmul(Tensor([112, 12, 0, 197],"float32"), Tensor([112, 12, 197, 64],"float32"), )

[accuracy error] paddle.Tensor.matmul(Tensor([112, 12, 0, 197],"float32"), Tensor([112, 12, 197, 64],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (112, 12, 0, 12, 197, 64), (112, 12, 0, 64) mismatch)
 x: array([], shape=(112, 12, 0, 12, 197, 64), dtype=float32)
 y: array([], shape=(112, 12, 0, 64), dtype=float32)
2025-04-21 10:10:44.789235 test begin: paddle.Tensor.matmul(Tensor([112, 12, 197, 197],"float16"), Tensor([112, 12, 197, 0],"float16"), )

[accuracy error] paddle.Tensor.matmul(Tensor([112, 12, 197, 197],"float16"), Tensor([112, 12, 197, 0],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (112, 12, 197, 12, 197, 0), (112, 12, 197, 0) mismatch)
 x: array([], shape=(112, 12, 197, 12, 197, 0), dtype=float16)
 y: array([], shape=(112, 12, 197, 0), dtype=float16)
2025-04-21 10:10:48.877948 test begin: paddle.Tensor.matmul(Tensor([112, 12, 197, 197],"float32"), Tensor([112, 12, 197, 0],"float32"), )

[accuracy error] paddle.Tensor.matmul(Tensor([112, 12, 197, 197],"float32"), Tensor([112, 12, 197, 0],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

(shapes (112, 12, 197, 12, 197, 0), (112, 12, 197, 0) mismatch)
 x: array([], shape=(112, 12, 197, 12, 197, 0), dtype=float32)
 y: array([], shape=(112, 12, 197, 0), dtype=float32)
2025-04-21 10:11:26.299739 test begin: paddle.Tensor.median(Tensor([0, 784],"float32"), )

 ** On entry to DGEMM  parameter number 8 had an illegal value
[paddle error] paddle.Tensor.median(Tensor([0, 784],"float32"), ) 
 In median, the size of input x should not be 0.
2025-04-21 10:11:26.532828 test begin: paddle.Tensor.median(Tensor([1000, 0],"float32"), )

[paddle error] paddle.Tensor.median(Tensor([1000, 0],"float32"), ) 
 In median, the size of input x should not be 0.
2025-04-21 10:11:26.944110 test begin: paddle.Tensor.mm(Tensor([10, 10],"float32"), Tensor([10, 0],"float32"), )

 ** On entry to SGEMM  parameter number 8 had an illegal value
W0421 10:11:27.185664 88585 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.Tensor.mm(Tensor([10, 10],"float32"), Tensor([10, 0],"float32"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)

2025-04-21 10:11:27.185976 test begin: paddle.Tensor.mode(Tensor([0, 2, 3],"float64"), )

element 1 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.mode(Tensor([0, 2, 3],"float64"), ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < in_dims[i], but received 0:0 >= in_dims[i]:0.] (at ../paddle/phi/kernels/gpu/mode_kernel.cu:36)

2025-04-21 10:11:27.323219 test begin: paddle.Tensor.mode(Tensor([0, 2, 3],"float64"), axis=1, keepdim=False, )

element 1 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.mode(Tensor([0, 2, 3],"float64"), axis=1, keepdim=False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < in_dims[i], but received 0:0 >= in_dims[i]:0.] (at ../paddle/phi/kernels/gpu/mode_kernel.cu:36)

2025-04-21 10:11:27.442712 test begin: paddle.Tensor.mode(Tensor([0, 2, 3],"float64"), axis=2, keepdim=True, )

element 1 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.mode(Tensor([0, 2, 3],"float64"), axis=2, keepdim=True, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < in_dims[i], but received 0:0 >= in_dims[i]:0.] (at ../paddle/phi/kernels/gpu/mode_kernel.cu:36)

2025-04-21 10:11:27.621976 test begin: paddle.Tensor.mode(Tensor([3, 0, 3],"float64"), )

element 1 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.mode(Tensor([3, 0, 3],"float64"), ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < in_dims[i], but received 0:0 >= in_dims[i]:0.] (at ../paddle/phi/kernels/gpu/mode_kernel.cu:36)

2025-04-21 10:11:27.863123 test begin: paddle.Tensor.mode(Tensor([3, 0, 3],"float64"), axis=2, keepdim=True, )

element 1 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.mode(Tensor([3, 0, 3],"float64"), axis=2, keepdim=True, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < in_dims[i], but received 0:0 >= in_dims[i]:0.] (at ../paddle/phi/kernels/gpu/mode_kernel.cu:36)

2025-04-21 10:11:28.067013 test begin: paddle.Tensor.mode(Tensor([3, 2, 0],"float64"), axis=1, keepdim=False, )

element 1 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.mode(Tensor([3, 2, 0],"float64"), axis=1, keepdim=False, ) 
 (InvalidArgument) The dims of Input(X) should be greater than 0.
  [Hint: Expected 0 < in_dims[i], but received 0:0 >= in_dims[i]:0.] (at ../paddle/phi/kernels/gpu/mode_kernel.cu:36)

2025-04-21 10:11:33.080295 test begin: paddle.Tensor.multigammaln(Tensor([0],"float32"), 3, )

[cuda error] paddle.Tensor.multigammaln(Tensor([0],"float32"), 3, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:33.247932 test begin: paddle.Tensor.nansum(Tensor([0, 2, 3, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, )

[cuda error] paddle.Tensor.nansum(Tensor([0, 2, 3, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:33.394145 test begin: paddle.Tensor.nansum(Tensor([0, 3, 3],"float32"), )

[cuda error] paddle.Tensor.nansum(Tensor([0, 3, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:33.525599 test begin: paddle.Tensor.nansum(Tensor([0, 3, 3],"float64"), )

[cuda error] paddle.Tensor.nansum(Tensor([0, 3, 3],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:33.705750 test begin: paddle.Tensor.nansum(Tensor([0, 3, 3],"float64"), axis=-1, )

[cuda error] paddle.Tensor.nansum(Tensor([0, 3, 3],"float64"), axis=-1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:33.837568 test begin: paddle.Tensor.nansum(Tensor([0, 3, 3],"float64"), axis=0, keepdim=True, )

[cuda error] paddle.Tensor.nansum(Tensor([0, 3, 3],"float64"), axis=0, keepdim=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:34.059273 test begin: paddle.Tensor.nansum(Tensor([3, 0, 3, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, )

[cuda error] paddle.Tensor.nansum(Tensor([3, 0, 3, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:34.250592 test begin: paddle.Tensor.nansum(Tensor([3, 0, 3],"float32"), )

[cuda error] paddle.Tensor.nansum(Tensor([3, 0, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:34.518139 test begin: paddle.Tensor.nansum(Tensor([3, 0, 3],"float64"), )

[cuda error] paddle.Tensor.nansum(Tensor([3, 0, 3],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:34.761029 test begin: paddle.Tensor.nansum(Tensor([3, 0, 3],"float64"), axis=-1, )

[cuda error] paddle.Tensor.nansum(Tensor([3, 0, 3],"float64"), axis=-1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:34.950136 test begin: paddle.Tensor.nansum(Tensor([3, 0, 3],"float64"), axis=0, keepdim=True, )

[cuda error] paddle.Tensor.nansum(Tensor([3, 0, 3],"float64"), axis=0, keepdim=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:35.156863 test begin: paddle.Tensor.nansum(Tensor([3, 2, 0, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, )

[cuda error] paddle.Tensor.nansum(Tensor([3, 2, 0, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:35.346458 test begin: paddle.Tensor.nansum(Tensor([3, 2, 3, 0, 5, 1, 2],"float64"), axis=3, keepdim=True, )

[cuda error] paddle.Tensor.nansum(Tensor([3, 2, 3, 0, 5, 1, 2],"float64"), axis=3, keepdim=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:35.520922 test begin: paddle.Tensor.nansum(Tensor([3, 2, 3, 4, 0, 1, 2],"float64"), axis=3, keepdim=True, )

[cuda error] paddle.Tensor.nansum(Tensor([3, 2, 3, 4, 0, 1, 2],"float64"), axis=3, keepdim=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:35.697413 test begin: paddle.Tensor.nansum(Tensor([3, 2, 3, 4, 5, 0, 2],"float64"), axis=3, keepdim=True, )

[cuda error] paddle.Tensor.nansum(Tensor([3, 2, 3, 4, 5, 0, 2],"float64"), axis=3, keepdim=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:35.953339 test begin: paddle.Tensor.nansum(Tensor([3, 2, 3, 4, 5, 1, 0],"float64"), axis=3, keepdim=True, )

[cuda error] paddle.Tensor.nansum(Tensor([3, 2, 3, 4, 5, 1, 0],"float64"), axis=3, keepdim=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:36.143472 test begin: paddle.Tensor.nansum(Tensor([3, 3, 0],"float32"), )

[cuda error] paddle.Tensor.nansum(Tensor([3, 3, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:36.351362 test begin: paddle.Tensor.nansum(Tensor([3, 3, 0],"float64"), )

[cuda error] paddle.Tensor.nansum(Tensor([3, 3, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:36.544316 test begin: paddle.Tensor.nansum(Tensor([3, 3, 0],"float64"), axis=-1, )

[cuda error] paddle.Tensor.nansum(Tensor([3, 3, 0],"float64"), axis=-1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:36.741659 test begin: paddle.Tensor.nansum(Tensor([3, 3, 0],"float64"), axis=0, keepdim=True, )

[cuda error] paddle.Tensor.nansum(Tensor([3, 3, 0],"float64"), axis=0, keepdim=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:37.438702 test begin: paddle.Tensor.outer(x=Tensor([4],"float64"), y=Tensor([0],"float64"), )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:11:37.616971 92181 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.Tensor.outer(x=Tensor([4],"float64"), y=Tensor([0],"float64"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:38.394902 test begin: paddle.Tensor.repeat_interleave(Tensor([0, 1, 1, 3],"float32"), 1, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([0, 1, 1, 3],"float32"), 1, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:38.528856 test begin: paddle.Tensor.repeat_interleave(Tensor([0, 1, 1, 3],"float32"), 2, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([0, 1, 1, 3],"float32"), 2, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:38.659736 test begin: paddle.Tensor.repeat_interleave(Tensor([0, 1, 1, 3],"float32"), 3, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([0, 1, 1, 3],"float32"), 3, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:38.881525 test begin: paddle.Tensor.repeat_interleave(Tensor([0, 1, 10, 10],"int64"), 1, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([0, 1, 10, 10],"int64"), 1, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:39.074343 test begin: paddle.Tensor.repeat_interleave(Tensor([0, 1, 10, 10],"int64"), 2, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([0, 1, 10, 10],"int64"), 2, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:39.205379 test begin: paddle.Tensor.repeat_interleave(Tensor([0, 1, 10, 10],"int64"), 3, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([0, 1, 10, 10],"int64"), 3, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:39.346472 test begin: paddle.Tensor.repeat_interleave(Tensor([0, 10],"int64"), 1, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([0, 10],"int64"), 1, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:39.512554 test begin: paddle.Tensor.repeat_interleave(Tensor([0, 10],"int64"), 2, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([0, 10],"int64"), 2, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:39.747438 test begin: paddle.Tensor.repeat_interleave(Tensor([0, 10],"int64"), 3, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([0, 10],"int64"), 3, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:39.937775 test begin: paddle.Tensor.repeat_interleave(Tensor([0, 1],"int64"), 1, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([0, 1],"int64"), 1, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:40.069145 test begin: paddle.Tensor.repeat_interleave(Tensor([0, 1],"int64"), 2, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([0, 1],"int64"), 2, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:40.264025 test begin: paddle.Tensor.repeat_interleave(Tensor([0, 1],"int64"), 3, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([0, 1],"int64"), 3, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:40.395443 test begin: paddle.Tensor.repeat_interleave(Tensor([0, 3, 16],"float32"), 1, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([0, 3, 16],"float32"), 1, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:40.591935 test begin: paddle.Tensor.repeat_interleave(Tensor([0, 3, 16],"float32"), 2, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([0, 3, 16],"float32"), 2, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:40.801064 test begin: paddle.Tensor.repeat_interleave(Tensor([0, 3, 16],"float32"), 3, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([0, 3, 16],"float32"), 3, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:41.012100 test begin: paddle.Tensor.repeat_interleave(Tensor([0, 3, 32],"float32"), 1, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([0, 3, 32],"float32"), 1, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:41.153245 test begin: paddle.Tensor.repeat_interleave(Tensor([0, 3, 32],"float32"), 2, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([0, 3, 32],"float32"), 2, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:41.349012 test begin: paddle.Tensor.repeat_interleave(Tensor([0, 3, 32],"float32"), 3, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([0, 3, 32],"float32"), 3, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:41.547341 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 0, 1, 3],"float32"), 1, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([2, 0, 1, 3],"float32"), 1, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:41.741547 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 0, 1, 3],"float32"), 2, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([2, 0, 1, 3],"float32"), 2, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:41.953210 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 0, 1, 3],"float32"), 3, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([2, 0, 1, 3],"float32"), 3, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:42.179264 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 0, 10, 10],"int64"), 1, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([2, 0, 10, 10],"int64"), 1, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:42.392314 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 0, 10, 10],"int64"), 2, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([2, 0, 10, 10],"int64"), 2, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:42.679525 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 0, 10, 10],"int64"), 3, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([2, 0, 10, 10],"int64"), 3, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:42.972063 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 0, 16],"float32"), 1, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([2, 0, 16],"float32"), 1, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:43.182936 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 0, 16],"float32"), 2, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([2, 0, 16],"float32"), 2, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:43.391870 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 0, 16],"float32"), 3, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([2, 0, 16],"float32"), 3, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:43.619040 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 0, 32],"float32"), 1, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([2, 0, 32],"float32"), 1, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:43.759367 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 0, 32],"float32"), 2, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([2, 0, 32],"float32"), 2, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:43.912454 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 0, 32],"float32"), 3, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([2, 0, 32],"float32"), 3, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:44.032194 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 0],"int64"), 1, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([2, 0],"int64"), 1, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:44.188148 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 0],"int64"), 2, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([2, 0],"int64"), 2, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:44.379073 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 0],"int64"), 3, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([2, 0],"int64"), 3, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:44.511359 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 0, 10],"int64"), 1, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 0, 10],"int64"), 1, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:44.703736 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 0, 10],"int64"), 2, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 0, 10],"int64"), 2, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:44.926487 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 0, 10],"int64"), 3, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 0, 10],"int64"), 3, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:45.060400 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 0, 3],"float32"), 1, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 0, 3],"float32"), 1, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:45.189233 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 0, 3],"float32"), 2, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 0, 3],"float32"), 2, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:45.385152 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 0, 3],"float32"), 3, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 0, 3],"float32"), 3, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:45.568041 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 1, 0],"float32"), 1, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 1, 0],"float32"), 1, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:45.694729 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 1, 0],"float32"), 2, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 1, 0],"float32"), 2, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:45.954486 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 1, 0],"float32"), 3, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 1, 0],"float32"), 3, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:46.124125 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 10, 0],"int64"), 1, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 10, 0],"int64"), 1, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:46.324210 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 10, 0],"int64"), 2, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 10, 0],"int64"), 2, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:46.529233 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 10, 0],"int64"), 3, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 10, 0],"int64"), 3, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:46.781015 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 3, 0],"float32"), 1, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([2, 3, 0],"float32"), 1, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:47.042455 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 3, 0],"float32"), 2, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([2, 3, 0],"float32"), 2, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:47.260773 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 3, 0],"float32"), 3, axis=0, )

[cuda error] paddle.Tensor.repeat_interleave(Tensor([2, 3, 0],"float32"), 3, axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:47.530766 test begin: paddle.Tensor.repeat_interleave(x=Tensor([0, 2, 4, 4, 5],"float64"), repeats=2, )

[cuda error] paddle.Tensor.repeat_interleave(x=Tensor([0, 2, 4, 4, 5],"float64"), repeats=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:47.742101 test begin: paddle.Tensor.repeat_interleave(x=Tensor([0, 2, 4, 4, 5],"float64"), repeats=2, axis=1, )

[cuda error] paddle.Tensor.repeat_interleave(x=Tensor([0, 2, 4, 4, 5],"float64"), repeats=2, axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:47.953129 test begin: paddle.Tensor.repeat_interleave(x=Tensor([0, 2, 4, 4, 5],"int32"), repeats=2, axis=3, )

element 0 of tensors does not require grad and does not have a grad_fn
[cuda error] paddle.Tensor.repeat_interleave(x=Tensor([0, 2, 4, 4, 5],"int32"), repeats=2, axis=3, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:48.264047 test begin: paddle.Tensor.repeat_interleave(x=Tensor([0, 2, 4, 5],"float64"), repeats=2, )

[cuda error] paddle.Tensor.repeat_interleave(x=Tensor([0, 2, 4, 5],"float64"), repeats=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:48.474652 test begin: paddle.Tensor.repeat_interleave(x=Tensor([0, 2, 4],"float64"), repeats=2, )

[cuda error] paddle.Tensor.repeat_interleave(x=Tensor([0, 2, 4],"float64"), repeats=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:48.680466 test begin: paddle.Tensor.repeat_interleave(x=Tensor([0, 2],"float64"), repeats=2, )

[cuda error] paddle.Tensor.repeat_interleave(x=Tensor([0, 2],"float64"), repeats=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:48.891824 test begin: paddle.Tensor.repeat_interleave(x=Tensor([0],"float32"), repeats=3, )

[cuda error] paddle.Tensor.repeat_interleave(x=Tensor([0],"float32"), repeats=3, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:49.146872 test begin: paddle.Tensor.repeat_interleave(x=Tensor([0],"float64"), repeats=3, )

[cuda error] paddle.Tensor.repeat_interleave(x=Tensor([0],"float64"), repeats=3, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:49.359782 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 0, 4, 4, 5],"float64"), repeats=2, )

[cuda error] paddle.Tensor.repeat_interleave(x=Tensor([4, 0, 4, 4, 5],"float64"), repeats=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:49.580885 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 0, 4, 4, 5],"float64"), repeats=2, axis=1, )

[cuda error] paddle.Tensor.repeat_interleave(x=Tensor([4, 0, 4, 4, 5],"float64"), repeats=2, axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:49.789150 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 0, 4, 4, 5],"int32"), repeats=2, axis=3, )

element 0 of tensors does not require grad and does not have a grad_fn
[cuda error] paddle.Tensor.repeat_interleave(x=Tensor([4, 0, 4, 4, 5],"int32"), repeats=2, axis=3, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:49.967960 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 0, 4, 5],"float64"), repeats=2, )

[cuda error] paddle.Tensor.repeat_interleave(x=Tensor([4, 0, 4, 5],"float64"), repeats=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:50.164339 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 0, 4],"float64"), repeats=2, )

[cuda error] paddle.Tensor.repeat_interleave(x=Tensor([4, 0, 4],"float64"), repeats=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:50.375079 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 0],"float64"), repeats=2, )

[cuda error] paddle.Tensor.repeat_interleave(x=Tensor([4, 0],"float64"), repeats=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:50.633936 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 0, 4, 5],"float64"), repeats=2, )

[cuda error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 0, 4, 5],"float64"), repeats=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:50.882625 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 0, 4, 5],"float64"), repeats=2, axis=1, )

[cuda error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 0, 4, 5],"float64"), repeats=2, axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:51.138112 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 0, 4, 5],"int32"), repeats=2, axis=3, )

element 0 of tensors does not require grad and does not have a grad_fn
[cuda error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 0, 4, 5],"int32"), repeats=2, axis=3, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:51.360683 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 0, 5],"float64"), repeats=2, )

[cuda error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 0, 5],"float64"), repeats=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:51.564202 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 0],"float64"), repeats=2, )

[cuda error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 0],"float64"), repeats=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:51.819036 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 0, 5],"float64"), repeats=2, )

[cuda error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 0, 5],"float64"), repeats=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:52.030999 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 0, 5],"float64"), repeats=2, axis=1, )

[cuda error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 0, 5],"float64"), repeats=2, axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:52.168014 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 0, 5],"int32"), repeats=2, axis=3, )

element 0 of tensors does not require grad and does not have a grad_fn
[cuda error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 0, 5],"int32"), repeats=2, axis=3, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:52.307295 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 0],"float64"), repeats=2, )

[cuda error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 0],"float64"), repeats=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:52.527566 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 4, 0],"float64"), repeats=2, )

[cuda error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 4, 0],"float64"), repeats=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:52.832619 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 4, 0],"float64"), repeats=2, axis=1, )

[cuda error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 4, 0],"float64"), repeats=2, axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:53.126178 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 4, 0],"int32"), repeats=2, axis=3, )

element 0 of tensors does not require grad and does not have a grad_fn
[cuda error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 4, 0],"int32"), repeats=2, axis=3, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:53.351799 test begin: paddle.Tensor.rot90(Tensor([0, 2],"float32"), 1, axes=list[0,1,], )

[cuda error] paddle.Tensor.rot90(Tensor([0, 2],"float32"), 1, axes=list[0,1,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:53.565994 test begin: paddle.Tensor.rot90(Tensor([3, 0],"float32"), 1, axes=list[0,1,], )

[cuda error] paddle.Tensor.rot90(Tensor([3, 0],"float32"), 1, axes=list[0,1,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:54.056612 test begin: paddle.Tensor.rot90(x=Tensor([0, 4, 4, 4],"float64"), )

[cuda error] paddle.Tensor.rot90(x=Tensor([0, 4, 4, 4],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:54.198713 test begin: paddle.Tensor.rot90(x=Tensor([0, 4, 4, 4],"float64"), k=-1, axes=list[1,2,], )

[cuda error] paddle.Tensor.rot90(x=Tensor([0, 4, 4, 4],"float64"), k=-1, axes=list[1,2,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:54.351345 test begin: paddle.Tensor.rot90(x=Tensor([0, 4, 4, 4],"float64"), k=-1, axes=tuple(2,3,), )

[cuda error] paddle.Tensor.rot90(x=Tensor([0, 4, 4, 4],"float64"), k=-1, axes=tuple(2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:54.490169 test begin: paddle.Tensor.rot90(x=Tensor([0, 4, 4],"float64"), )

[cuda error] paddle.Tensor.rot90(x=Tensor([0, 4, 4],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:54.734790 test begin: paddle.Tensor.rot90(x=Tensor([0, 4],"float32"), )

[cuda error] paddle.Tensor.rot90(x=Tensor([0, 4],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:54.943275 test begin: paddle.Tensor.rot90(x=Tensor([0, 4],"float64"), )

[cuda error] paddle.Tensor.rot90(x=Tensor([0, 4],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:55.151977 test begin: paddle.Tensor.rot90(x=Tensor([0, 4],"float64"), k=-1, )

[cuda error] paddle.Tensor.rot90(x=Tensor([0, 4],"float64"), k=-1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:55.908751 test begin: paddle.Tensor.rot90(x=Tensor([3, 0],"float64"), )

[cuda error] paddle.Tensor.rot90(x=Tensor([3, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:56.126932 test begin: paddle.Tensor.rot90(x=Tensor([4, 0, 4, 4],"float64"), )

[cuda error] paddle.Tensor.rot90(x=Tensor([4, 0, 4, 4],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:56.311366 test begin: paddle.Tensor.rot90(x=Tensor([4, 0, 4, 4],"float64"), k=-1, axes=list[1,2,], )

[cuda error] paddle.Tensor.rot90(x=Tensor([4, 0, 4, 4],"float64"), k=-1, axes=list[1,2,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:56.519173 test begin: paddle.Tensor.rot90(x=Tensor([4, 0, 4, 4],"float64"), k=-1, axes=tuple(2,3,), )

[cuda error] paddle.Tensor.rot90(x=Tensor([4, 0, 4, 4],"float64"), k=-1, axes=tuple(2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:56.792189 test begin: paddle.Tensor.rot90(x=Tensor([4, 0, 4],"float64"), )

[cuda error] paddle.Tensor.rot90(x=Tensor([4, 0, 4],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:56.996996 test begin: paddle.Tensor.rot90(x=Tensor([4, 0],"float32"), )

[cuda error] paddle.Tensor.rot90(x=Tensor([4, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:57.190342 test begin: paddle.Tensor.rot90(x=Tensor([4, 0],"float64"), k=-1, )

[cuda error] paddle.Tensor.rot90(x=Tensor([4, 0],"float64"), k=-1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:57.575123 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 0, 4],"float64"), )

[cuda error] paddle.Tensor.rot90(x=Tensor([4, 4, 0, 4],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:57.804205 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 0, 4],"float64"), k=-1, axes=list[1,2,], )

[cuda error] paddle.Tensor.rot90(x=Tensor([4, 4, 0, 4],"float64"), k=-1, axes=list[1,2,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:57.958126 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 0, 4],"float64"), k=-1, axes=tuple(2,3,), )

[cuda error] paddle.Tensor.rot90(x=Tensor([4, 4, 0, 4],"float64"), k=-1, axes=tuple(2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:58.144511 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 0],"float64"), )

[cuda error] paddle.Tensor.rot90(x=Tensor([4, 4, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:58.366434 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 4, 0],"float64"), )

[cuda error] paddle.Tensor.rot90(x=Tensor([4, 4, 4, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:58.508688 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 4, 0],"float64"), k=-1, axes=list[1,2,], )

[cuda error] paddle.Tensor.rot90(x=Tensor([4, 4, 4, 0],"float64"), k=-1, axes=list[1,2,], ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:11:58.687590 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 4, 0],"float64"), k=-1, axes=tuple(2,3,), )

[cuda error] paddle.Tensor.rot90(x=Tensor([4, 4, 4, 0],"float64"), k=-1, axes=tuple(2,3,), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:12:30.319804 test begin: paddle.Tensor.std(Tensor([0, 1, 36],"float32"), axis=-1, keepdim=True, )

/usr/local/lib/python3.9/dist-packages/torch/utils/_device.py:106: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1823.)
  return func(*args, **kwargs)
W0421 10:12:30.503590 110735 dygraph_functions.cc:84735] got different data type, run type promotion automatically, this may cause data type been changed.
W0421 10:12:30.505270 110735 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.Tensor.std(Tensor([0, 1, 36],"float32"), axis=-1, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:12:30.505595 test begin: paddle.Tensor.std(Tensor([0, 1, 45],"float32"), axis=-1, keepdim=True, )

W0421 10:12:30.695541 110743 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.Tensor.std(Tensor([0, 1, 45],"float32"), axis=-1, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:12:30.695895 test begin: paddle.Tensor.std(Tensor([0, 1024, 8],"float32"), )

[accuracy error] paddle.Tensor.std(Tensor([0, 1024, 8],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array(0., dtype=float32)
 y: array(nan, dtype=float32)
2025-04-21 10:12:30.897731 test begin: paddle.Tensor.std(Tensor([0, 1024, 8],"float64"), )

[accuracy error] paddle.Tensor.std(Tensor([0, 1024, 8],"float64"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array(0.)
 y: array(nan)
2025-04-21 10:12:31.126888 test begin: paddle.Tensor.std(Tensor([1, 0, 36],"float32"), axis=-1, keepdim=True, )

W0421 10:12:31.248107 110811 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.Tensor.std(Tensor([1, 0, 36],"float32"), axis=-1, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:12:31.248503 test begin: paddle.Tensor.std(Tensor([1, 0, 45],"float32"), axis=-1, keepdim=True, )

W0421 10:12:31.360846 110959 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.Tensor.std(Tensor([1, 0, 45],"float32"), axis=-1, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:12:31.361245 test begin: paddle.Tensor.std(Tensor([1, 1, 0],"float32"), axis=-1, keepdim=True, )

[accuracy error] paddle.Tensor.std(Tensor([1, 1, 0],"float32"), axis=-1, keepdim=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[0.]]], dtype=float32)
 y: array([[[nan]]], dtype=float32)
2025-04-21 10:12:31.602085 test begin: paddle.Tensor.std(Tensor([1024, 0, 8],"float32"), )

[accuracy error] paddle.Tensor.std(Tensor([1024, 0, 8],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array(0., dtype=float32)
 y: array(nan, dtype=float32)
2025-04-21 10:12:31.818468 test begin: paddle.Tensor.std(Tensor([1024, 0, 8],"float64"), )

[accuracy error] paddle.Tensor.std(Tensor([1024, 0, 8],"float64"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array(0.)
 y: array(nan)
2025-04-21 10:12:32.014326 test begin: paddle.Tensor.std(Tensor([1024, 1024, 0],"float32"), )

[accuracy error] paddle.Tensor.std(Tensor([1024, 1024, 0],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array(0., dtype=float32)
 y: array(nan, dtype=float32)
2025-04-21 10:12:32.222093 test begin: paddle.Tensor.std(Tensor([1024, 1024, 0],"float64"), )

[accuracy error] paddle.Tensor.std(Tensor([1024, 1024, 0],"float64"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array(0.)
 y: array(nan)
2025-04-21 10:13:45.236019 test begin: paddle.Tensor.topk(Tensor([0, 1000],"float32"), 5, 1, True, True, )

element 1 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.topk(Tensor([0, 1000],"float32"), 5, 1, True, True, ) 
 (InvalidArgument) x has only 0 element, can not find 5 top values.
  [Hint: Expected x.numel() >= k, but received x.numel():0 < k:5.] (at ../paddle/phi/kernels/gpu/top_k_kernel.cu:80)

2025-04-21 10:14:29.040038 test begin: paddle.std(Tensor([0, 3, 4, 10],"float32"), list[1,3,], True, False, )

W0421 10:14:29.232887 152534 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.std(Tensor([0, 3, 4, 10],"float32"), list[1,3,], True, False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:14:29.233347 test begin: paddle.std(Tensor([0, 3, 4, 10],"float64"), 2, True, False, )

W0421 10:14:29.430220 152539 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.std(Tensor([0, 3, 4, 10],"float64"), 2, True, False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:14:29.430696 test begin: paddle.std(Tensor([0, 3, 4, 10],"float64"), list[1,2,], True, False, )

W0421 10:14:29.571935 152610 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.std(Tensor([0, 3, 4, 10],"float64"), list[1,2,], True, False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:14:29.572386 test begin: paddle.std(Tensor([0, 3, 4, 10],"float64"), list[1,3,], False, False, )

W0421 10:14:29.736665 152688 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.std(Tensor([0, 3, 4, 10],"float64"), list[1,3,], False, False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:14:29.737055 test begin: paddle.std(Tensor([0, 3, 4, 10],"float64"), list[1,3,], True, False, )

W0421 10:14:29.897831 152694 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.std(Tensor([0, 3, 4, 10],"float64"), list[1,3,], True, False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:14:29.898186 test begin: paddle.std(Tensor([0, 3, 4, 10],"float64"), tuple(1,3,), True, False, )

W0421 10:14:30.125329 152698 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.std(Tensor([0, 3, 4, 10],"float64"), tuple(1,3,), True, False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:14:30.125809 test begin: paddle.std(Tensor([0, 32],"float32"), )

[accuracy error] paddle.std(Tensor([0, 32],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array(0., dtype=float32)
 y: array(nan, dtype=float32)
2025-04-21 10:14:30.270386 test begin: paddle.std(Tensor([0, 5],"float32"), )

[accuracy error] paddle.std(Tensor([0, 5],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array(0., dtype=float32)
 y: array(nan, dtype=float32)
2025-04-21 10:14:30.474937 test begin: paddle.std(Tensor([0, 9],"float32"), axis=1, )

W0421 10:14:30.621212 152919 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.std(Tensor([0, 9],"float32"), axis=1, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:14:30.621691 test begin: paddle.std(Tensor([0],"float32"), )

[accuracy error] paddle.std(Tensor([0],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array(0., dtype=float32)
 y: array(nan, dtype=float32)
2025-04-21 10:14:30.745487 test begin: paddle.std(Tensor([1, 0, 4, 10],"float32"), list[1,3,], True, False, )

[accuracy error] paddle.std(Tensor([1, 0, 4, 10],"float32"), list[1,3,], True, False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[0., 0., 0., 0.]], dtype=float32)
 y: array([[nan, nan, nan, nan]], dtype=float32)
2025-04-21 10:14:30.983465 test begin: paddle.std(Tensor([1, 0, 4, 10],"float64"), 2, True, False, )

W0421 10:14:31.116706 152946 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.std(Tensor([1, 0, 4, 10],"float64"), 2, True, False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:14:31.117143 test begin: paddle.std(Tensor([1, 0, 4, 10],"float64"), list[1,2,], True, False, )

[accuracy error] paddle.std(Tensor([1, 0, 4, 10],"float64"), list[1,2,], True, False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
 y: array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])
2025-04-21 10:14:31.315222 test begin: paddle.std(Tensor([1, 0, 4, 10],"float64"), list[1,3,], False, False, )

W0421 10:14:31.583735 153103 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.std(Tensor([1, 0, 4, 10],"float64"), list[1,3,], False, False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:14:31.584137 test begin: paddle.std(Tensor([1, 0, 4, 10],"float64"), list[1,3,], True, False, )

[accuracy error] paddle.std(Tensor([1, 0, 4, 10],"float64"), list[1,3,], True, False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[0., 0., 0., 0.]])
 y: array([[nan, nan, nan, nan]])
2025-04-21 10:14:31.893567 test begin: paddle.std(Tensor([1, 0, 4, 10],"float64"), tuple(1,3,), True, False, )

[accuracy error] paddle.std(Tensor([1, 0, 4, 10],"float64"), tuple(1,3,), True, False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[0., 0., 0., 0.]])
 y: array([[nan, nan, nan, nan]])
2025-04-21 10:14:32.242107 test begin: paddle.std(Tensor([1, 3, 0, 10],"float32"), list[1,3,], True, False, )

W0421 10:14:32.631182 153678 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.std(Tensor([1, 3, 0, 10],"float32"), list[1,3,], True, False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:14:32.632012 test begin: paddle.std(Tensor([1, 3, 0, 10],"float64"), 2, True, False, )

[accuracy error] paddle.std(Tensor([1, 3, 0, 10],"float64"), 2, True, False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])
 y: array([[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]]])
2025-04-21 10:14:32.885948 test begin: paddle.std(Tensor([1, 3, 0, 10],"float64"), list[1,2,], True, False, )

[accuracy error] paddle.std(Tensor([1, 3, 0, 10],"float64"), list[1,2,], True, False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
 y: array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])
2025-04-21 10:14:33.095269 test begin: paddle.std(Tensor([1, 3, 0, 10],"float64"), list[1,3,], False, False, )

W0421 10:14:33.301765 154095 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.std(Tensor([1, 3, 0, 10],"float64"), list[1,3,], False, False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:14:33.302127 test begin: paddle.std(Tensor([1, 3, 0, 10],"float64"), list[1,3,], True, False, )

W0421 10:14:33.517962 154101 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.std(Tensor([1, 3, 0, 10],"float64"), list[1,3,], True, False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:14:33.518415 test begin: paddle.std(Tensor([1, 3, 0, 10],"float64"), tuple(1,3,), True, False, )

W0421 10:14:33.734014 154180 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.std(Tensor([1, 3, 0, 10],"float64"), tuple(1,3,), True, False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:14:33.734435 test begin: paddle.std(Tensor([1, 3, 4, 0],"float32"), list[1,3,], True, False, )

[accuracy error] paddle.std(Tensor([1, 3, 4, 0],"float32"), list[1,3,], True, False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[0., 0., 0., 0.]], dtype=float32)
 y: array([[nan, nan, nan, nan]], dtype=float32)
2025-04-21 10:14:33.951161 test begin: paddle.std(Tensor([1, 3, 4, 0],"float64"), 2, True, False, )

W0421 10:14:34.159863 154233 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.std(Tensor([1, 3, 4, 0],"float64"), 2, True, False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:14:34.160356 test begin: paddle.std(Tensor([1, 3, 4, 0],"float64"), list[1,2,], True, False, )

W0421 10:14:34.378000 154309 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.std(Tensor([1, 3, 4, 0],"float64"), list[1,2,], True, False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:14:34.378501 test begin: paddle.std(Tensor([1, 3, 4, 0],"float64"), list[1,3,], False, False, )

W0421 10:14:34.594480 154385 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.std(Tensor([1, 3, 4, 0],"float64"), list[1,3,], False, False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:14:34.594920 test begin: paddle.std(Tensor([1, 3, 4, 0],"float64"), list[1,3,], True, False, )

[accuracy error] paddle.std(Tensor([1, 3, 4, 0],"float64"), list[1,3,], True, False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[0., 0., 0., 0.]])
 y: array([[nan, nan, nan, nan]])
2025-04-21 10:14:34.880547 test begin: paddle.std(Tensor([1, 3, 4, 0],"float64"), tuple(1,3,), True, False, )

[accuracy error] paddle.std(Tensor([1, 3, 4, 0],"float64"), tuple(1,3,), True, False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[0., 0., 0., 0.]])
 y: array([[nan, nan, nan, nan]])
2025-04-21 10:14:35.092663 test begin: paddle.std(Tensor([3, 0],"float32"), )

[accuracy error] paddle.std(Tensor([3, 0],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array(0., dtype=float32)
 y: array(nan, dtype=float32)
2025-04-21 10:14:35.357870 test begin: paddle.std(Tensor([32, 0],"float32"), )

[accuracy error] paddle.std(Tensor([32, 0],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array(0., dtype=float32)
 y: array(nan, dtype=float32)
2025-04-21 10:14:35.586758 test begin: paddle.std(Tensor([6, 0],"float32"), axis=1, )

[accuracy error] paddle.std(Tensor([6, 0],"float32"), axis=1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([0., 0., 0., 0., 0., 0.], dtype=float32)
 y: array([nan, nan, nan, nan, nan, nan], dtype=float32)
2025-04-21 10:14:35.884582 test begin: paddle.std(x=Tensor([0, 3, 3],"float64"), )

[accuracy error] paddle.std(x=Tensor([0, 3, 3],"float64"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array(0.)
 y: array(nan)
2025-04-21 10:14:36.124029 test begin: paddle.std(x=Tensor([0, 3, 3],"float64"), axis=0, )

[accuracy error] paddle.std(x=Tensor([0, 3, 3],"float64"), axis=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.]])
 y: array([[nan, nan, nan],
       [nan, nan, nan],
       [nan, nan, nan]])
2025-04-21 10:14:36.427862 test begin: paddle.std(x=Tensor([0, 3, 3],"float64"), axis=0, unbiased=False, )

W0421 10:14:36.701287 155158 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.std(x=Tensor([0, 3, 3],"float64"), axis=0, unbiased=False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:14:36.701763 test begin: paddle.std(x=Tensor([0, 3, 3],"float64"), axis=list[0,1,], )

[accuracy error] paddle.std(x=Tensor([0, 3, 3],"float64"), axis=list[0,1,], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([0., 0., 0.])
 y: array([nan, nan, nan])
2025-04-21 10:14:36.983811 test begin: paddle.std(x=Tensor([0, 3, 3],"float64"), axis=tuple(0,1,), )

[accuracy error] paddle.std(x=Tensor([0, 3, 3],"float64"), axis=tuple(0,1,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([0., 0., 0.])
 y: array([nan, nan, nan])
2025-04-21 10:14:37.263931 test begin: paddle.std(x=Tensor([0, 3, 3],"float64"), axis=tuple(0,1,), keepdim=True, )

[accuracy error] paddle.std(x=Tensor([0, 3, 3],"float64"), axis=tuple(0,1,), keepdim=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[0., 0., 0.]]])
 y: array([[[nan, nan, nan]]])
2025-04-21 10:14:37.483399 test begin: paddle.std(x=Tensor([0, 3],"float32"), )

[accuracy error] paddle.std(x=Tensor([0, 3],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array(0., dtype=float32)
 y: array(nan, dtype=float32)
2025-04-21 10:14:37.788286 test begin: paddle.std(x=Tensor([0, 3],"float64"), )

[accuracy error] paddle.std(x=Tensor([0, 3],"float64"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array(0.)
 y: array(nan)
2025-04-21 10:14:37.989848 test begin: paddle.std(x=Tensor([2, 0],"float32"), )

[accuracy error] paddle.std(x=Tensor([2, 0],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array(0., dtype=float32)
 y: array(nan, dtype=float32)
2025-04-21 10:14:38.200026 test begin: paddle.std(x=Tensor([2, 0],"float64"), )

[accuracy error] paddle.std(x=Tensor([2, 0],"float64"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array(0.)
 y: array(nan)
2025-04-21 10:14:38.431274 test begin: paddle.std(x=Tensor([3, 0, 3],"float64"), )

[accuracy error] paddle.std(x=Tensor([3, 0, 3],"float64"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array(0.)
 y: array(nan)
2025-04-21 10:14:38.683956 test begin: paddle.std(x=Tensor([3, 0, 3],"float64"), axis=0, )

W0421 10:14:38.890228 155867 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.std(x=Tensor([3, 0, 3],"float64"), axis=0, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:14:38.890652 test begin: paddle.std(x=Tensor([3, 0, 3],"float64"), axis=0, unbiased=False, )

W0421 10:14:39.098881 155869 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.std(x=Tensor([3, 0, 3],"float64"), axis=0, unbiased=False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:14:39.099286 test begin: paddle.std(x=Tensor([3, 0, 3],"float64"), axis=list[0,1,], )

[accuracy error] paddle.std(x=Tensor([3, 0, 3],"float64"), axis=list[0,1,], ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([0., 0., 0.])
 y: array([nan, nan, nan])
2025-04-21 10:14:39.387869 test begin: paddle.std(x=Tensor([3, 0, 3],"float64"), axis=tuple(0,1,), )

[accuracy error] paddle.std(x=Tensor([3, 0, 3],"float64"), axis=tuple(0,1,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([0., 0., 0.])
 y: array([nan, nan, nan])
2025-04-21 10:14:39.538452 test begin: paddle.std(x=Tensor([3, 0, 3],"float64"), axis=tuple(0,1,), keepdim=True, )

[accuracy error] paddle.std(x=Tensor([3, 0, 3],"float64"), axis=tuple(0,1,), keepdim=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[0., 0., 0.]]])
 y: array([[[nan, nan, nan]]])
2025-04-21 10:14:39.756368 test begin: paddle.std(x=Tensor([3, 3, 0],"float64"), )

[accuracy error] paddle.std(x=Tensor([3, 3, 0],"float64"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array(0.)
 y: array(nan)
2025-04-21 10:14:40.018433 test begin: paddle.std(x=Tensor([3, 3, 0],"float64"), axis=0, )

W0421 10:14:40.146874 156133 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.std(x=Tensor([3, 3, 0],"float64"), axis=0, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:14:40.147211 test begin: paddle.std(x=Tensor([3, 3, 0],"float64"), axis=0, unbiased=False, )

W0421 10:14:40.309303 156137 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.std(x=Tensor([3, 3, 0],"float64"), axis=0, unbiased=False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:14:40.309711 test begin: paddle.std(x=Tensor([3, 3, 0],"float64"), axis=list[0,1,], )

W0421 10:14:40.424574 156182 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.std(x=Tensor([3, 3, 0],"float64"), axis=list[0,1,], ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:14:40.424947 test begin: paddle.std(x=Tensor([3, 3, 0],"float64"), axis=tuple(0,1,), )

W0421 10:14:40.613866 156215 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.std(x=Tensor([3, 3, 0],"float64"), axis=tuple(0,1,), ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:14:40.614260 test begin: paddle.std(x=Tensor([3, 3, 0],"float64"), axis=tuple(0,1,), keepdim=True, )

W0421 10:14:40.821346 156220 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.std(x=Tensor([3, 3, 0],"float64"), axis=tuple(0,1,), keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:07:27.611930 test begin: paddle.Tensor.tril(Tensor([0, 2, 2],"float32"), -1, )

[cuda error] paddle.Tensor.tril(Tensor([0, 2, 2],"float32"), -1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:28.106919 test begin: paddle.Tensor.tril(Tensor([0, 2],"float32"), -1, )

[cuda error] paddle.Tensor.tril(Tensor([0, 2],"float32"), -1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:28.347841 test begin: paddle.Tensor.tril(Tensor([1, 0, 2],"float32"), -1, )

[cuda error] paddle.Tensor.tril(Tensor([1, 0, 2],"float32"), -1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:28.479463 test begin: paddle.Tensor.tril(Tensor([1, 2, 0],"float32"), -1, )

[cuda error] paddle.Tensor.tril(Tensor([1, 2, 0],"float32"), -1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:28.599815 test begin: paddle.Tensor.tril(Tensor([2, 0, 2],"float32"), -1, )

[cuda error] paddle.Tensor.tril(Tensor([2, 0, 2],"float32"), -1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:28.838420 test begin: paddle.Tensor.tril(Tensor([2, 0],"float32"), -1, )

[cuda error] paddle.Tensor.tril(Tensor([2, 0],"float32"), -1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:07:29.003583 test begin: paddle.Tensor.tril(Tensor([2, 2, 0],"float32"), -1, )

[cuda error] paddle.Tensor.tril(Tensor([2, 2, 0],"float32"), -1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-04-21 10:08:08.541044 test begin: paddle.Tensor.var(Tensor([0, 2, 3],"float32"), axis=0, )

/usr/local/lib/python3.9/dist-packages/torch/utils/_device.py:106: UserWarning: var(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1823.)
  return func(*args, **kwargs)
W0421 10:08:08.737815 17797 dygraph_functions.cc:84735] got different data type, run type promotion automatically, this may cause data type been changed.
[accuracy error] paddle.Tensor.var(Tensor([0, 2, 3],"float32"), axis=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[0., 0., 0.],
       [0., 0., 0.]], dtype=float32)
 y: array([[nan, nan, nan],
       [nan, nan, nan]], dtype=float32)
2025-04-21 10:08:08.740237 test begin: paddle.Tensor.var(Tensor([0, 2, 3],"float64"), axis=0, )

[accuracy error] paddle.Tensor.var(Tensor([0, 2, 3],"float64"), axis=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[0., 0., 0.],
       [0., 0., 0.]])
 y: array([[nan, nan, nan],
       [nan, nan, nan]])
2025-04-21 10:08:08.950556 test begin: paddle.Tensor.var(Tensor([0, 4],"float64"), axis=0, )

[accuracy error] paddle.Tensor.var(Tensor([0, 4],"float64"), axis=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([0., 0., 0., 0.])
 y: array([nan, nan, nan, nan])
2025-04-21 10:08:09.153962 test begin: paddle.Tensor.var(Tensor([0, 784],"float32"), axis=0, )

[accuracy error] paddle.Tensor.var(Tensor([0, 784],"float32"), axis=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,...
 y: array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,...
2025-04-21 10:08:09.346254 test begin: paddle.Tensor.var(Tensor([0],"float32"), axis=0, )

[accuracy error] paddle.Tensor.var(Tensor([0],"float32"), axis=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array(0., dtype=float32)
 y: array(nan, dtype=float32)
2025-04-21 10:08:09.543770 test begin: paddle.Tensor.var(Tensor([0],"float64"), axis=0, )

[accuracy error] paddle.Tensor.var(Tensor([0],"float64"), axis=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array(0.)
 y: array(nan)
2025-04-21 10:08:09.712632 test begin: paddle.Tensor.var(Tensor([1000, 0],"float32"), axis=0, )

W0421 10:08:09.922670 18235 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.Tensor.var(Tensor([1000, 0],"float32"), axis=0, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:08:09.923355 test begin: paddle.Tensor.var(Tensor([10000, 0, 3],"float32"), axis=0, )

W0421 10:08:10.194298 18448 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.Tensor.var(Tensor([10000, 0, 3],"float32"), axis=0, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:08:10.195734 test begin: paddle.Tensor.var(Tensor([10000, 0, 3],"float64"), axis=0, )

W0421 10:08:10.449615 18540 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.Tensor.var(Tensor([10000, 0, 3],"float64"), axis=0, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:08:10.451653 test begin: paddle.Tensor.var(Tensor([10000, 2, 0],"float32"), axis=0, )

W0421 10:08:10.674252 18556 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.Tensor.var(Tensor([10000, 2, 0],"float32"), axis=0, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:08:10.674906 test begin: paddle.Tensor.var(Tensor([10000, 2, 0],"float64"), axis=0, )

W0421 10:08:10.891433 18776 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.Tensor.var(Tensor([10000, 2, 0],"float64"), axis=0, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:08:10.892163 test begin: paddle.Tensor.var(Tensor([100000, 0],"float64"), axis=0, )

W0421 10:08:11.084606 18789 backward.cc:437] While running Node (SubtractGradNode) raises an EnforceNotMet exception
[paddle error] paddle.Tensor.var(Tensor([100000, 0],"float64"), axis=0, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1052)

2025-04-21 10:08:28.990408 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[0,1,3,2,],list[0,2,1,3,],], )

W0421 10:08:29.220497 24536 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[0,1,3,2,],list[0,2,1,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 5, 1, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 0, 5, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:29.224744 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[0,1,3,],list[0,3,1,],], )

W0421 10:08:29.362237 24608 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[0,1,3,],list[0,3,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 5, 1, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 0, 5, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:29.362836 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[0,2,3,],list[0,1,2,],], )

W0421 10:08:29.589289 24752 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[0,2,3,],list[0,1,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:29.589753 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[0,2,3,],list[0,2,1,],], )

W0421 10:08:29.800099 24845 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[0,2,3,],list[0,2,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 5, 1, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 0, 5, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:29.800568 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[0,3,1,2,],list[3,2,1,0,],], )

W0421 10:08:29.995118 24862 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[0,3,1,2,],list[3,2,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:29.995555 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[0,3,2,1,],list[2,1,3,0,],], )

W0421 10:08:30.186256 24878 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[0,3,2,1,],list[2,1,3,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 5, 1, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 0, 5, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:30.186889 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[1,0,2,3,],list[3,0,1,2,],], )

W0421 10:08:30.375567 24980 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[1,0,2,3,],list[3,0,1,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:30.378913 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[1,0,3,2,],list[3,0,1,2,],], )

W0421 10:08:30.589496 25116 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[1,0,3,2,],list[3,0,1,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 5, 1, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 0, 5, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:30.595554 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[1,2,0,],list[1,2,3,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:08:30.737922 25199 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[1,2,0,],list[1,2,3,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:08:30.738176 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[1,2,0,],list[1,3,2,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:08:30.878947 25220 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[1,2,0,],list[1,3,2,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:08:30.880944 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[1,2,3,0,],list[3,2,1,0,],], )

W0421 10:08:31.067405 25355 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[1,2,3,0,],list[3,2,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 5, 1, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 0, 5, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:31.070165 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[1,3,2,0,],list[1,3,2,0,],], )

W0421 10:08:31.280328 25431 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[1,3,2,0,],list[1,3,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:31.280750 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[1,3,2,0,],list[2,1,0,3,],], )

W0421 10:08:31.521067 25449 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[1,3,2,0,],list[2,1,0,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 5, 1, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 0, 5, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:31.548470 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[2,1,0,3,],list[2,0,3,1,],], )

W0421 10:08:31.681914 25544 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[2,1,0,3,],list[2,0,3,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 5, 1, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 0, 5, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:31.837528 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[2,3,0,],list[1,2,0,],], )

W0421 10:08:31.985839 25581 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[2,3,0,],list[1,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:31.986178 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[2,3,0,],list[3,1,0,],], )

W0421 10:08:32.170037 25717 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[2,3,0,],list[3,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 5, 1, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 0, 5, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:32.172144 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[2,3,],list[1,3,],], )

W0421 10:08:32.314190 25751 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[2,3,],list[1,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:32.318770 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,1,0,2,],list[3,1,0,2,],], )

W0421 10:08:32.512144 25827 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,1,0,2,],list[3,1,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:32.514655 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,1,2,0,],list[1,2,3,0,],], )

W0421 10:08:32.721688 25844 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,1,2,0,],list[1,2,3,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 5, 1, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 0, 5, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:32.722219 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,1,2,0,],list[2,3,1,0,],], )

W0421 10:08:32.918115 25854 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,1,2,0,],list[2,3,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:32.932552 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,1,2,],list[1,2,3,],], )

W0421 10:08:33.205101 25871 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,1,2,],list[1,2,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 5, 1, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 0, 5, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:33.205437 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,1,2,],list[1,3,2,],], )

W0421 10:08:33.341316 25955 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,1,2,],list[1,3,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 5, 1, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 0, 5, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:33.341643 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,1,2,],list[2,3,1,],], )

W0421 10:08:33.530856 25970 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,1,2,],list[2,3,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:33.531208 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,1,2,],list[3,1,0,],], )

W0421 10:08:33.667248 26055 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,1,2,],list[3,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:33.668188 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,2,0,],list[2,1,0,],], )

W0421 10:08:33.788192 26129 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,2,0,],list[2,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:33.788536 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,2,0,],list[3,2,0,],], )

W0421 10:08:34.032120 26139 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,2,0,],list[3,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:34.034077 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,2,1,],list[2,0,1,],], )

W0421 10:08:34.292493 26289 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,2,1,],list[2,0,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:34.292960 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,2,],list[3,1,],], )

W0421 10:08:34.412241 26439 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,2,],list[3,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:37.280955 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[1,2,3,0,],list[1,3,0,2,],], )

W0421 10:08:37.533496 27257 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[1,2,3,0,],list[1,3,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:37.800921 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[1,3,0,],list[3,0,2,],], )

W0421 10:08:37.994221 27431 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[1,3,0,],list[3,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:37.994671 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[1,3,2,0,],list[1,3,2,0,],], )

W0421 10:08:38.237823 27451 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[1,3,2,0,],list[1,3,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:38.597166 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[1,3,2,0,],list[3,0,1,2,],], )

W0421 10:08:38.796239 27834 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[1,3,2,0,],list[3,0,1,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:38.796724 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[1,3,],list[1,0,],], )

W0421 10:08:39.085541 27856 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[1,3,],list[1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:39.788420 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[2,3,0,],list[1,2,0,],], )

W0421 10:08:39.967527 28261 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[2,3,0,],list[1,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:40.161770 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[2,3,1,],list[1,0,2,],], )

W0421 10:08:40.357753 28289 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[2,3,1,],list[1,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:40.358131 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[2,3,],list[1,3,],], )

W0421 10:08:40.633334 28374 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[2,3,],list[1,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:40.638697 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,1,0,2,],list[3,1,0,2,],], )

W0421 10:08:40.830690 28518 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,1,0,2,],list[3,1,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:41.008139 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,1,2,0,],list[2,3,1,0,],], )

W0421 10:08:41.231177 28552 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,1,2,0,],list[2,3,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:41.231640 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,1,2,],list[0,3,2,],], )

W0421 10:08:41.426299 28567 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,1,2,],list[0,3,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:41.921221 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,1,2,],list[2,3,1,],], )

W0421 10:08:42.155745 28739 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,1,2,],list[2,3,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:42.156193 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,1,2,],list[3,1,0,],], )

W0421 10:08:42.392709 28783 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,1,2,],list[3,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:42.393139 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,2,0,],list[2,1,0,],], )

W0421 10:08:42.625818 28874 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,2,0,],list[2,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:42.626231 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,2,0,],list[3,2,0,],], )

W0421 10:08:42.822252 28967 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,2,0,],list[3,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:42.822898 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,2,1,0,],list[0,1,3,2,],], )

W0421 10:08:43.047474 29116 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,2,1,0,],list[0,1,3,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:43.047964 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,2,1,],list[2,0,1,],], )

W0421 10:08:43.267769 29333 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,2,1,],list[2,0,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:43.268199 test begin: paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,2,],list[3,1,],], )

W0421 10:08:43.472219 29548 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,2,],list[3,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:43.666476 test begin: paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[0,1,3,2,],list[0,2,1,3,],], )

W0421 10:08:43.870183 29584 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[0,1,3,2,],list[0,2,1,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 5, 5, Tensor layout is NCHW, Tensor stride is 25, 5, 1. New dims is 0, 5, 1, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:43.882257 test begin: paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[0,1,3,],list[0,3,1,],], )

W0421 10:08:44.007431 29604 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[0,1,3,],list[0,3,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 5, 1, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 0, 5, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:44.007845 test begin: paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[0,2,3,],list[0,1,2,],], )

W0421 10:08:44.158996 29617 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[0,2,3,],list[0,1,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 0, 1, 5, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:44.159558 test begin: paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[0,2,3,],list[0,2,1,],], )

W0421 10:08:44.341816 29629 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[0,2,3,],list[0,2,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 0, 1, 1, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:44.568900 test begin: paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[1,0,2,3,],list[3,0,1,2,],], )

W0421 10:08:44.700062 30153 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[1,0,2,3,],list[3,0,1,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 0, 1, 5, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:44.702172 test begin: paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[1,0,3,2,],list[3,0,1,2,],], )

W0421 10:08:44.819795 30246 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[1,0,3,2,],list[3,0,1,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 0, 1, 1, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:44.824550 test begin: paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[1,2,0,],list[1,3,2,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:08:45.013191 30255 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[1,2,0,],list[1,3,2,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:08:45.015135 test begin: paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[1,2,3,0,],list[3,2,1,0,],], )

W0421 10:08:45.147495 30273 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[1,2,3,0,],list[3,2,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 0, 1, 1, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:45.149264 test begin: paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[1,3,2,0,],list[1,3,2,0,],], )

W0421 10:08:45.328277 30290 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[1,3,2,0,],list[1,3,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 0, 1, 1, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:45.338631 test begin: paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[2,3,0,],list[1,2,0,],], )

W0421 10:08:45.472872 30309 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[2,3,0,],list[1,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 0, 1, 5, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:45.898810 test begin: paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[3,1,0,2,],list[3,1,0,2,],], )

W0421 10:08:46.109933 30481 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[3,1,0,2,],list[3,1,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 0, 1, 1, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:46.111961 test begin: paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[3,1,2,0,],list[1,2,3,0,],], )

W0421 10:08:46.311084 30498 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[3,1,2,0,],list[1,2,3,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 5, 5, Tensor layout is NCHW, Tensor stride is 25, 5, 1. New dims is 0, 5, 1, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:46.311552 test begin: paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[3,1,2,0,],list[2,3,1,0,],], )

W0421 10:08:46.506713 30584 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[3,1,2,0,],list[2,3,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 0, 1, 5, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:46.523694 test begin: paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[3,1,2,],list[1,2,3,],], )

W0421 10:08:46.657822 30604 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[3,1,2,],list[1,2,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 5, 5, Tensor layout is NCHW, Tensor stride is 25, 5, 1. New dims is 0, 5, 1, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:46.658227 test begin: paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[3,1,2,],list[1,3,2,],], )

W0421 10:08:46.779700 30626 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[3,1,2,],list[1,3,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 0, 1, 1, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:46.780439 test begin: paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[3,1,2,],list[2,3,1,],], )

W0421 10:08:46.901301 30711 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[3,1,2,],list[2,3,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 0, 1, 5, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:46.914822 test begin: paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[3,2,0,],list[2,1,0,],], )

W0421 10:08:47.125420 30797 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[3,2,0,],list[2,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 0, 1, 5, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:47.125832 test begin: paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[3,2,0,],list[3,2,0,],], )

W0421 10:08:47.371737 30902 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[3,2,0,],list[3,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 0, 1, 1, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:50.036410 test begin: paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([5, 5, 1, 5],"float64"), list[list[3,1,2,],list[0,3,2,],], )

W0421 10:08:50.217130 31857 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([5, 5, 1, 5],"float64"), list[list[3,1,2,],list[0,3,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 0, 1, 1, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:50.425140 test begin: paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([5, 5, 1, 5],"float64"), list[list[3,1,2,],list[1,3,2,],], )

W0421 10:08:50.640026 31948 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([5, 5, 1, 5],"float64"), list[list[3,1,2,],list[1,3,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 0, 1, 1, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:50.640492 test begin: paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([5, 5, 1, 5],"float64"), list[list[3,1,2,],list[2,3,1,],], )

W0421 10:08:50.836745 31975 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([5, 5, 1, 5],"float64"), list[list[3,1,2,],list[2,3,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 0, 1, 5, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:51.356249 test begin: paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([5, 5, 1, 5],"float64"), list[list[3,2,1,],list[2,0,1,],], )

W0421 10:08:51.493393 32304 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([0, 1, 5, 5],"float64"), Tensor([5, 5, 1, 5],"float64"), list[list[3,2,1,],list[2,0,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 0, 1, 5, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:54.514492 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[0,1,2,],list[2,0,1,],], )

W0421 10:08:54.653587 33239 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[0,1,2,],list[2,0,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:54.838290 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[0,1,3,2,],list[2,3,0,1,],], )

W0421 10:08:55.075980 33329 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[0,1,3,2,],list[2,3,0,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:55.078606 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[0,2,3,],list[0,1,2,],], )

W0421 10:08:55.272213 33352 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[0,2,3,],list[0,1,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:55.275089 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[0,3,1,2,],list[3,2,1,0,],], )

W0421 10:08:55.406697 33442 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[0,3,1,2,],list[3,2,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:55.408524 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[1,0,2,3,],list[3,0,1,2,],], )

W0421 10:08:55.520071 33464 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[1,0,2,3,],list[3,0,1,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:55.520478 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[1,0,3,2,],list[2,3,0,1,],], )

W0421 10:08:55.687024 33471 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[1,0,3,2,],list[2,3,0,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:55.748317 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[1,0,3,],list[2,3,0,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:08:55.864859 33694 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[1,0,3,],list[2,3,0,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:08:55.865399 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[1,2,0,3,],list[0,2,1,3,],], )

W0421 10:08:55.988757 33773 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[1,2,0,3,],list[0,2,1,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:55.989254 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[1,2,0,],list[1,2,3,],], )

W0421 10:08:56.186383 34048 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[1,2,0,],list[1,2,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:56.186724 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[1,2,0,],list[1,3,2,],], )

W0421 10:08:56.385056 34057 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[1,2,0,],list[1,3,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:56.386003 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[1,2,3,0,],list[1,3,0,2,],], )

W0421 10:08:56.538689 34215 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[1,2,3,0,],list[1,3,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:56.542731 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[1,3,0,],list[3,0,2,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:08:56.775835 34237 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[1,3,0,],list[3,0,2,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:08:56.776194 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[1,3,2,0,],list[1,3,2,0,],], )

W0421 10:08:56.928452 34327 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[1,3,2,0,],list[1,3,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:57.142754 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[1,3,],list[1,0,],], )

W0421 10:08:57.272949 34449 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[1,3,],list[1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:57.273425 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[2,0,1,3,],list[3,1,0,2,],], )

W0421 10:08:57.375797 34459 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[2,0,1,3,],list[3,1,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:57.376190 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[2,0,1,],list[0,1,3,],], )

W0421 10:08:57.587134 34474 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[2,0,1,],list[0,1,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:57.587525 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[2,1,0,3,],list[2,0,1,3,],], )

W0421 10:08:57.790057 34490 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[2,1,0,3,],list[2,0,1,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:57.793559 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[2,1,0,3,],list[3,1,2,0,],], )

W0421 10:08:58.003921 34504 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[2,1,0,3,],list[3,1,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:58.004334 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[2,1,],list[0,2,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:08:58.128506 34590 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[2,1,],list[0,2,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:08:58.128832 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[2,3,0,],list[1,2,0,],], )

W0421 10:08:58.243249 34599 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[2,3,0,],list[1,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:58.245350 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[2,3,1,],list[1,0,2,],], )

W0421 10:08:58.432389 34610 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[2,3,1,],list[1,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:58.432779 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[2,3,],list[1,3,],], )

W0421 10:08:58.637200 34622 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[2,3,],list[1,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:58.637595 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,0,2,1,],list[2,1,0,3,],], )

W0421 10:08:58.774933 34698 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,0,2,1,],list[2,1,0,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:58.775301 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,0,],list[2,1,],], )

W0421 10:08:58.893126 34711 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,0,],list[2,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:58.893509 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,1,0,2,],list[2,3,1,0,],], )

W0421 10:08:59.079149 34782 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,1,0,2,],list[2,3,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:59.079616 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,1,0,2,],list[3,1,0,2,],], )

W0421 10:08:59.317361 34794 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,1,0,2,],list[3,1,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:59.317804 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,1,0,],list[3,2,1,],], )

W0421 10:08:59.518239 34824 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,1,0,],list[3,2,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:59.520669 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,1,2,0,],list[2,3,1,0,],], )

W0421 10:08:59.718364 34902 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,1,2,0,],list[2,3,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:08:59.718886 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,1,2,],list[0,3,2,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:08:59.949604 34933 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,1,2,],list[0,3,2,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:08:59.953202 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,1,2,],list[2,3,1,],], )

W0421 10:09:00.151295 35143 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,1,2,],list[2,3,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:00.151861 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,1,2,],list[3,1,0,],], )

W0421 10:09:00.457669 35288 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,1,2,],list[3,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:00.458061 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,2,0,],list[2,1,0,],], )

W0421 10:09:00.670042 35518 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,2,0,],list[2,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:00.670461 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,2,0,],list[3,2,0,],], )

W0421 10:09:00.851982 35604 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,2,0,],list[3,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:00.852396 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,2,1,0,],list[0,1,3,2,],], )

W0421 10:09:01.045956 35622 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,2,1,0,],list[0,1,3,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:01.046350 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,2,1,],list[2,0,1,],], )

W0421 10:09:01.241801 35767 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,2,1,],list[2,0,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:01.242254 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,2,],list[3,1,],], )

W0421 10:09:01.433624 35778 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,2,],list[3,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:02.534324 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[0,2,3,],list[0,1,2,],], )

W0421 10:09:02.772476 35965 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[0,2,3,],list[0,1,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:06.201030 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[2,3,0,],list[1,2,0,],], )

W0421 10:09:06.450268 37216 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[2,3,0,],list[1,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:06.583680 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[2,3,1,],list[1,0,2,],], )

W0421 10:09:06.775806 37306 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[2,3,1,],list[1,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:06.776246 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[2,3,],list[1,3,],], )

W0421 10:09:06.984545 37313 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[2,3,],list[1,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:06.984973 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,0,2,1,],list[2,1,0,3,],], )

W0421 10:09:07.188334 37397 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,0,2,1,],list[2,1,0,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:07.188806 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,0,],list[2,1,],], )

W0421 10:09:07.394093 37434 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,0,],list[2,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:07.394528 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,1,0,2,],list[2,3,1,0,],], )

W0421 10:09:07.602746 37455 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,1,0,2,],list[2,3,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:07.606699 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,1,0,],list[3,2,1,],], )

W0421 10:09:07.830133 37538 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,1,0,],list[3,2,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:08.071434 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,1,2,0,],list[2,3,1,0,],], )

W0421 10:09:08.266180 37784 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,1,2,0,],list[2,3,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:08.266738 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,1,2,],list[0,3,2,],], )

W0421 10:09:08.523320 37872 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,1,2,],list[0,3,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:08.987907 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,1,2,],list[2,3,1,],], )

W0421 10:09:09.261411 38389 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,1,2,],list[2,3,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:09.263871 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,2,0,],list[2,1,0,],], )

W0421 10:09:09.449725 38403 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,2,0,],list[2,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:09.450113 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,2,0,],list[3,2,0,],], )

W0421 10:09:09.642622 38419 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,2,0,],list[3,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:09.643010 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,2,1,0,],list[0,1,3,2,],], )

W0421 10:09:09.836521 38432 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,2,1,0,],list[0,1,3,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:09.838903 test begin: paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,2,],list[3,1,],], )

W0421 10:09:10.030980 38742 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 1, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,2,],list[3,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:10.040034 test begin: paddle.tensordot(Tensor([1, 0, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[0,3,1,2,],list[3,2,1,0,],], )

W0421 10:09:10.237874 38922 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[0,3,1,2,],list[3,2,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 1, 0, 5, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:10.243394 test begin: paddle.tensordot(Tensor([1, 0, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[1,0,3,],list[2,3,0,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:09:10.430572 38933 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[1,0,3,],list[2,3,0,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:10.432204 test begin: paddle.tensordot(Tensor([1, 0, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[1,2,0,],list[1,2,3,],], )

W0421 10:09:10.685107 38948 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[1,2,0,],list[1,2,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 1, 0, 1, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:10.685599 test begin: paddle.tensordot(Tensor([1, 0, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[1,2,0,],list[1,3,2,],], )

W0421 10:09:10.875152 39028 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[1,2,0,],list[1,3,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 0, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 5, 0, 1, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:10.875520 test begin: paddle.tensordot(Tensor([1, 0, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[1,2,3,0,],list[1,3,0,2,],], )

W0421 10:09:11.114243 39042 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[1,2,3,0,],list[1,3,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 0, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 5, 0, 1, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:11.116758 test begin: paddle.tensordot(Tensor([1, 0, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[1,3,2,0,],list[1,3,2,0,],], )

W0421 10:09:11.323618 39248 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[1,3,2,0,],list[1,3,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 1, 0, 1, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:11.546798 test begin: paddle.tensordot(Tensor([1, 0, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[2,1,0,3,],list[3,1,2,0,],], )

W0421 10:09:11.740010 39549 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[2,1,0,3,],list[3,1,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 0, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 5, 0, 1, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:11.740801 test begin: paddle.tensordot(Tensor([1, 0, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[2,1,],list[0,2,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:09:12.013334 39684 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[2,1,],list[0,2,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:12.019040 test begin: paddle.tensordot(Tensor([1, 0, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[3,0,],list[2,1,],], )

W0421 10:09:12.218317 39731 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[3,0,],list[2,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 1, 0, 5, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:12.225572 test begin: paddle.tensordot(Tensor([1, 0, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[3,1,0,2,],list[3,1,0,2,],], )

W0421 10:09:12.472136 39916 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[3,1,0,2,],list[3,1,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 1, 0, 1, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:12.990013 test begin: paddle.tensordot(Tensor([1, 0, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[3,2,0,],list[3,2,0,],], )

W0421 10:09:13.219163 40415 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[3,2,0,],list[3,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 1, 0, 1, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:13.221061 test begin: paddle.tensordot(Tensor([1, 0, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[3,2,1,],list[2,0,1,],], )

W0421 10:09:13.441352 40564 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[3,2,1,],list[2,0,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 1, 0, 5, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:13.654699 test begin: paddle.tensordot(Tensor([1, 0, 5, 5],"float64"), Tensor([5, 5, 1, 5],"float64"), list[list[0,2,3,],list[0,1,2,],], )

W0421 10:09:13.880982 40616 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 5, 5],"float64"), Tensor([5, 5, 1, 5],"float64"), list[list[0,2,3,],list[0,1,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 1, 0, 5, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:13.881451 test begin: paddle.tensordot(Tensor([1, 0, 5, 5],"float64"), Tensor([5, 5, 1, 5],"float64"), list[list[0,2,3,],list[0,2,1,],], )

W0421 10:09:14.181006 40638 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 5, 5],"float64"), Tensor([5, 5, 1, 5],"float64"), list[list[0,2,3,],list[0,2,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 1, 0, 1, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:15.173090 test begin: paddle.tensordot(Tensor([1, 0, 5, 5],"float64"), Tensor([5, 5, 1, 5],"float64"), list[list[2,3,0,],list[1,2,0,],], )

W0421 10:09:15.364077 40930 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 5, 5],"float64"), Tensor([5, 5, 1, 5],"float64"), list[list[2,3,0,],list[1,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 1, 0, 5, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:15.976326 test begin: paddle.tensordot(Tensor([1, 0, 5, 5],"float64"), Tensor([5, 5, 1, 5],"float64"), list[list[3,0,],list[2,1,],], )

W0421 10:09:16.224081 41058 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 5, 5],"float64"), Tensor([5, 5, 1, 5],"float64"), list[list[3,0,],list[2,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 1, 0, 5, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:16.952179 test begin: paddle.tensordot(Tensor([1, 0, 5, 5],"float64"), Tensor([5, 5, 1, 5],"float64"), list[list[3,2,0,],list[2,1,0,],], )

W0421 10:09:17.182300 41591 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 5, 5],"float64"), Tensor([5, 5, 1, 5],"float64"), list[list[3,2,0,],list[2,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 1, 0, 5, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:17.182895 test begin: paddle.tensordot(Tensor([1, 0, 5, 5],"float64"), Tensor([5, 5, 1, 5],"float64"), list[list[3,2,0,],list[3,2,0,],], )

W0421 10:09:17.478127 41723 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 0, 5, 5],"float64"), Tensor([5, 5, 1, 5],"float64"), list[list[3,2,0,],list[3,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 1, 0, 1, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:17.694314 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[0,1,3,2,],list[0,2,1,3,],], )

W0421 10:09:17.901999 41984 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[0,1,3,2,],list[0,2,1,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 5, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:17.902432 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[0,1,3,2,],list[1,3,0,2,],], )

W0421 10:09:18.112564 42000 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[0,1,3,2,],list[1,3,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:18.117991 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[0,1,3,],list[0,3,1,],], )

W0421 10:09:18.389231 42080 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[0,1,3,],list[0,3,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 5, 0, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 1, 5, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:18.391387 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[0,2,3,],list[0,2,1,],], )

W0421 10:09:18.587780 42093 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[0,2,3,],list[0,2,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 5, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:18.813430 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,0,3,2,],list[3,0,1,2,],], )

W0421 10:09:19.069468 42193 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,0,3,2,],list[3,0,1,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 5, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:19.069903 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,0,3,],list[2,3,0,],], )

W0421 10:09:19.268736 42199 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,0,3,],list[2,3,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:19.269203 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,2,0,3,],list[0,2,1,3,],], )

W0421 10:09:19.537091 42216 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,2,0,3,],list[0,2,1,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:19.537574 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,2,0,],list[1,2,3,],], )

W0421 10:09:19.757705 42298 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,2,0,],list[1,2,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:19.758107 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,2,0,],list[1,3,2,],], )

W0421 10:09:19.961139 42383 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,2,0,],list[1,3,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:19.961603 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,2,3,0,],list[1,3,0,2,],], )

W0421 10:09:20.094609 42394 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,2,3,0,],list[1,3,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:20.094949 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,2,3,0,],list[3,2,1,0,],], )

W0421 10:09:20.336015 42410 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,2,3,0,],list[3,2,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 5, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:20.336387 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,3,0,],list[3,0,2,],], )

W0421 10:09:20.538300 42570 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,3,0,],list[3,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:20.538654 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,3,2,0,],list[1,3,2,0,],], )

W0421 10:09:20.672338 42648 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,3,2,0,],list[1,3,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:20.880753 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,3,],list[1,0,],], )

W0421 10:09:21.067117 42825 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,3,],list[1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:21.070638 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[2,0,1,],list[0,1,3,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:09:21.204648 42856 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[2,0,1,],list[0,1,3,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:21.205313 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[2,1,0,3,],list[2,0,1,3,],], )

W0421 10:09:21.431094 42946 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[2,1,0,3,],list[2,0,1,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:21.431547 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[2,1,0,3,],list[2,0,3,1,],], )

W0421 10:09:21.718674 43162 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[2,1,0,3,],list[2,0,3,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 5, 0, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 1, 5, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:21.719201 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[2,1,0,3,],list[3,1,2,0,],], )

W0421 10:09:21.923702 43382 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[2,1,0,3,],list[3,1,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:22.057644 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[2,3,0,],list[3,1,0,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:09:22.180758 43412 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[2,3,0,],list[3,1,0,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:22.186275 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[3,1,0,2,],list[3,1,0,2,],], )

W0421 10:09:22.414932 43450 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[3,1,0,2,],list[3,1,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:22.415359 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[3,1,0,],list[3,2,1,],], )

W0421 10:09:22.682086 43533 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[3,1,0,],list[3,2,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:22.819911 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[3,1,2,],list[0,3,2,],], )

W0421 10:09:22.939041 43563 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[3,1,2,],list[0,3,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:23.073215 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[3,1,2,],list[1,3,2,],], )

W0421 10:09:23.216944 43641 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[3,1,2,],list[1,3,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 5, 0, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 1, 5, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:23.218785 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[3,1,2,],list[3,1,0,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:09:23.406723 43653 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[3,1,2,],list[3,1,0,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:23.409939 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[3,2,0,],list[3,2,0,],], )

W0421 10:09:23.614504 43734 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[3,2,0,],list[3,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:23.815719 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[0,1,3,2,],list[1,3,0,2,],], )

W0421 10:09:24.020062 43772 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[0,1,3,2,],list[1,3,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:24.518909 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[0,3,1,2,],list[3,2,1,0,],], )

W0421 10:09:24.738471 43969 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[0,3,1,2,],list[3,2,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:25.135454 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[1,0,3,],list[2,3,0,],], )

W0421 10:09:25.344758 44240 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[1,0,3,],list[2,3,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:26.419576 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[1,3,0,],list[3,0,2,],], )

W0421 10:09:26.623296 44878 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[1,3,0,],list[3,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:26.623724 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[1,3,2,0,],list[1,3,2,0,],], )

W0421 10:09:26.819522 44896 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[1,3,2,0,],list[1,3,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:27.028214 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[1,3,],list[1,0,],], )

W0421 10:09:27.270459 44925 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[1,3,],list[1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:28.764733 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,0,2,1,],list[2,1,0,3,],], )

W0421 10:09:28.980980 45828 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,0,2,1,],list[2,1,0,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:28.981527 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,0,],list[2,1,],], )

W0421 10:09:29.184954 46044 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,0,],list[2,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:29.185428 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,1,0,2,],list[2,3,1,0,],], )

W0421 10:09:29.470079 46060 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,1,0,2,],list[2,3,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:29.470855 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,1,0,2,],list[3,1,0,2,],], )

W0421 10:09:29.720664 46431 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,1,0,2,],list[3,1,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:29.721173 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,1,0,],list[3,2,1,],], )

W0421 10:09:29.943724 46443 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,1,0,],list[3,2,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:30.157064 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,1,2,],list[0,3,2,],], )

W0421 10:09:30.371402 46672 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,1,2,],list[0,3,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:30.822628 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,1,2,],list[3,1,0,],], )

W0421 10:09:30.993058 47284 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,1,2,],list[3,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:30.995968 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,2,0,],list[3,2,0,],], )

W0421 10:09:31.187734 47303 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,2,0,],list[3,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:31.192860 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,2,1,],list[2,0,1,],], )

W0421 10:09:31.400645 47383 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([1, 5, 1, 1],"float64"), list[list[3,2,1,],list[2,0,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:31.407411 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[0,1,3,2,],list[1,3,0,2,],], )

W0421 10:09:31.545265 47402 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[0,1,3,2,],list[1,3,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 5, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:31.548357 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[0,1,3,],list[0,3,1,],], )

W0421 10:09:31.666869 47474 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[0,1,3,],list[0,3,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 5, 0, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 1, 5, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:31.668738 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[0,2,3,],list[0,2,1,],], )

W0421 10:09:31.787161 47492 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[0,2,3,],list[0,2,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 0, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 1, 5, 0, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:31.798446 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[1,0,3,2,],list[3,0,1,2,],], )

W0421 10:09:31.920886 47504 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[1,0,3,2,],list[3,0,1,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 5, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:32.110417 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[1,2,0,3,],list[0,2,1,3,],], )

W0421 10:09:32.241969 47568 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[1,2,0,3,],list[0,2,1,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 1, 1, 0, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:32.242438 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[1,2,0,],list[1,2,3,],], )

W0421 10:09:32.383715 47644 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[1,2,0,],list[1,2,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 5, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:32.386463 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[1,2,3,0,],list[3,2,1,0,],], )

W0421 10:09:32.502816 47659 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[1,2,3,0,],list[3,2,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 5, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:32.503334 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[1,3,0,],list[3,0,2,],], )

W0421 10:09:32.718901 47737 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[1,3,0,],list[3,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 5, 0, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 5, 5, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:32.719307 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[1,3,2,0,],list[1,3,2,0,],], )

W0421 10:09:32.926306 47751 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[1,3,2,0,],list[1,3,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 1, 1, 0, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:32.930418 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[1,3,],list[1,0,],], )

W0421 10:09:33.164065 47837 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[1,3,],list[1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 0, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 5, 1, 0, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:33.166902 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[2,1,0,3,],list[2,0,1,3,],], )

W0421 10:09:33.305634 47927 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[2,1,0,3,],list[2,0,1,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 1, 1, 0, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:33.306081 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[2,1,0,3,],list[2,0,3,1,],], )

W0421 10:09:33.506664 47942 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[2,1,0,3,],list[2,0,3,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 5, 0, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 1, 5, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:33.534184 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[3,1,0,2,],list[3,1,0,2,],], )

W0421 10:09:33.909871 48043 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[3,1,0,2,],list[3,1,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 1, 1, 0, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:34.286876 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[3,1,2,],list[0,3,2,],], )

W0421 10:09:34.493549 48577 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[3,1,2,],list[0,3,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 5, 0, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 5, 5, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:34.499407 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[3,1,2,],list[1,3,2,],], )

W0421 10:09:34.658664 48594 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[3,1,2,],list[1,3,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 5, 0, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 5, 5, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:34.666782 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[3,2,0,],list[3,2,0,],], )

W0421 10:09:34.829140 48687 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[3,2,0,],list[3,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 0, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 1, 5, 0, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:37.208693 test begin: paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([5, 5, 1, 5],"float64"), list[list[3,0,],list[2,1,],], )

W0421 10:09:37.332540 49319 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 0, 5],"float64"), Tensor([5, 5, 1, 5],"float64"), list[list[3,0,],list[2,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:38.153885 test begin: paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[0,1,2,],list[2,0,1,],], )

W0421 10:09:38.277240 49685 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[0,1,2,],list[2,0,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:38.279778 test begin: paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[0,1,3,2,],list[1,3,0,2,],], )

W0421 10:09:38.462484 49699 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[0,1,3,2,],list[1,3,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:38.462873 test begin: paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[0,1,3,2,],list[2,3,0,1,],], )

W0421 10:09:38.614890 49975 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[0,1,3,2,],list[2,3,0,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 5, 0, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 1, 5, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:38.634335 test begin: paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[0,2,3,],list[0,1,2,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:09:38.845181 49983 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[0,2,3,],list[0,1,2,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:39.133049 test begin: paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[1,0,3,2,],list[2,3,0,1,],], )

W0421 10:09:39.275485 50100 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[1,0,3,2,],list[2,3,0,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 5, 0, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 1, 5, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:39.349533 test begin: paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[1,0,3,],list[2,3,0,],], )

W0421 10:09:39.871531 50178 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[1,0,3,],list[2,3,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 5, 0, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 1, 5, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:39.871923 test begin: paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[1,2,0,3,],list[0,2,1,3,],], )

W0421 10:09:40.299474 50267 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[1,2,0,3,],list[0,2,1,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:40.299883 test begin: paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[1,2,0,],list[1,2,3,],], )

W0421 10:09:40.622040 50286 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[1,2,0,],list[1,2,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:40.622343 test begin: paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[1,2,0,],list[1,3,2,],], )

W0421 10:09:40.733139 50372 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[1,2,0,],list[1,3,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:40.733445 test begin: paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[1,2,3,0,],list[1,3,0,2,],], )

W0421 10:09:40.886137 50378 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[1,2,3,0,],list[1,3,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:41.014043 test begin: paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[1,3,2,0,],list[1,3,2,0,],], )

W0421 10:09:41.229640 50400 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[1,3,2,0,],list[1,3,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:41.486790 test begin: paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[1,3,],list[1,0,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:09:41.713672 50569 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[1,3,],list[1,0,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:41.861437 test begin: paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[2,0,1,],list[0,1,3,],], )

W0421 10:09:42.287899 50665 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[2,0,1,],list[0,1,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:42.288545 test begin: paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[2,1,0,3,],list[2,0,1,3,],], )

W0421 10:09:42.857496 50775 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[2,1,0,3,],list[2,0,1,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:43.331299 test begin: paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[2,1,],list[0,2,],], )

W0421 10:09:43.444797 51015 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[2,1,],list[0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 5, 0, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 1, 5, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:43.445183 test begin: paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[2,3,0,],list[1,2,0,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:09:43.616081 51026 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[2,3,0,],list[1,2,0,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:43.619596 test begin: paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[2,3,1,],list[1,0,2,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:09:43.785854 51037 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[2,3,1,],list[1,0,2,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:43.786194 test begin: paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[2,3,],list[1,3,],], )

W0421 10:09:44.004238 51042 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[2,3,],list[1,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:44.004676 test begin: paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[3,0,2,1,],list[2,1,0,3,],], )

W0421 10:09:44.255252 51052 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[3,0,2,1,],list[2,1,0,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:44.255776 test begin: paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[3,0,],list[2,1,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:09:44.514591 51134 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[3,0,],list[2,1,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:44.707745 test begin: paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[3,1,0,2,],list[3,1,0,2,],], )

W0421 10:09:44.903779 51199 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[3,1,0,2,],list[3,1,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:44.904259 test begin: paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[3,1,0,],list[3,2,1,],], )

W0421 10:09:45.206511 51371 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[3,1,0,],list[3,2,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:45.768160 test begin: paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[3,1,2,],list[3,1,0,],], )

W0421 10:09:45.911033 51819 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[3,1,2,],list[3,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:45.911369 test begin: paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[3,2,0,],list[2,1,0,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:09:46.091077 51827 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[3,2,0,],list[2,1,0,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:46.091455 test begin: paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[3,2,0,],list[3,2,0,],], )

W0421 10:09:46.258406 51834 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[3,2,0,],list[3,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 5, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:46.258806 test begin: paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[3,2,1,0,],list[0,1,3,2,],], )

W0421 10:09:46.447767 51846 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[3,2,1,0,],list[0,1,3,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:46.448157 test begin: paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[3,2,1,],list[2,0,1,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:09:46.645520 51856 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[3,2,1,],list[2,0,1,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:46.645921 test begin: paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[3,2,],list[3,1,],], )

W0421 10:09:46.848388 51872 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[3,2,],list[3,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:54.653271 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[0,1,2,],list[2,0,1,],], )

W0421 10:09:54.844209 54645 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[0,1,2,],list[2,0,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 5, 1, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 0, 5, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:55.386428 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[0,3,1,2,],list[3,2,1,0,],], )

W0421 10:09:55.576767 54777 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[0,3,1,2,],list[3,2,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:55.577235 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[0,3,2,1,],list[2,1,3,0,],], )

W0421 10:09:55.787034 54799 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[0,3,2,1,],list[2,1,3,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 5, 1, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 0, 5, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:55.787489 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[1,0,2,3,],list[3,0,1,2,],], )

W0421 10:09:55.925537 54832 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[1,0,2,3,],list[3,0,1,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 5, 1, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 0, 5, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:55.929703 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[1,0,3,2,],list[3,0,1,2,],], )

W0421 10:09:56.049319 54849 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[1,0,3,2,],list[3,0,1,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 5, 1, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 0, 5, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:56.366795 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[1,2,0,],list[1,2,3,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:09:56.598248 55147 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[1,2,0,],list[1,2,3,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:56.598614 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[1,2,0,],list[1,3,2,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:09:56.811285 55290 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[1,2,0,],list[1,3,2,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:56.815207 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[1,2,3,0,],list[3,2,1,0,],], )

W0421 10:09:56.979563 55324 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[1,2,3,0,],list[3,2,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 5, 1, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 0, 5, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:56.982104 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[1,3,2,0,],list[1,3,2,0,],], )

W0421 10:09:57.101347 55335 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[1,3,2,0,],list[1,3,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:57.101764 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[1,3,2,0,],list[2,1,0,3,],], )

W0421 10:09:57.295991 55363 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[1,3,2,0,],list[2,1,0,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 5, 1, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 0, 5, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:57.315286 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[2,0,1,3,],list[3,1,0,2,],], )

W0421 10:09:57.525835 55649 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[2,0,1,3,],list[3,1,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:57.657342 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[2,1,0,3,],list[2,0,1,3,],], )

W0421 10:09:57.773931 55751 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[2,1,0,3,],list[2,0,1,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 5, 1, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 0, 5, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:57.774292 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[2,1,0,3,],list[2,0,3,1,],], )

W0421 10:09:57.962205 55756 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[2,1,0,3,],list[2,0,3,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 5, 1, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 0, 5, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:58.266797 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[2,3,0,],list[1,2,0,],], )

W0421 10:09:58.471665 56067 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[2,3,0,],list[1,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:58.472143 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[2,3,0,],list[3,1,0,],], )

W0421 10:09:58.610760 56074 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[2,3,0,],list[3,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 5, 1, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 0, 5, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:58.613694 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[2,3,],list[1,3,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:09:58.804288 56087 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[2,3,],list[1,3,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:58.804620 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,0,2,1,],list[2,1,0,3,],], )

W0421 10:09:58.964296 56109 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,0,2,1,],list[2,1,0,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:58.964758 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,0,],list[2,1,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:09:59.198940 56121 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,0,],list[2,1,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:59.199280 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,1,0,2,],list[2,3,1,0,],], )

W0421 10:09:59.404292 56144 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,1,0,2,],list[2,3,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:59.404706 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,1,0,2,],list[3,1,0,2,],], )

W0421 10:09:59.648927 56219 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,1,0,2,],list[3,1,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:59.649331 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,1,0,],list[3,2,1,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:09:59.854759 56294 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,1,0,],list[3,2,1,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:09:59.855186 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,1,2,0,],list[1,2,3,0,],], )

W0421 10:09:59.987702 56309 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,1,2,0,],list[1,2,3,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 5, 1, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 0, 5, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:09:59.988096 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,1,2,0,],list[2,3,1,0,],], )

W0421 10:10:00.108220 56314 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,1,2,0,],list[2,3,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:00.110881 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,1,2,],list[1,2,3,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:00.300137 56334 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,1,2,],list[1,2,3,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:00.300459 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,1,2,],list[1,3,2,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:00.504865 56411 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,1,2,],list[1,3,2,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:00.505166 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,1,2,],list[2,3,1,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:00.806466 56489 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,1,2,],list[2,3,1,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:00.808090 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,1,2,],list[3,1,0,],], )

W0421 10:10:01.024266 56789 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,1,2,],list[3,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:01.024668 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,2,0,],list[2,1,0,],], )

W0421 10:10:01.167438 56814 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,2,0,],list[2,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 0, 1, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:01.167844 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,2,0,],list[3,2,0,],], )

W0421 10:10:01.410660 56827 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,2,0,],list[3,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 5, 1, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 0, 5, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:01.665573 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,2,],list[3,1,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:01.845356 57080 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([0, 5, 1, 1],"float64"), list[list[3,2,],list[3,1,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:01.845749 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[0,1,2,],list[2,0,1,],], )

W0421 10:10:02.039670 57155 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[0,1,2,],list[2,0,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:02.209733 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[0,1,3,2,],list[2,3,0,1,],], )

W0421 10:10:02.359108 57448 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[0,1,3,2,],list[2,3,0,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:02.363266 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[0,2,3,],list[0,1,2,],], )

W0421 10:10:02.591528 57478 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[0,2,3,],list[0,1,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:02.594706 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[0,3,1,2,],list[3,2,1,0,],], )

W0421 10:10:02.801252 57502 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[0,3,1,2,],list[3,2,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:02.806989 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[1,0,2,3,],list[3,0,1,2,],], )

W0421 10:10:02.944139 57515 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[1,0,2,3,],list[3,0,1,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:02.944500 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[1,0,3,2,],list[2,3,0,1,],], )

W0421 10:10:03.100555 57536 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[1,0,3,2,],list[2,3,0,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:03.103573 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[1,0,3,],list[2,3,0,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:03.224233 57555 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[1,0,3,],list[2,3,0,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:03.224544 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[1,2,0,3,],list[0,2,1,3,],], )

W0421 10:10:03.343489 57562 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[1,2,0,3,],list[0,2,1,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:03.826967 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[1,3,0,],list[3,0,2,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:04.031351 57718 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[1,3,0,],list[3,0,2,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:04.241346 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[1,3,2,0,],list[3,0,1,2,],], )

W0421 10:10:04.448571 57762 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[1,3,2,0,],list[3,0,1,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:04.609280 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[2,0,1,3,],list[3,1,0,2,],], )

W0421 10:10:04.880095 58007 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[2,0,1,3,],list[3,1,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:04.880626 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[2,0,1,],list[0,1,3,],], )

W0421 10:10:05.174640 58020 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[2,0,1,],list[0,1,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:05.175164 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[2,1,0,3,],list[2,0,1,3,],], )

W0421 10:10:05.429152 58251 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[2,1,0,3,],list[2,0,1,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:05.440161 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[2,1,0,3,],list[3,1,2,0,],], )

W0421 10:10:05.564364 58340 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[2,1,0,3,],list[3,1,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:05.564739 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[2,1,],list[0,2,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:05.777216 58355 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[2,1,],list[0,2,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:06.835474 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,1,0,2,],list[2,3,1,0,],], )

W0421 10:10:07.023939 59011 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,1,0,2,],list[2,3,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:07.273515 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,1,0,],list[3,2,1,],], )

W0421 10:10:07.479947 59097 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,1,0,],list[3,2,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:07.482633 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,1,2,0,],list[2,3,1,0,],], )

W0421 10:10:07.679190 59113 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,1,2,0,],list[2,3,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:07.679666 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,1,2,],list[0,3,2,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:07.872754 59123 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,1,2,],list[0,3,2,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:07.876397 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,1,2,],list[2,3,1,],], )

W0421 10:10:08.066449 59268 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,1,2,],list[2,3,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:08.478034 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,2,0,],list[3,2,0,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:08.698592 59383 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,2,0,],list[3,2,0,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:08.968520 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,2,1,],list[2,0,1,],], )

W0421 10:10:09.167266 59561 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 0, 1, 1],"float64"), list[list[3,2,1,],list[2,0,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:09.623391 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[0,1,3,2,],list[0,2,1,3,],], )

W0421 10:10:09.815740 59880 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[0,1,3,2,],list[0,2,1,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 5, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:09.816269 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[0,1,3,2,],list[1,3,0,2,],], )

W0421 10:10:09.992432 60035 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[0,1,3,2,],list[1,3,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:10.211817 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[0,1,3,],list[0,3,1,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:10.411365 60131 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[0,1,3,],list[0,3,1,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:10.414869 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[0,2,3,],list[0,2,1,],], )

W0421 10:10:10.614526 60216 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[0,2,3,],list[0,2,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 5, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:11.054589 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,0,3,2,],list[3,0,1,2,],], )

W0421 10:10:11.194607 60486 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,0,3,2,],list[3,0,1,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 5, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:11.312680 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,2,0,3,],list[0,2,1,3,],], )

W0421 10:10:11.504637 60515 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,2,0,3,],list[0,2,1,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 5, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:11.505110 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,2,0,],list[1,2,3,],], )

W0421 10:10:11.712697 60540 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,2,0,],list[1,2,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:11.713106 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,2,0,],list[1,3,2,],], )

W0421 10:10:11.958938 60562 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,2,0,],list[1,3,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:11.959541 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,2,3,0,],list[1,3,0,2,],], )

W0421 10:10:12.163295 60709 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,2,3,0,],list[1,3,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:12.163781 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,2,3,0,],list[3,2,1,0,],], )

W0421 10:10:12.358861 60742 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,2,3,0,],list[3,2,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 5, 0, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 1, 5, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:12.359346 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,3,0,],list[3,0,2,],], )

W0421 10:10:12.485060 60826 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,3,0,],list[3,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 5, 0, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 1, 5, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:12.485471 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,3,2,0,],list[1,3,2,0,],], )

W0421 10:10:12.602411 60844 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,3,2,0,],list[1,3,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:12.790809 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,3,2,0,],list[3,0,1,2,],], )

W0421 10:10:12.997673 60880 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,3,2,0,],list[3,0,1,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:12.998137 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,3,],list[1,0,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:13.125634 60963 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[1,3,],list[1,0,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:13.128263 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[2,0,1,],list[0,1,3,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:13.259831 61104 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[2,0,1,],list[0,1,3,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:13.711068 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[2,1,0,3,],list[3,1,2,0,],], )

W0421 10:10:13.859122 61422 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[2,1,0,3,],list[3,1,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:13.859575 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[2,1,],list[0,2,],], )

W0421 10:10:14.014022 61426 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[2,1,],list[0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 5, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:14.016236 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[2,3,0,],list[3,1,0,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:14.125620 61516 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[2,3,0,],list[3,1,0,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:14.125867 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[2,3,1,],list[1,0,2,],], )

W0421 10:10:14.299680 61588 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[2,3,1,],list[1,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:14.300098 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[2,3,],list[1,3,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:14.467743 61663 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[2,3,],list[1,3,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:14.472845 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[3,1,0,2,],list[3,1,0,2,],], )

W0421 10:10:14.713531 61686 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[3,1,0,2,],list[3,1,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:14.971256 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[3,1,2,],list[0,3,2,],], )

W0421 10:10:15.160852 61780 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[3,1,2,],list[0,3,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 5, 0, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 1, 5, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:15.360941 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[3,1,2,],list[1,3,2,],], )

W0421 10:10:15.500245 61937 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[3,1,2,],list[1,3,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 5, 0, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 1, 5, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:15.502398 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[3,1,2,],list[3,1,0,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:15.678917 61945 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[3,1,2,],list[3,1,0,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:15.872803 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[3,2,1,0,],list[0,1,3,2,],], )

W0421 10:10:16.086822 61970 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[3,2,1,0,],list[0,1,3,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:16.091187 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[3,2,],list[3,1,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:16.338250 62119 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 0, 1],"float64"), list[list[3,2,],list[3,1,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:16.338636 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[0,1,2,],list[2,0,1,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:16.565408 62132 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[0,1,2,],list[2,0,1,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:16.565732 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[0,1,3,2,],list[0,2,1,3,],], )

W0421 10:10:16.701550 62218 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[0,1,3,2,],list[0,2,1,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 5, 0, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 1, 5, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:16.701937 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[0,1,3,2,],list[1,3,0,2,],], )

W0421 10:10:16.854545 62236 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[0,1,3,2,],list[1,3,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:16.854938 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[0,1,3,2,],list[2,3,0,1,],], )

W0421 10:10:17.043870 62247 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[0,1,3,2,],list[2,3,0,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 5, 0, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 1, 5, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:17.044344 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[0,1,3,],list[0,3,1,],], )

W0421 10:10:17.240356 62290 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[0,1,3,],list[0,3,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 5, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:17.240958 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[0,2,3,],list[0,1,2,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:17.455916 62752 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[0,2,3,],list[0,1,2,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:17.456306 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[0,2,3,],list[0,2,1,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:17.606711 62762 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[0,2,3,],list[0,2,1,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:17.829641 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[0,3,2,1,],list[2,1,3,0,],], )

W0421 10:10:18.052335 63007 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[0,3,2,1,],list[2,1,3,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 5, 0, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 1, 5, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:18.246180 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[1,0,3,2,],list[2,3,0,1,],], )

W0421 10:10:18.510960 63108 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[1,0,3,2,],list[2,3,0,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 5, 0, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 1, 5, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:18.684385 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[1,0,3,],list[2,3,0,],], )

W0421 10:10:18.800743 63410 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[1,0,3,],list[2,3,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 5, 0, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 1, 5, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:18.803243 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[1,2,0,],list[1,2,3,],], )

W0421 10:10:18.915870 63418 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[1,2,0,],list[1,2,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:18.916238 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[1,2,0,],list[1,3,2,],], )

W0421 10:10:19.144471 63425 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[1,2,0,],list[1,3,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:19.145067 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[1,2,3,0,],list[1,3,0,2,],], )

W0421 10:10:19.388860 63508 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[1,2,3,0,],list[1,3,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:19.798272 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[1,3,2,0,],list[2,1,0,3,],], )

W0421 10:10:19.944643 63621 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[1,3,2,0,],list[2,1,0,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 5, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:20.062297 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[1,3,],list[1,0,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:20.198282 63637 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[1,3,],list[1,0,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:20.400688 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[2,0,1,],list[0,1,3,],], )

W0421 10:10:20.537598 63786 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[2,0,1,],list[0,1,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:20.540588 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[2,1,0,3,],list[2,0,3,1,],], )

W0421 10:10:20.718281 63801 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[2,1,0,3,],list[2,0,3,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 5, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:20.854719 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[2,1,],list[0,2,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:20.978768 63885 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[2,1,],list[0,2,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:20.979064 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[2,3,0,],list[1,2,0,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:21.105726 63891 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[2,3,0,],list[1,2,0,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:21.374751 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[2,3,1,],list[1,0,2,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:21.579674 64123 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[2,3,1,],list[1,0,2,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:21.582273 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[3,0,2,1,],list[2,1,0,3,],], )

W0421 10:10:21.787068 64139 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[3,0,2,1,],list[2,1,0,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:21.788150 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[3,0,],list[2,1,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:21.999830 64291 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[3,0,],list[2,1,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:22.160112 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[3,1,2,0,],list[1,2,3,0,],], )

W0421 10:10:22.350477 64482 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[3,1,2,0,],list[1,2,3,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 5, 0, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 1, 5, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:22.772606 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[3,1,2,],list[1,2,3,],], )

W0421 10:10:22.907675 64794 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[3,1,2,],list[1,2,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 5, 0, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 1, 5, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:23.357669 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[3,2,0,],list[2,1,0,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:23.544669 64973 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[3,2,0,],list[2,1,0,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:23.552525 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[3,2,1,0,],list[0,1,3,2,],], )

W0421 10:10:23.675750 64986 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[3,2,1,0,],list[0,1,3,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:23.676097 test begin: paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[3,2,1,],list[2,0,1,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:23.812593 64997 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 1, 5],"float64"), Tensor([1, 5, 1, 0],"float64"), list[list[3,2,1,],list[2,0,1,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:23.814519 test begin: paddle.tensordot(Tensor([1, 1, 5, 0],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[0,1,2,],list[2,0,1,],], )

W0421 10:10:23.923226 65007 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 0],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[0,1,2,],list[2,0,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 5, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:23.929580 test begin: paddle.tensordot(Tensor([1, 1, 5, 0],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[0,2,3,],list[0,1,2,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:24.109004 65020 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 0],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[0,2,3,],list[0,1,2,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:24.457845 test begin: paddle.tensordot(Tensor([1, 1, 5, 0],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[1,2,0,3,],list[0,2,1,3,],], )

W0421 10:10:24.645121 65184 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 0],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[1,2,0,3,],list[0,2,1,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:24.645487 test begin: paddle.tensordot(Tensor([1, 1, 5, 0],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[1,2,0,],list[1,2,3,],], )

W0421 10:10:24.776504 65203 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 0],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[1,2,0,],list[1,2,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:24.781735 test begin: paddle.tensordot(Tensor([1, 1, 5, 0],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[1,3,2,0,],list[1,3,2,0,],], )

W0421 10:10:24.903403 65229 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 0],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[1,3,2,0,],list[1,3,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:24.919079 test begin: paddle.tensordot(Tensor([1, 1, 5, 0],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[2,0,1,],list[0,1,3,],], )

W0421 10:10:25.043586 65330 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 0],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[2,0,1,],list[0,1,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 5, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:25.043900 test begin: paddle.tensordot(Tensor([1, 1, 5, 0],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[2,1,0,3,],list[2,0,1,3,],], )

W0421 10:10:25.161109 65338 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 0],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[2,1,0,3,],list[2,0,1,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:25.182393 test begin: paddle.tensordot(Tensor([1, 1, 5, 0],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[2,1,],list[0,2,],], )

W0421 10:10:25.300818 65421 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 0],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[2,1,],list[0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 5, 0, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 5, 5, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:25.301208 test begin: paddle.tensordot(Tensor([1, 1, 5, 0],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[2,3,0,],list[1,2,0,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:25.494529 65501 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 0],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[2,3,0,],list[1,2,0,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:25.691094 test begin: paddle.tensordot(Tensor([1, 1, 5, 0],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[3,0,2,1,],list[2,1,0,3,],], )

W0421 10:10:25.850559 65609 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 0],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[3,0,2,1,],list[2,1,0,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 5, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:25.851009 test begin: paddle.tensordot(Tensor([1, 1, 5, 0],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[3,0,],list[2,1,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:26.052697 65616 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 0],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[3,0,],list[2,1,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:26.286976 test begin: paddle.tensordot(Tensor([1, 1, 5, 0],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[3,1,0,2,],list[3,1,0,2,],], )

W0421 10:10:26.477566 65915 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 0],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[3,1,0,2,],list[3,1,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:26.477973 test begin: paddle.tensordot(Tensor([1, 1, 5, 0],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[3,1,0,],list[3,2,1,],], )

W0421 10:10:26.608986 65993 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 0],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[3,1,0,],list[3,2,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 5, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:27.110722 test begin: paddle.tensordot(Tensor([1, 1, 5, 0],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[3,1,2,],list[3,1,0,],], )

W0421 10:10:27.318725 66457 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 0],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[3,1,2,],list[3,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 5, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:27.319174 test begin: paddle.tensordot(Tensor([1, 1, 5, 0],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[3,2,0,],list[2,1,0,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:27.552130 66539 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 0],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[3,2,0,],list[2,1,0,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:27.552515 test begin: paddle.tensordot(Tensor([1, 1, 5, 0],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[3,2,0,],list[3,2,0,],], )

W0421 10:10:27.720698 66623 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 0],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[3,2,0,],list[3,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:27.722649 test begin: paddle.tensordot(Tensor([1, 1, 5, 0],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[3,2,1,],list[2,0,1,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:27.897248 66636 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 0],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[3,2,1,],list[2,0,1,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:29.005086 test begin: paddle.tensordot(Tensor([1, 1, 5, 0],"float64"), Tensor([5, 5, 1, 5],"float64"), list[list[1,2,0,],list[1,2,3,],], )

W0421 10:10:29.267001 66951 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 0],"float64"), Tensor([5, 5, 1, 5],"float64"), list[list[1,2,0,],list[1,2,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 1, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:31.840943 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[0,1,2,],list[2,0,1,],], )

W0421 10:10:32.010715 68670 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[0,1,2,],list[2,0,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 5, 5, Tensor layout is NCHW, Tensor stride is 25, 5, 1. New dims is 0, 5, 1, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:32.814079 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[0,3,2,1,],list[2,1,3,0,],], )

W0421 10:10:32.933496 68961 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[0,3,2,1,],list[2,1,3,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 5, 5, Tensor layout is NCHW, Tensor stride is 25, 5, 1. New dims is 0, 5, 1, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:32.933906 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[1,0,2,3,],list[3,0,1,2,],], )

W0421 10:10:33.111481 68973 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[1,0,2,3,],list[3,0,1,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 5, 1, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 0, 5, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:33.114842 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[1,0,3,2,],list[3,0,1,2,],], )

W0421 10:10:33.320101 68982 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[1,0,3,2,],list[3,0,1,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 5, 1, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 0, 5, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:33.495334 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[1,2,0,],list[1,2,3,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:33.650900 69209 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[1,2,0,],list[1,2,3,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:33.651250 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[1,2,0,],list[1,3,2,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:33.801354 69226 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[1,2,0,],list[1,3,2,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:33.807706 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[1,2,3,0,],list[3,2,1,0,],], )

W0421 10:10:34.040127 69300 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[1,2,3,0,],list[3,2,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 5, 1, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 0, 5, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:34.042486 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[1,3,2,0,],list[1,3,2,0,],], )

W0421 10:10:34.250190 69323 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[1,3,2,0,],list[1,3,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 0, 1, 1, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:34.256485 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[2,0,1,3,],list[3,1,0,2,],], )

W0421 10:10:34.484323 69472 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[2,0,1,3,],list[3,1,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 0, 1, 1, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:35.175437 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[2,3,],list[1,3,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:35.377341 69812 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[2,3,],list[1,3,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:35.380343 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[3,0,],list[2,1,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:35.582516 69891 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[3,0,],list[2,1,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:35.585309 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[3,1,0,2,],list[3,1,0,2,],], )

W0421 10:10:35.712697 69906 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[3,1,0,2,],list[3,1,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 0, 1, 1, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:35.713080 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[3,1,0,],list[3,2,1,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:35.880190 69985 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[3,1,0,],list[3,2,1,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:35.880475 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[3,1,2,0,],list[1,2,3,0,],], )

W0421 10:10:36.001323 70058 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[3,1,2,0,],list[1,2,3,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 5, 5, Tensor layout is NCHW, Tensor stride is 25, 5, 1. New dims is 0, 5, 1, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:36.001679 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[3,1,2,0,],list[2,3,1,0,],], )

W0421 10:10:36.253149 70073 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[3,1,2,0,],list[2,3,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 5, 1, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 0, 5, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:36.257528 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[3,1,2,],list[1,2,3,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:36.456717 70147 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[3,1,2,],list[1,2,3,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:36.457160 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[3,1,2,],list[1,3,2,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:36.592093 70164 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[3,1,2,],list[1,3,2,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:36.592525 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[3,1,2,],list[2,3,1,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:36.787994 70244 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[3,1,2,],list[2,3,1,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:37.093939 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[3,2,],list[3,1,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:37.273248 70353 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([0, 5, 1, 5],"float64"), list[list[3,2,],list[3,1,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:37.517677 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[0,3,1,2,],list[3,2,1,0,],], )

W0421 10:10:37.722488 70543 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[0,3,1,2,],list[3,2,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 5, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:37.728602 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[1,0,3,],list[2,3,0,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:37.907310 70682 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[1,0,3,],list[2,3,0,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:37.907657 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[1,2,0,3,],list[0,2,1,3,],], )

W0421 10:10:38.098881 70697 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[1,2,0,3,],list[0,2,1,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 1, 0, 1, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:38.689633 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[1,3,0,],list[3,0,2,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:38.825337 70957 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[1,3,0,],list[3,0,2,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:39.592161 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[2,1,0,3,],list[2,0,1,3,],], )

W0421 10:10:39.788389 71294 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[2,1,0,3,],list[2,0,1,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 0, 1, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 1, 0, 1, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:39.993182 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[2,1,],list[0,2,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:40.151780 71525 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[2,1,],list[0,2,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:40.503805 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[3,1,0,2,],list[2,3,1,0,],], )

W0421 10:10:40.677363 71694 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[3,1,0,2,],list[2,3,1,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 0, 1, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 5, 0, 1, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:40.804653 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[3,1,0,],list[3,2,1,],], )

W0421 10:10:40.925769 71705 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[3,1,0,],list[3,2,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 0, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 5, 0, 1, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:40.930377 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[3,1,2,],list[0,3,2,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:41.119649 71720 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[3,1,2,],list[0,3,2,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:41.320892 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[3,2,0,],list[3,2,0,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:41.446910 71826 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 0, 1, 5],"float64"), list[list[3,2,0,],list[3,2,0,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:41.721599 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[0,1,3,2,],list[0,2,1,3,],], )

W0421 10:10:41.941881 72048 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[0,1,3,2,],list[0,2,1,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 0, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 1, 5, 0, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:42.104601 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[0,1,3,],list[0,3,1,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:42.292536 72144 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[0,1,3,],list[0,3,1,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:42.882129 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[1,2,0,],list[1,3,2,],], )

W0421 10:10:43.040982 72413 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[1,2,0,],list[1,3,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 0, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 5, 1, 0, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:43.041449 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[1,2,3,0,],list[1,3,0,2,],], )

W0421 10:10:43.230592 72487 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[1,2,3,0,],list[1,3,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 0, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 5, 1, 0, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:43.261035 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[1,3,0,],list[3,0,2,],], )

W0421 10:10:43.528002 72637 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[1,3,0,],list[3,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 5, 0, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 5, 5, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:43.670275 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[1,3,2,0,],list[3,0,1,2,],], )

W0421 10:10:43.984342 72745 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[1,3,2,0,],list[3,0,1,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 5, 0, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 5, 5, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:43.984954 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[1,3,],list[1,0,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:44.121579 72831 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[1,3,],list[1,0,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:44.126661 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[2,0,1,],list[0,1,3,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:44.349345 72910 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[2,0,1,],list[0,1,3,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:44.355168 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[2,1,0,3,],list[3,1,2,0,],], )

W0421 10:10:44.503479 72988 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[2,1,0,3,],list[3,1,2,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 0, 5, Tensor layout is NCHW, Tensor stride is 5, 5, 1. New dims is 5, 1, 0, 5.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:44.700681 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[2,3,0,],list[3,1,0,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:44.848994 73468 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[2,3,0,],list[3,1,0,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:44.977117 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[2,3,],list[1,3,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:45.099066 73477 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[2,3,],list[1,3,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:45.697541 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[3,1,2,],list[3,1,0,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:45.823978 73595 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[3,1,2,],list[3,1,0,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:45.865037 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[3,2,1,0,],list[0,1,3,2,],], )

W0421 10:10:45.996194 73801 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[3,2,1,0,],list[0,1,3,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 5, 0, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 5, 5, 0, 1.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:45.998838 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[3,2,],list[3,1,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:46.145138 73809 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 0, 5],"float64"), list[list[3,2,],list[3,1,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:46.145415 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[0,1,2,],list[2,0,1,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:46.301784 73819 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[0,1,2,],list[2,0,1,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:46.304479 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[0,1,3,2,],list[1,3,0,2,],], )

W0421 10:10:46.441664 73892 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[0,1,3,2,],list[1,3,0,2,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 5, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:46.442045 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[0,1,3,2,],list[2,3,0,1,],], )

W0421 10:10:46.635586 73906 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[0,1,3,2,],list[2,3,0,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 5, 0, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 5, 5, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:46.636152 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[0,1,3,],list[0,3,1,],], )

W0421 10:10:46.818747 73981 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[0,1,3,],list[0,3,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 5, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:46.819179 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[0,2,3,],list[0,1,2,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:46.978821 73996 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[0,2,3,],list[0,1,2,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:46.979346 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[0,2,3,],list[0,2,1,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:47.133026 74137 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[0,2,3,],list[0,2,1,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:47.556148 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[1,0,3,2,],list[2,3,0,1,],], )

W0421 10:10:47.678499 74323 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[1,0,3,2,],list[2,3,0,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 5, 0, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 5, 5, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:47.915008 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[1,0,3,],list[2,3,0,],], )

W0421 10:10:48.137270 74417 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[1,0,3,],list[2,3,0,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 5, 0, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 5, 5, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:48.144582 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[1,2,0,],list[1,2,3,],], )

W0421 10:10:48.300220 74562 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[1,2,0,],list[1,2,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 5, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:48.710475 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[1,3,2,0,],list[2,1,0,3,],], )

W0421 10:10:48.848132 74794 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[1,3,2,0,],list[2,1,0,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 5, 0, Tensor layout is NCHW, Tensor stride is 5, 1, 1. New dims is 5, 5, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:49.085588 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[1,3,],list[1,0,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:49.345628 74819 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[1,3,],list[1,0,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:49.348146 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[2,0,1,],list[0,1,3,],], )

W0421 10:10:49.515455 74832 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[2,0,1,],list[0,1,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 5, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:49.517685 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[2,1,0,3,],list[2,0,3,1,],], )

W0421 10:10:49.637733 74909 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[2,1,0,3,],list[2,0,3,1,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 1, 5, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:49.639923 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[2,1,],list[0,2,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:49.757385 74919 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[2,1,],list[0,2,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:49.757685 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[2,3,0,],list[1,2,0,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:49.884512 74930 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[2,3,0,],list[1,2,0,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:49.897851 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[2,3,1,],list[1,0,2,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:50.018934 75009 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[2,3,1,],list[1,0,2,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:50.021635 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[3,0,2,1,],list[2,1,0,3,],], )

W0421 10:10:50.271170 75019 backward.cc:437] While running Node (SumGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[3,0,2,1,],list[2,1,0,3,],], ) 
 (InvalidArgument) Right now Resize is only supported for contiguous Tensor. Tensor dims is 5, 1, 0, Tensor layout is NCHW, Tensor stride is 1, 1, 1. New dims is 5, 1, 1, 0.
  [Hint: Expected meta_.is_contiguous() == true, but received meta_.is_contiguous():0 != true:1.] (at ../paddle/phi/core/dense_tensor_impl.cc:283)

2025-04-21 10:10:50.271709 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[3,0,],list[2,1,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:50.422324 75173 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[3,0,],list[2,1,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:51.235236 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[3,2,0,],list[2,1,0,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:51.400131 75580 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[3,2,0,],list[2,1,0,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:51.554738 test begin: paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[3,2,1,],list[2,0,1,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:51.706264 75745 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([1, 1, 5, 5],"float64"), Tensor([5, 5, 1, 0],"float64"), list[list[3,2,1,],list[2,0,1,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:54.060587 test begin: paddle.tensordot(Tensor([3, 4, 4],"float32"), Tensor([0, 4, 5],"float32"), 0, )

 ** On entry to SGEMM  parameter number 8 had an illegal value
W0421 10:10:54.264125 76519 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([3, 4, 4],"float32"), Tensor([0, 4, 5],"float32"), 0, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)

2025-04-21 10:10:54.271585 test begin: paddle.tensordot(Tensor([3, 4, 4],"float32"), Tensor([4, 0, 5],"float32"), 0, )

 ** On entry to SGEMM  parameter number 8 had an illegal value
W0421 10:10:54.474556 76554 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([3, 4, 4],"float32"), Tensor([4, 0, 5],"float32"), 0, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)

2025-04-21 10:10:54.475292 test begin: paddle.tensordot(Tensor([3, 4, 4],"float32"), Tensor([4, 0, 5],"float32"), 1, )

 ** On entry to SGEMM  parameter number 8 had an illegal value
W0421 10:10:54.648420 76631 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([3, 4, 4],"float32"), Tensor([4, 0, 5],"float32"), 1, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)

2025-04-21 10:10:54.654111 test begin: paddle.tensordot(Tensor([3, 4, 4],"float32"), Tensor([4, 4, 0],"float32"), 0, )

 ** On entry to SGEMM  parameter number 8 had an illegal value
W0421 10:10:54.772701 76706 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([3, 4, 4],"float32"), Tensor([4, 4, 0],"float32"), 0, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)

2025-04-21 10:10:54.773027 test begin: paddle.tensordot(Tensor([3, 4, 4],"float32"), Tensor([4, 4, 0],"float32"), 1, )

 ** On entry to SGEMM  parameter number 8 had an illegal value
W0421 10:10:54.916169 76781 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([3, 4, 4],"float32"), Tensor([4, 4, 0],"float32"), 1, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)

2025-04-21 10:10:54.916454 test begin: paddle.tensordot(Tensor([3, 4, 4],"float32"), Tensor([4, 4, 0],"float32"), 2, )

 ** On entry to SGEMM  parameter number 8 had an illegal value
W0421 10:10:55.069135 76924 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([3, 4, 4],"float32"), Tensor([4, 4, 0],"float32"), 2, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)

2025-04-21 10:10:55.069424 test begin: paddle.tensordot(Tensor([3, 4, 4],"float32"), Tensor([4, 4, 0],"float32"), list[list[1,2,],list[0,1,],], )

 ** On entry to SGEMM  parameter number 8 had an illegal value
W0421 10:10:55.262996 76936 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([3, 4, 4],"float32"), Tensor([4, 4, 0],"float32"), list[list[1,2,],list[0,1,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)

2025-04-21 10:10:55.263331 test begin: paddle.tensordot(Tensor([3, 4, 4],"float32"), Tensor([4, 4, 0],"float32"), tuple(list[1,2,],list[0,1,],), )

 ** On entry to SGEMM  parameter number 8 had an illegal value
W0421 10:10:55.432567 76953 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([3, 4, 4],"float32"), Tensor([4, 4, 0],"float32"), tuple(list[1,2,],list[0,1,],), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)

2025-04-21 10:10:55.432941 test begin: paddle.tensordot(Tensor([3, 4, 4],"float32"), Tensor([4, 4, 0],"float32"), tuple(tuple(1,2,),tuple(0,1,),), )

 ** On entry to SGEMM  parameter number 8 had an illegal value
W0421 10:10:55.623070 77038 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([3, 4, 4],"float32"), Tensor([4, 4, 0],"float32"), tuple(tuple(1,2,),tuple(0,1,),), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)

2025-04-21 10:10:55.623428 test begin: paddle.tensordot(Tensor([3, 4, 4],"float64"), Tensor([0, 4, 5],"float64"), 0, )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:55.836046 77133 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([3, 4, 4],"float64"), Tensor([0, 4, 5],"float64"), 0, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:55.857323 test begin: paddle.tensordot(Tensor([3, 4, 4],"float64"), Tensor([4, 0, 5],"float64"), 0, )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:56.088515 77268 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([3, 4, 4],"float64"), Tensor([4, 0, 5],"float64"), 0, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:56.088849 test begin: paddle.tensordot(Tensor([3, 4, 4],"float64"), Tensor([4, 0, 5],"float64"), 1, )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:56.292850 77351 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([3, 4, 4],"float64"), Tensor([4, 0, 5],"float64"), 1, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:56.298996 test begin: paddle.tensordot(Tensor([3, 4, 4],"float64"), Tensor([4, 4, 0],"float64"), 0, )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:56.503086 77377 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([3, 4, 4],"float64"), Tensor([4, 4, 0],"float64"), 0, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:56.503464 test begin: paddle.tensordot(Tensor([3, 4, 4],"float64"), Tensor([4, 4, 0],"float64"), 1, )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:56.713027 77528 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([3, 4, 4],"float64"), Tensor([4, 4, 0],"float64"), 1, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:56.713355 test begin: paddle.tensordot(Tensor([3, 4, 4],"float64"), Tensor([4, 4, 0],"float64"), 2, )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:56.916157 77543 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([3, 4, 4],"float64"), Tensor([4, 4, 0],"float64"), 2, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:56.916568 test begin: paddle.tensordot(Tensor([3, 4, 4],"float64"), Tensor([4, 4, 0],"float64"), list[list[1,2,],list[0,1,],], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:57.150357 77624 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([3, 4, 4],"float64"), Tensor([4, 4, 0],"float64"), list[list[1,2,],list[0,1,],], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:57.150690 test begin: paddle.tensordot(Tensor([3, 4, 4],"float64"), Tensor([4, 4, 0],"float64"), tuple(list[1,2,],list[0,1,],), )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:57.355269 77795 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([3, 4, 4],"float64"), Tensor([4, 4, 0],"float64"), tuple(list[1,2,],list[0,1,],), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:10:57.355623 test begin: paddle.tensordot(Tensor([3, 4, 4],"float64"), Tensor([4, 4, 0],"float64"), tuple(tuple(1,2,),tuple(0,1,),), )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:10:57.549990 77804 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(Tensor([3, 4, 4],"float64"), Tensor([4, 4, 0],"float64"), tuple(tuple(1,2,),tuple(0,1,),), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:01.621721 test begin: paddle.tensordot(x=Tensor([2, 3, 3, 4],"float64"), y=Tensor([0, 4, 3, 4],"float64"), axes=0, )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:11:01.831262 79174 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(x=Tensor([2, 3, 3, 4],"float64"), y=Tensor([0, 4, 3, 4],"float64"), axes=0, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:01.834918 test begin: paddle.tensordot(x=Tensor([2, 3, 3, 4],"float64"), y=Tensor([3, 0, 3, 4],"float64"), axes=0, )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:11:01.970548 79190 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(x=Tensor([2, 3, 3, 4],"float64"), y=Tensor([3, 0, 3, 4],"float64"), axes=0, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:01.970888 test begin: paddle.tensordot(x=Tensor([2, 3, 3, 4],"float64"), y=Tensor([3, 4, 0, 4],"float64"), )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:11:02.088891 79197 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(x=Tensor([2, 3, 3, 4],"float64"), y=Tensor([3, 4, 0, 4],"float64"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:02.089230 test begin: paddle.tensordot(x=Tensor([2, 3, 3, 4],"float64"), y=Tensor([3, 4, 0, 4],"float64"), axes=0, )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:11:02.311201 79272 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(x=Tensor([2, 3, 3, 4],"float64"), y=Tensor([3, 4, 0, 4],"float64"), axes=0, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:02.311507 test begin: paddle.tensordot(x=Tensor([2, 3, 3, 4],"float64"), y=Tensor([3, 4, 3, 0],"float64"), )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:11:02.531674 79304 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(x=Tensor([2, 3, 3, 4],"float64"), y=Tensor([3, 4, 3, 0],"float64"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:02.531949 test begin: paddle.tensordot(x=Tensor([2, 3, 3, 4],"float64"), y=Tensor([3, 4, 3, 0],"float64"), axes=0, )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:11:02.734853 79386 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(x=Tensor([2, 3, 3, 4],"float64"), y=Tensor([3, 4, 3, 0],"float64"), axes=0, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:03.204642 test begin: paddle.tensordot(x=Tensor([2, 3, 4, 4],"float64"), y=Tensor([0, 3, 4, 4],"float64"), axes=0, )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:11:03.405130 79698 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(x=Tensor([2, 3, 4, 4],"float64"), y=Tensor([0, 3, 4, 4],"float64"), axes=0, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:03.405482 test begin: paddle.tensordot(x=Tensor([2, 3, 4, 4],"float64"), y=Tensor([2, 0, 4, 4],"float64"), axes=0, )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:11:03.595160 79774 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(x=Tensor([2, 3, 4, 4],"float64"), y=Tensor([2, 0, 4, 4],"float64"), axes=0, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:03.595486 test begin: paddle.tensordot(x=Tensor([2, 3, 4, 4],"float64"), y=Tensor([2, 3, 0, 4],"float64"), axes=0, )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:11:03.732796 79838 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(x=Tensor([2, 3, 4, 4],"float64"), y=Tensor([2, 3, 0, 4],"float64"), axes=0, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:03.733137 test begin: paddle.tensordot(x=Tensor([2, 3, 4, 4],"float64"), y=Tensor([2, 3, 4, 0],"float64"), axes=0, )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:11:03.873924 79884 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(x=Tensor([2, 3, 4, 4],"float64"), y=Tensor([2, 3, 4, 0],"float64"), axes=0, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:04.685421 test begin: paddle.tensordot(x=Tensor([2, 7, 4, 2],"float64"), y=Tensor([7, 0, 4, 2],"float64"), axes=list[tuple(1,2,3,),tuple(0,2,3,),], )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:11:04.875694 80245 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(x=Tensor([2, 7, 4, 2],"float64"), y=Tensor([7, 0, 4, 2],"float64"), axes=list[tuple(1,2,3,),tuple(0,2,3,),], ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:04.875968 test begin: paddle.tensordot(x=Tensor([2, 7, 4, 2],"float64"), y=Tensor([7, 0, 4, 2],"float64"), axes=tuple(list[1,2,3,],list[0,2,3,],), )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:11:05.072055 80328 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(x=Tensor([2, 7, 4, 2],"float64"), y=Tensor([7, 0, 4, 2],"float64"), axes=tuple(list[1,2,3,],list[0,2,3,],), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:06.281601 test begin: paddle.tensordot(x=Tensor([3, 4, 3, 4],"float64"), y=Tensor([4, 0, 3, 4],"float64"), axes=1, )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:11:06.488101 80769 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(x=Tensor([3, 4, 3, 4],"float64"), y=Tensor([4, 0, 3, 4],"float64"), axes=1, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:06.488489 test begin: paddle.tensordot(x=Tensor([3, 4, 3, 4],"float64"), y=Tensor([4, 4, 0, 4],"float64"), axes=1, )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:11:06.721676 80790 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(x=Tensor([3, 4, 3, 4],"float64"), y=Tensor([4, 4, 0, 4],"float64"), axes=1, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:06.722016 test begin: paddle.tensordot(x=Tensor([3, 4, 3, 4],"float64"), y=Tensor([4, 4, 3, 0],"float64"), axes=1, )

 ** On entry to DGEMM  parameter number 8 had an illegal value
W0421 10:11:06.909938 80808 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.tensordot(x=Tensor([3, 4, 3, 4],"float64"), y=Tensor([4, 4, 3, 0],"float64"), axes=1, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)

2025-04-21 10:11:12.804022 test begin: paddle.tile(Tensor([0, 1],"float32"), list[1,1,49,], )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 3>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 3> const&, Eigen::DSizes<long, 6> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201472 (unix time) try "date -d @1745201472" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f971bbb70ef) received by PID 161499 (TID 0x7f953dfff700) from PID 465268975 ***]

2025-04-21 10:11:29.957196 test begin: paddle.tile(Tensor([0, 7],"float32"), list[40,1,1,], )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 3>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 3> const&, Eigen::DSizes<long, 6> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201490 (unix time) try "date -d @1745201490" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f1dcae200ef) received by PID 84302 (TID 0x7f1d37f48700) from PID 18446744072818393327 ***]

2025-04-21 10:11:41.337713 test begin: paddle.tile(Tensor([0],"float32"), list[245,1,], )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 2>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 2> const&, Eigen::DSizes<long, 4> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201501 (unix time) try "date -d @1745201501" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f77c51330ef) received by PID 89900 (TID 0x7f7747dc2700) from PID 18446744072720953583 ***]

2025-04-21 10:11:45.511975 test begin: paddle.tile(Tensor([0],"float32"), list[7,1,], )

W0421 10:11:51.783674 94991 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:11:51.784904 94991 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 2>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 2> const&, Eigen::DSizes<long, 4> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201511 (unix time) try "date -d @1745201511" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f5d855050ef) received by PID 93727 (TID 0x7f5d107c3700) from PID 18446744071651217647 ***]

2025-04-21 10:12:02.285666 test begin: paddle.tile(Tensor([1, 0, 1, 1, 1, 1],"float32"), list[1,3,4,4,1,1,], )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 6>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 6> const&, Eigen::DSizes<long, 12> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201522 (unix time) try "date -d @1745201522" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f4ecc8640ef) received by PID 97438 (TID 0x7f4e1bb85700) from PID 18446744072845934831 ***]

2025-04-21 10:12:20.414308 test begin: paddle.tile(Tensor([1, 0, 1, 1, 1, 3],"float32"), list[216,248,1,1,2,1,], )

W0421 10:12:26.723151 107371 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:12:26.724510 107371 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 6>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 6> const&, Eigen::DSizes<long, 12> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201546 (unix time) try "date -d @1745201546" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7faab89160ef) received by PID 106167 (TID 0x7faa4ddc2700) from PID 18446744072511119599 ***]

2025-04-21 10:12:30.663986 test begin: paddle.tile(Tensor([1, 0, 1, 1],"float32"), list[3,1,1,1,], )

W0421 10:12:37.772703 110760 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:12:37.773916 110760 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 4>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 4> const&, Eigen::DSizes<long, 8> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201557 (unix time) try "date -d @1745201557" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f57774de0ef) received by PID 109420 (TID 0x7f5712949700) from PID 2001592559 ***]

2025-04-21 10:12:41.566660 test begin: paddle.tile(Tensor([1, 0, 1, 64, 16],"float32"), list[1,1,4,1,1,], )

W0421 10:12:47.676096 114608 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:12:47.677281 114608 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 5>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 5> const&, Eigen::DSizes<long, 10> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201567 (unix time) try "date -d @1745201567" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f31625bc0ef) received by PID 113234 (TID 0x7f30f1abb700) from PID 1650180335 ***]

2025-04-21 10:13:13.222196 test begin: paddle.tile(Tensor([1, 0, 13, 13],"float32"), list[3,1,1,1,], )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 4>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 4> const&, Eigen::DSizes<long, 8> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201593 (unix time) try "date -d @1745201593" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f1e0fdea0ef) received by PID 122453 (TID 0x7f1d7c949700) from PID 266248431 ***]

2025-04-21 10:13:24.983854 test begin: paddle.tile(Tensor([1, 0, 2, 2],"float32"), list[1,10,1,1,], )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 4>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 4> const&, Eigen::DSizes<long, 8> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201605 (unix time) try "date -d @1745201605" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f98cb6a80ef) received by PID 127098 (TID 0x7f985a949700) from PID 18446744072827338991 ***]

2025-04-21 10:13:36.486014 test begin: paddle.tile(Tensor([1, 0, 2],"float32"), list[1,1,2,], )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 3>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 3> const&, Eigen::DSizes<long, 6> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201617 (unix time) try "date -d @1745201617" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f6a15a860ef) received by PID 131078 (TID 0x7f69ba7c3700) from PID 363356399 ***]

2025-04-21 10:13:47.162788 test begin: paddle.tile(Tensor([1, 0, 64, 64, 2],"float32"), tuple(16,1,1,1,1,), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 5>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 5> const&, Eigen::DSizes<long, 10> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201627 (unix time) try "date -d @1745201627" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7ff10ee4c0ef) received by PID 135026 (TID 0x7ff0a5f48700) from PID 249872623 ***]

2025-04-21 10:13:58.533053 test begin: paddle.tile(Tensor([1, 0],"float32"), list[256,1,], )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 2>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 2> const&, Eigen::DSizes<long, 4> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201639 (unix time) try "date -d @1745201639" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f4234a140ef) received by PID 138421 (TID 0x7f41b5dc2700) from PID 882983151 ***]

2025-04-21 10:14:03.458346 test begin: paddle.tile(Tensor([1, 0],"float32"), list[5,1,], )

W0421 10:14:10.191716 143701 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:14:10.193600 143701 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 2>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 2> const&, Eigen::DSizes<long, 4> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201650 (unix time) try "date -d @1745201650" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f21a4f890ef) received by PID 142635 (TID 0x7f212fdc2700) from PID 18446744072182337775 ***]

2025-04-21 10:14:14.249174 test begin: paddle.tile(Tensor([1, 0],"float32"), list[58,1,], )

W0421 10:14:20.650080 147192 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:14:20.651546 147192 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 2>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 2> const&, Eigen::DSizes<long, 4> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201660 (unix time) try "date -d @1745201660" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fdb8fe9f0ef) received by PID 146300 (TID 0x7fdb1734a700) from PID 18446744071829057775 ***]

2025-04-21 10:14:24.801039 test begin: paddle.tile(Tensor([1, 0],"float32"), list[64,1,], )

W0421 10:14:31.011567 151047 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:14:31.012822 151047 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 2>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 2> const&, Eigen::DSizes<long, 4> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201671 (unix time) try "date -d @1745201671" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f7f414c20ef) received by PID 149592 (TID 0x7f7ee27c3700) from PID 1095508207 ***]

2025-04-21 10:14:35.710118 test begin: paddle.tile(Tensor([1, 0],"float32"), repeat_times=list[2,1,], )

W0421 10:14:44.028906 154716 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:14:44.030639 154716 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 2>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 2> const&, Eigen::DSizes<long, 4> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201684 (unix time) try "date -d @1745201684" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f6466d1b0ef) received by PID 153192 (TID 0x7f63e1f48700) from PID 1725018351 ***]

2025-04-21 10:14:54.774892 test begin: paddle.tile(Tensor([1, 1, 0, 1, 1, 3],"float32"), list[216,248,1,1,2,1,], )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 6>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 6> const&, Eigen::DSizes<long, 12> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201695 (unix time) try "date -d @1745201695" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7ff968b5e0ef) received by PID 157916 (TID 0x7ff8fbdc2700) from PID 1756750063 ***]

2025-04-21 10:14:59.657902 test begin: paddle.tile(Tensor([1, 1, 0, 13],"float32"), list[3,1,1,1,], )

W0421 10:15:05.583518 162907 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:15:05.584846 162907 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 4>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 4> const&, Eigen::DSizes<long, 8> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201705 (unix time) try "date -d @1745201705" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fd8d75280ef) received by PID 161481 (TID 0x7fd86a949700) from PID 18446744073027092719 ***]

2025-04-21 10:15:22.991224 test begin: paddle.tile(Tensor([1, 1, 0, 1],"float32"), list[3,1,1,1,], )

W0421 10:15:29.274331  9045 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:15:29.276189  9045 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 4>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 4> const&, Eigen::DSizes<long, 8> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201729 (unix time) try "date -d @1745201729" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f46c974a0ef) received by PID 7735 (TID 0x7f46567c3700) from PID 18446744072794448111 ***]

2025-04-21 10:15:48.067424 test begin: paddle.tile(Tensor([1, 1, 0, 2],"float32"), list[1,10,1,1,], )

W0421 10:15:53.978402 16852 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:15:53.979981 16852 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 4>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 4> const&, Eigen::DSizes<long, 8> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201753 (unix time) try "date -d @1745201753" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fa131e7c0ef) received by PID 15304 (TID 0x7fa0df2b7700) from PID 837271791 ***]

2025-04-21 10:16:12.015201 test begin: paddle.tile(Tensor([1, 1, 0, 64, 2],"float32"), tuple(16,1,1,1,1,), )

W0421 10:16:18.053994 24820 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:16:18.055410 24820 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 5>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 5> const&, Eigen::DSizes<long, 10> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201778 (unix time) try "date -d @1745201778" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f2d09d3b0ef) received by PID 23374 (TID 0x7f2c9f237700) from PID 164868335 ***]

2025-04-21 10:16:46.133009 test begin: paddle.tile(Tensor([1, 1, 1, 0, 1, 3],"float32"), list[216,248,1,1,2,1,], )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 6>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 6> const&, Eigen::DSizes<long, 12> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201806 (unix time) try "date -d @1745201806" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f68730cd0ef) received by PID 32822 (TID 0x7f6805f48700) from PID 1930219759 ***]

2025-04-21 10:17:04.280510 test begin: paddle.tile(Tensor([1, 1, 1, 0],"float32"), list[3,1,1,1,], )

W0421 10:17:10.513151 44095 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:17:10.514495 44095 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 4>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 4> const&, Eigen::DSizes<long, 8> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201830 (unix time) try "date -d @1745201830" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f43894280ef) received by PID 42864 (TID 0x7f43207c3700) from PID 18446744071717421295 ***]

2025-04-21 10:17:28.479854 test begin: paddle.tile(Tensor([1, 1, 1, 1, 0, 3],"float32"), list[216,248,1,1,2,1,], )

W0421 10:17:34.674968 53035 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:17:34.676584 53035 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 6>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 6> const&, Eigen::DSizes<long, 12> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201854 (unix time) try "date -d @1745201854" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f5c02ec30ef) received by PID 51731 (TID 0x7f5b6df48700) from PID 49033455 ***]

2025-04-21 10:17:52.941037 test begin: paddle.tile(Tensor([1, 1, 1, 1, 1, 0],"float32"), list[216,248,1,1,2,1,], )

W0421 10:17:59.069200 62028 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:17:59.070515 62028 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 6>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 6> const&, Eigen::DSizes<long, 12> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201879 (unix time) try "date -d @1745201879" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fddd6b180ef) received by PID 60551 (TID 0x7fdd45f48700) from PID 18446744073016541423 ***]

2025-04-21 10:18:17.322543 test begin: paddle.tile(Tensor([1, 1, 13, 0],"float32"), list[3,1,1,1,], )

W0421 10:18:23.643338 71362 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:18:23.644713 71362 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 4>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 4> const&, Eigen::DSizes<long, 8> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201903 (unix time) try "date -d @1745201903" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f2710d520ef) received by PID 69948 (TID 0x7f26a5dc2700) from PID 282403055 ***]

2025-04-21 10:18:41.907544 test begin: paddle.tile(Tensor([1, 1, 2, 0],"float32"), list[1,10,1,1,], )

W0421 10:18:47.749841 80659 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:18:47.751045 80659 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 4>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 4> const&, Eigen::DSizes<long, 8> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201927 (unix time) try "date -d @1745201927" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fa93ed9b0ef) received by PID 79249 (TID 0x7fa8abf48700) from PID 1054453999 ***]

2025-04-21 10:19:06.276815 test begin: paddle.tile(Tensor([1, 1, 64, 0, 2],"float32"), tuple(16,1,1,1,1,), )

W0421 10:19:12.496882 90628 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:19:12.498030 90628 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 5>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 5> const&, Eigen::DSizes<long, 10> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201952 (unix time) try "date -d @1745201952" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f60a7ae30ef) received by PID 89141 (TID 0x7f6024949700) from PID 18446744072227795183 ***]

2025-04-21 10:19:29.740514 test begin: paddle.tile(Tensor([1, 1, 64, 64, 0],"float32"), tuple(16,1,1,1,1,), )

W0421 10:19:35.652508 100428 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:19:35.653798 100428 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 5>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 5> const&, Eigen::DSizes<long, 10> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745201975 (unix time) try "date -d @1745201975" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f2103e8c0ef) received by PID 98936 (TID 0x7f206f34a700) from PID 65585391 ***]

2025-04-21 10:20:01.105678 test begin: paddle.tile(Tensor([1, 192, 0],"float32"), list[1,1,2,], )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 3>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 3> const&, Eigen::DSizes<long, 6> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202001 (unix time) try "date -d @1745202001" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fe2d3ba20ef) received by PID 107486 (TID 0x7fe274949700) from PID 18446744072966775023 ***]

2025-04-21 10:20:19.017245 test begin: paddle.tile(Tensor([1, 196, 0],"float32"), list[1,1,2,], )

W0421 10:20:25.052250 118087 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:20:25.053929 118087 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 3>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 3> const&, Eigen::DSizes<long, 6> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202025 (unix time) try "date -d @1745202025" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f68fbd1d0ef) received by PID 116590 (TID 0x7f687a949700) from PID 18446744073639416047 ***]

2025-04-21 10:20:43.938382 test begin: paddle.tile(Tensor([1, 2, 0, 64, 16],"float32"), list[1,1,4,1,1,], )

W0421 10:20:50.472057 127044 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:20:50.473469 127044 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 5>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 5> const&, Eigen::DSizes<long, 10> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202050 (unix time) try "date -d @1745202050" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f4e899c00ef) received by PID 125608 (TID 0x7f4e187c3700) from PID 18446744071723286767 ***]

2025-04-21 10:21:07.772744 test begin: paddle.tile(Tensor([1, 2, 1, 0, 16],"float32"), list[1,1,4,1,1,], )

W0421 10:21:14.169239 136326 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:21:14.170449 136326 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 5>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 5> const&, Eigen::DSizes<long, 10> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202074 (unix time) try "date -d @1745202074" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f1f153430ef) received by PID 134873 (TID 0x7f1e9a7c3700) from PID 355741935 ***]

2025-04-21 10:21:32.740338 test begin: paddle.tile(Tensor([1, 2, 1, 64, 0],"float32"), list[1,1,4,1,1,], )

W0421 10:21:39.405728 146770 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:21:39.406850 146770 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 5>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 5> const&, Eigen::DSizes<long, 10> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202099 (unix time) try "date -d @1745202099" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f79d6f970ef) received by PID 145191 (TID 0x7f7979f48700) from PID 18446744073021255919 ***]

2025-04-21 10:21:43.967751 test begin: paddle.tile(Tensor([1, 3, 0, 1, 1, 1],"float32"), list[1,3,4,4,1,1,], )

W0421 10:21:50.084702 151124 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:21:50.085873 151124 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 6>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 6> const&, Eigen::DSizes<long, 12> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202110 (unix time) try "date -d @1745202110" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fb5366820ef) received by PID 149397 (TID 0x7fb4b7abb700) from PID 912793839 ***]

2025-04-21 10:21:54.269031 test begin: paddle.tile(Tensor([1, 3, 1, 0, 1, 1],"float32"), list[1,3,4,4,1,1,], )

W0421 10:22:00.703773 155153 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:22:00.705073 155153 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 6>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 6> const&, Eigen::DSizes<long, 12> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202120 (unix time) try "date -d @1745202120" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fad3af050ef) received by PID 153504 (TID 0x7fac7df48700) from PID 988827887 ***]

2025-04-21 10:22:18.504757 test begin: paddle.tile(Tensor([1, 3, 1, 1, 0, 1],"float32"), list[1,3,4,4,1,1,], )

W0421 10:22:25.103842   426 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:22:25.104957   426 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 6>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 6> const&, Eigen::DSizes<long, 12> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202145 (unix time) try "date -d @1745202145" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f47556ca0ef) received by PID 162901 (TID 0x7f46e67c3700) from PID 1433182447 ***]

2025-04-21 10:22:29.216140 test begin: paddle.tile(Tensor([1, 3, 1, 1, 1, 0],"float32"), list[1,3,4,4,1,1,], )

W0421 10:22:36.179647  4848 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:22:36.182909  4848 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 6>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 6> const&, Eigen::DSizes<long, 12> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202156 (unix time) try "date -d @1745202156" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f612e1440ef) received by PID 3209 (TID 0x7f60994f4700) from PID 773079279 ***]

2025-04-21 10:22:46.768930 test begin: paddle.tile(Tensor([13, 0, 16, 16],"float32"), repeat_times=list[1,1,4,1,], )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 4>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 4> const&, Eigen::DSizes<long, 8> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202167 (unix time) try "date -d @1745202167" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fc99ae4a0ef) received by PID 7677 (TID 0x7fc91bf48700) from PID 18446744072013258991 ***]

2025-04-21 10:23:05.623751 test begin: paddle.tile(Tensor([13, 0, 32],"float32"), list[1,1,4,], )

W0421 10:23:11.568797 18945 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:23:11.569814 18945 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 3>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 3> const&, Eigen::DSizes<long, 6> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202191 (unix time) try "date -d @1745202191" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f9bbaf380ef) received by PID 17535 (TID 0x7f9b47f48700) from PID 18446744072551104751 ***]

2025-04-21 10:23:15.574504 test begin: paddle.tile(Tensor([13, 0, 7],"float32"), repeat_times=list[4,1,1,], )

W0421 10:23:21.791848 23484 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:23:21.793108 23484 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 3>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 3> const&, Eigen::DSizes<long, 6> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202201 (unix time) try "date -d @1745202201" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f94210140ef) received by PID 22283 (TID 0x7f93bddc2700) from PID 553730287 ***]

2025-04-21 10:23:31.343652 test begin: paddle.tile(Tensor([13, 1, 0],"float32"), list[1,1,4,], )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 3>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 3> const&, Eigen::DSizes<long, 6> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202211 (unix time) try "date -d @1745202211" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f656eba10ef) received by PID 26000 (TID 0x7f650bf48700) from PID 1857687791 ***]

2025-04-21 10:23:49.685558 test begin: paddle.tile(Tensor([13, 1, 0],"float32"), repeat_times=list[4,1,1,], )

W0421 10:23:55.968479 36310 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:23:55.970041 36310 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 3>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 3> const&, Eigen::DSizes<long, 6> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202235 (unix time) try "date -d @1745202235" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f7279e630ef) received by PID 34982 (TID 0x7f71fb2b7700) from PID 2045128943 ***]

2025-04-21 10:24:00.259963 test begin: paddle.tile(Tensor([13, 2, 0, 16],"float32"), repeat_times=list[1,1,4,1,], )

W0421 10:24:05.936574 40609 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:24:05.938727 40609 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 4>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 4> const&, Eigen::DSizes<long, 8> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202245 (unix time) try "date -d @1745202245" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f23c1fc80ef) received by PID 39129 (TID 0x7f23192b7700) from PID 18446744072669135087 ***]

2025-04-21 10:24:16.439125 test begin: paddle.tile(Tensor([13, 2, 16, 0],"float32"), repeat_times=list[1,1,4,1,], )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 4>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 4> const&, Eigen::DSizes<long, 8> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202256 (unix time) try "date -d @1745202256" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f645fccc0ef) received by PID 42555 (TID 0x7f63fe949700) from PID 1607254255 ***]

2025-04-21 10:24:34.659617 test begin: paddle.tile(Tensor([13, 7, 0],"float32"), repeat_times=list[4,1,1,], )

W0421 10:24:41.835525 53176 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:24:41.836625 53176 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 3>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 3> const&, Eigen::DSizes<long, 6> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202281 (unix time) try "date -d @1745202281" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7ff26f4dc0ef) received by PID 51816 (TID 0x7ff204949700) from PID 1867366639 ***]

2025-04-21 10:24:51.753092 test begin: paddle.tile(Tensor([16, 0, 1, 1, 4],"float32"), list[1,1,64,64,1,], )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 5>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 5> const&, Eigen::DSizes<long, 10> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202292 (unix time) try "date -d @1745202292" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fec60b440ef) received by PID 55883 (TID 0x7febf1dc2700) from PID 1622425839 ***]

2025-04-21 10:24:56.185586 test begin: paddle.tile(Tensor([16, 0, 1, 3, 64, 64],"float32"), list[1,11,1,1,1,1,], )

W0421 10:25:02.355479 61431 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:25:02.356623 61431 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 6>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 6> const&, Eigen::DSizes<long, 12> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202302 (unix time) try "date -d @1745202302" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f244932c0ef) received by PID 59907 (TID 0x7f239a7c3700) from PID 1228062959 ***]

2025-04-21 10:25:19.768245 test begin: paddle.tile(Tensor([16, 0, 1, 58, 58],"float32"), list[1,1,4,1,1,], )

W0421 10:25:25.631646 71199 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:25:25.632845 71199 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 5>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 5> const&, Eigen::DSizes<long, 10> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202325 (unix time) try "date -d @1745202325" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f9bb03d70ef) received by PID 70076 (TID 0x7f9b316f8700) from PID 18446744072371400943 ***]

2025-04-21 10:25:29.669342 test begin: paddle.tile(Tensor([16, 1, 0, 3, 64, 64],"float32"), list[1,11,1,1,1,1,], )

W0421 10:25:35.845851 75110 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:25:35.846974 75110 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 6>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 6> const&, Eigen::DSizes<long, 12> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202335 (unix time) try "date -d @1745202335" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f5a2ec3a0ef) received by PID 73649 (TID 0x7f599bf48700) from PID 784572655 ***]

2025-04-21 10:25:40.243189 test begin: paddle.tile(Tensor([16, 1, 1, 0, 64, 64],"float32"), list[1,11,1,1,1,1,], )

W0421 10:25:46.158838 78799 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:25:46.160156 78799 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 6>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 6> const&, Eigen::DSizes<long, 12> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202346 (unix time) try "date -d @1745202346" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fb0e30f70ef) received by PID 77430 (TID 0x7fb047f48700) from PID 18446744073224024303 ***]

2025-04-21 10:26:03.911773 test begin: paddle.tile(Tensor([16, 1, 1, 3, 0, 64],"float32"), list[1,11,1,1,1,1,], )

W0421 10:26:09.818845 88109 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:26:09.819947 88109 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 6>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 6> const&, Eigen::DSizes<long, 12> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202369 (unix time) try "date -d @1745202369" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f66e81d50ef) received by PID 86802 (TID 0x7f66856f8700) from PID 18446744073308819695 ***]

2025-04-21 10:26:28.718229 test begin: paddle.tile(Tensor([16, 1, 1, 3, 64, 0],"float32"), list[1,11,1,1,1,1,], )

W0421 10:26:35.660902 97821 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:26:35.662681 97821 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 6>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 6> const&, Eigen::DSizes<long, 12> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202395 (unix time) try "date -d @1745202395" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f003e4420ef) received by PID 96523 (TID 0x7effc387e700) from PID 1044652271 ***]

2025-04-21 10:26:40.058835 test begin: paddle.tile(Tensor([16, 10, 0, 1, 4],"float32"), list[1,1,64,64,1,], )

W0421 10:26:46.452602 102210 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:26:46.453760 102210 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 5>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 5> const&, Eigen::DSizes<long, 10> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202406 (unix time) try "date -d @1745202406" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f3d016ad0ef) received by PID 100765 (TID 0x7f3c607c3700) from PID 23777519 ***]

2025-04-21 10:26:50.325407 test begin: paddle.tile(Tensor([16, 10, 0, 58, 58],"float32"), list[1,1,4,1,1,], )

W0421 10:26:56.216549 106476 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:26:56.217712 106476 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 5>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 5> const&, Eigen::DSizes<long, 10> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202416 (unix time) try "date -d @1745202416" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f9eed6bf0ef) received by PID 104989 (TID 0x7f9e807c3700) from PID 18446744073397858543 ***]

2025-04-21 10:27:00.547899 test begin: paddle.tile(Tensor([16, 10, 1, 0, 4],"float32"), list[1,1,64,64,1,], )

W0421 10:27:07.262104 110365 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:27:07.263281 110365 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 5>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 5> const&, Eigen::DSizes<long, 10> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202427 (unix time) try "date -d @1745202427" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f08896ec0ef) received by PID 109002 (TID 0x7f07ea7c3700) from PID 18446744071720321263 ***]

2025-04-21 10:27:11.354691 test begin: paddle.tile(Tensor([16, 10, 1, 0, 58],"float32"), list[1,1,4,1,1,], )

W0421 10:27:17.055441 115473 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:27:17.056584 115473 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 5>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 5> const&, Eigen::DSizes<long, 10> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202437 (unix time) try "date -d @1745202437" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f63b99080ef) received by PID 113058 (TID 0x7f632a7c3700) from PID 18446744072527839471 ***]

2025-04-21 10:27:20.992899 test begin: paddle.tile(Tensor([16, 10, 1, 1, 0],"float32"), list[1,1,64,64,1,], )

W0421 10:27:27.143352 118787 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:27:27.144634 118787 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 5>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 5> const&, Eigen::DSizes<long, 10> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202447 (unix time) try "date -d @1745202447" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f43e00fe0ef) received by PID 117388 (TID 0x7f434934a700) from PID 18446744073173721327 ***]

2025-04-21 10:27:31.193575 test begin: paddle.tile(Tensor([16, 10, 1, 58, 0],"float32"), list[1,1,4,1,1,], )

W0421 10:27:38.240398 122663 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:27:38.242244 122663 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 5>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 5> const&, Eigen::DSizes<long, 10> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202458 (unix time) try "date -d @1745202458" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fd2115cc0ef) received by PID 121532 (TID 0x7fd1847c3700) from PID 291291375 ***]

2025-04-21 10:27:56.829035 test begin: paddle.tile(Tensor([18, 0],"float32"), repeat_times=list[1,18,], )

W0421 10:28:03.115396 131939 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:28:03.116503 131939 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 2>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 2> const&, Eigen::DSizes<long, 4> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202483 (unix time) try "date -d @1745202483" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fc6177a30ef) received by PID 130291 (TID 0x7fc5c4949700) from PID 393883887 ***]

2025-04-21 10:28:14.605835 test begin: paddle.tile(Tensor([256, 0],"float32"), list[1,256,], )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 2>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 2> const&, Eigen::DSizes<long, 4> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202494 (unix time) try "date -d @1745202494" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fe40e0ec0ef) received by PID 134639 (TID 0x7fe3a94f4700) from PID 235847919 ***]

2025-04-21 10:28:19.457477 test begin: paddle.tile(Tensor([3, 0, 1, 64, 32],"float16"), list[1,1,8,1,1,], )

W0421 10:28:26.271472 140383 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:28:26.273346 140383 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, phi::dtype::float16, 5>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<phi::dtype::float16, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<phi::dtype::float16 const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 5> const&, Eigen::DSizes<long, 10> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202506 (unix time) try "date -d @1745202506" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fccb39a50ef) received by PID 138604 (TID 0x7fcc58949700) from PID 18446744072427819247 ***]

2025-04-21 10:28:30.582646 test begin: paddle.tile(Tensor([3, 2, 0, 64, 32],"float16"), list[1,1,8,1,1,], )

W0421 10:28:37.906857 144436 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:28:37.908332 144436 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, phi::dtype::float16, 5>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<phi::dtype::float16, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<phi::dtype::float16 const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 5> const&, Eigen::DSizes<long, 10> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202517 (unix time) try "date -d @1745202517" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f2ad1e460ef) received by PID 143229 (TID 0x7f2a6b2b7700) from PID 18446744072935989487 ***]

2025-04-21 10:28:42.401676 test begin: paddle.tile(Tensor([3, 2, 1, 0, 32],"float16"), list[1,1,8,1,1,], )

W0421 10:28:48.897348 148726 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:28:48.898533 148726 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, phi::dtype::float16, 5>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<phi::dtype::float16, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<phi::dtype::float16 const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 5> const&, Eigen::DSizes<long, 10> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202528 (unix time) try "date -d @1745202528" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f006c7300ef) received by PID 147021 (TID 0x7f0013b85700) from PID 1819476207 ***]

2025-04-21 10:29:08.740635 test begin: paddle.tile(Tensor([3, 2, 1, 64, 0],"float16"), list[1,1,8,1,1,], )

W0421 10:29:14.854878 158548 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:29:14.856927 158548 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, phi::dtype::float16, 5>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<phi::dtype::float16, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<phi::dtype::float16 const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 5> const&, Eigen::DSizes<long, 10> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202554 (unix time) try "date -d @1745202554" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7ff55f86c0ef) received by PID 157023 (TID 0x7ff4f4949700) from PID 1602666735 ***]

2025-04-21 10:29:25.795343 test begin: paddle.tile(Tensor([4, 0, 16],"float32"), list[1,4,1,], )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 3>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 3> const&, Eigen::DSizes<long, 6> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202566 (unix time) try "date -d @1745202566" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fda5afa70ef) received by PID 161954 (TID 0x7fd9edf48700) from PID 1526362351 ***]

2025-04-21 10:29:37.143993 test begin: paddle.tile(Tensor([4, 0, 1],"float32"), list[1,1,16,], )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 3>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 3> const&, Eigen::DSizes<long, 6> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202577 (unix time) try "date -d @1745202577" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fa234bd40ef) received by PID 2525 (TID 0x7fa1bbdc2700) from PID 884818159 ***]

2025-04-21 10:29:43.096385 test begin: paddle.tile(Tensor([4, 0, 1],"float32"), list[1,1,32,], )

W0421 10:29:49.764784  8572 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:29:49.767532  8572 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 3>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 3> const&, Eigen::DSizes<long, 6> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202589 (unix time) try "date -d @1745202589" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fafa7db20ef) received by PID 6576 (TID 0x7faf3e949700) from PID 18446744072230740207 ***]

2025-04-21 10:29:54.225688 test begin: paddle.tile(Tensor([4, 0, 32],"float32"), list[1,4,1,], )

W0421 10:30:00.261973 12777 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:30:00.263161 12777 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 3>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 3> const&, Eigen::DSizes<long, 6> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202600 (unix time) try "date -d @1745202600" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fefea2f30ef) received by PID 11082 (TID 0x7fef77744700) from PID 18446744073343545583 ***]

2025-04-21 10:30:04.454719 test begin: paddle.tile(Tensor([4, 0, 32],"float32"), list[1,8,1,], )

W0421 10:30:10.969585 16870 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:30:10.971016 16870 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 3>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 3> const&, Eigen::DSizes<long, 6> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202610 (unix time) try "date -d @1745202610" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f7334d9b0ef) received by PID 15327 (TID 0x7f72abdc2700) from PID 886681839 ***]

2025-04-21 10:30:15.435346 test begin: paddle.tile(Tensor([4, 0],"float32"), repeat_times=list[1,4,], )

W0421 10:30:21.630620 20741 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:30:21.631771 20741 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 2>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 2> const&, Eigen::DSizes<long, 4> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202621 (unix time) try "date -d @1745202621" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f43bc7670ef) received by PID 19008 (TID 0x7f4361b85700) from PID 18446744072576463087 ***]

2025-04-21 10:30:45.994034 test begin: paddle.tile(Tensor([4, 1, 0],"float32"), list[1,4,1,], )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 3>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 3> const&, Eigen::DSizes<long, 6> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202646 (unix time) try "date -d @1745202646" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f36a50ec0ef) received by PID 28619 (TID 0x7f363fdc2700) from PID 18446744072183791855 ***]

2025-04-21 10:31:04.350515 test begin: paddle.tile(Tensor([4, 1, 0],"float32"), list[1,8,1,], )

W0421 10:31:10.629215 40112 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:31:10.630523 40112 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 3>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 3> const&, Eigen::DSizes<long, 6> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202670 (unix time) try "date -d @1745202670" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fe97acd50ef) received by PID 38688 (TID 0x7fe8e1f48700) from PID 2060275951 ***]

2025-04-21 10:31:34.618310 test begin: paddle.tile(Tensor([4, 4, 0],"float32"), list[1,1,16,], )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 3>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 3> const&, Eigen::DSizes<long, 6> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202695 (unix time) try "date -d @1745202695" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f6df75c70ef) received by PID 49257 (TID 0x7f6d86949700) from PID 18446744073564614895 ***]

2025-04-21 10:31:52.959255 test begin: paddle.tile(Tensor([4, 4, 0],"float32"), list[1,1,32,], )

W0421 10:31:59.607839 60848 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:31:59.609028 60848 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 3>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 3> const&, Eigen::DSizes<long, 6> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202719 (unix time) try "date -d @1745202719" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f77117f90ef) received by PID 59089 (TID 0x7f76a27c3700) from PID 293572847 ***]

2025-04-21 10:32:17.388711 test begin: paddle.tile(Tensor([4, 8, 0],"float32"), list[1,1,32,], )

W0421 10:32:24.495499 71313 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:32:24.497053 71313 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 3>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 3> const&, Eigen::DSizes<long, 6> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202744 (unix time) try "date -d @1745202744" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fd0665470ef) received by PID 69627 (TID 0x7fd00f87e700) from PID 1716809967 ***]

2025-04-21 10:32:42.489798 test begin: paddle.tile(Tensor([40, 0],"float32"), list[1,1,49,], )

W0421 10:32:48.514739 81619 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:32:48.515905 81619 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 3>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 3> const&, Eigen::DSizes<long, 6> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202768 (unix time) try "date -d @1745202768" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f2a406990ef) received by PID 80093 (TID 0x7f29a7b85700) from PID 1080660207 ***]

2025-04-21 10:33:07.566773 test begin: paddle.tile(Tensor([5, 0],"float32"), list[1,5,], )

W0421 10:33:14.061470 92244 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:33:14.062667 92244 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 2>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 2> const&, Eigen::DSizes<long, 4> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202794 (unix time) try "date -d @1745202794" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fd2bf2130ef) received by PID 90632 (TID 0x7fd249f48700) from PID 18446744072621207791 ***]

2025-04-21 10:33:32.645637 test begin: paddle.tile(Tensor([5, 0],"float32"), list[8,1,], )

W0421 10:33:40.105979 103167 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:33:40.107270 103167 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 2>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 2> const&, Eigen::DSizes<long, 4> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202820 (unix time) try "date -d @1745202820" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f41debed0ef) received by PID 101429 (TID 0x7f413df48700) from PID 18446744073151631599 ***]

2025-04-21 10:33:57.721956 test begin: paddle.tile(Tensor([58, 0],"float32"), list[1,58,], )

W0421 10:34:03.557068 113568 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:34:03.558187 113568 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 2>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 2> const&, Eigen::DSizes<long, 4> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202843 (unix time) try "date -d @1745202843" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f25cd2920ef) received by PID 111960 (TID 0x7f2551dc2700) from PID 18446744072856609007 ***]

2025-04-21 10:34:21.250286 test begin: paddle.tile(Tensor([64, 0],"float32"), list[1,64,], )

W0421 10:34:27.371551 122793 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:34:27.373459 122793 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 2>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 2> const&, Eigen::DSizes<long, 4> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202867 (unix time) try "date -d @1745202867" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f1486d6f0ef) received by PID 121411 (TID 0x7f141ff48700) from PID 18446744071676817647 ***]

2025-04-21 10:34:45.871261 test begin: paddle.tile(Tensor([7, 0],"float32"), list[40,1,1,], )

W0421 10:34:51.951267 132449 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:34:51.952365 132449 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 3>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 3> const&, Eigen::DSizes<long, 6> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202891 (unix time) try "date -d @1745202891" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f4c468660ef) received by PID 130563 (TID 0x7f4bd9d0b700) from PID 1183211759 ***]

2025-04-21 10:35:18.090511 test begin: paddle.tile(Tensor([8, 0, 1],"float64"), list[1,1,100,], )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, double, 3>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<double, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<double const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 3> const&, Eigen::DSizes<long, 6> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202918 (unix time) try "date -d @1745202918" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f6372ac30ef) received by PID 140868 (TID 0x7f6199fff700) from PID 1923887343 ***]

2025-04-21 10:35:37.766040 test begin: paddle.tile(Tensor([8, 4, 0],"float64"), list[1,1,100,], )

W0421 10:35:44.149194 154097 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:35:44.150331 154097 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, double, 3>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<double, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<double const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 3> const&, Eigen::DSizes<long, 6> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202944 (unix time) try "date -d @1745202944" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f12a492d0ef) received by PID 152414 (TID 0x7f1239dc2700) from PID 18446744072175669487 ***]

2025-04-21 10:36:07.998071 test begin: paddle.tile(x=Tensor([1, 0, 2],"float64"), repeat_times=tuple(2,3,), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, double, 3>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<double, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<double const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 3> const&, Eigen::DSizes<long, 6> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202968 (unix time) try "date -d @1745202968" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f6ef41b30ef) received by PID 162647 (TID 0x7f6e8134a700) from PID 18446744073510007023 ***]

2025-04-21 10:36:26.133931 test begin: paddle.tile(x=Tensor([1, 0],"float64"), repeat_times=list[471,1,], )

W0421 10:36:32.532708 10914 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:36:32.533871 10914 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, double, 2>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<double, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<double const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 2> const&, Eigen::DSizes<long, 4> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745202992 (unix time) try "date -d @1745202992" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f721f4ee0ef) received by PID 9312 (TID 0x7f7170949700) from PID 525263087 ***]

2025-04-21 10:36:50.872132 test begin: paddle.tile(x=Tensor([1, 3, 0],"float64"), repeat_times=tuple(2,3,), )

W0421 10:36:57.190613 21015 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:36:57.191699 21015 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, double, 3>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<double, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<double const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 3> const&, Eigen::DSizes<long, 6> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745203017 (unix time) try "date -d @1745203017" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fd2cb5590ef) received by PID 19964 (TID 0x7fd254949700) from PID 18446744072825966831 ***]

2025-04-21 10:37:15.978586 test begin: paddle.tile(x=Tensor([2, 0],"float64"), repeat_times=list[3,2,], )

W0421 10:37:22.363143 32293 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0421 10:37:22.365497 32293 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   TileGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::tile_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::TileGradKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, double, 2>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<double, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<double const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 2> const&, Eigen::DSizes<long, 4> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1745203042 (unix time) try "date -d @1745203042" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f59ab9990ef) received by PID 30472 (TID 0x7f5924949700) from PID 18446744072293552367 ***]

