paddle.nn.functional.conv1d(x=Tensor([2, 3, 4],"float64"), weight=Tensor([1, 3, 3],"float64"), bias=Tensor([0],"float64"), stride=2, padding=1, )
paddle.nn.functional.softmax(Tensor([0, 7, 7],"float32"), axis=-1, )
paddle.nn.functional.pad(Tensor([1, 0, 1, 13],"float32"), pad=list[0,0,0,1,], mode="replicate", value=0.0, data_format="NCHW", name=None, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 100, 1, 64],"float16"), Tensor([2, 0, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 100],"float16"), is_causal=False, )
paddle.Tensor.tile(Tensor([1, 5, 0],"float32"), list[367,1,1,], )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 4, 128],"float16"), Tensor([1, 2048, 4, 128],"float16"), Tensor([1, 2048, 0, 128],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), dropout_p=0.0, training=True, is_causal=False, )
paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), rtol=1e-05, atol=1e-08, equal_nan=True, name="test_6", )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 0, 8, 32, 32],"float32"), output_size=list[3,3,3,], )
paddle.nn.functional.max_pool1d(Tensor([2, 0, 8],"float64"), list[3,], 1, 1, False, False, None, )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 0, 8, 32, 32],"float64"), output_size=list[2,3,3,], )
paddle.searchsorted(sorted_sequence=Tensor([7],"float32"), values=Tensor([0, 2, 2],"float32"), right=True, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 0, 4, 4],"float32"), output_size=list[3,3,], data_format="NCHW", name=None, )
paddle.nn.functional.conv1d(Tensor([16, 64, 0],"float32"), Tensor([128, 64, 3],"float32"), bias=Tensor([128],"float32"), padding=256, stride=list[1,], dilation=list[256,], groups=1, data_format="NCL", )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[617,1,1,], )
paddle.nn.functional.conv3d_transpose(Tensor([4, 4, 8, 8, 8],"float32"), Tensor([4, 3, 3, 3, 0],"float32"), Tensor([6],"float32"), output_size=None, padding=list[list[0,0,],list[0,0,],list[1,1,],list[1,1,],list[2,2,],], stride=1, dilation=1, groups=2, data_format="NCDHW", )
paddle.nn.functional.pad(Tensor([1, 0, 3],"float32"), pad=list[1,2,], mode="replicate", value=0.0, data_format="NCL", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([0, 3, 2, 2],"float32"), Tensor([3, 1, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=1, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nonzero(Tensor([0, 2, 28, 28],"float32"), )
paddle.einsum("ij,kj->ik", Tensor([4, 5],"float64"), Tensor([0, 5],"float64"), )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 0, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 0],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )
paddle.nn.functional.max_unpool1d(Tensor([0, 3, 8],"float64"), Tensor([0, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
paddle.nn.quant.weight_only_linear(Tensor([100, 512],"float16"), weight=Tensor([0, 512],"int8"), bias=Tensor([1024],"float16"), weight_scale=Tensor([1024],"float16"), weight_dtype="int8", )
paddle.nn.functional.max_pool2d(Tensor([0, 3, 224, 224],"float32"), kernel_size=2, )
paddle.Tensor.tile(Tensor([1360, 0],"float32"), list[1,34,], )
paddle.nn.functional.pad(Tensor([3, 0, 5, 6],"complex128"), pad=list[1,2,2,1,], mode="replicate", value=0.0, data_format="NCHW", name=None, )
paddle.slice(Tensor([2, 0, 104],"float32"), list[0,1,2,], list[0,0,24,], list[2,2,40,], )
paddle.nn.functional.pad(Tensor([0, 3, 4],"float32"), list[1,1,], mode="circular", data_format="NCL", )
paddle.nn.functional.pad(Tensor([0, 4, 5, 6],"complex128"), pad=list[1,2,2,1,], mode="constant", value=100, data_format="NCHW", name=None, )
paddle.vision.ops.generate_proposals(Tensor([1, 4, 0, 16],"float32"), Tensor([1, 16, 16, 16],"float32"), Tensor([1, 2],"float32"), Tensor([16, 16, 4, 4],"float32"), Tensor([16, 16, 4, 4],"float32"), pre_nms_top_n=12000, post_nms_top_n=5000, nms_thresh=0.7, min_size=3.0, eta=1.0, pixel_offset=True, return_rois_num=True, )
paddle.Tensor.tile(Tensor([1, 6, 0],"float32"), list[3240,1,1,], )
paddle.nn.functional.conv2d(Tensor([1, 128, 256, 256],"float32"), Tensor([3, 128, 1, 0],"float32"), padding=0, groups=1, )
paddle.nn.functional.l1_loss(Tensor([0, 500, 10],"float32"), Tensor([0, 500, 10],"float32"), reduction="none", )
paddle.nn.functional.conv2d(Tensor([2, 3, 37, 37],"float32"), Tensor([64, 3, 0, 7],"float32"), None, list[2,2,], 0, list[1,1,], 1, "NCHW", )
paddle.nn.functional.pad(Tensor([1, 2, 0, 4, 5],"complex128"), pad=list[1,2,1,1,3,4,], mode="constant", value=100, data_format="NCDHW", )
paddle.searchsorted(sorted_sequence=Tensor([7],"float32"), values=Tensor([2, 0, 2],"float32"), right=True, )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[210,1,1,], )
paddle.allclose(tuple(Tensor([0, 20, 100],"float32"),), tuple(Tensor([0, 20, 100],"float32"),), rtol=0.0001, atol=0.0001, )
paddle.isin(Tensor([4, 0],"bfloat16"), Tensor([2, 0],"bfloat16"), False, True, )
paddle.slice(Tensor([0, 1, 104],"float32"), list[0,1,2,], list[0,0,8,], list[16,1,24,], )
paddle.nn.functional.avg_pool2d(Tensor([0, 1536, 7, 7],"float16"), kernel_size=7, stride=1, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nanmedian(Tensor([2, 3, 0, 5],"float32"), axis=-1, keepdim=False, mode="min", )
paddle.vision.ops.generate_proposals(Tensor([1, 9, 10, 8],"float32"), Tensor([1, 0, 10, 8],"float32"), Tensor([1, 2],"float32"), Tensor([720, 4],"float32"), Tensor([720, 4],"float32"), pre_nms_top_n=4000, post_nms_top_n=4000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 0, 2],"float64"), Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=1, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float32"), weight=Tensor([1, 3, 3, 3],"float32"), bias=Tensor([0],"float32"), stride=list[2,1,], padding=1, dilation=2, )
paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 0, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 1, 50, 50],"float16"), scale=0.125, )
paddle.nn.functional.conv2d(Tensor([4, 16, 16, 3],"float32"), Tensor([5, 3, 0, 3],"float32"), Tensor([5],"float32"), padding=list[1,1,2,2,], stride=1, dilation=2, groups=1, data_format="NHWC", )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 2],"float32"), weight=Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([0],"float32"), stride=1, padding=list[1,0,1,], dilation=1, )
paddle.vision.ops.generate_proposals(Tensor([2, 3, 4, 4],"float32"), Tensor([2, 0, 4, 4],"float32"), Tensor([2, 3],"float32"), Tensor([4, 4, 3, 4],"float32"), Tensor([4, 4, 3, 4],"float32"), pre_nms_top_n=10, post_nms_top_n=5, return_rois_num=True, )
paddle.nn.functional.l1_loss(Tensor([0, 2],"float64"), label=Tensor([0, 2],"float64"), reduction="mean", name=None, )
paddle.einsum("nbka,ahc->nbkhc", Tensor([0, 3, 5, 6],"float16"), Tensor([6, 2, 4],"float16"), )
paddle.searchsorted(Tensor([1024],"float16"), Tensor([0],"float16"), )
paddle.nn.functional.pad(Tensor([0, 16, 14, 15, 384],"float16"), tuple(0,6,0,0,0,0,), data_format="NDHWC", )
paddle.Tensor.imag(Tensor([2, 20, 0, 3],"complex64"), )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 4, 128],"float16"), Tensor([1, 2048, 4, 0],"float16"), Tensor([1, 2048, 4, 128],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.conv1d_transpose(Tensor([4, 6, 16],"float32"), Tensor([6, 0, 3],"float32"), bias=Tensor([8],"float32"), output_size=list[36,], output_padding=0, padding=0, stride=list[2,], dilation=list[2,], groups=1, data_format="NCL", )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([0, 768, 16, 7, 10],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
paddle.nn.functional.max_pool2d(x=Tensor([0, 3, 32, 32],"float64"), kernel_size=list[3,3,], padding=list[1,1,], stride=list[1,1,], )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 0, 96],"float16"), Tensor([2, 101, 0, 96],"float16"), Tensor([2, 101, 0, 96],"float16"), attn_mask=None, is_causal=False, )
paddle.slice(Tensor([0, 1, 104],"float32"), list[0,1,2,], list[0,0,88,], list[13,1,104,], )
paddle.nn.functional.avg_pool2d(x=Tensor([2, 0, 32, 32],"float32"), kernel_size=list[2,2,], )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 2],"float64"), weight=Tensor([3, 1, 3, 0, 3],"float64"), bias=Tensor([3],"float64"), stride=1, padding=list[1,0,1,], groups=3, dilation=1, )
paddle.einsum("b h i d, b h j d -> b h i j", Tensor([1, 8, 1, 64],"float32"), Tensor([1, 8, 0, 64],"float32"), )
paddle.slice(Tensor([0, 1, 4],"float32"), axes=list[2,], starts=list[2,], ends=list[3,], )
paddle.nn.functional.max_pool2d(Tensor([13, 256, 7, 0],"float32"), kernel_size=1, stride=2, padding=0, )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 3, 7, 0],"float32"), output_size=list[None,3,], )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[86,1,1,], )
paddle.slice(Tensor([3, 0, 104],"float32"), list[0,1,2,], list[0,0,72,], list[3,1,88,], )
paddle.nn.functional.prelu(x=Tensor([1, 2, 0, 4],"float32"), weight=Tensor([1],"float32"), )
paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 0],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0014022786635905504, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.nn.functional.avg_pool1d(Tensor([0, 3, 32],"float32"), 2, None, 0, True, False, None, )
paddle.nn.functional.poisson_nll_loss(Tensor([0, 2],"float32"), Tensor([0, 2],"float32"), log_input=False, full=False, epsilon=1e-08, reduction="mean", name=None, )
paddle.nn.functional.lp_pool1d(Tensor([2, 3, 0],"float32"), 7.0, 2, None, 1, True, "NCL", None, )
paddle.nn.functional.fractional_max_pool2d(x=Tensor([2, 0, 7, 7],"float32"), output_size=list[3,None,], random_u=0.6, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 4, 4],"float64"), weight=Tensor([3, 0, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding="vaLiD", output_padding=0, dilation=1, groups=1, output_size=None, data_format="NCHW", )
paddle.matmul(x=Tensor([10],"float32"), y=Tensor([0],"float32"), )
paddle.nn.functional.avg_pool2d(Tensor([0, 1024, 8, 8],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.conv1d(Tensor([13, 64, 7],"float32"), Tensor([32, 64, 0],"float32"), bias=Tensor([32],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.pad(Tensor([0, 8, 14, 12],"float32"), pad=list[1,0,1,2,], mode="constant", value=0.0, data_format="NCHW", name=None, )
paddle.slice(Tensor([3, 4, 0],"float32"), axes=list[0,], starts=list[2,], ends=list[3,], )
paddle.nn.functional.pad(Tensor([1, 1, 100, 0],"float32"), pad=list[0,1,0,0,], mode="constant", value=-10000.0, data_format="NCHW", name=None, )
paddle.polar(Tensor([0],"float64"), Tensor([0],"float64"), )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 1024, 14, 0],"float32"), output_size=1, data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([1, 2, 3, 0, 5],"complex64"), pad=list[1,2,1,1,3,4,], mode="constant", value=100, data_format="NDHWC", )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
paddle.nn.functional.pad(Tensor([1024, 0, 1, 1],"float32"), list[2,2,2,2,], )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 100, 1, 64],"float16"), Tensor([2, 100, 0, 64],"float16"), attn_mask=Tensor([2, 1, 1, 100],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.poisson_nll_loss(Tensor([0, 2],"float32"), Tensor([0, 2],"float32"), log_input=True, full=False, epsilon=1e-08, reduction="mean", name=None, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[287,1,1,], )
paddle.nn.functional.pad(Tensor([0, 3, 4, 4],"float64"), list[1,1,1,1,], mode="replicate", data_format="NCHW", )
paddle.nn.functional.max_pool3d(x=Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], ceil_mode=True, )
paddle.nn.functional.pad(Tensor([4, 128, 0, 70],"float32"), pad=list[1,1,1,1,], mode="constant", value=0.0, data_format="NCHW", name=None, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([0, 3, 7, 7],"float32"), output_size=5, data_format="NCHW", name=None, )
paddle.nn.functional.avg_pool2d(Tensor([1, 0, 10, 10],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.temporal_shift(Tensor([0, 1024, 14, 14],"float32"), 8, 0.125, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([0, 128, 80, 80],"float32"), Tensor([128, 128, 2, 2],"float32"), bias=Tensor([128],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.fill_diagonal_(Tensor([3, 0, 3],"float64"), 1, offset=0, wrap=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 1, 2, 40],"float16"), Tensor([1, 1, 2, 40],"float16"), Tensor([1, 0, 2, 40],"float16"), attn_mask=Tensor([1, 2, 1, 1],"float16"), )
paddle.einsum("ijk, ikl->ijl", Tensor([3, 10, 0],"float64"), Tensor([3, 3, 0],"float64"), )
paddle.nn.functional.sequence_mask(Tensor([0, 3],"int64"), maxlen=30, dtype=type(numpy.int32), )
paddle.einsum("blq,bhlk->bhlqk", Tensor([1, 3, 5],"float64"), Tensor([0, 2, 3, 4],"float64"), )
paddle.nn.functional.hinge_embedding_loss(Tensor([4, 3, 0],"float64"), Tensor([4, 3, 0],"float64"), reduction="mean", margin=-4.0, name=None, )
paddle.Tensor.tile(Tensor([1, 7, 0],"float32"), list[385,1,1,], )
paddle.nanmedian(Tensor([0, 3, 4, 5],"float32"), axis=tuple(1,2,3,), keepdim=False, )
paddle.nn.functional.max_pool2d(Tensor([2, 0, 32, 32],"float64"), kernel_size=list[3,3,], stride=None, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([0, 512, 2, 2],"float32"), output_size=tuple(7,7,), data_format="NCHW", name=None, )
paddle.nn.functional.conv2d(Tensor([64, 1, 0, 28],"float32"), Tensor([6, 1, 0, 3],"float32"), bias=None, padding=1, stride=list[1,1,], dilation=list[1,1,], groups=1, data_format="NCHW", )
paddle.nn.functional.conv2d(Tensor([8, 256, 128, 128],"float32"), Tensor([256, 256, 3, 0],"float32"), bias=None, stride=1, padding=1, )
paddle.nn.functional.max_pool3d(Tensor([1, 5, 6, 8, 0],"float32"), list[3,3,3,], stride=list[1,1,1,], padding=1, data_format="NDHWC", )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[1662,1,1,], )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 0, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
paddle.nn.functional.max_pool2d(Tensor([2, 0, 40, 40],"float64"), kernel_size=2, stride=None, padding=0, return_mask=True, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.max_pool2d(Tensor([1, 0, 112, 112],"float32"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nonzero(x=Tensor([0],"float64"), as_tuple=False, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 0, 32, 32],"float32"), output_size=tuple(2,2,), data_format="NCHW", name=None, )
paddle.nn.functional.avg_pool2d(x=Tensor([0, 3, 4, 4],"float64"), kernel_size=list[3,3,], stride=list[1,1,], padding=list[1,1,], exclusive=False, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 0, 2],"float64"), Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[2,2,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 16, 16, 4],"float32"), Tensor([4, 3, 0, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[list[0,0,],list[1,2,],list[2,1,],list[0,0,],], stride=1, dilation=1, groups=2, data_format="NHWC", )
paddle.nn.functional.temporal_shift(Tensor([240, 1024, 14, 0],"float32"), 8, 0.125, data_format="NCHW", )
paddle.nn.functional.softmax(Tensor([128, 1, 0, 49, 49],"float16"), -1, name=None, )
paddle.nn.functional.avg_pool2d(Tensor([0, 128, 8, 50],"float32"), kernel_size=tuple(2,1,), stride=tuple(2,1,), padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([1, 16, 14, 14, 0],"float16"), tuple(0,0,0,0,0,0,), data_format="NDHWC", )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float64"), weight=Tensor([1, 3, 3, 0],"float64"), bias=Tensor([1],"float64"), stride=list[2,1,], padding=1, dilation=2, )
paddle.nn.functional.max_pool3d(x=Tensor([2, 3, 8, 0, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,2,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([4, 16, 16, 4],"float32"), Tensor([4, 3, 0, 3],"float32"), Tensor([6],"float32"), output_size=None, padding="same", stride=1, dilation=1, groups=2, data_format="NHWC", )
paddle.nn.functional.conv2d_transpose(Tensor([1, 1024, 128, 0],"float32"), Tensor([1024, 128, 3, 3],"float32"), padding=0, stride=2, groups=4, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[161,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([4, 3, 7, 7],"float32"), Tensor([3, 0, 5, 5],"float32"), bias=Tensor([6],"float32"), padding=2, output_padding=list[1,1,], stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 32, 0],"float64"), output_size=list[1,3,2,], )
paddle.nn.functional.conv1d_transpose(Tensor([4, 16, 6],"float32"), Tensor([6, 8, 0],"float32"), bias=Tensor([8],"float32"), output_size=None, output_padding=0, padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NLC", )
paddle.nn.functional.conv2d(Tensor([1024, 1, 131, 131],"float32"), Tensor([1, 1, 4, 0],"float32"), )
paddle.nn.functional.avg_pool3d(Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,1,1,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
paddle.matmul(Tensor([2, 2],"float32"), Tensor([50000, 2, 0],"float32"), )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float32"), Tensor([3, 0, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=1, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.pad(Tensor([1, 0, 14, 15, 384],"float16"), tuple(0,1,0,0,0,0,), data_format="NDHWC", )
paddle.nn.functional.pad(Tensor([3, 4, 5, 0, 7],"complex64"), pad=list[1,2,2,1,1,0,], mode="constant", value=100, data_format="NCDHW", name=None, )
paddle.nn.functional.pad(Tensor([3, 0, 5, 6, 7],"complex64"), pad=list[1,2,2,1,1,0,], mode="replicate", value=0.0, data_format="NCDHW", name=None, )
paddle.nn.functional.conv2d_transpose(x=Tensor([0, 3, 2, 2],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=0, dilation=1, )
paddle.nanmedian(Tensor([2, 3, 4, 0],"float32"), axis=tuple(1,2,), keepdim=False, mode="min", )
paddle.Tensor.lgamma(Tensor([0, 3],"float32"), )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 32, 0],"float64"), output_size=tuple(3,3,3,), data_format="NDHWC", name=None, )
paddle.nn.functional.avg_pool2d(x=Tensor([0, 3, 4, 4],"float64"), kernel_size=list[3,3,], stride=tuple(1,1,), padding=list[0,0,], )
paddle.nn.functional.pad(Tensor([1024, 1, 0, 256],"float32"), list[2,2,2,2,], )
paddle.allclose(tuple(Tensor([13, 7, 0],"float32"),Tensor([13, 7, 0],"float32"),), tuple(Tensor([13, 7, 0],"float32"),Tensor([13, 7, 0],"float32"),), rtol=0.0001, atol=0.0001, )
paddle.nn.functional.max_pool2d(Tensor([2, 3, 0, 32],"float32"), kernel_size=2, stride=2, padding=1, ceil_mode=False, )
paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 0, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,0,0,], exclusive=True, )
paddle.strided_slice(x=Tensor([5, 8, 6, 4, 0, 6],"float64"), axes=list[1,2,5,], starts=list[-3,3,4,], ends=list[3,0,1,], strides=list[-1,-1,-2,], )
paddle.nn.functional.conv2d_transpose(Tensor([2, 96, 2, 2],"float32"), Tensor([96, 96, 4, 0],"float32"), bias=Tensor([96],"float32"), padding=0, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.layer_norm(Tensor([0, 6, 6, 3],"float64"), list[6,6,3,], weight=Tensor([108],"float64"), bias=None, epsilon=1e-05, )
paddle.kthvalue(Tensor([0, 128, 10],"float64"), 2, -1, )
paddle.nn.functional.pad(Tensor([0, 2, 3],"float64"), pad=list[2,1,], mode="replicate", value=0.0, data_format="NCL", name=None, )
paddle.nn.functional.max_unpool3d(Tensor([0, 3, 2, 2, 3],"float32"), Tensor([0, 3, 2, 2, 3],"int64"), kernel_size=2, stride=2, output_size=list[1,3,4,4,6,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[1376,1,1,], )
paddle.nn.functional.conv2d(Tensor([1, 1024, 0, 256],"float32"), Tensor([24, 128, 1, 1],"float32"), padding=0, groups=8, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 0],"float32"), Tensor([6, 8, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[2,2,2,], dilation=list[2,1,2,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 2, 2, 2, 3],"float64"), Tensor([3, 1, 3, 0, 3],"float64"), bias=Tensor([3],"float64"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=3, output_size=None, data_format="NDHWC", )
paddle.nn.functional.max_pool2d(Tensor([2, 0, 40, 40],"float64"), kernel_size=4, stride=2, padding=2, return_mask=True, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.max_pool3d(x=Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=1, )
paddle.nn.functional.conv3d(Tensor([4, 3, 0, 8, 8],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[0,0,],list[1,1,],list[2,2,],list[2,2,],], stride=1, dilation=1, groups=1, data_format="NCDHW", )
paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 0, 768],"float16"), Tensor([768, 3072],"float16"), Tensor([3072, 768],"float16"), Tensor([3072],"float16"), Tensor([768],"float16"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 3, 4, 0],"float64"), output_size=list[3,3,], data_format="NCHW", name=None, )
paddle.frexp(Tensor([0, 12],"float64"), )
paddle.vision.ops.generate_proposals(Tensor([1, 9, 0, 9],"float32"), Tensor([1, 36, 12, 9],"float32"), Tensor([1, 2],"float32"), Tensor([972, 4],"float32"), Tensor([972, 4],"float32"), pre_nms_top_n=4000, post_nms_top_n=4000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.slice(Tensor([4, 20, 0],"float16"), list[0,], list[2,], list[4,], )
paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), atol=0.0001, )
paddle.nn.functional.avg_pool2d(Tensor([0, 3, 32, 32],"float32"), kernel_size=2, stride=None, padding="SAME", )
paddle.nn.functional.pad(Tensor([1, 16, 31, 28, 0],"float16"), tuple(0,0,0,4,0,0,), data_format="NDHWC", )
paddle.nn.functional.softmax(Tensor([0, 16, 128],"float64"), 1, )
paddle.slice(Tensor([2, 10, 0],"float32"), list[0,], list[1,], list[2,], )
paddle.nn.functional.max_pool2d(Tensor([0, 3, 32, 32],"float64"), kernel_size=list[3,3,], stride=list[1,1,], padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.Tensor.fill_diagonal_(Tensor([0, 3],"float32"), 4, 0, True, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([1, 3, 0, 2, 2],"float32"), bias=Tensor([1],"float32"), stride=1, padding=0, )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[300,1,1,], )
paddle.nn.functional.conv2d(Tensor([1, 256, 128, 128],"float32"), Tensor([3, 256, 1, 0],"float32"), padding=0, groups=1, )
paddle.nn.functional.max_pool3d(Tensor([0, 4, 4, 4, 4],"float32"), list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], data_format="NDHWC", )
paddle.subtract(Tensor([0],"float64"), Tensor([0],"float64"), name="Cauchy_log_prob", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 3, 16, 16],"float32"), Tensor([3, 0, 3, 3],"float32"), None, output_size=None, padding=0, stride=1, dilation=1, groups=1, data_format="NCHW", )
paddle.nn.functional.avg_pool3d(Tensor([2, 0, 32, 32, 32],"float32"), kernel_size=2, stride=None, padding="SAME", ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 8, 3, 0, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[1,2,1,], dilation=list[2,2,2,], groups=1, output_size=None, data_format="NCDHW", )
paddle.subtract(x=Tensor([0, 2],"float64"), y=Tensor([0, 2],"float64"), )
paddle.nn.functional.pixel_shuffle(Tensor([2, 4, 0, 9],"float64"), 3, "NHWC", )
paddle.nn.functional.conv2d(Tensor([8, 128, 257, 0],"float32"), Tensor([256, 128, 3, 3],"float32"), bias=None, stride=2, padding=0, )
paddle.Tensor.tile(Tensor([1, 6, 0],"float32"), list[423,1,1,], )
paddle.nn.functional.conv2d(Tensor([64, 1, 0, 28],"float32"), Tensor([6, 1, 3, 3],"float32"), bias=None, padding=1, stride=list[1,1,], dilation=list[1,1,], groups=1, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([12, 1, 16, 0],"float32"), list[2,1,2,1,], )
paddle.nn.functional.conv3d_transpose(Tensor([2, 2, 2, 2, 3],"float64"), Tensor([3, 0, 3, 3, 3],"float64"), bias=Tensor([3],"float64"), padding=list[1,1,1,], output_padding=1, stride=list[2,2,2,], dilation=list[1,1,1,], groups=3, output_size=None, data_format="NDHWC", )
paddle.nn.functional.pad(Tensor([0, 3, 28, 24],"float32"), pad=list[1,1,2,2,], mode="reflect", value=0.0, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[140,1,1,], )
paddle.nn.functional.max_unpool2d(Tensor([0, 3, 3, 3],"float32"), Tensor([0, 3, 3, 3],"int32"), kernel_size=2, padding=0, output_size=list[7,7,], )
paddle.trace(x=Tensor([2, 0, 2],"float64"), offset=1, axis1=0, axis2=2, )
paddle.nn.functional.conv3d_transpose(Tensor([0, 3, 2, 2, 2],"float32"), Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([3],"float32"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=3, output_size=None, data_format="NCDHW", )
paddle.nn.functional.conv2d(Tensor([2, 48, 4, 4],"float32"), Tensor([48, 48, 3, 0],"float32"), None, list[1,1,], 1, list[1,1,], 1, "NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([2, 2, 3, 0],"float64"), Tensor([2, 2, 1, 1],"float64"), groups=1, padding="SAME", )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([0, 3, 8, 32, 32],"float64"), output_size=3, )
paddle.nn.functional.pad(Tensor([1, 3, 0, 239, 224],"float32"), tuple(0,0,0,1,0,0,), data_format="NCDHW", )
paddle.einsum("blq,bhlk->bhlqk", Tensor([1, 3, 0],"float64"), Tensor([1, 2, 3, 4],"float64"), )
paddle.slice(Tensor([16, 0, 104],"float32"), list[0,1,2,], list[0,0,56,], list[16,1,72,], )
paddle.nn.functional.conv3d_transpose(Tensor([0, 6, 8, 8, 8],"float32"), Tensor([6, 8, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[2,2,2,], dilation=list[2,1,2,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.conv1d_transpose(Tensor([4, 6, 16],"float32"), Tensor([6, 0, 3],"float32"), bias=Tensor([8],"float32"), output_size=None, output_padding=0, padding=0, stride=list[2,], dilation=list[2,], groups=1, data_format="NCL", )
paddle.nn.functional.kl_div(Tensor([5, 0],"float64"), Tensor([5, 0],"float64"), "sum", False, )
paddle.Tensor.trunc(Tensor([8, 0],"float32"), )
paddle.nn.functional.scaled_dot_product_attention(query=Tensor([2, 64, 12, 0],"float16"), key=Tensor([2, 64, 12, 0],"float16"), value=Tensor([2, 64, 12, 0],"float16"), is_causal=True, )
paddle.nn.functional.conv1d(Tensor([2, 3, 4],"float64"), Tensor([1, 3, 3],"float64"), bias=Tensor([0],"float64"), padding=0, stride=list[2,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.vision.ops.prior_box(Tensor([4, 48, 80, 0],"float32"), Tensor([4, 3, 640, 0],"float32"), list[16.0,24.0,], list[], list[1.0,], list[0.1,0.1,0.2,0.2,], False, False, list[8.0,8.0,], 0.5, False, None, )
paddle.logcumsumexp(Tensor([10, 0],"float32"), dtype="float32", axis=1, )
paddle.nn.functional.max_pool2d(Tensor([2, 0, 40, 40],"float64"), kernel_size=tuple(2,4,), stride=None, padding=0, return_mask=True, ceil_mode=False, data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[804,1,1,], )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[80,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([1, 2048, 0, 128],"float32"), Tensor([2048, 128, 3, 3],"float32"), padding=0, stride=2, groups=8, )
paddle.nn.functional.hinge_embedding_loss(Tensor([10, 0, 5],"float64"), Tensor([10, 0, 5],"float64"), )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float64"), weight=Tensor([1, 3, 3, 0],"float64"), bias=Tensor([1],"float64"), stride=list[2,1,], padding=1, dilation=list[2,2,], )
paddle.nn.functional.pad(Tensor([1, 1, 2, 0],"float32"), pad=list[0,1,0,0,], mode="constant", value=-1000000.0, data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([0, 4, 5, 6, 7],"complex64"), pad=list[1,2,2,1,1,0,], mode="reflect", value=0.0, data_format="NCDHW", name=None, )
paddle.nn.functional.sequence_mask(Tensor([2, 0],"int64"), maxlen=Tensor([1],"int32"), dtype=VarType(int64), )
paddle.incubate.softmax_mask_fuse(Tensor([0, 1, 8, 32],"float32"), Tensor([0, 1, 8, 32],"float32"), )
paddle.einsum("...i->...", Tensor([0, 3, 10],"float64"), )
paddle.frexp(Tensor([0, 5, 2],"float32"), )
paddle.nn.functional.pad(Tensor([1, 3, 280, 0],"float32"), pad=list[2,2,2,2,], mode="constant", value=0.0, data_format="NCHW", )
paddle.nn.functional.conv2d(Tensor([1, 3, 0, 224],"float32"), Tensor([3, 3, 3, 3],"float32"), Tensor([3],"float32"), list[3,2,], 0, list[1,1,], 1, "NCHW", )
paddle.matmul(Tensor([1, 3, 2, 5, 5],"float16"), Tensor([0, 3, 2, 5, 4],"float16"), )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float64"), weight=Tensor([1, 3, 3, 0],"float64"), bias=Tensor([1],"float64"), stride=list[2,1,], padding=1, dilation=tuple(2,2,), )
paddle.einsum("ak, kn-> an", Tensor([0, 11],"float32"), Tensor([11, 50],"float32"), )
paddle.Tensor.tile(Tensor([1, 300, 0],"float16"), list[2,1,1,], )
paddle.roll(Tensor([4, 0, 4, 4],"complex128"), Tensor([1],"int64"), 3, name=None, )
paddle.Tensor.tile(Tensor([1, 0, 4, 4],"float32"), tuple(4,1,1,1,), )
paddle.nn.functional.max_unpool2d(Tensor([0, 1, 2, 2],"float32"), Tensor([0, 1, 2, 2],"int32"), kernel_size=2, stride=None, output_size=tuple(5,5,), )
paddle.nn.functional.layer_norm(Tensor([4, 10, 0, 4],"float32"), 4, )
paddle.Tensor.fill_diagonal_(Tensor([0, 3, 3],"float64"), 1, offset=0, wrap=True, )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 0, 7, 7],"float32"), output_size=list[3,3,], data_format="NHWC", )
paddle.nn.functional.max_pool3d(x=Tensor([0, 8, 8, 8, 3],"float64"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], data_format="NDHWC", )
paddle.nn.functional.pixel_unshuffle(Tensor([2, 0, 12, 12],"float64"), 3, "NCHW", None, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 16, 16, 6],"float32"), Tensor([6, 0, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=list[list[0,0,],list[1,1,],list[2,2,],list[0,0,],], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NHWC", )
paddle.vision.ops.generate_proposals(Tensor([1, 3, 10, 15],"float32"), Tensor([0, 12, 10, 15],"float32"), Tensor([1, 2],"float32"), Tensor([450, 4],"float32"), Tensor([450, 4],"float32"), pre_nms_top_n=2000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 0, 8, 8],"float32"), Tensor([6, 4, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding="valid", output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=2, output_size=None, data_format="NCDHW", )
paddle.nn.functional.fractional_max_pool3d(x=Tensor([0, 3, 7, 7, 7],"float64"), output_size=list[3,3,3,], random_u=None, )
paddle.Tensor.tile(Tensor([1, 5, 0],"float32"), list[238,1,1,], )
paddle.einsum("ij,kl->ijkl", Tensor([0, 5],"float64"), Tensor([0, 7],"float64"), )
paddle.nn.functional.max_pool3d(Tensor([0, 3, 4, 4, 6],"float64"), kernel_size=2, stride=2, padding=0, return_mask=True, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.einsum("bhlqk,bhkd->bhlqd", Tensor([52, 4, 3, 1, 1],"float32"), Tensor([52, 4, 1, 0],"float32"), )
paddle.Tensor.lgamma(Tensor([5, 0, 8],"float64"), )
paddle.nn.functional.softmax(Tensor([16, 16, 0],"float64"), 2, )
paddle.strided_slice(x=Tensor([3, 4, 0, 6],"float64"), axes=list[1,2,3,], starts=list[-3,0,2,], ends=list[3,2,4,], strides=list[1,1,1,], )
paddle.allclose(Tensor([13, 0, 128],"float32"), Tensor([13, 0, 128],"float32"), rtol=0.0001, atol=0.0001, )
paddle.nn.functional.pad(Tensor([1, 0, 3, 4, 5],"complex64"), pad=list[1,2,1,1,3,4,], mode="constant", value=100, data_format="NCDHW", )
paddle.nn.functional.softmax(Tensor([11, 4, 0, 7],"float32"), axis=3, )
paddle.nn.functional.conv1d(x=Tensor([2, 3, 0],"float64"), weight=Tensor([6, 1, 3],"float64"), bias=Tensor([6],"float64"), stride=2, padding=0, groups=3, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[150,1,1,], )
paddle.nn.functional.max_pool3d(x=Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[1,2,3,], padding=list[0,0,0,], )
paddle.reverse(Tensor([1, 1, 0],"float32"), axis=list[0,], )
paddle.nn.functional.conv2d_transpose(Tensor([4, 1024, 16, 16],"float32"), Tensor([1024, 1, 0, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1024, output_size=None, data_format="NCHW", )
paddle.allclose(Tensor([0, 10],"float64"), Tensor([0, 10],"float64"), rtol=1e-05, atol=1e-08, )
paddle.nn.functional.pad(Tensor([1, 0, 3, 4, 5],"complex128"), pad=list[1,2,1,1,3,4,], mode="constant", value=100, data_format="NCDHW", )
paddle.Tensor.tile(Tensor([34000, 0],"float32"), list[1,215,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[469,1,1,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[504,1,1,], )
paddle.nn.functional.avg_pool2d(Tensor([0, 1024, 4, 4],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([19125, 0],"float32"), list[1,73,], )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[159,1,1,], )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 3, 0, 4],"float32"), output_size=list[3,3,], data_format="NCHW", name=None, )
paddle.nn.functional.pad(x=Tensor([0, 3, 3],"float32"), pad=list[1,1,], mode="constant", value=0.0, data_format="NCL", )
paddle.nn.functional.max_pool2d(Tensor([2, 0, 32, 32],"float64"), kernel_size=list[3,3,], stride=tuple(1,2,), padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.temporal_shift(Tensor([240, 1024, 0, 14],"float16"), 8, 0.125, data_format="NCHW", )
paddle.nn.functional.max_pool1d(Tensor([0, 3, 8],"float32"), 2, 2, 0, False, False, None, )
paddle.einsum("ak, kn-> an", Tensor([60000, 11],"float32"), Tensor([11, 0],"float32"), )
paddle.nn.functional.conv1d(x=Tensor([2, 3, 4],"float64"), weight=Tensor([1, 3, 3],"float64"), bias=Tensor([0],"float64"), stride=list[1,], padding=list[1,], dilation=tuple(2,), )
paddle.nn.functional.avg_pool2d(Tensor([4, 0, 4, 80],"float32"), list[4,2,], )
paddle.einsum("ijk, ikl->ijl", Tensor([3, 10, 3],"float64"), Tensor([3, 3, 0],"float64"), )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 0],"float32"), weight=Tensor([1, 3, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=2, padding=0, )
paddle.searchsorted(Tensor([0],"bfloat16"), Tensor([0],"bfloat16"), )
paddle.nn.functional.pad(x=Tensor([3, 3, 0],"float64"), pad=list[1,1,], mode="constant", value=0.0, data_format="NCL", )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[395,1,1,], )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([0, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=True, )
paddle.einsum("b h i d, b h j d -> b h i j", Tensor([1, 8, 1, 64],"float32"), Tensor([0, 8, 1, 64],"float32"), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[584,1,1,], )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([0, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
paddle.nn.functional.pad(Tensor([1, 3, 0, 224, 258],"float32"), tuple(0,2,0,0,0,0,), data_format="NCDHW", )
paddle.Tensor.tile(Tensor([121, 0],"float32"), list[32,1,1,], )
paddle.nn.functional.avg_pool2d(Tensor([2, 0, 4, 4],"float64"), kernel_size=list[3,3,], stride=tuple(1,1,), padding=list[0,0,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.lp_pool1d(Tensor([2, 32, 0],"float32"), 7.0, 2, None, 1, False, "NLC", None, )
paddle.nn.functional.max_pool3d(Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,1,1,], return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.nn.functional.conv1d_transpose(Tensor([1, 256, 0],"float32"), Tensor([256, 128, 8],"float32"), bias=Tensor([128],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.einsum("nbts,nbsc->nbtc", Tensor([2, 0, 7, 14],"float32"), Tensor([2, 0, 14, 8],"float32"), )
paddle.nn.functional.maxout(Tensor([9, 2, 0, 6],"float64"), 2, 3, None, )
paddle.nn.functional.avg_pool1d(Tensor([2, 0, 8],"float64"), 3, 4, 0, True, False, None, )
paddle.nn.quant.weight_only_linear(Tensor([101, 64],"float16"), weight=Tensor([192, 64],"int8"), bias=Tensor([192],"float16"), weight_scale=Tensor([0],"float16"), weight_dtype="int8", )
paddle.nn.quant.weight_quantize(Tensor([64, 0],"float16"), algo="weight_only_int8", arch=75, group_size=-1, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[1434,1,1,], )
paddle.einsum("ij,kl->ijkl", Tensor([0, 5],"float64"), Tensor([3, 7],"float64"), )
paddle.nn.functional.conv2d_transpose(Tensor([2, 96, 2, 0],"float32"), Tensor([96, 96, 4, 4],"float32"), bias=Tensor([96],"float32"), padding=0, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d(Tensor([1, 1, 0, 261],"float32"), Tensor([64, 1, 0, 7],"float32"), None, list[2,2,], 0, list[1,1,], 1, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([1, 3, 0, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=list[2,2,1,], padding=1, dilation=tuple(2,2,2,), )
paddle.nn.functional.conv2d_transpose(Tensor([4, 128, 32, 32],"float32"), Tensor([128, 1, 4, 0],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=128, output_size=None, data_format="NCHW", )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 0, 64],"float16"), Tensor([2, 100, 1, 64],"float16"), Tensor([2, 100, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 100],"float16"), is_causal=False, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[452,1,1,], )
paddle.nn.functional.fractional_max_pool3d(x=Tensor([0, 3, 7, 7, 7],"float32"), output_size=5, random_u=0.5, )
paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 0],"int32"), bias=Tensor([10],"float16"), dequant_scales=Tensor([10],"float32"), act_method="gelu", compute_dtype="fp16", )
paddle.isin(Tensor([8, 0],"bfloat16"), Tensor([4, 0],"bfloat16"), False, False, )
paddle.nn.functional.mse_loss(Tensor([3, 3, 0, 10],"float32"), Tensor([3, 3, 0, 10],"float32"), "mean", )
paddle.strided_slice(x=Tensor([5, 8, 0, 4, 2, 6],"float64"), axes=list[1,2,5,], starts=list[-3,3,4,], ends=list[3,0,1,], strides=list[-1,-1,-2,], )
paddle.subtract(Tensor([1, 0, 30, 30],"float32"), Tensor([1, 0, 30, 30],"float32"), )
paddle.reverse(Tensor([0, 4, 16],"float64"), axis=list[0,], )
paddle.subtract(x=Tensor([0],"float64"), y=Tensor([0],"float64"), )
paddle.Tensor.tile(Tensor([12240, 0],"float32"), list[1,41,], )
paddle.nn.functional.softmax(Tensor([1024, 0, 49, 49],"float32"), -1, name=None, )
paddle.nn.functional.conv2d(Tensor([4, 6, 16, 16],"float32"), Tensor([12, 1, 3, 0],"float32"), None, padding="valid", stride=1, dilation=1, groups=6, data_format="NCHW", )
paddle.einsum("i,d->id", Tensor([16],"float32"), Tensor([0],"float32"), )
paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 8, 0, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,1,1,], exclusive=True, )
paddle.nn.functional.pixel_shuffle(Tensor([2, 9, 0, 4],"float64"), 3, "NCHW", None, )
paddle.nn.functional.conv3d(Tensor([4, 6, 8, 8, 0],"float32"), Tensor([12, 1, 3, 3, 0],"float32"), None, padding="valid", stride=1, dilation=1, groups=6, data_format="NCDHW", )
paddle.nn.functional.conv2d_transpose(Tensor([0, 256, 47, 35],"float32"), Tensor([256, 128, 4, 4],"float32"), bias=None, padding=0, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.selu(x=Tensor([0, 2],"float64"), )
paddle.nn.functional.conv1d(Tensor([16, 64, 25500],"float32"), Tensor([128, 64, 0],"float32"), bias=Tensor([128],"float32"), padding=32, stride=list[1,], dilation=list[32,], groups=1, data_format="NCL", )
paddle.nn.functional.hinge_embedding_loss(Tensor([10, 10, 0],"float64"), Tensor([10, 10, 0],"float64"), reduction="none", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 8, 3, 0, 3],"float32"), bias=Tensor([8],"float32"), padding="valid", output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.Tensor.tile(Tensor([1, 1024, 0, 1, 16],"float16"), list[1,1,1,4,1,], )
paddle.einsum("ibm,hm->ibh", Tensor([7, 0, 32],"float32"), Tensor([32, 32],"float32"), )
paddle.nn.functional.prelu(x=Tensor([1, 0, 3, 4],"float32"), weight=Tensor([1],"float32"), )
paddle.nn.functional.pad(Tensor([1, 4410, 0],"float32"), pad=list[200,200,], mode="reflect", data_format="NLC", )
paddle.nn.functional.softmax(Tensor([13, 4, 5, 0, 7],"float32"), )
paddle.nn.functional.pad(Tensor([1024, 0, 256, 256],"float32"), list[2,2,2,2,], )
paddle.nn.functional.fractional_max_pool3d(x=Tensor([0, 3, 7, 7, 7],"float32"), kernel_size=2, output_size=list[3,3,3,], random_u=0.6, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 0],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
paddle.nn.functional.pad(Tensor([12, 1, 0, 128],"float32"), list[2,1,2,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 0],"float32"), Tensor([6, 8, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[2,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float64"), weight=Tensor([1, 3, 3, 0],"float64"), bias=Tensor([1],"float64"), stride=list[2,1,], padding=tuple(1,2,), dilation=tuple(2,2,), )
paddle.gammaln(Tensor([0, 3, 4, 5],"float32"), )
paddle.nn.functional.conv2d(Tensor([1, 256, 128, 128],"float32"), Tensor([256, 256, 0, 3],"float32"), padding=1, groups=1, )
paddle.incubate.softmax_mask_fuse(x=Tensor([2, 0, 8, 32],"float16"), mask=Tensor([2, 1, 8, 32],"float16"), )
paddle.nn.functional.mse_loss(Tensor([3, 0, 10, 10],"float32"), Tensor([3, 0, 10, 10],"float32"), "sum", )
paddle.nanmedian(Tensor([2, 3, 0, 5],"float32"), axis=2, keepdim=False, mode="min", )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 0, 32, 32],"float64"), output_size=tuple(3,3,3,), )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[150,1,1,], )
paddle.nn.functional.max_pool2d(Tensor([1, 256, 0, 30],"float32"), 1, stride=2, )
paddle.einsum("...qk,...kd->...qd", Tensor([52, 4, 3, 1, 0],"float32"), Tensor([52, 4, 3, 2, 0],"float32"), )
paddle.nn.functional.pad(x=Tensor([3, 0, 3],"float64"), pad=list[1,1,], mode="constant", value=0.0, data_format="NCL", )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 128, 32, 0],"float32"), output_size=tuple(2,2,), data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([4, 2, 64, 0],"float32"), list[1,72,1,1,], )
paddle.nn.functional.avg_pool3d(Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCDHW", name=None, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 2, 2, 2, 3],"float32"), Tensor([3, 1, 3, 0, 3],"float32"), bias=Tensor([3],"float32"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=3, output_size=None, data_format="NDHWC", )
paddle.nn.functional.max_unpool2d(Tensor([0, 1, 2, 2],"int64"), Tensor([0, 1, 2, 2],"int32"), kernel_size=2, stride=None, output_size=list[1,1,4,5,], )
paddle.index_fill(Tensor([0],"int64"), Tensor([4],"int64"), 0, 2, )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 0, 2],"float64"), weight=Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,1,], dilation=1, )
paddle.slice(Tensor([2, 3, 4, 0, 6],"float32"), axes=list[0,1,2,], starts=list[1,0,2,], ends=list[3,3,4,], )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 0, 2],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=1, output_padding=0, dilation=1, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([16660, 0],"float32"), list[1,71,], )
paddle.nn.functional.max_pool2d(Tensor([0, 4, 40, 40],"float64"), kernel_size=4, stride=2, padding=2, return_mask=True, ceil_mode=False, data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([1, 5, 0],"float32"), list[562,1,1,], )
paddle.Tensor.__getitem__(Tensor([512, 0],"float32"), slice(None,None,-1), )
paddle.nn.functional.pad(Tensor([30, 64, 16, 0],"float32"), list[0,1,0,1,], value=-math.inf, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([1, 3, 0, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=list[2,2,1,], padding=tuple(1,2,2,), dilation=tuple(2,2,2,), )
paddle.nn.functional.selu(x=Tensor([0, 2],"float32"), )
paddle.nn.functional.pixel_shuffle(Tensor([2, 9, 0, 4],"float32"), upscale_factor=3, )
paddle.nn.functional.pad(Tensor([1, 16, 0, 18, 384],"float16"), tuple(0,3,0,0,0,0,), data_format="NDHWC", )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[311,1,1,], )
paddle.nn.functional.conv2d(Tensor([1, 3, 224, 224],"float32"), Tensor([3, 3, 0, 3],"float32"), Tensor([3],"float32"), list[4,3,], 0, list[1,1,], 1, "NCHW", )
paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 1, 1, 7],"float32"), Tensor([13, 4, 0, 7, 8],"float32"), )
paddle.meshgrid(Tensor([0],"float32"), Tensor([0],"float32"), Tensor([0],"float32"), Tensor([0],"float32"), )
paddle.slice(Tensor([8, 2, 100, 0],"float16"), axes=list[1,], starts=list[1,], ends=list[2,], )
paddle.Tensor.tile(Tensor([0, 4],"float32"), list[32,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 0],"float64"), Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=1, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.softmax(Tensor([0, 16, 128],"float64"), 1, name=None, )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 2],"float32"), weight=Tensor([3, 2, 3, 3, 0],"float32"), bias=Tensor([1],"float32"), stride=1, padding=0, dilation=1, )
paddle.nn.functional.conv2d_transpose(Tensor([1, 1024, 128, 128],"float32"), Tensor([1024, 128, 3, 0],"float32"), padding=0, stride=2, groups=4, )
paddle.nn.functional.conv1d(Tensor([2, 3, 0],"float64"), Tensor([6, 1, 3],"float64"), bias=Tensor([6],"float64"), padding=0, stride=list[2,], dilation=list[1,], groups=3, data_format="NCL", )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 0, 4, 4],"float64"), weight=Tensor([1, 3, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=list[2,2,1,], padding=1, dilation=tuple(2,2,2,), )
paddle.nn.functional.avg_pool1d(Tensor([2, 3, 0],"float64"), 3, 4, 0, True, False, None, )
paddle.Tensor.tile(Tensor([1, 4, 0],"float32"), list[444,1,1,], )
paddle.Tensor.__sub__(Tensor([1, 0],"float32"), Tensor([1, 0],"float32"), )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[270,1,1,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[540,1,1,], )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[2019,1,1,], )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 0, 16, 32],"float32"), output_size=2, data_format="NCHW", name=None, )
paddle.nn.functional.conv1d_transpose(Tensor([4, 6, 16],"float32"), Tensor([6, 0, 1],"float32"), bias=Tensor([8],"float32"), output_size=None, output_padding=0, padding=3, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.channel_shuffle(Tensor([2, 9, 4, 0],"float64"), 3, "NCHW", )
paddle.nn.functional.avg_pool2d(Tensor([0, 3, 4, 4],"float64"), kernel_size=list[3,3,], stride=list[1,1,], padding=list[0,0,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 256, 0, 32],"float32"), Tensor([256, 1, 8, 8],"float32"), bias=None, padding=2, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=256, output_size=None, data_format="NCHW", )
paddle.slice(Tensor([2, 3, 0],"float32"), axes=list[1,], starts=list[2,], ends=list[3,], )
paddle.nn.functional.conv3d_transpose(Tensor([4, 4, 8, 8, 8],"float32"), Tensor([4, 0, 3, 3, 3],"float32"), Tensor([4],"float32"), output_size=tuple(10,17,10,), padding="valid", stride=tuple(1,2,1,), dilation=1, groups=1, data_format="NCDHW", )
paddle.Tensor.tile(Tensor([1, 5, 0],"float32"), list[3801,1,1,], )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 0, 32],"float64"), output_size=tuple(3,3,3,), data_format="NDHWC", name=None, )
paddle.Tensor.fill_diagonal_(Tensor([0, 3],"float64"), 1, 0, False, )
paddle.nn.functional.lp_pool1d(Tensor([2, 0, 32],"float64"), norm_type=5, kernel_size=5, stride=3, padding=list[0,], )
paddle.nn.functional.max_pool2d(Tensor([0, 112, 112, 64],"float16"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NHWC", name=None, )
paddle.allclose(x=Tensor([2, 0, 2, 3],"float64"), y=Tensor([2, 0, 2, 3],"float64"), rtol=-3.0, atol=-2.0, )
paddle.nn.functional.pixel_shuffle(Tensor([2, 0, 4, 4],"float64"), 3, "NCHW", None, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 128, 94, 0],"float32"), Tensor([128, 128, 2, 2],"float32"), bias=None, padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[444,1,1,], )
paddle.take(Tensor([3, 0],"float64"), Tensor([5, 8],"int64"), mode="wrap", )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 2],"float64"), weight=Tensor([3, 0, 3],"float64"), bias=Tensor([1],"float64"), stride=2, padding=list[1,], groups=1, dilation=1, )
paddle.nn.functional.max_pool1d(Tensor([1, 0, 6],"float32"), kernel_size=5, stride=5, padding=0, ceil_mode=True, return_mask=True, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 0],"float32"), weight=Tensor([1, 3, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=list[2,2,1,], padding=list[1,2,2,], dilation=tuple(2,2,2,), )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([0, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 0, 16],"float16"), Tensor([1, 2048, 0, 16],"float16"), Tensor([1, 2048, 0, 16],"float16"), attn_mask=Tensor([1, 1, 0, 2048],"float16"), is_causal=True, )
paddle.einsum("..., f -> ... f", Tensor([0],"float32"), Tensor([0],"float32"), )
paddle.nn.functional.softmax(Tensor([2, 17, 0, 4],"float32"), axis=1, )
paddle.einsum("sec,ecm->sm", Tensor([10, 60, 0],"float32"), Tensor([60, 10, 0],"float32"), )
paddle.einsum("ak, kn-> an", Tensor([15000, 11],"float32"), Tensor([11, 0],"float32"), )
paddle.nn.functional.avg_pool3d(x=Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], ceil_mode=True, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 2, 2, 6],"float64"), weight=Tensor([6, 0, 3, 3],"float64"), bias=Tensor([3],"float64"), output_padding=1, stride=2, padding=list[1,0,], groups=3, data_format="NHWC", dilation=1, )
paddle.vision.ops.prior_box(Tensor([4, 96, 40, 0],"float32"), Tensor([4, 3, 640, 640],"float32"), list[32.0,48.0,64.0,80.0,96.0,128.0,], list[], list[1.0,], list[0.1,0.1,0.2,0.2,], False, False, list[16.0,16.0,], 0.5, False, None, )
paddle.nn.functional.max_pool1d(x=Tensor([2, 0, 8],"float64"), kernel_size=2, stride=1, padding=list[1,], )
paddle.nn.functional.pad(Tensor([1, 0, 200, 150],"float64"), pad=list[1,1,2,2,], mode="constant", value=0.0, data_format="NCHW", )
paddle.nn.functional.conv2d(Tensor([1, 128, 12, 32],"float32"), Tensor([128, 128, 0, 3],"float32"), None, list[1,1,], 1, list[1,1,], 1, )
paddle.nn.functional.avg_pool2d(Tensor([16, 0, 8, 8],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 0, 8],"float32"), Tensor([6, 8, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding="valid", output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.einsum("bnij,bjnd->bind", Tensor([13, 4, 0, 4],"float32"), Tensor([13, 4, 4, 4],"float32"), )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float64"), weight=Tensor([1, 3, 0, 3],"float64"), bias=Tensor([1],"float64"), stride=list[2,1,], padding=1, dilation=list[2,2,], )
paddle.nn.functional.maxout(Tensor([9, 2, 2, 0],"float64"), 2, 3, None, )
paddle.nn.functional.pad(Tensor([1, 0, 14, 19, 384],"float16"), tuple(0,2,0,0,0,0,), data_format="NDHWC", )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 1024, 0, 128],"bfloat16"), Tensor([1, 1024, 0, 128],"bfloat16"), Tensor([1, 1024, 0, 128],"bfloat16"), attn_mask=None, is_causal=True, )
paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 8, 1, 109],"float32"), Tensor([0, 8, 109, 64],"float32"), )
paddle.nn.functional.pixel_shuffle(x=Tensor([2, 0, 4, 4],"float64"), upscale_factor=3, data_format="NCHW", )
paddle.nn.functional.max_pool3d(x=Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[1,2,3,], padding=list[0,0,0,], )
paddle.nn.functional.avg_pool3d(Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[1,2,3,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCDHW", name=None, )
paddle.einsum("ibnd,snd->ibns", Tensor([7, 14, 4, 8],"float32"), Tensor([0, 4, 8],"float32"), )
paddle.nn.functional.softmax(x=Tensor([0, 3, 4],"float32"), )
paddle.nn.functional.zeropad2d(Tensor([4, 0, 224, 224],"float64"), list[2,2,2,2,], )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([1, 3, 3, 0, 3],"float32"), bias=Tensor([1],"float32"), stride=2, padding=1, )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 0, 7],"float32"), output_size=list[2,3,5,], data_format="NCDHW", name=None, )
paddle.nn.functional.fractional_max_pool3d(x=Tensor([2, 0, 7, 7, 7],"float32"), output_size=list[3,None,3,], random_u=0.6, )
paddle.nn.functional.conv2d(Tensor([1, 2048, 128, 128],"float32"), Tensor([24, 256, 0, 1],"float32"), padding=0, groups=8, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[1041,1,1,], )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([0, 3, 7, 7],"float32"), output_size=list[3,3,], data_format="NHWC", )
paddle.nn.functional.max_pool3d(Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 16],"float32"), Tensor([6, 0, 3, 3],"float32"), bias=Tensor([8],"float32"), padding="valid", output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 0],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0016999575309455395, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 0, 2],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=0, dilation=1, )
paddle.nn.functional.pad(Tensor([2, 64, 16, 0],"float32"), list[0,1,0,1,], value=0, )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[116,1,1,], )
paddle.nn.functional.conv1d(x=Tensor([2, 3, 4],"float32"), weight=Tensor([1, 3, 3],"float32"), bias=Tensor([0],"float32"), stride=1, padding=0, )
paddle.nn.functional.l1_loss(Tensor([0, 10, 5],"float32"), Tensor([0, 10, 5],"float32"), "none", name=None, )
paddle.nn.functional.conv2d(Tensor([16, 3, 260, 0],"float32"), weight=Tensor([3, 1, 5, 0],"float32"), groups=3, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([4, 134, 4, 128],"float16"), Tensor([4, 134, 0, 128],"float16"), Tensor([4, 134, 4, 128],"float16"), attn_mask=Tensor([4, 1, 134, 134],"float16"), dropout_p=0.0, training=True, is_causal=False, )
paddle.nn.functional.conv2d_transpose(Tensor([1, 128, 128, 128],"float32"), Tensor([128, 0, 4, 4],"float32"), bias=Tensor([3],"float32"), padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.subtract(x=Tensor([2, 0],"float64"), y=Tensor([2, 0],"float64"), )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([0, 4, 4, 3],"float64"), output_size=tuple(3,3,), data_format="NHWC", name=None, )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 0, 7],"float32"), output_size=list[None,3,None,], data_format="NCDHW", name=None, )
paddle.Tensor.__getitem__(Tensor([3, 3, 0],"float32"), tuple(slice(None,-1,None),slice(None,None,-1),slice(-1,None,None),), )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 0, 4],"float32"), weight=Tensor([1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=list[2,1,], padding=1, dilation=2, )
paddle.nn.functional.max_pool2d(Tensor([1, 1, 4, 0],"float32"), kernel_size=2, stride=2, return_mask=True, )
paddle.nn.functional.hinge_embedding_loss(Tensor([10, 0, 5],"float64"), Tensor([10, 0, 5],"float64"), reduction="sum", margin=1.0, name=None, )
paddle.nn.functional.conv1d(x=Tensor([2, 3, 0],"float64"), weight=Tensor([1, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=2, padding=0, )
paddle.nn.functional.max_pool3d(Tensor([2, 0, 32, 32, 32],"float32"), kernel_size=2, stride=None, padding=0, return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([0, 256, 56, 56],"float32"), output_size=list[7,7,], data_format="NCHW", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([1, 256, 14, 14],"float32"), Tensor([256, 256, 2, 0],"float32"), bias=Tensor([256],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.fractional_max_pool2d(x=Tensor([2, 0, 7, 7],"float32"), return_mask=False, output_size=list[3,3,], random_u=0.3, )
paddle.nn.functional.max_pool1d(x=Tensor([2, 0, 8],"float64"), kernel_size=2, stride=list[1,], padding=1, )
paddle.nn.functional.max_pool2d(Tensor([2, 0, 32, 32],"float32"), kernel_size=2, stride=2, padding=list[list[0,0,],list[0,0,],list[0,0,],list[0,0,],], return_mask=False, )
paddle.nn.functional.pad(Tensor([0, 1, 2, 2, 2],"float32"), pad=list[2,2,2,2,2,2,], mode="replicate", value=0.0, data_format="NCDHW", name=None, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[162,1,1,], )
paddle.einsum("i,d->id", Tensor([0],"float32"), Tensor([0],"float32"), )
paddle.einsum("ij,k->ijk", Tensor([1, 4096],"float32"), Tensor([0],"float32"), )
paddle.nanmedian(Tensor([0, 3, 4, 5],"float32"), axis=tuple(1,2,), keepdim=False, mode="min", )
paddle.nn.functional.sequence_mask(Tensor([0, 3],"int64"), maxlen=10, dtype=VarType(float32), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[685,1,1,], )
paddle.einsum("bind,bjnd->bnij", Tensor([13, 2, 0, 4],"float32"), Tensor([13, 2, 0, 4],"float32"), )
paddle.nn.functional.conv2d_transpose(Tensor([0, 96, 2, 2],"float32"), Tensor([96, 96, 4, 4],"float32"), bias=Tensor([96],"float32"), padding=0, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([0, 16, 31, 28, 192],"float16"), tuple(0,0,0,4,0,0,), data_format="NDHWC", )
paddle.Tensor.tile(Tensor([4, 2, 0, 64],"float32"), list[1,72,1,1,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[351,1,1,], )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 128, 32, 0],"float32"), output_size=tuple(6,6,), data_format="NCHW", name=None, )
paddle.nn.functional.max_pool2d(Tensor([0, 3, 32, 32],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, )
paddle.nn.quant.weight_only_linear(Tensor([2, 0, 64],"float16"), weight=Tensor([192, 64],"int8"), bias=Tensor([192],"float16"), weight_scale=Tensor([192],"float16"), weight_dtype="int8", )
paddle.slice(Tensor([0, 2, 104],"float32"), list[0,1,2,], list[0,0,72,], list[2,2,88,], )
paddle.Tensor.imag(Tensor([2, 20, 0, 3],"complex128"), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[385,1,1,], )
paddle.nn.functional.interpolate(x=Tensor([2, 3, 7, 0],"float32"), mode="area", size=list[2,5,], )
paddle.Tensor.tile(Tensor([1, 5, 0],"float32"), list[1,1,2,], )
paddle.nn.functional.conv2d_transpose(Tensor([0, 6, 16, 16],"float32"), Tensor([6, 8, 1, 1],"float32"), bias=Tensor([8],"float32"), padding=tuple(2,3,), output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([1, 3, 686, 0],"float32"), tuple(0,0,0,338,), )
paddle.unique(Tensor([0, 5, 5],"float32"), return_index=True, return_inverse=True, return_counts=True, axis=0, )
paddle.Tensor.nonzero(Tensor([0, 11],"bool"), )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 0, 8, 32, 32],"float64"), output_size=list[1,3,2,], )
paddle.nn.functional.conv2d_transpose(Tensor([2, 6, 2, 2],"float32"), Tensor([6, 1, 3, 0],"float32"), bias=None, padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=3, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv1d(Tensor([2, 3, 4],"float64"), Tensor([1, 3, 0],"float64"), bias=Tensor([1],"float64"), padding=0, stride=list[2,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.pad(Tensor([3, 4, 5, 0],"complex64"), pad=list[1,2,2,1,], mode="constant", value=100, data_format="NCHW", name=None, )
paddle.nn.functional.max_pool2d(Tensor([1, 64, 112, 0],"float32"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([10285, 0],"float32"), list[1,394,], )
paddle.nn.functional.pad(Tensor([0, 16, 14, 17, 384],"float32"), tuple(0,4,0,0,0,0,), data_format="NDHWC", )
paddle.Tensor.fill_diagonal_(Tensor([3, 0],"float32"), 1, offset=2, wrap=True, )
paddle.nn.functional.log_softmax(Tensor([2, 3, 4, 0],"float32"), 1, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 1, 2, 40],"float16"), Tensor([0, 1, 2, 40],"float16"), Tensor([1, 1, 2, 40],"float16"), attn_mask=Tensor([1, 2, 1, 1],"float16"), )
paddle.nn.functional.conv2d_transpose(x=Tensor([0, 3, 2, 2],"float32"), weight=Tensor([3, 1, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=1, output_padding=0, dilation=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 2],"float64"), weight=Tensor([3, 0, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,], groups=1, dilation=2, )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 2],"float64"), weight=Tensor([3, 0, 3],"float64"), bias=Tensor([2],"float64"), stride=1, padding=0, output_padding=0, groups=1, dilation=1, output_size=None, data_format="NCL", name=None, )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 32, 0],"float64"), output_size=list[2,2,2,], )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 3, 4, 0],"float64"), output_size=list[1,1,], )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([1, 3, 3, 0, 3],"float32"), bias=Tensor([1],"float32"), stride=list[2,2,1,], padding=tuple(1,2,2,), dilation=tuple(2,2,2,), )
paddle.allclose(tuple(Tensor([2, 0, 32],"float32"),), tuple(Tensor([2, 0, 32],"float32"),), rtol=0.0001, atol=0.0001, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 0],"float64"), Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([1024, 0, 256, 256],"float32"), list[1,1,1,1,], )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 0, 32],"float32"), output_size=list[3,3,3,], data_format="NCDHW", name=None, )
paddle.nn.functional.conv2d(Tensor([2, 192, 4, 4],"float32"), Tensor([96, 192, 1, 0],"float32"), None, list[1,1,], 0, list[1,1,], 1, "NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([1, 1024, 128, 128],"float32"), Tensor([1024, 0, 3, 3],"float32"), padding=0, stride=2, groups=4, )
paddle.einsum("ibnd,jbnd->bnij", Tensor([0, 10, 4, 8],"float32"), Tensor([0, 10, 4, 8],"float32"), )
paddle.nn.functional.pad(Tensor([3, 1, 3, 40, 0],"float32"), pad=list[0,0,0,0,2,2,], data_format="NCDHW", )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[528,1,1,], )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float64"), weight=Tensor([1, 3, 3, 3, 3],"float64"), bias=Tensor([0],"float64"), stride=list[2,2,1,], padding=1, dilation=2, )
paddle.vision.ops.generate_proposals(Tensor([1, 4, 16, 16],"float32"), Tensor([1, 16, 16, 0],"float32"), Tensor([1, 2],"float32"), Tensor([16, 16, 4, 4],"float32"), Tensor([16, 16, 4, 4],"float32"), pre_nms_top_n=12000, post_nms_top_n=5000, nms_thresh=0.7, min_size=3.0, eta=1.0, pixel_offset=True, return_rois_num=True, )
paddle.isin(Tensor([8, 0],"bfloat16"), Tensor([4, 256],"bfloat16"), False, False, )
paddle.reverse(Tensor([6, 13, 0],"int64"), list[0,], )
paddle.nn.functional.max_pool3d(x=Tensor([2, 3, 0, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,1,1,], )
paddle.einsum("ijk,lk->ijl", Tensor([3, 4, 5],"float64"), Tensor([0, 5],"float64"), )
paddle.nn.functional.conv2d(Tensor([3, 16, 2, 2],"float32"), Tensor([16, 16, 0, 3],"float32"), None, list[2,2,], 1, list[1,1,], 1, "NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([0, 3, 2, 2],"float64"), Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.__getitem__(Tensor([3, 3, 0],"float32"), tuple(slice(1,-1,None),slice(0,2,None),slice(None,None,-1),), )
paddle.Tensor.tile(Tensor([1, 4, 0],"float32"), list[254,1,1,], )
paddle.vision.ops.generate_proposals(Tensor([1, 3, 10, 14],"float32"), Tensor([0, 12, 10, 14],"float32"), Tensor([1, 2],"float32"), Tensor([420, 4],"float32"), Tensor([420, 4],"float32"), pre_nms_top_n=2000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.vision.ops.yolo_box(Tensor([2, 16, 0, 8],"float32"), img_size=Tensor([2, 2],"int32"), anchors=list[10,13,16,30,], class_num=2, conf_thresh=0.01, downsample_ratio=8, clip_bbox=True, scale_x_y=1.0, iou_aware=True, iou_aware_factor=0.5, )
paddle.nn.functional.pad(Tensor([1, 22071, 0],"float32"), list[1,0,], value=7, mode="constant", data_format="NCL", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 128, 94, 70],"float32"), Tensor([128, 128, 0, 2],"float32"), bias=None, padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.margin_ranking_loss(Tensor([10, 0],"float64"), Tensor([10, 0],"float64"), Tensor([10, 0],"float64"), 0.2, "mean", None, )
paddle.vision.ops.generate_proposals(Tensor([1, 3, 10, 15],"float32"), Tensor([1, 12, 0, 15],"float32"), Tensor([1, 2],"float32"), Tensor([450, 4],"float32"), Tensor([450, 4],"float32"), pre_nms_top_n=2000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 0, 8, 96],"float16"), Tensor([2, 101, 8, 96],"float16"), Tensor([2, 101, 8, 96],"float16"), attn_mask=None, is_causal=False, )
paddle.nn.functional.conv2d(Tensor([1, 1024, 128, 128],"float32"), Tensor([12, 256, 1, 0],"float32"), padding=0, groups=4, )
paddle.nn.functional.pad(Tensor([1, 0, 200, 150],"float64"), pad=list[10,10,10,10,], mode="constant", value=0.0, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 0, 2],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,], dilation=1, )
paddle.nn.functional.max_pool1d(Tensor([2, 0, 8],"float32"), 2, 2, 0, False, False, None, )
paddle.roll(Tensor([4, 5, 4, 0],"complex128"), Tensor([1],"int64"), 3, name=None, )
paddle.vision.ops.generate_proposals(Tensor([1, 4, 16, 16],"float32"), Tensor([1, 16, 16, 16],"float32"), Tensor([1, 0],"float32"), Tensor([16, 16, 4, 4],"float32"), Tensor([16, 16, 4, 4],"float32"), pre_nms_top_n=12000, post_nms_top_n=5000, nms_thresh=0.7, min_size=3.0, eta=1.0, pixel_offset=True, return_rois_num=True, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 2],"float32"), Tensor([3, 1, 3, 3],"float32"), bias=Tensor([0],"float32"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[2,2,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([0, 4, 4, 3],"float64"), output_size=list[3,3,], data_format="NHWC", name=None, )
paddle.nn.quant.weight_quantize(Tensor([0, 256],"float16"), algo="weight_only_int8", arch=70, group_size=-1, )
paddle.nn.functional.conv2d_transpose(x=Tensor([0, 3, 2, 2],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=1, output_padding=0, dilation=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([1, 2, 3, 4, 0],"complex128"), pad=list[1,2,1,1,3,4,], mode="constant", value=100, data_format="NCDHW", )
paddle.einsum("blkd,bldq->blkq", Tensor([52, 5, 1, 1],"float32"), Tensor([52, 5, 1, 0],"float32"), )
paddle.slice(Tensor([1024, 16, 0],"float16"), axes=list[0,], starts=list[128,], ends=list[256,], )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 2],"float64"), Tensor([3, 0, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d(Tensor([1, 1024, 256, 0],"float32"), Tensor([1024, 128, 3, 0],"float32"), padding=1, groups=8, )
paddle.reverse(Tensor([0, 1, 3],"float32"), axis=0, )
paddle.nn.functional.max_pool1d(Tensor([2, 0, 32],"float32"), kernel_size=2, stride=2, padding=0, )
paddle.Tensor.__sub__(Tensor([3, 6, 0, 4, 1, 5],"float64"), Tensor([3, 6, 0, 4, 1, 5],"float64"), )
paddle.nn.functional.pad(Tensor([1, 1, 0, 3, 2],"float64"), pad=list[1,0,1,2,1,0,], mode="constant", value=0.0, data_format="NCDHW", name=None, )
paddle.Tensor.tile(Tensor([19125, 0],"float32"), list[1,43,], )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([0, 3, 8, 32, 32],"float64"), output_size=list[2,2,2,], data_format="NCDHW", name=None, )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 0],"float64"), weight=Tensor([3, 2, 3],"float64"), bias=Tensor([2],"float64"), stride=1, padding=0, output_padding=0, groups=1, dilation=1, output_size=None, data_format="NCL", name=None, )
paddle.nn.functional.softmax(Tensor([2, 3, 4, 0],"float32"), dtype=type(numpy.float64), )
paddle.allclose(Tensor([13, 13, 0],"float32"), Tensor([13, 13, 0],"float32"), rtol=0.0001, atol=0.0001, )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 2],"float32"), weight=Tensor([3, 0, 3, 3, 3],"float32"), bias=Tensor([3],"float32"), stride=1, padding=list[1,0,1,], groups=3, dilation=1, )
paddle.nn.functional.mse_loss(Tensor([2, 10, 0],"float32"), Tensor([2, 10, 0],"float32"), "none", )
paddle.Tensor.tile(Tensor([2, 2, 38, 64, 0],"float16"), list[1,1,1,1,2,], )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 0, 4],"float32"), weight=Tensor([1, 3, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=2, padding=1, )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 2],"float64"), weight=Tensor([3, 0, 3],"float64"), bias=Tensor([3],"float64"), stride=1, padding=list[1,], output_padding=0, groups=3, dilation=1, output_size=None, data_format="NCL", name=None, )
paddle.Tensor.fill_diagonal_(Tensor([3, 3, 0],"float64"), 1, offset=0, wrap=True, )
paddle.einsum("ik, ijk->j", Tensor([4, 5],"float64"), Tensor([4, 0, 5],"float64"), )
paddle.slice(Tensor([0, 1, 104],"float32"), list[0,1,2,], list[0,0,40,], list[16,1,56,], )
paddle.subtract(Tensor([16, 0, 1],"float32"), Tensor([16, 0, 1],"float32"), )
paddle.einsum("...ij,...i->j...", Tensor([10, 0],"float64"), Tensor([3, 4, 5, 10],"float64"), )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 0, 2],"float32"), Tensor([3, 1, 3, 3],"float32"), bias=None, padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[2,2,], groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[175,1,1,], )
paddle.nn.functional.prelu(x=Tensor([1, 2, 3, 0],"float64"), weight=Tensor([1],"float64"), )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 32, 0],"float64"), output_size=tuple(3,3,3,), data_format="NCDHW", name=None, )
paddle.nn.functional.pad(Tensor([0, 4, 5],"complex128"), pad=list[1,2,], mode="constant", value=100, data_format="NCL", name=None, )
paddle.nn.functional.square_error_cost(Tensor([3, 0, 1, 2],"float64"), label=Tensor([3, 0, 1, 2],"float64"), )
paddle.nn.functional.pad(Tensor([0, 2, 3, 4, 5],"complex64"), pad=list[1,2,1,1,3,4,], mode="constant", value=100, data_format="NDHWC", )
paddle.slice(Tensor([0, 2, 104],"float32"), list[0,1,2,], list[0,0,64,], list[16,2,80,], )
paddle.Tensor.__rpow__(Tensor([4, 0, 2],"float32"), 2, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 160, 16, 0],"float32"), output_size=4, data_format="NCHW", name=None, )
paddle.einsum("i...->...", Tensor([2, 0, 10],"float64"), )
paddle.einsum("k...,jk", Tensor([2, 4, 5, 0],"float64"), Tensor([2, 2],"float64"), )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 0, 2],"float32"), weight=Tensor([3, 1, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=1, dilation=1, )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[206,1,1,], )
paddle.nn.functional.conv2d(Tensor([4, 16, 16, 3],"float32"), Tensor([5, 3, 3, 0],"float32"), Tensor([5],"float32"), padding=0, stride=1, dilation=1, groups=1, data_format="NHWC", )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 3, 4, 0],"float64"), output_size=list[3,3,], )
paddle.nn.functional.pad(Tensor([0, 6, 6],"float32"), pad=list[2,2,], mode="constant", value=0.0, data_format="NCL", name=None, )
paddle.Tensor.digamma(Tensor([4, 0],"float64"), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[3801,1,1,], )
paddle.nn.functional.pad(Tensor([0, 2, 3, 4, 5],"complex128"), pad=list[1,2,1,1,3,4,], mode="constant", value=100, data_format="NCDHW", )
paddle.index_fill(Tensor([128],"int64"), Tensor([0],"int64"), 0, 7, )
paddle.nn.functional.conv2d_transpose(Tensor([0, 6, 16, 16],"float32"), Tensor([6, 4, 3, 3],"float32"), bias=Tensor([8],"float32"), padding="valid", output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=2, output_size=None, data_format="NCHW", )
paddle.nn.functional.lp_pool2d(Tensor([0, 3, 32, 32],"float32"), 2, kernel_size=2, stride=1, ceil_mode=False, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[505,1,1,], )
paddle.nn.functional.softmax(Tensor([2, 0],"float32"), axis=0, dtype="float64", )
paddle.nn.functional.l1_loss(Tensor([10, 0, 5],"float32"), Tensor([10, 0, 5],"float32"), reduction="sum", )
paddle.nn.functional.pad(Tensor([0, 16, 61, 56, 96],"float32"), tuple(0,0,0,2,0,0,), data_format="NDHWC", )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 1024, 32, 128],"bfloat16"), Tensor([1, 1024, 32, 128],"bfloat16"), Tensor([0, 1024, 32, 128],"bfloat16"), attn_mask=None, is_causal=True, )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 2],"float64"), weight=Tensor([3, 0, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,1,], dilation=1, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float64"), weight=Tensor([1, 3, 3, 0, 3],"float64"), bias=Tensor([1],"float64"), stride=list[2,2,1,], padding=list[1,2,2,], dilation=tuple(2,2,2,), )
paddle.nn.functional.conv1d(x=Tensor([2, 3, 4],"float64"), weight=Tensor([1, 3, 0],"float64"), bias=Tensor([1],"float64"), stride=2, padding=0, )
paddle.Tensor.tile(Tensor([1, 1024, 2, 0, 16],"float16"), list[1,1,1,4,1,], )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float64"), weight=Tensor([1, 3, 3, 3, 3],"float64"), bias=Tensor([0],"float64"), stride=list[2,2,1,], padding=list[1,2,2,], dilation=tuple(2,2,2,), )
paddle.Tensor.tile(Tensor([1000, 0, 80],"float32"), list[1,4,1,], )
paddle.slice(Tensor([2, 0, 104],"float32"), list[0,1,2,], list[0,0,8,], list[2,2,24,], )
paddle.einsum("i,j->ij", Tensor([0],"float32"), Tensor([0],"float32"), )
paddle.incubate.nn.functional.fused_feedforward(x=Tensor([0, 2, 2],"float32"), linear1_weight=Tensor([2, 2],"float32"), linear2_weight=Tensor([2, 2],"float32"), dropout1_rate=0, dropout2_rate=0, )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 2],"float64"), weight=Tensor([3, 2, 3, 0, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=0, dilation=1, )
paddle.Tensor.tile(Tensor([4, 1, 0],"float32"), tuple(1,4,1,), )
paddle.nn.functional.mse_loss(Tensor([2, 10, 0],"float32"), Tensor([2, 10, 0],"float32"), "mean", )
paddle.nn.functional.max_pool3d(Tensor([0, 3, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.slice(Tensor([0, 1, 104],"float32"), list[0,1,2,], list[0,0,8,], list[3,1,24,], )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 0, 2],"float64"), Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.max_pool2d(Tensor([1, 0, 17, 273],"float32"), tuple(3,3,), tuple(2,2,), tuple(0,0,), False, )
paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 0],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0009313154732808471, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.nn.functional.max_pool3d(Tensor([2, 32, 32, 32, 0],"float32"), kernel_size=2, stride=2, padding=0, data_format="NDHWC", return_mask=False, )
paddle.nn.functional.pad(Tensor([0, 1, 16, 16],"float32"), list[2,1,2,1,], )
paddle.nn.functional.conv1d(Tensor([1, 20, 0],"float32"), Tensor([512, 20, 7],"float32"), bias=Tensor([512],"float32"), padding=3, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.Tensor.tile(Tensor([2, 2, 0, 64, 1],"float16"), list[1,1,1,1,2,], )
paddle.nn.functional.conv1d(Tensor([2, 3, 4],"float32"), Tensor([1, 3, 3],"float32"), bias=Tensor([0],"float32"), padding=list[1,], stride=list[1,], dilation=list[2,], groups=1, data_format="NCL", )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 0, 4, 4],"float64"), output_size=list[1,4,], data_format="NCHW", name=None, )
paddle.nn.functional.l1_loss(Tensor([4, 500, 0],"float32"), Tensor([4, 500, 0],"float32"), reduction="none", )
paddle.Tensor.tile(Tensor([8, 0, 512],"float32"), tuple(1,7,1,), )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 0],"float64"), weight=Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=1, dilation=1, )
paddle.nn.functional.mse_loss(Tensor([3, 0, 10, 10],"float32"), Tensor([3, 0, 10, 10],"float32"), "mean", )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[107,1,1,], )
paddle.nn.functional.max_pool2d(Tensor([1, 3, 6, 0],"float32"), kernel_size=2, stride=2, padding=0, return_mask=True, )
paddle.quantile(Tensor([0],"float32"), list[0.55,0.7,], 0, )
paddle.nn.functional.conv2d(Tensor([1, 256, 0, 128],"float32"), Tensor([3, 256, 0, 1],"float32"), padding=0, groups=1, )
paddle.nn.functional.conv2d(Tensor([3, 3, 32, 0],"float32"), Tensor([64, 3, 7, 0],"float32"), None, list[2,2,], 3, list[1,1,], 1, "NCHW", )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([0, 3, 5, 7, 7],"float32"), output_size=list[None,3,None,], data_format="NCDHW", name=None, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 0, 4],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding="vaLiD", output_padding=0, dilation=1, groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 5, 0],"float32"), list[362,1,1,], )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 100, 1, 64],"float16"), Tensor([2, 100, 1, 0],"float16"), attn_mask=Tensor([2, 1, 1, 100],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.conv1d(Tensor([1, 256, 28],"float32"), Tensor([256, 256, 0],"float32"), bias=Tensor([256],"float32"), padding=25, stride=list[1,], dilation=list[5,], groups=1, data_format="NCL", )
paddle.nn.functional.conv1d(Tensor([52, 7, 32],"float32"), Tensor([16, 32, 0],"float32"), bias=None, padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NLC", )
paddle.nn.functional.avg_pool2d(x=Tensor([2, 3, 4, 0],"float64"), kernel_size=list[3,3,], stride=list[3,3,], padding=list[0,0,], ceil_mode=True, exclusive=False, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[535,1,1,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[531,1,1,], )
paddle.nn.functional.unfold(Tensor([3, 0, 20, 20],"float64"), kernel_sizes=list[3,3,], strides=list[1,1,], paddings=list[1,1,1,1,], dilations=list[1,1,], name=None, )
paddle.nn.functional.pad(Tensor([0, 3, 32, 32],"float32"), list[2,3,2,3,], value=0, )
paddle.Tensor.tile(Tensor([11585, 0, 1],"float32"), list[1,418,1,], )
paddle.nn.functional.softmax(Tensor([16, 16, 0],"float64"), 2, name=None, )
paddle.nn.functional.fractional_max_pool2d(x=Tensor([2, 0, 7, 7],"float32"), output_size=5, random_u=0.5, )
paddle.nn.functional.pad(Tensor([1, 0, 28, 24],"float32"), pad=list[1,1,2,2,], mode="reflect", value=0.0, data_format="NCHW", )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float64"), weight=Tensor([1, 3, 3, 0, 3],"float64"), bias=Tensor([1],"float64"), stride=list[2,2,1,], padding=1, dilation=2, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([0],"float64"), stride=1, padding=0, dilation=1, )
paddle.nn.functional.pad(Tensor([1, 0, 3],"float64"), pad=list[1,2,], mode="constant", value=0.0, data_format="NCL", name=None, )
paddle.nn.functional.avg_pool2d(Tensor([16, 0, 4, 4],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.kthvalue(x=Tensor([0, 2, 4],"float64"), k=4, axis=2, keepdim=False, )
paddle.Tensor.tile(Tensor([4, 1, 0],"float32"), tuple(1,7,1,), )
paddle.nn.functional.sequence_mask(Tensor([2, 0],"int64"), maxlen=10, dtype=VarType(float32), )
paddle.nn.functional.pad(Tensor([3, 0, 5, 6, 7],"complex64"), pad=list[1,1,1,1,1,1,], mode="constant", value=100, data_format="NCDHW", name=None, )
paddle.einsum("a...a->...", Tensor([5, 3, 2, 1, 0, 5],"float64"), )
paddle.nn.functional.conv2d_transpose(Tensor([2, 2, 2, 6],"float32"), Tensor([6, 0, 3, 3],"float32"), bias=Tensor([3],"float32"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=3, output_size=None, data_format="NHWC", )
paddle.nn.functional.l1_loss(Tensor([0, 10, 5],"float32"), Tensor([0, 10, 5],"float32"), )
paddle.nn.functional.softmax(Tensor([104, 16, 0, 18],"float16"), )
paddle.Tensor.tile(Tensor([1, 0],"float32"), tuple(21,1,), )
paddle.nn.functional.conv2d_transpose(Tensor([0, 64, 248, 216],"float32"), Tensor([64, 128, 1, 1],"float32"), bias=None, padding=0, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv1d(Tensor([1, 256, 0],"float32"), Tensor([256, 256, 3],"float32"), bias=Tensor([256],"float32"), padding=5, stride=list[1,], dilation=list[5,], groups=1, data_format="NCL", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 256, 16, 16],"float32"), Tensor([256, 1, 16, 0],"float32"), bias=None, padding=4, output_padding=0, stride=list[8,8,], dilation=list[1,1,], groups=256, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([1, 4096, 0, 16],"float32"), Tensor([4096, 512, 3, 3],"float32"), padding=0, stride=2, groups=8, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 4, 3, 0, 3],"float32"), bias=Tensor([8],"float32"), padding="valid", output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=2, output_size=None, data_format="NCDHW", )
paddle.nn.functional.pad(Tensor([16, 3, 256, 0],"float32"), list[6,6,6,6,], )
paddle.nn.functional.pixel_unshuffle(Tensor([2, 1, 0, 12],"float32"), 3, "NCHW", )
paddle.Tensor.tile(Tensor([8, 0, 512],"float32"), tuple(1,10,1,), )
paddle.allclose(tuple(Tensor([2, 0],"float32"),), tuple(Tensor([2, 0],"float32"),), rtol=0.0001, atol=0.0001, )
paddle.nanmedian(Tensor([0, 3, 4, 5],"float32"), axis=list[0,1,3,], keepdim=False, )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[193,1,1,], )
paddle.roll(Tensor([0, 5, 4, 4],"complex128"), Tensor([4],"int64"), list[0,1,2,3,], name=None, )
paddle.Tensor.tile(Tensor([1, 0],"float32"), tuple(300,1,), )
paddle.nn.functional.max_pool1d(Tensor([2, 3, 0],"float64"), 2, 1, list[1,], False, False, None, )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[140,1,1,], )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 8, 96],"float16"), Tensor([0, 100, 8, 96],"float16"), Tensor([2, 100, 8, 96],"float16"), attn_mask=None, is_causal=False, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 0],"float32"), weight=Tensor([6, 1, 3, 3, 3],"float32"), bias=Tensor([6],"float32"), stride=2, padding=0, groups=3, )
paddle.nn.functional.temporal_shift(Tensor([0, 1024, 14, 14],"float16"), 8, 0.125, )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[81,1,1,], )
paddle.nn.functional.conv1d_transpose(x=Tensor([0, 3, 2],"float64"), weight=Tensor([3, 1, 3],"float64"), bias=Tensor([1],"float64"), stride=2, padding=list[1,], groups=1, dilation=1, )
paddle.nn.functional.group_norm(Tensor([2, 4, 0],"float64"), num_groups=2, weight=Tensor([4],"float64"), bias=Tensor([4],"float64"), data_format="NCL", )
paddle.nn.functional.l1_loss(Tensor([10, 0, 5],"float32"), Tensor([10, 0, 5],"float32"), )
paddle.nn.functional.pad(Tensor([1, 1, 0, 3],"float64"), pad=list[1,0,1,2,], mode="constant", value=0.0, data_format="NCHW", name=None, )
paddle.nn.functional.log_softmax(Tensor([0, 3, 4, 5],"float32"), 1, None, )
paddle.einsum("i..., i...", Tensor([1, 3, 2],"float64"), Tensor([0],"float64"), )
paddle.nn.functional.selu(Tensor([3, 0, 5, 10],"float64"), 1.5, 2.0, )
paddle.vision.ops.generate_proposals(Tensor([1, 0, 10, 8],"float32"), Tensor([1, 36, 10, 8],"float32"), Tensor([1, 2],"float32"), Tensor([720, 4],"float32"), Tensor([720, 4],"float32"), pre_nms_top_n=4000, post_nms_top_n=4000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.einsum("iox,ojx->ijx", Tensor([0, 3, 1],"complex64"), Tensor([3, 2, 1],"complex64"), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[179,1,1,], )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float32"), weight=Tensor([1, 3, 3, 0],"float32"), bias=Tensor([1],"float32"), stride=list[2,1,], padding=list[1,2,], dilation=tuple(2,2,), )
paddle.nn.functional.max_pool1d(x=Tensor([2, 0, 8],"float64"), kernel_size=1, stride=1, padding=0, )
paddle.einsum("blkd,bldq->blkq", Tensor([52, 5, 0, 1],"float32"), Tensor([52, 5, 1, 3],"float32"), )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 0, 7, 7],"float32"), output_size=tuple(7,7,), data_format="NCHW", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 16],"float32"), Tensor([6, 8, 3, 0],"float32"), bias=Tensor([8],"float32"), padding="same", output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([1, 128, 80, 80],"float32"), Tensor([128, 128, 0, 2],"float32"), bias=Tensor([128],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nanmedian(Tensor([2, 3, 4, 0],"float32"), axis=list[0,2,1,3,], keepdim=False, mode="min", )
paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 8, 1, 113],"float32"), Tensor([1, 8, 113, 0],"float32"), )
paddle.nn.functional.pad(Tensor([2, 0, 4, 4],"float64"), list[1,1,1,1,], mode="replicate", data_format="NCHW", )
paddle.nn.functional.pad(x=Tensor([1, 0, 1, 2, 3],"float64"), pad=tuple(2,2,1,1,0,0,), mode="reflect", data_format="NCDHW", )
paddle.nn.functional.max_pool1d(Tensor([0, 3, 16],"float32"), kernel_size=2, stride=2, return_mask=True, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[99,1,1,], )
paddle.nn.functional.conv3d(Tensor([4, 6, 0, 8, 8],"float32"), Tensor([8, 3, 3, 3, 3],"float32"), Tensor([8],"float32"), padding="same", stride=1, dilation=1, groups=2, data_format="NCDHW", )
paddle.nn.functional.max_pool3d(Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=list[1,0,0,], return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 0, 5, 2, 4],"float16"), Tensor([1, 0, 5, 2, 4],"float16"), )
paddle.nn.functional.pad(Tensor([4, 64, 0, 140],"float32"), pad=list[1,1,1,1,], mode="constant", value=0.0, data_format="NCHW", name=None, )
paddle.kthvalue(Tensor([30, 0, 40],"float32"), k=2, )
paddle.Tensor.tile(Tensor([1, 7, 0],"float32"), list[455,1,1,], )
paddle.slice(Tensor([8, 0, 20],"float16"), list[0,], list[4,], list[8,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[241,1,1,], )
paddle.nn.functional.conv3d_transpose(x=Tensor([0, 3, 2, 2, 2],"float64"), weight=Tensor([3, 2, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=0, dilation=1, )
paddle.nn.functional.conv2d(Tensor([1, 1, 101, 165],"float32"), Tensor([64, 1, 7, 0],"float32"), None, list[2,2,], 0, list[1,1,], 1, )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float64"), weight=Tensor([1, 3, 3, 3],"float64"), bias=Tensor([0],"float64"), stride=list[2,1,], padding=tuple(1,2,), dilation=tuple(2,2,), )
paddle.vision.ops.generate_proposals(Tensor([1, 15, 0, 60],"float32"), Tensor([1, 60, 40, 60],"float32"), Tensor([1, 2],"float32"), Tensor([36000, 4],"float32"), Tensor([36000, 4],"float32"), pre_nms_top_n=12000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float64"), weight=Tensor([3, 0, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,], dilation=2, )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 0, 7, 7],"float32"), output_size=list[None,3,], )
paddle.einsum("bind,snd->bnis", Tensor([0, 2, 4, 4],"float32"), Tensor([2, 4, 4],"float32"), )
paddle.nn.functional.group_norm(Tensor([0, 1024, 10, 26],"float32"), num_groups=32, epsilon=1e-05, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), )
paddle.nn.functional.conv1d_transpose(Tensor([0, 3, 2],"float32"), Tensor([3, 1, 3],"float32"), bias=Tensor([1],"float32"), output_size=None, output_padding=0, padding=list[1,], stride=list[1,], dilation=list[2,], groups=1, data_format="NCL", )
paddle.nn.functional.max_pool3d(Tensor([2, 3, 6, 6, 0],"float32"), 3, data_format="NDHWC", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 16, 16, 4],"float32"), Tensor([4, 3, 3, 0],"float32"), Tensor([6],"float32"), output_size=None, padding="valid", stride=tuple(1,2,), dilation=tuple(2,1,), groups=2, data_format="NHWC", )
paddle.nn.functional.max_pool3d(Tensor([2, 3, 0, 32, 32],"float32"), kernel_size=2, stride=None, padding="SAME", return_mask=True, )
paddle.nn.functional.mse_loss(Tensor([2, 96, 0],"float32"), Tensor([2, 96, 0],"float32"), reduction="none", )
paddle.nn.functional.log_softmax(Tensor([2, 0, 4, 5],"float32"), -1, None, )
paddle.allclose(tuple(Tensor([13, 0, 16],"float32"),Tensor([13, 0, 16],"float32"),), tuple(Tensor([13, 0, 16],"float32"),Tensor([13, 0, 16],"float32"),), rtol=0.0001, atol=0.0001, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 0],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )
paddle.logcumsumexp(Tensor([0, 12],"float16"), dtype="float16", axis=None, )
paddle.reverse(Tensor([6, 0, 3],"int64"), list[0,], )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([0, 3, 5, 7, 7],"float32"), output_size=list[None,3,None,], )
paddle.nn.functional.hinge_embedding_loss(Tensor([3, 0],"float32"), Tensor([3, 0],"float32"), reduction="none", margin=1.0, name=None, )
paddle.nn.functional.pad(Tensor([1, 16, 18, 14, 0],"float16"), tuple(0,0,0,3,0,0,), data_format="NDHWC", )
paddle.nn.functional.avg_pool1d(Tensor([2, 0, 8],"float64"), 2, 1, 0, True, True, None, )
paddle.nn.functional.conv2d(Tensor([4, 16, 16, 3],"float32"), Tensor([5, 3, 3, 0],"float32"), Tensor([5],"float32"), padding=list[1,1,2,2,], stride=1, dilation=2, groups=1, data_format="NHWC", )
paddle.nn.functional.conv2d_transpose(Tensor([1, 128, 0, 80],"float32"), Tensor([128, 128, 2, 2],"float32"), bias=Tensor([128],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 0, 8, 16],"bfloat16"), Tensor([1, 0, 2, 16],"bfloat16"), Tensor([1, 0, 2, 16],"bfloat16"), attn_mask=None, is_causal=True, )
paddle.Tensor.tile(Tensor([10285, 0],"float32"), list[1,45,], )
paddle.nn.functional.conv1d(x=Tensor([2, 3, 4],"float32"), weight=Tensor([1, 3, 3],"float32"), bias=Tensor([0],"float32"), stride=list[1,], padding=list[1,], dilation=tuple(2,), )
paddle.nn.functional.pad(Tensor([2, 0, 4, 4, 4],"float32"), list[1,1,1,1,1,1,], mode="circular", data_format="NCDHW", )
paddle.nn.functional.conv1d_transpose(x=Tensor([0, 3, 2],"float64"), weight=Tensor([3, 1, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,], groups=3, dilation=1, )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 0, 32],"float64"), output_size=list[1,3,2,], data_format="NCDHW", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 128, 32, 32],"float32"), Tensor([128, 1, 0, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=128, output_size=None, data_format="NCHW", )
paddle.nn.functional.log_softmax(x=Tensor([2, 0, 1],"float32"), axis=0, )
paddle.nn.functional.hinge_embedding_loss(Tensor([4, 0, 3, 4],"float64"), Tensor([4, 0, 3, 4],"float64"), reduction="sum", margin=-4.0, name=None, )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 0, 32, 32],"float64"), output_size=3, data_format="NCDHW", name=None, )
paddle.vision.ops.box_coder(prior_box=Tensor([80, 4],"float32"), prior_box_var=Tensor([80, 4],"float32"), target_box=Tensor([0, 80, 4],"float32"), code_type="decode_center_size", box_normalized=False, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 2, 2, 2, 3],"float32"), Tensor([3, 0, 3, 3, 3],"float32"), bias=Tensor([3],"float32"), padding=list[1,1,1,], output_padding=1, stride=list[2,2,2,], dilation=list[1,1,1,], groups=3, output_size=None, data_format="NDHWC", )
paddle.Tensor.tile(Tensor([1, 6, 0],"float32"), list[376,1,1,], )
paddle.einsum("ji,j", Tensor([1, 0],"float64"), Tensor([1],"float64"), )
paddle.einsum("ibm,hm->ibh", Tensor([1, 14, 32],"float32"), Tensor([0, 32],"float32"), )
paddle.nanmedian(Tensor([2, 0],"float32"), axis=1, mode="min", )
paddle.nn.functional.avg_pool2d(Tensor([64, 0, 28, 28],"float32"), kernel_size=3, stride=2, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.Tensor.__rpow__(Tensor([0, 16],"float32"), 10000.0, )
paddle.nn.functional.pad(x=Tensor([3, 0, 1, 2],"float64"), pad=list[1,1,2,3,], mode="constant", value=2.0, data_format="NCHW", )
paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), rtol=1e-05, atol=1e-08, equal_nan=True, name="test_2", )
paddle.nn.functional.prelu(Tensor([0, 2, 3, 4],"float32"), Tensor([1],"float32"), data_format="NCHW", )
paddle.logaddexp(Tensor([4, 0, 2],"float16"), Tensor([4, 0, 2],"float32"), )
paddle.Tensor.tile(Tensor([1360, 0],"float32"), list[1,31,], )
paddle.Tensor.tile(Tensor([140, 188, 0, 1, 1, 6],"float32"), list[1,1,1,1,2,1,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[425,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([2, 2, 3, 3],"float64"), Tensor([2, 0, 1, 1],"float64"), groups=1, padding=list[1,0,0,1,], )
paddle.slice(Tensor([0, 2, 100, 100],"float16"), axes=list[1,], starts=list[1,], ends=list[2,], )
paddle.nn.functional.max_pool3d(x=Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=tuple(1,0,0,), )
paddle.Tensor.__pow__(Tensor([4, 3, 0],"float16"), Tensor([4, 3, 0],"float32"), )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 8, 0],"float16"), Tensor([1, 2048, 2, 0],"float16"), Tensor([1, 2048, 2, 0],"float16"), attn_mask=Tensor([1, 1, 2048, 0],"float16"), is_causal=True, )
paddle.incubate.nn.functional.fused_rms_norm(Tensor([0, 512],"float16"), Tensor([512],"float16"), None, 1e-05, begin_norm_axis=1, )
paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 64],"float16"), Tensor([64],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, )
paddle.nn.functional.l1_loss(Tensor([0, 2],"float64"), Tensor([0, 2],"float64"), "mean", name=None, )
paddle.einsum("ij,kl->ijkl", Tensor([4, 0],"float64"), Tensor([3, 0],"float64"), )
paddle.Tensor.tile(Tensor([1, 0, 768],"float32"), list[32,1,1,], )
paddle.Tensor.tile(Tensor([1, 1, 64, 0, 2],"float32"), tuple(16,10,1,1,1,), )
paddle.Tensor.tile(Tensor([4, 0, 512],"float32"), tuple(1,12,1,), )
paddle.slice(Tensor([1024, 2, 0],"float16"), axes=list[0,], starts=list[896,], ends=list[1024,], )
paddle.nn.functional.conv2d(Tensor([1024, 1, 258, 258],"float32"), Tensor([1, 1, 4, 0],"float32"), )
paddle.nn.functional.max_pool2d(x=Tensor([2, 0, 24, 24],"float32"), kernel_size=2, stride=2, padding=0, )
paddle.einsum("se,sec->sec", Tensor([2, 60],"float32"), Tensor([2, 60, 0],"float32"), )
paddle.nn.functional.conv2d_transpose(x=Tensor([0, 3, 2, 2],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,], dilation=1, output_size=None, data_format="NCHW", )
paddle.einsum("blkd,bldq->blkq", Tensor([52, 0, 1, 1],"float32"), Tensor([52, 0, 1, 3],"float32"), )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 0, 2],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,], dilation=1, output_size=None, data_format="NCHW", )
paddle.slice(Tensor([0, 1, 104],"float32"), list[0,1,2,], list[0,0,80,], list[3,1,96,], )
paddle.nn.functional.lp_pool2d(Tensor([2, 0, 32, 32],"float32"), math.inf, kernel_size=list[2,4,], stride=2, ceil_mode=False, )
paddle.nn.functional.max_pool1d(Tensor([2, 0, 8],"float64"), 2, 2, 0, False, False, None, )
paddle.nn.functional.kl_div(input=Tensor([32, 128, 0],"float32"), label=Tensor([32, 128, 0],"float32"), reduction="batchmean", )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 512, 0, 128],"float32"), output_size=tuple(6,6,), data_format="NCHW", name=None, )
paddle.nn.functional.max_pool1d(Tensor([0, 3, 16],"float64"), kernel_size=2, stride=2, return_mask=True, )
paddle.nn.functional.pad(Tensor([1, 0, 14, 14, 384],"float32"), tuple(0,0,0,0,0,0,), data_format="NDHWC", )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 4, 0, 3],"float64"), output_size=tuple(3,3,), data_format="NHWC", )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[489,1,1,], )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 0],"float32"), weight=Tensor([3, 1, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=1, output_padding=0, dilation=1, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[144,1,1,], )
paddle.nn.functional.pad(Tensor([1, 0, 184, 204],"float64"), pad=list[52,52,0,0,], mode="constant", value=0.0, data_format="NCHW", )
paddle.nn.functional.max_pool2d(x=Tensor([2, 0, 32, 32],"float32"), kernel_size=list[2,2,], )
paddle.nn.functional.max_pool2d(Tensor([0, 64, 17, 257],"float32"), tuple(3,3,), tuple(2,2,), tuple(0,0,), False, )
paddle.allclose(Tensor([0],"int64"), Tensor([0],"int64"), 50.0, 49.0, False, )
paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 0, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln1_scale=Tensor([2],"float32"), ln1_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 2, 2, 6],"float64"), weight=Tensor([6, 0, 3, 3],"float64"), bias=Tensor([3],"float64"), stride=1, padding=list[1,0,], groups=3, data_format="NHWC", dilation=1, )
paddle.nn.functional.conv3d_transpose(Tensor([4, 8, 8, 8, 4],"float32"), Tensor([4, 3, 0, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding="valid", stride=tuple(1,2,1,), dilation=tuple(2,1,1,), groups=2, data_format="NDHWC", )
paddle.nn.functional.maxout(x=Tensor([100, 4, 0, 3],"float64"), groups=2, )
paddle.nn.functional.pad(Tensor([0, 16, 14, 19, 384],"float32"), tuple(0,2,0,0,0,0,), data_format="NDHWC", )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 0],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
paddle.nn.functional.conv1d(Tensor([16, 64, 25500],"float32"), Tensor([128, 64, 0],"float32"), bias=Tensor([128],"float32"), padding=16, stride=list[1,], dilation=list[16,], groups=1, data_format="NCL", )
paddle.nn.functional.conv2d(Tensor([2, 192, 4, 4],"float32"), Tensor([384, 192, 0, 1],"float32"), None, list[2,2,], 0, list[1,1,], 1, "NCHW", )
paddle.allclose(x=Tensor([2, 2, 0, 3],"float64"), y=Tensor([2, 2, 0, 3],"float64"), )
paddle.nn.functional.conv2d_transpose(Tensor([4, 16, 16, 3],"float32"), Tensor([3, 5, 0, 3],"float32"), Tensor([5],"float32"), output_size=None, padding=0, stride=1, dilation=1, groups=1, data_format="NHWC", )
paddle.Tensor.tile(Tensor([19125, 0],"float32"), list[1,131,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[398,1,1,], )
paddle.slice(Tensor([0, 8, 24],"float32"), axes=list[1,], starts=list[4,], ends=list[5,], )
paddle.nn.functional.pixel_unshuffle(Tensor([2, 0, 12, 1],"float64"), 3, "NHWC", None, )
paddle.allclose(Tensor([0, 5],"float32"), Tensor([0, 5],"float32"), atol=1e-06, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[80,1,1,], )
paddle.nn.functional.channel_shuffle(Tensor([2, 9, 0, 4],"float64"), 3, "NCHW", )
paddle.nn.functional.max_pool2d(Tensor([2, 3, 32, 0],"float32"), kernel_size=2, stride=2, padding=1, ceil_mode=False, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 2, 2, 6],"float32"), Tensor([6, 1, 0, 3],"float32"), bias=None, padding=list[1,0,], output_padding=1, stride=list[2,2,], dilation=list[1,1,], groups=3, output_size=None, data_format="NHWC", )
paddle.nn.functional.pad(Tensor([12, 1, 128, 0],"float32"), list[2,1,2,1,], )
paddle.index_fill(Tensor([128],"int64"), Tensor([0],"int64"), 0, 5, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 0, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[161,1,1,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[405,1,1,], )
paddle.nn.functional.temporal_shift(Tensor([128, 0, 14, 14],"float16"), 8, 0.125, )
paddle.roll(Tensor([4, 5, 0],"float64"), Tensor([3],"int64"), list[0,1,2,], name=None, )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([0, 3, 8, 32, 32],"float64"), output_size=tuple(3,3,3,), data_format="NCDHW", )
paddle.nn.functional.avg_pool3d(Tensor([2, 3, 8, 8, 0],"float64"), kernel_size=list[3,3,3,], stride=2, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 0],"float32"), Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.conv2d(Tensor([3, 8, 4, 4],"float32"), Tensor([8, 8, 3, 0],"float32"), None, list[2,2,], 1, list[1,1,], 1, "NCHW", )
paddle.nn.functional.pad(Tensor([0, 3, 4],"float64"), list[1,1,], mode="replicate", data_format="NCL", )
paddle.nn.functional.max_pool2d(Tensor([0, 64, 112, 112],"float32"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float64"), weight=Tensor([1, 3, 0, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=2, padding=1, )
paddle.nn.functional.pad(Tensor([1, 21955, 0],"float32"), list[1,0,], value=0, mode="constant", data_format="NCL", )
paddle.nn.functional.max_pool2d(Tensor([12, 0, 40, 40],"float32"), 3, stride=1, padding=1, data_format="NCHW", )
paddle.nn.functional.softmax(Tensor([0, 1, 16, 49, 49],"float32"), -1, name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 256, 47, 0],"float32"), Tensor([256, 128, 4, 4],"float32"), bias=None, padding=0, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 8, 0, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[1,2,1,], dilation=list[2,2,2,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.avg_pool2d(Tensor([0, 528, 13, 13],"float32"), kernel_size=5, stride=3, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[401,1,1,], )
paddle.nanquantile(Tensor([4, 7, 0],"float64"), q=0.1, axis=list[1,2,], keepdim=True, )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 3, 7, 0],"float32"), output_size=list[3,3,], data_format="NHWC", )
paddle.nn.functional.avg_pool3d(Tensor([2, 3, 8, 0, 8],"float64"), kernel_size=list[3,3,3,], stride=2, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
paddle.nn.functional.fractional_max_pool2d(x=Tensor([0, 3, 7, 7],"float32"), output_size=5, random_u=0.5, )
paddle.nn.functional.max_pool1d(Tensor([1, 0, 16],"float64"), kernel_size=2, stride=2, return_mask=True, )
paddle.nn.functional.conv2d_transpose(x=Tensor([0, 3, 8, 8],"float64"), weight=Tensor([3, 1, 5, 5],"float64"), bias=Tensor([1],"float64"), stride=1, padding="sAmE", output_padding=0, dilation=1, groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.flashmask_attention(Tensor([1, 2048, 8, 96],"float16"), Tensor([1, 2048, 8, 96],"float16"), Tensor([1, 0, 8, 96],"float16"), startend_row_indices=Tensor([1, 1, 2048, 1],"int32"), causal=True, )
paddle.nn.functional.avg_pool2d(Tensor([0, 104, 28, 28],"float16"), kernel_size=3, stride=2, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.conv1d(Tensor([2, 4, 3],"float32"), Tensor([2, 3, 0],"float32"), bias=Tensor([2],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NLC", )
paddle.nn.functional.pad(Tensor([1, 16, 15, 14, 0],"float16"), tuple(0,0,0,6,0,0,), data_format="NDHWC", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[564,1,1,], )
paddle.nn.functional.avg_pool3d(Tensor([2, 8, 8, 8, 0],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NDHWC", name=None, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 128, 0, 32],"float32"), output_size=tuple(6,6,), data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[1,100,1,], )
paddle.Tensor.imag(Tensor([0, 257, 511],"complex64"), )
paddle.Tensor.tile(Tensor([1, 10, 0],"float32"), list[870,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([4, 16, 16, 6],"float32"), Tensor([6, 8, 3, 0],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NHWC", )
paddle.Tensor.tile(Tensor([1, 6, 0],"float32"), list[612,1,1,], )
paddle.slice(Tensor([4, 0, 5],"float32"), list[0,], list[2,], list[4,], )
paddle.nn.functional.hinge_embedding_loss(Tensor([10, 10, 0],"float64"), Tensor([10, 10, 0],"float64"), reduction="sum", )
paddle.nn.functional.selu(Tensor([2, 0],"float32"), 1.0507009873554805, 1.6732632423543772, None, )
paddle.vision.ops.prior_box(Tensor([4, 48, 0, 40],"float32"), Tensor([4, 3, 640, 640],"float32"), list[32.0,48.0,64.0,80.0,96.0,128.0,], list[], list[1.0,], list[0.1,0.1,0.2,0.2,], False, False, list[16.0,16.0,], 0.5, False, None, )
paddle.nn.functional.l1_loss(Tensor([3, 0],"float32"), Tensor([3, 0],"float32"), reduction="mean", )
paddle.nn.quant.weight_only_linear(Tensor([0, 32, 64],"float16"), Tensor([128, 64],"int8"), bias=Tensor([256],"float16"), weight_scale=Tensor([256],"float16"), weight_dtype="int4", group_size=-1, )
paddle.vision.ops.prior_box(Tensor([4, 96, 0, 40],"float32"), Tensor([4, 3, 640, 640],"float32"), list[32.0,48.0,64.0,80.0,96.0,128.0,], list[], list[1.0,], list[0.1,0.1,0.2,0.2,], False, False, list[16.0,16.0,], 0.5, False, None, )
paddle.einsum("ibnd,jbnd->bnij", Tensor([7, 10, 4, 0],"float32"), Tensor([7, 10, 4, 0],"float32"), )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 5, 7, 0],"float32"), output_size=list[2,3,5,], )
paddle.nn.functional.conv2d_transpose(Tensor([2, 128, 124, 108],"float32"), Tensor([128, 128, 2, 0],"float32"), bias=None, padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([0, 3, 280, 350],"float32"), pad=list[2,2,2,2,], mode="constant", value=0.0, data_format="NCHW", )
paddle.nn.functional.multi_margin_loss(Tensor([5, 0],"float64"), Tensor([5],"int64"), p=1, margin=1.0, weight=None, reduction="none", name=None, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([0, 128, 28, 28],"float16"), output_size=7, data_format="NCHW", name=None, )
paddle.Tensor.__pow__(Tensor([4, 3, 0],"float16"), Tensor([4, 3, 0],"float64"), )
paddle.Tensor.tile(Tensor([1, 4, 0],"float32"), list[461,1,1,], )
paddle.nn.functional.prelu(x=Tensor([1, 2, 0, 4],"float64"), weight=Tensor([1],"float64"), )
paddle.Tensor.tile(Tensor([1, 0],"float32"), tuple(6380,1,), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[648,1,1,], )
paddle.Tensor.tile(Tensor([1, 7, 0],"float32"), list[540,1,1,], )
paddle.einsum("bind,snd->bnis", Tensor([13, 4, 4, 4],"float32"), Tensor([0, 4, 4],"float32"), )
paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 512],"int32"), bias=Tensor([512],"float16"), dequant_scales=Tensor([0],"float32"), shift=Tensor([512],"float16"), smooth=Tensor([512],"float16"), act_method="gelu", compute_dtype="fp16", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.nn.functional.fractional_max_pool2d(x=Tensor([0, 3, 7, 7],"float32"), output_size=list[2,5,], random_u=0.7, )
paddle.nn.functional.pad(Tensor([0, 1, 1, 1],"float32"), list[0,0,0,0,], )
paddle.nn.functional.kl_div(input=Tensor([32, 0, 128],"float32"), label=Tensor([32, 0, 128],"float32"), reduction="batchmean", )
paddle.nn.functional.conv2d(Tensor([1, 1024, 256, 256],"float32"), Tensor([1024, 128, 3, 0],"float32"), padding=1, groups=8, )
paddle.nanmedian(Tensor([2, 3, 0, 5],"float32"), axis=2, keepdim=False, )
paddle.allclose(Tensor([0, 13],"float32"), Tensor([0, 13],"float32"), rtol=0.0001, atol=0.0001, )
paddle.nn.functional.conv1d(Tensor([13, 24, 0],"float32"), Tensor([24, 12, 16],"float32"), bias=Tensor([24],"float32"), padding=8, stride=list[1,], dilation=list[1,], groups=2, data_format="NCL", )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float64"), weight=Tensor([1, 3, 0, 3],"float64"), bias=Tensor([1],"float64"), stride=2, padding=0, )
paddle.nn.functional.conv3d_transpose(Tensor([4, 8, 8, 8, 4],"float32"), Tensor([4, 3, 0, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=0, stride=1, dilation=1, groups=2, data_format="NDHWC", )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float64"), weight=Tensor([3, 0, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,], dilation=2, output_size=None, data_format="NCHW", )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 0, 128],"float16"), Tensor([1, 2048, 0, 128],"float16"), Tensor([1, 2048, 0, 128],"float16"), attn_mask=Tensor([1, 1, 0, 2048],"float16"), dropout_p=0.0, training=True, is_causal=False, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 0, 2],"float32"), weight=Tensor([3, 1, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=list[1,0,], dilation=2, )
paddle.nn.functional.lp_pool2d(Tensor([0, 3, 32, 32],"float64"), norm_type=2.0, kernel_size=5, stride=3, padding=0, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([1, 256, 14, 0],"float32"), Tensor([256, 256, 2, 2],"float32"), bias=Tensor([256],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.allclose(tuple(Tensor([0, 7],"float32"),Tensor([0, 7],"float32"),), tuple(Tensor([0, 7],"float32"),Tensor([0, 7],"float32"),), rtol=0.0001, atol=0.0001, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[723,1,1,], )
paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 0, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln2_scale=Tensor([2],"float32"), ln2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, )
paddle.nn.functional.layer_norm(Tensor([0, 10, 60, 30],"float32"), list[10,60,30,], weight=Tensor([18000],"float32"), bias=Tensor([18000],"float32"), epsilon=1e-05, )
paddle.nn.functional.conv2d(Tensor([4, 16, 0, 3],"float32"), Tensor([5, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[1,2,3,1,], stride=2, dilation=1, groups=1, data_format="NHWC", )
paddle.nn.functional.pad(Tensor([4, 6, 0],"float32"), pad=list[2,2,], mode="constant", value=0.0, data_format="NCL", name=None, )
paddle.nn.functional.max_pool3d(Tensor([1, 4, 4, 4, 0],"float32"), list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], data_format="NDHWC", )
paddle.Tensor.tile(Tensor([12906, 1, 0],"float32"), list[1,215,1,], )
paddle.einsum("aaa->a", Tensor([0, 5, 5],"float64"), )
paddle.einsum("blqd,bmdk->blqk", Tensor([13, 5, 0, 1],"float32"), Tensor([13, 1, 0, 1],"float32"), )
paddle.nn.functional.hinge_embedding_loss(Tensor([3, 0],"float64"), Tensor([3, 0],"float64"), reduction="mean", margin=1.0, name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 16],"float32"), Tensor([6, 8, 3, 0],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[2,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[373,1,1,], )
paddle.nn.functional.pad(Tensor([0, 1, 1, 1],"float16"), pad=list[0,0,0,1,], mode="replicate", value=0.0, data_format="NCHW", name=None, )
paddle.nn.functional.conv3d(Tensor([4, 8, 8, 8, 3],"float32"), Tensor([5, 3, 0, 3, 3],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[1,1,],list[2,2,],list[1,1,],list[0,0,],], stride=1, dilation=1, groups=1, data_format="NDHWC", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[155,1,1,], )
paddle.slice(Tensor([13, 0, 104],"float32"), list[0,1,2,], list[0,0,8,], list[13,1,24,], )
paddle.einsum("blkd,bldq->blkq", Tensor([13, 5, 1, 1],"float32"), Tensor([13, 5, 0, 3],"float32"), )
paddle.nn.functional.temporal_shift(x=Tensor([2, 4, 0, 3],"float64"), seg_num=2, shift_ratio=0.4, data_format="NHWC", )
paddle.nn.functional.flashmask_attention(Tensor([1, 2048, 8, 96],"float16"), Tensor([1, 2048, 8, 0],"float16"), Tensor([1, 2048, 8, 96],"float16"), startend_row_indices=Tensor([1, 1, 2048, 1],"int32"), causal=True, )
paddle.nn.functional.conv1d_transpose(Tensor([0, 3, 2],"float64"), Tensor([3, 1, 3],"float64"), bias=Tensor([1],"float64"), output_size=None, output_padding=0, padding=list[1,], stride=list[2,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv3d(Tensor([4, 3, 8, 8, 8],"float32"), Tensor([5, 3, 3, 0, 3],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[0,0,],list[1,1,],list[2,2,],list[2,2,],], stride=1, dilation=1, groups=1, data_format="NCDHW", )
paddle.nn.functional.pad(Tensor([1, 0, 31, 28, 192],"float16"), tuple(0,0,0,4,0,0,), data_format="NDHWC", )
paddle.nn.functional.margin_ranking_loss(Tensor([0],"float16"), Tensor([0],"float16"), Tensor([0],"float16"), 0.5, "mean", None, )
paddle.nn.functional.softmax(x=Tensor([2, 3, 0],"float64"), )
paddle.vision.ops.prior_box(Tensor([4, 96, 80, 0],"float32"), Tensor([4, 3, 640, 640],"float32"), list[16.0,24.0,], list[], list[1.0,], list[0.1,0.1,0.2,0.2,], False, False, list[8.0,8.0,], 0.5, False, None, )
paddle.nn.functional.avg_pool3d(x=Tensor([2, 8, 8, 8, 0],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NDHWC", exclusive=False, )
paddle.nn.functional.hinge_embedding_loss(Tensor([10, 0, 5],"float64"), Tensor([10, 0, 5],"float64"), reduction="none", )
paddle.nn.functional.fractional_max_pool2d(x=Tensor([2, 0, 7, 7],"float16"), output_size=list[3,3,], random_u=0.3, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 2],"float32"), Tensor([3, 1, 3, 3],"float32"), bias=Tensor([0],"float32"), padding=0, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.max_pool1d(Tensor([1, 0, 2],"float64"), 2, 2, 0, True, False, None, )
paddle.einsum("b h i d, b h j d -> b h i j", Tensor([0, 8, 1, 64],"float32"), Tensor([0, 8, 109, 64],"float32"), )
paddle.Tensor.tile(Tensor([121, 0],"float32"), list[2,1,1,], )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[208,1,1,], )
paddle.nn.functional.lp_pool2d(Tensor([0, 3, 32, 32],"float64"), 2, kernel_size=5, stride=3, ceil_mode=False, )
paddle.nanmedian(Tensor([2, 3, 4, 0],"float32"), axis=list[0,1,3,], keepdim=False, mode="min", )
paddle.nn.functional.max_pool2d(Tensor([0, 512, 1, 40],"float16"), kernel_size=tuple(1,1,), stride=1, padding=0, )
paddle.nn.functional.pad(x=Tensor([0, 1, 1, 2, 3],"float64"), pad=tuple(2,1,3,0,2,0,), mode="replicate", data_format="NDHWC", )
paddle.einsum("bhlqk,bhkd->bhlqd", Tensor([52, 4, 0, 1, 1],"float32"), Tensor([52, 4, 1, 8],"float32"), )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float64"), weight=Tensor([1, 3, 3, 0, 3],"float64"), bias=Tensor([1],"float64"), stride=list[2,2,1,], padding=1, dilation=list[2,2,2,], )
paddle.Tensor.tile(Tensor([1, 5, 0],"float32"), list[508,1,1,], )
paddle.einsum("bnij,jbnd->ibnd", Tensor([10, 4, 7, 0],"float32"), Tensor([7, 10, 4, 0],"float32"), )
paddle.Tensor.fill_diagonal_(Tensor([0, 3],"float32"), 1, offset=2, wrap=True, )
paddle.nn.quant.weight_only_linear(Tensor([0, 32, 128],"float16"), Tensor([288, 128],"int8"), bias=Tensor([288],"float16"), weight_scale=Tensor([288],"float16"), weight_dtype="int8", group_size=-1, )
paddle.nn.functional.pad(Tensor([2, 0, 4],"float32"), list[1,1,], mode="replicate", data_format="NCL", )
paddle.Tensor.isnan(Tensor([3, 6, 3, 4, 2, 0],"float64"), )
paddle.nn.functional.max_pool1d(x=Tensor([0, 3, 8],"float64"), kernel_size=2, stride=2, padding=0, )
paddle.nn.functional.lp_pool2d(Tensor([2, 0, 32, 32],"float32"), norm_type=math.inf, kernel_size=list[2,4,], stride=2, padding=0, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 3, 0, 4],"float64"), output_size=list[1,1,], data_format="NCHW", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([1, 512, 16, 16],"float32"), Tensor([512, 512, 3, 0],"float32"), padding=0, stride=2, groups=1, )
paddle.nn.functional.softmax(Tensor([0, 165, 126],"float32"), axis=-1, )
paddle.nn.functional.softmax(Tensor([2, 3, 0],"float64"), 1, name=None, )
paddle.Tensor.tile(Tensor([1, 512, 4, 0],"float32"), tuple(8,1,1,1,), )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[569,1,1,], )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 8, 16],"float16"), Tensor([1, 2048, 2, 0],"float16"), Tensor([1, 2048, 2, 16],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), is_causal=True, )
paddle.reverse(Tensor([12, 0, 8],"float64"), axis=0, )
paddle.nn.functional.avg_pool2d(Tensor([13, 1, 0, 32],"float32"), tuple(2,1,), stride=tuple(2,1,), ceil_mode=True, )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[534,1,1,], )
paddle.nn.functional.pad(Tensor([1, 0, 2, 3, 2],"float64"), pad=list[1,0,1,0,0,1,], mode="replicate", value=0.0, data_format="NCDHW", name=None, )
paddle.nn.functional.group_norm(Tensor([2, 0, 4],"float64"), num_groups=2, weight=Tensor([4],"float64"), bias=Tensor([4],"float64"), data_format="NLC", )
paddle.searchsorted(Tensor([5],"float64"), Tensor([2, 0],"float64"), right=True, )
paddle.nn.functional.conv2d(Tensor([1, 2048, 0, 16],"float32"), Tensor([2048, 512, 0, 3],"float32"), padding=1, groups=4, )
paddle.nn.functional.conv2d(Tensor([1, 2048, 16, 0],"float32"), Tensor([2048, 512, 3, 0],"float32"), padding=1, groups=4, )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float32"), weight=Tensor([1, 3, 3, 3],"float32"), bias=Tensor([0],"float32"), stride=1, padding=0, )
paddle.nn.functional.pad(Tensor([12, 16, 16, 0],"float32"), tuple(0,0,0,0,), data_format="NHWC", )
paddle.nn.functional.pixel_unshuffle(Tensor([2, 0, 12, 12],"float32"), 3, "NCHW", )
paddle.nn.functional.conv1d_transpose(Tensor([2, 2, 3],"float32"), Tensor([3, 0, 3],"float32"), bias=Tensor([3],"float32"), output_size=None, output_padding=0, padding=list[1,], stride=list[1,], dilation=list[1,], groups=3, data_format="NLC", )
paddle.nn.functional.conv2d(Tensor([2, 192, 0, 4],"float32"), Tensor([96, 192, 1, 1],"float32"), None, list[1,1,], 0, list[1,1,], 1, "NCHW", )
paddle.nn.functional.margin_ranking_loss(Tensor([2, 0],"float64"), Tensor([2, 0],"float64"), Tensor([2, 0],"float64"), 0.0, "mean", None, )
paddle.nn.functional.softmax(Tensor([1024, 4, 0, 49],"float32"), -1, name=None, )
paddle.nn.functional.max_pool1d(Tensor([2, 0, 8],"float64"), 2, list[1,], 1, False, False, None, )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 2],"float64"), weight=Tensor([3, 0, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,1,], dilation=2, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[367,1,1,], )
paddle.strided_slice(x=Tensor([6, 0],"float32"), axes=list[0,1,], starts=list[3,4,], ends=list[5,2,], strides=list[1,-2,], )
paddle.einsum("blkd,bldq->blkq", Tensor([13, 5, 1, 0],"float32"), Tensor([13, 5, 1, 0],"float32"), )
paddle.strided_slice(x=Tensor([0, 6],"float32"), axes=list[0,1,], starts=list[3,4,], ends=list[5,2,], strides=list[1,-2,], )
paddle.nn.functional.conv3d_transpose(Tensor([4, 4, 8, 8, 8],"float32"), Tensor([4, 3, 3, 0, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,1,2,2,1,1,], stride=1, dilation=1, groups=2, data_format="NCDHW", )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[238,1,1,], )
paddle.nn.functional.conv1d(Tensor([4, 16, 3],"float32"), Tensor([6, 1, 0],"float32"), bias=Tensor([6],"float32"), padding="valid", stride=list[1,], dilation=list[1,], groups=3, data_format="NLC", )
paddle.gammaln(Tensor([0, 20, 1],"float32"), )
paddle.reverse(Tensor([12, 0],"float64"), axis=list[0,], )
paddle.nn.functional.maxout(Tensor([2, 0, 5, 4],"float64"), 2, -1, )
paddle.nn.functional.avg_pool3d(Tensor([0, 1, 7, 40, 40],"float32"), kernel_size=tuple(5,1,1,), stride=1, )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 0],"float32"), weight=Tensor([1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=list[2,1,], padding=tuple(1,2,), dilation=tuple(2,2,), )
paddle.slice(Tensor([16, 0, 104],"float32"), list[0,1,2,], list[0,0,24,], list[16,2,40,], )
paddle.nn.functional.conv2d_transpose(Tensor([2, 2, 2, 6],"float64"), Tensor([6, 1, 3, 0],"float64"), bias=Tensor([3],"float64"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=3, output_size=None, data_format="NHWC", )
paddle.nn.functional.pad(Tensor([1, 16, 31, 0, 192],"float16"), tuple(0,0,0,4,0,0,), data_format="NDHWC", )
paddle.nn.functional.pad(Tensor([1048576, 1, 1, 0],"float32"), list[0,0,0,0,], )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float64"), weight=Tensor([1, 3, 3, 3, 0],"float64"), bias=Tensor([1],"float64"), stride=list[2,2,1,], padding=1, dilation=tuple(2,2,2,), )
paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 0],"float32"), bias=Tensor([512],"float32"), act_method="gelu", compute_dtype="fp32", )
paddle.nn.functional.pad(Tensor([1, 16, 14, 0, 384],"float16"), tuple(0,6,0,0,0,0,), data_format="NDHWC", )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([1, 3, 3, 3, 0],"float32"), bias=Tensor([1],"float32"), stride=list[2,2,1,], padding=1, dilation=list[2,2,2,], )
paddle.Tensor.tile(Tensor([144, 0],"float32"), list[32,1,1,], )
paddle.nn.functional.layer_norm(Tensor([14, 0, 384],"float32"), 384, weight=Tensor([384],"float32"), bias=Tensor([384],"float32"), epsilon=1e-05, )
paddle.Tensor.tile(Tensor([1, 0, 512],"float32"), tuple(1,14,1,), )
paddle.roll(Tensor([2, 0, 2],"float64"), Tensor([3],"int64"), list[0,1,2,], name=None, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 0, 1, 64],"float16"), Tensor([2, 100, 1, 64],"float16"), Tensor([2, 100, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 100],"float16"), is_causal=False, )
paddle.nn.functional.maxout(Tensor([2, 6, 0, 4],"float64"), 2, -1, )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[280,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([2, 2, 2, 6],"float64"), Tensor([6, 0, 3, 3],"float64"), bias=Tensor([3],"float64"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=3, output_size=None, data_format="NHWC", )
paddle.nn.quant.weight_only_linear(Tensor([123, 768],"float16"), weight=Tensor([0, 768],"int8"), bias=None, weight_scale=Tensor([2304],"float16"), weight_dtype="int8", )
paddle.Tensor.tile(Tensor([1, 5, 0],"float32"), list[351,1,1,], )
paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 8, 0, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=1, exclusive=True, )
paddle.nn.functional.pad(Tensor([1536, 1, 0, 1],"float32"), list[0,1,0,1,], )
paddle.einsum("bhlqk,bhkd->bhlqd", Tensor([13, 4, 3, 0, 1],"float32"), Tensor([13, 4, 1, 8],"float32"), )
paddle.nn.functional.conv2d_transpose(Tensor([4, 256, 16, 16],"float32"), Tensor([256, 0, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=256, output_size=None, data_format="NCHW", )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 0, 128, 128],"float32"), output_size=tuple(3,3,), data_format="NCHW", name=None, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([0, 1024, 4, 80],"float32"), list[1,40,], )
paddle.meshgrid(list[Tensor([10],"float64"),Tensor([0],"float64"),Tensor([10],"float64"),Tensor([10],"float64"),Tensor([10],"float64"),], )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float32"), weight=Tensor([1, 3, 3, 3],"float32"), bias=Tensor([0],"float32"), stride=2, padding=1, )
paddle.nn.functional.prelu(x=Tensor([1, 2, 3, 0],"float32"), weight=Tensor([1],"float32"), )
paddle.nn.functional.pad(Tensor([0, 64, 188, 140],"float32"), pad=list[1,1,1,1,], mode="constant", value=0.0, data_format="NCHW", name=None, )
paddle.nn.functional.max_pool2d(Tensor([13, 256, 0, 7],"float32"), kernel_size=1, stride=2, padding=0, )
paddle.slice(Tensor([3, 0, 104],"float32"), list[0,1,2,], list[0,0,40,], list[3,1,56,], )
paddle.nn.functional.avg_pool1d(x=Tensor([0, 3, 8],"float64"), kernel_size=2, stride=1, padding=0, ceil_mode=True, )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 5, 0, 7],"float32"), output_size=list[2,3,5,], )
paddle.vision.ops.generate_proposals(Tensor([1, 4, 16, 16],"float32"), Tensor([1, 16, 16, 16],"float32"), Tensor([1, 2],"float32"), Tensor([0, 16, 4, 4],"float32"), Tensor([16, 16, 4, 4],"float32"), pre_nms_top_n=12000, post_nms_top_n=5000, nms_thresh=0.7, min_size=3.0, eta=1.0, pixel_offset=True, return_rois_num=True, )
paddle.nn.functional.conv1d_transpose(Tensor([4, 6, 16],"float32"), Tensor([6, 8, 0],"float32"), bias=Tensor([8],"float32"), output_size=None, output_padding=0, padding=list[1,2,], stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.strided_slice(x=Tensor([5, 8, 6, 0, 2, 6],"float64"), axes=list[1,2,5,], starts=list[6,5,4,], ends=list[2,0,1,], strides=list[-1,-2,-3,], )
paddle.Tensor.digamma(Tensor([2, 0],"float32"), )
paddle.roll(Tensor([0, 5, 4, 4],"complex128"), Tensor([2],"int64"), tuple(1,3,), name=None, )
paddle.nn.functional.conv2d(Tensor([3, 8, 4, 0],"float32"), Tensor([8, 8, 3, 0],"float32"), None, list[2,2,], 1, list[1,1,], 1, "NCHW", )
paddle.nn.functional.max_pool3d(Tensor([2, 3, 32, 32, 0],"float32"), kernel_size=2, stride=None, padding="SAME", return_mask=True, )
paddle.nn.functional.max_unpool3d(Tensor([0, 3, 2, 2, 3],"float64"), Tensor([0, 3, 2, 2, 3],"int32"), kernel_size=2, stride=None, )
paddle.nn.functional.maxout(x=Tensor([0, 9, 3, 3],"float64"), groups=3, )
paddle.einsum("ij,ij->", Tensor([2, 0],"float32"), Tensor([2, 0],"float32"), )
paddle.nn.functional.pad(Tensor([3, 0, 5, 6],"complex64"), pad=list[1,2,2,1,], mode="circular", value=0.0, data_format="NCHW", name=None, )
paddle.nn.functional.conv3d_transpose(Tensor([0, 6, 8, 8, 8],"float32"), Tensor([6, 8, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=list[list[0,0,],list[0,0,],list[2,3,],list[1,2,],list[2,1,],], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float32"), Tensor([3, 0, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[2,2,2,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.fractional_max_pool2d(x=Tensor([0, 3, 7, 7],"float32"), return_mask=False, output_size=list[3,3,], random_u=0.3, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[768,1,1,], )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 0],"float32"), weight=Tensor([3, 2, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=0, dilation=1, )
paddle.slice(Tensor([3, 0, 104],"float32"), list[0,1,2,], list[0,0,16,], list[3,1,32,], )
paddle.nn.functional.pad(Tensor([1, 16, 61, 56, 0],"float32"), tuple(0,0,0,2,0,0,), data_format="NDHWC", )
paddle.Tensor.tile(Tensor([1, 6, 0],"float32"), list[469,1,1,], )
paddle.nn.functional.max_pool2d(Tensor([1, 256, 0, 27],"float32"), 1, stride=2, )
paddle.nn.functional.avg_pool2d(Tensor([0, 1, 44, 40],"float32"), kernel_size=tuple(5,1,), stride=1, )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([0, 3, 5, 7, 7],"float32"), output_size=5, )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 32, 0],"float64"), output_size=list[1,3,2,], data_format="NCDHW", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([0, 6, 2, 2],"float32"), Tensor([6, 1, 3, 3],"float32"), bias=Tensor([3],"float32"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=3, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[650,1,1,], )
paddle.nn.functional.conv2d(x=Tensor([2, 4, 4, 3],"float32"), weight=Tensor([2, 3, 0, 3],"float32"), bias=Tensor([2],"float32"), stride=1, padding=0, data_format="NHWC", )
paddle.slice(Tensor([0, 1, 104],"float32"), list[0,1,2,], list[0,0,64,], list[3,1,80,], )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float32"), weight=Tensor([1, 3, 3, 0],"float32"), bias=Tensor([1],"float32"), stride=list[2,1,], padding=1, dilation=tuple(2,2,), )
paddle.nn.functional.kl_div(Tensor([40, 20, 0],"float32"), Tensor([40, 20, 0],"float32"), "batchmean", True, )
paddle.nn.functional.pad(Tensor([16, 3, 0, 256],"float32"), list[6,6,6,6,], )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float64"), weight=Tensor([1, 3, 3, 3, 0],"float64"), bias=Tensor([1],"float64"), stride=list[2,2,1,], padding=list[1,2,2,], dilation=tuple(2,2,2,), )
paddle.nn.functional.conv2d(Tensor([8, 128, 0, 256],"float32"), Tensor([128, 128, 3, 3],"float32"), bias=None, stride=1, padding=1, )
paddle.incubate.nn.functional.fused_bias_act(Tensor([1, 0],"float16"), None, act_method="swiglu", )
paddle.nn.functional.pad(Tensor([0, 2, 3],"float64"), pad=list[1,2,], mode="replicate", value=0.0, data_format="NLC", name=None, )
paddle.Tensor.fill_diagonal_(Tensor([3, 0],"float64"), 4, 1, False, )
paddle.Tensor.__getitem__(Tensor([3, 0, 3],"float32"), tuple(slice(1,2,None),slice(2,None,None),slice(None,None,-1),), )
paddle.nn.functional.avg_pool2d(x=Tensor([0, 3, 32, 32],"float64"), kernel_size=list[2,2,], )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([1, 3, 3, 0, 3],"float32"), bias=Tensor([1],"float32"), stride=list[2,2,1,], padding=1, dilation=tuple(2,2,2,), )
paddle.nn.functional.max_pool2d(Tensor([1, 0, 4, 4],"float32"), kernel_size=2, stride=2, return_mask=True, )
paddle.nn.functional.pad(Tensor([1, 2, 0, 4, 5],"complex64"), pad=list[1,2,1,1,3,4,], mode="constant", value=100, )
paddle.nn.functional.conv2d_transpose(Tensor([20, 64, 76, 136],"float32"), Tensor([64, 1, 4, 0],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=64, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([10285, 0],"float32"), list[1,20,], )
paddle.nn.functional.conv2d_transpose(Tensor([1, 1024, 2, 2],"float32"), Tensor([1024, 512, 0, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[1434,1,1,], )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 0, 7, 7],"float32"), output_size=list[2,5,], data_format="NCHW", name=None, )
paddle.nn.functional.softmax(Tensor([0, 3, 4],"float32"), axis=1, dtype="float64", )
paddle.nn.functional.conv3d_transpose(Tensor([0, 4, 8, 8, 8],"float32"), Tensor([4, 4, 3, 3, 3],"float32"), Tensor([4],"float32"), output_size=tuple(10,17,10,), padding="valid", stride=tuple(1,2,1,), dilation=1, groups=1, data_format="NCDHW", )
paddle.nn.functional.fractional_max_pool3d(x=Tensor([0, 3, 7, 7, 7],"float32"), output_size=list[2,3,5,], random_u=0.7, )
paddle.einsum("a...a->...", Tensor([5, 3, 0, 1, 4, 5],"float64"), )
paddle.nn.functional.pad(Tensor([3, 1, 0, 40, 3],"float32"), pad=list[2,2,0,0,0,0,], data_format="NCDHW", )
paddle.nn.functional.selu(Tensor([3, 5, 0, 10],"float64"), 1.5, 2.0, )
paddle.nn.functional.max_pool3d(Tensor([2, 0, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding=list[0,0,0,0,0,0,], )
paddle.nn.functional.softmax(Tensor([2, 0, 1, 129],"float16"), -1, )
paddle.nn.functional.hinge_embedding_loss(Tensor([10, 10, 0],"float64"), Tensor([10, 10, 0],"float64"), reduction="none", margin=1.0, name=None, )
paddle.Tensor.tile(Tensor([1, 5, 0],"float32"), list[679,1,1,], )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 128, 28, 0],"float16"), output_size=7, data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([16, 0, 256, 256],"float32"), list[6,6,6,6,], )
paddle.nn.functional.conv3d_transpose(Tensor([0, 3, 2, 2, 2],"float64"), Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[336,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([4, 4, 16, 16],"float32"), Tensor([4, 3, 0, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[list[0,0,],list[0,0,],list[1,1,],list[2,2,],], stride=1, dilation=1, groups=2, data_format="NCHW", )
paddle.slice(Tensor([0, 1, 104],"float32"), list[0,1,2,], list[0,0,64,], list[16,1,80,], )
paddle.logcumsumexp(Tensor([0],"float32"), )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 0, 64],"float16"), Tensor([1, 2048, 0, 64],"float16"), Tensor([1, 2048, 0, 64],"float16"), attn_mask=Tensor([1, 1, 0, 2048],"float16"), is_causal=False, )
paddle.nn.functional.max_unpool1d(Tensor([1, 0, 8],"float64"), Tensor([1, 0, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
paddle.nn.functional.fractional_max_pool2d(x=Tensor([2, 3, 7, 0],"float32"), output_size=list[3,None,], random_u=0.6, )
paddle.nn.functional.max_unpool2d(Tensor([0, 1, 2, 2],"float32"), Tensor([0, 1, 2, 2],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCHW", output_size=None, name=None, )
paddle.nn.functional.pad(Tensor([1, 16, 14, 18, 0],"float16"), tuple(0,3,0,0,0,0,), data_format="NDHWC", )
paddle.einsum("bhwc,hkc->bhwk", Tensor([0, 14, 14, 64],"float32"), Tensor([14, 14, 64],"float32"), )
paddle.strided_slice(x=Tensor([0, 4, 5, 6],"float64"), axes=list[1,2,3,], starts=list[-3,0,2,], ends=list[3,2,4,], strides=list[1,1,1,], )
paddle.Tensor.tile(Tensor([1, 5, 0],"float32"), list[897,1,1,], )
paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([0],"float16"), residual=Tensor([16, 256],"float16"), residual_alpha=0.69204696, quant_scale=0.15, quant_round_type=1, quant_max_bound=127, quant_min_bound=-127, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[401,1,1,], )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 0, 32, 32],"float64"), output_size=list[2,2,2,], data_format="NCDHW", name=None, )
paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 0, 50, 50],"float16"), scale=0.125, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 0],"float32"), Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[2,2,2,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([1, 3, 3, 3, 0],"float32"), bias=Tensor([1],"float32"), stride=2, padding=0, )
paddle.einsum("ijkl, lmn->kmn", Tensor([2, 4, 5, 3],"float64"), Tensor([3, 0, 4],"float64"), )
paddle.nn.functional.softmax(Tensor([2, 3, 4, 0],"float32"), 0, name=None, )
paddle.nn.functional.avg_pool3d(x=Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=2, padding=1, exclusive=True, )
paddle.Tensor.__rpow__(Tensor([0],"float32"), 1000000.0, )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float64"), weight=Tensor([1, 3, 3, 0],"float64"), bias=Tensor([1],"float64"), stride=2, padding=0, )
paddle.nn.functional.avg_pool3d(Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=list[1,0,0,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
paddle.nn.functional.conv3d(Tensor([4, 8, 8, 8, 3],"float32"), Tensor([5, 3, 0, 3, 3],"float32"), Tensor([5],"float32"), padding=list[1,1,2,2,3,3,], stride=1, dilation=2, groups=1, data_format="NDHWC", )
paddle.nn.functional.conv1d(Tensor([2, 3, 4],"float32"), Tensor([1, 3, 0],"float32"), bias=Tensor([1],"float32"), padding=list[1,], stride=list[1,], dilation=list[2,], groups=1, data_format="NCL", )
paddle.Tensor.tile(Tensor([1, 9, 0],"float32"), list[724,1,1,], )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 1024, 8, 16],"bfloat16"), Tensor([1, 1024, 2, 16],"bfloat16"), Tensor([1, 0, 2, 16],"bfloat16"), attn_mask=None, is_causal=True, )
paddle.nn.functional.conv2d(Tensor([16, 3, 260, 260],"float32"), weight=Tensor([3, 1, 5, 0],"float32"), groups=3, )
paddle.nn.functional.softmax(Tensor([10, 0, 153, 153],"float32"), axis=-1, )
paddle.nn.functional.pad(Tensor([1, 0, 16, 14, 384],"float16"), tuple(0,0,0,5,0,0,), data_format="NDHWC", )
paddle.Tensor.fill_diagonal_(Tensor([0, 3],"float32"), 4, 1, False, )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 0, 32, 32],"float32"), output_size=list[3,3,3,], data_format="NCDHW", name=None, )
paddle.nn.functional.pad(Tensor([0, 1, 1, 1],"float32"), list[2,2,2,2,], )
paddle.nn.functional.pad(Tensor([1, 16, 14, 0, 384],"float16"), tuple(0,5,0,0,0,0,), data_format="NDHWC", )
paddle.nn.functional.conv2d_transpose(Tensor([2, 6, 2, 2],"float64"), Tensor([6, 0, 3, 3],"float64"), bias=Tensor([3],"float64"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=3, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[276,1,1,], )
paddle.nn.functional.conv1d_transpose(Tensor([2, 2, 3],"float64"), Tensor([3, 0, 3],"float64"), bias=Tensor([3],"float64"), output_size=None, output_padding=0, padding=list[1,], stride=list[1,], dilation=list[1,], groups=3, data_format="NLC", )
paddle.allclose(Tensor([1, 0, 200],"float32"), Tensor([1, 0, 200],"float32"), )
paddle.nn.functional.conv2d(Tensor([8, 128, 255, 255],"float32"), Tensor([256, 128, 0, 1],"float32"), bias=None, stride=2, padding=0, )
paddle.slice(Tensor([16, 0, 104],"float32"), list[0,1,2,], list[0,0,48,], list[16,2,64,], )
paddle.vision.ops.box_coder(prior_box=Tensor([80, 4],"float32"), prior_box_var=tuple(1,2,3,4,), target_box=Tensor([0, 80, 4],"float32"), code_type="decode_center_size", box_normalized=False, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([0, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=False, )
paddle.nn.functional.softmax(Tensor([0, 6, 5, 4],"float64"), axis=-3, dtype=None, name=None, )
paddle.nn.functional.conv1d(Tensor([16, 64, 0],"float32"), Tensor([128, 64, 3],"float32"), bias=Tensor([128],"float32"), padding=64, stride=list[1,], dilation=list[64,], groups=1, data_format="NCL", )
paddle.Tensor.tile(Tensor([24565, 0],"float32"), list[1,136,], )
paddle.subtract(Tensor([10, 0],"float32"), Tensor([10, 0],"float32"), )
paddle.allclose(Tensor([0, 20, 200],"float32"), Tensor([0, 20, 200],"float32"), )
paddle.einsum("...qk,...kd->...qd", Tensor([13, 0, 3, 1, 2],"float32"), Tensor([13, 0, 3, 2, 8],"float32"), )
paddle.nn.functional.avg_pool2d(Tensor([0, 1024, 14, 14],"float32"), kernel_size=2, stride=2, padding="SAME", ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.pad(x=Tensor([0, 1, 1, 2, 3],"float64"), pad=tuple(0,1,1,1,2,0,), mode="circular", value=0, data_format="NCDHW", )
paddle.nn.functional.pad(Tensor([0, 4, 5, 6],"complex64"), pad=list[1,1,1,1,], mode="constant", value=100, data_format="NCHW", name=None, )
paddle.nn.functional.softmax(Tensor([0, 16, 1, 6],"float32"), axis=-1, dtype="float32", )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[241,1,1,], )
paddle.nn.functional.log_softmax(Tensor([0, 3, 4],"float64"), 1, )
paddle.nn.functional.hinge_embedding_loss(Tensor([3, 0],"float64"), Tensor([3, 0],"float64"), reduction="mean", margin=-4.0, name=None, )
paddle.nn.functional.pad(Tensor([0, 2, 3],"float64"), pad=list[1,2,], mode="replicate", value=0.0, data_format="NCL", name=None, )
paddle.polar(Tensor([0],"float32"), Tensor([0],"float32"), )
paddle.nn.functional.pad(Tensor([1, 2, 0, 4, 5],"complex128"), pad=list[1,2,1,1,3,4,], mode="constant", value=100, data_format="NDHWC", )
paddle.Tensor.tile(Tensor([140, 188, 1, 1, 1, 0],"float32"), list[1,1,1,1,2,1,], )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 512, 0, 128],"float32"), output_size=tuple(3,3,), data_format="NCHW", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([0, 6, 2, 2],"float64"), Tensor([6, 1, 3, 3],"float64"), bias=Tensor([3],"float64"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=3, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([0, 4, 16, 16],"float32"), Tensor([4, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[list[0,0,],list[0,0,],list[1,1,],list[2,2,],], stride=1, dilation=1, groups=2, data_format="NCHW", )
paddle.nn.functional.conv2d(Tensor([3, 3, 0, 32],"float32"), Tensor([64, 3, 7, 7],"float32"), None, list[2,2,], 3, list[1,1,], 1, "NCHW", )
paddle.nn.functional.margin_ranking_loss(Tensor([0, 10],"float64"), Tensor([0, 10],"float64"), Tensor([0, 10],"float64"), 0.2, "sum", )
paddle.nn.functional.pad(Tensor([1024, 0, 129, 129],"float32"), list[1,1,1,1,], )
paddle.nn.functional.conv2d_transpose(x=Tensor([0, 6, 2, 2],"float64"), weight=Tensor([6, 1, 3, 3],"float64"), bias=Tensor([3],"float64"), stride=1, padding=list[1,0,], output_padding=0, dilation=1, groups=3, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([14, 0, 768],"float32"), tuple(1,8,1,), )
paddle.nn.quant.weight_quantize(Tensor([64, 0],"float16"), algo="weight_only_int8", group_size=-1, )
paddle.nn.functional.conv1d_transpose(Tensor([4, 3, 16],"float32"), Tensor([3, 0, 3],"float32"), bias=Tensor([6],"float32"), output_size=None, output_padding=0, padding="valid", stride=list[1,], dilation=list[1,], groups=3, data_format="NCL", )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 2],"float32"), Tensor([3, 0, 3, 3],"float32"), bias=None, padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([4, 1, 0],"float32"), tuple(1,3,1,), )
paddle.einsum("...,...", Tensor([0],"float64"), Tensor([0],"float64"), )
paddle.einsum("ijkl, lmn->kmn", Tensor([2, 4, 5, 3],"float64"), Tensor([3, 2, 0],"float64"), )
paddle.nn.functional.pad(Tensor([1, 16, 15, 0, 384],"float32"), tuple(0,0,0,1,0,0,), data_format="NDHWC", )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[947,1,1,], )
paddle.Tensor.tile(Tensor([1, 7, 0],"float32"), list[852,1,1,], )
paddle.nn.functional.pad(Tensor([1, 5551, 0],"float32"), list[1,0,], value=3, mode="constant", data_format="NCL", )
paddle.nn.functional.selu(Tensor([0, 5, 5, 10],"float64"), 1.5, 2.0, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float64"), weight=Tensor([1, 3, 3, 3, 3],"float64"), bias=Tensor([0],"float64"), stride=list[2,2,1,], padding=1, dilation=list[2,2,2,], )
paddle.vision.ops.generate_proposals(Tensor([1, 9, 12, 0],"float32"), Tensor([1, 36, 12, 9],"float32"), Tensor([1, 2],"float32"), Tensor([972, 4],"float32"), Tensor([972, 4],"float32"), pre_nms_top_n=4000, post_nms_top_n=4000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.cartesian_prod(list[Tensor([0],"complex128"),Tensor([0],"complex128"),Tensor([0],"complex128"),], )
paddle.nn.functional.conv3d(Tensor([4, 6, 8, 8, 8],"float32"), Tensor([12, 1, 3, 0, 3],"float32"), None, padding="valid", stride=1, dilation=1, groups=6, data_format="NCDHW", )
paddle.nn.functional.max_pool2d(Tensor([0, 3, 32, 32],"float32"), kernel_size=2, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.max_pool2d(Tensor([1, 0, 20, 20],"float32"), kernel_size=5, stride=1, padding=2, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([1, 0, 140, 240],"float64"), pad=list[0,0,40,40,], mode="constant", value=0.0, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 128, 0, 32],"float32"), Tensor([128, 1, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=128, output_size=None, data_format="NCHW", )
paddle.nn.functional.l1_loss(Tensor([0, 3, 32, 32],"float32"), Tensor([0, 3, 32, 32],"float32"), "mean", name=None, )
paddle.allclose(Tensor([10, 0],"float64"), Tensor([10, 0],"float64"), rtol=1e-05, atol=1e-08, )
paddle.nonzero(x=Tensor([3, 4, 0],"float16"), )
paddle.nn.functional.conv1d_transpose(Tensor([2, 3, 2],"float64"), Tensor([3, 0, 3],"float64"), bias=Tensor([2],"float64"), output_size=None, output_padding=0, padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.slice(Tensor([16, 0, 104],"float32"), list[0,1,2,], list[0,0,72,], list[16,1,88,], )
paddle.Tensor.frexp(Tensor([4, 0, 2],"float64"), )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[105,1,1,], )
paddle.nn.functional.pad(Tensor([1, 0, 14, 16, 384],"float16"), tuple(0,5,0,0,0,0,), data_format="NDHWC", )
paddle.nn.functional.prelu(Tensor([1, 2, 3, 0],"float32"), Tensor([2],"float32"), data_format="NCHW", )
paddle.Tensor.fill_diagonal_(Tensor([0, 3],"float64"), 1, offset=0, wrap=True, )
paddle.Tensor.tile(Tensor([1, 2, 0, 1, 8],"float32"), list[1,1,1,2,1,], )
paddle.nn.functional.conv1d(Tensor([16, 80, 0],"float32"), Tensor([80, 80, 0],"float32"), bias=None, padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.Tensor.__pow__(Tensor([4, 0, 2],"float16"), Tensor([4, 0, 2],"float64"), )
paddle.nn.functional.conv2d_transpose(Tensor([0, 3, 16, 16],"float32"), Tensor([3, 5, 3, 3],"float32"), None, output_size=None, padding=0, stride=1, dilation=1, groups=1, data_format="NCHW", )
paddle.meshgrid(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),], )
paddle.einsum("...i->...", Tensor([2, 0, 11],"float64"), )
paddle.nn.functional.lp_pool1d(Tensor([2, 0, 32],"float32"), norm_type=math.inf, kernel_size=2, stride=2, padding=list[1,], ceil_mode=True, )
paddle.nn.functional.pad(Tensor([3, 4, 0, 6, 7],"complex64"), pad=list[1,2,2,1,1,0,], mode="constant", value=100, data_format="NCDHW", name=None, )
paddle.reverse(Tensor([4, 12, 0],"float64"), axis=1, )
paddle.vision.ops.prior_box(Tensor([4, 96, 80, 0],"float32"), Tensor([4, 3, 640, 0],"float32"), list[16.0,24.0,], list[], list[1.0,], list[0.1,0.1,0.2,0.2,], False, False, list[8.0,8.0,], 0.5, False, None, )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[99,1,1,], )
paddle.nn.functional.conv3d_transpose(Tensor([0, 6, 8, 8, 8],"float32"), Tensor([6, 4, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding="valid", output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=2, output_size=None, data_format="NCDHW", )
paddle.nn.functional.max_pool2d(x=Tensor([0, 3, 32, 32],"float32"), kernel_size=list[2,2,], )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 100, 1, 64],"float16"), Tensor([2, 100, 1, 0],"float16"), attn_mask=Tensor([2, 1, 1, 100],"float16"), is_causal=False, )
paddle.einsum("bhwc,hkc->bhwk", Tensor([48, 32, 0, 64],"float32"), Tensor([32, 32, 64],"float32"), )
paddle.nn.functional.max_unpool3d(Tensor([1, 3, 0, 2, 3],"float64"), Tensor([1, 3, 0, 2, 3],"int32"), kernel_size=2, stride=None, )
paddle.vision.ops.generate_proposals(Tensor([1, 9, 10, 8],"float32"), Tensor([0, 36, 10, 8],"float32"), Tensor([1, 2],"float32"), Tensor([720, 4],"float32"), Tensor([720, 4],"float32"), pre_nms_top_n=4000, post_nms_top_n=4000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.allclose(Tensor([13, 0],"float32"), Tensor([13, 0],"float32"), atol=0.01, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 256, 32, 32],"float32"), Tensor([256, 0, 8, 8],"float32"), bias=None, padding=2, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=256, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([0, 128, 32, 32],"float32"), Tensor([128, 1, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=128, output_size=None, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([0, 1, 3, 40, 40],"float32"), pad=list[0,0,0,0,2,2,], data_format="NCDHW", )
paddle.nn.functional.conv2d(Tensor([1, 1024, 0, 26],"float32"), Tensor([256, 1024, 1, 1],"float32"), None, list[1,1,], 0, list[1,1,], 1, )
paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 0],"float16"), bias=Tensor([512],"float16"), act_method="gelu", )
paddle.nn.functional.hinge_embedding_loss(Tensor([0, 10, 5],"float64"), Tensor([0, 10, 5],"float64"), reduction="sum", )
paddle.nonzero(x=Tensor([2, 0, 2],"float32"), )
paddle.Tensor.fill_diagonal_(Tensor([7, 0],"float32"), 4, 0, True, )
paddle.einsum("...qk,...kd->...qd", Tensor([52, 4, 0, 1, 2],"float32"), Tensor([52, 4, 0, 2, 8],"float32"), )
paddle.logaddexp(Tensor([10, 200, 0],"float32"), Tensor([10, 200, 0],"float32"), )
paddle.nn.functional.avg_pool3d(x=Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], ceil_mode=True, )
paddle.nn.functional.pixel_shuffle(x=Tensor([4, 81, 4, 0],"float64"), upscale_factor=3, data_format="NCHW", )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 0, 4],"float32"), weight=Tensor([1, 3, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=list[2,2,1,], padding=1, dilation=tuple(2,2,2,), )
paddle.einsum("sec,sm->ecm", Tensor([10, 0, 10],"float32"), Tensor([10, 64],"float32"), )
paddle.nn.functional.pad(Tensor([1, 16, 16, 0, 384],"float32"), tuple(0,0,0,5,0,0,), data_format="NDHWC", )
paddle.linalg.slogdet(x=Tensor([0, 4, 4],"float64"), )
paddle.slice(Tensor([0, 1, 104],"float32"), list[0,1,2,], list[0,0,32,], list[16,1,48,], )
paddle.nn.functional.conv2d(Tensor([1, 1024, 10, 0],"float32"), Tensor([256, 1024, 1, 1],"float32"), None, list[1,1,], 0, list[1,1,], 1, )
paddle.nn.functional.softmax(Tensor([2, 6, 5, 0],"float64"), axis=-3, dtype=None, name=None, )
paddle.frac(Tensor([0, 3],"int64"), )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 0, 2],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,], dilation=2, )
paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 0, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, )
paddle.nn.functional.max_pool2d(Tensor([0, 24, 368, 368],"float32"), kernel_size=2, stride=1, padding="SAME", return_mask=False, ceil_mode=True, data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([8, 1, 0],"float32"), tuple(1,2,1,), )
paddle.nn.functional.max_pool3d(Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[0,0,0,], return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.Tensor.tile(Tensor([0, 4],"float32"), list[2,1,1,], )
paddle.nn.functional.max_pool3d(Tensor([2, 6, 7, 9, 0],"float32"), list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], data_format="NDHWC", )
paddle.nn.functional.pad(Tensor([3, 0, 40, 40],"float32"), pad=list[2,2,0,0,], )
paddle.nn.functional.max_unpool1d(Tensor([1, 3, 0],"float64"), Tensor([1, 3, 0],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
paddle.nn.functional.conv2d(Tensor([3, 3, 32, 32],"float32"), Tensor([64, 3, 7, 0],"float32"), None, list[2,2,], 3, list[1,1,], 1, "NCHW", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 8, 8, 8, 6],"float32"), Tensor([6, 8, 0, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NDHWC", )
paddle.nn.functional.conv3d_transpose(Tensor([4, 4, 8, 8, 8],"float32"), Tensor([4, 3, 3, 0, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,2,1,], stride=1, dilation=1, groups=2, data_format="NCDHW", )
paddle.Tensor.tile(Tensor([1, 5, 0],"float32"), list[3066,1,1,], )
paddle.Tensor.tile(Tensor([4, 1, 0],"float32"), tuple(1,11,1,), )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([0, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )
paddle.logcumsumexp(Tensor([0, 4],"float32"), dtype="float32", )
paddle.nn.functional.margin_ranking_loss(Tensor([0, 10],"float64"), Tensor([0, 10],"float64"), Tensor([0, 10],"float64"), 0.0, "none", None, )
paddle.nn.functional.conv2d_transpose(Tensor([1, 2048, 0, 16],"float32"), Tensor([2048, 512, 3, 3],"float32"), padding=0, stride=2, groups=4, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[426,1,1,], )
paddle.logcumsumexp(Tensor([10, 10, 0],"float32"), )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 0, 4, 4],"float32"), weight=Tensor([1, 3, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=list[2,2,1,], padding=1, dilation=list[2,2,2,], )
paddle.nn.functional.softmax(Tensor([11, 0, 7, 7],"float32"), axis=3, )
paddle.nn.functional.conv2d(Tensor([2, 48, 4, 0],"float32"), Tensor([48, 48, 3, 0],"float32"), None, list[1,1,], 1, list[1,1,], 1, "NCHW", )
paddle.einsum("ibm,hm->ibh", Tensor([0, 10, 32],"float32"), Tensor([32, 32],"float32"), )
paddle.einsum("bind,bjnd->bnij", Tensor([13, 2, 4, 4],"float32"), Tensor([13, 0, 4, 4],"float32"), )
paddle.Tensor.tile(Tensor([1024, 0],"float32"), list[64,1,1,], )
paddle.nn.functional.max_pool2d(Tensor([2, 4, 0, 40],"float64"), kernel_size=2, stride=None, padding=0, return_mask=True, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.conv1d(Tensor([1, 256, 28],"float32"), Tensor([256, 256, 0],"float32"), bias=Tensor([256],"float32"), padding=1, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.Tensor.__sub__(Tensor([3, 6, 3, 0, 1, 5],"float64"), Tensor([3, 6, 3, 0, 1, 5],"float64"), )
paddle.nn.functional.pad(Tensor([0, 2, 3, 4, 5],"complex64"), pad=list[1,2,1,1,3,4,], mode="constant", value=100, data_format="NCDHW", )
paddle.nn.functional.pad(Tensor([2, 0, 4],"float32"), list[1,1,], mode="reflect", data_format="NCL", )
paddle.nonzero(Tensor([0],"bool"), )
paddle.nn.functional.pad(Tensor([0, 2, 3, 4, 5],"complex64"), pad=list[1,2,1,1,3,4,], mode="constant", value=100, )
paddle.einsum("ijk, jil -> kl", Tensor([3, 4, 0],"float64"), Tensor([4, 3, 0],"float64"), )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([0, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 160, 0, 10],"float32"), tuple(1,1,), )
paddle.nn.functional.fractional_max_pool3d(Tensor([2, 0, 7, 7, 7],"float32"), output_size=list[3,3,3,], kernel_size=2, random_u=0.6, return_mask=False, name=None, )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 2],"float32"), weight=Tensor([3, 1, 3, 0, 3],"float32"), bias=Tensor([3],"float32"), stride=1, padding=list[1,0,1,], groups=3, dilation=1, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 6, 2, 2],"float32"), weight=Tensor([6, 1, 3, 0],"float32"), bias=Tensor([3],"float32"), stride=1, padding=list[1,0,], groups=3, dilation=1, )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 2],"float64"), weight=Tensor([3, 0, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=0, dilation=1, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 128, 0, 28],"float16"), output_size=7, data_format="NCHW", name=None, )
paddle.subtract(Tensor([2, 0],"complex128"), Tensor([2, 0],"float64"), name="Normal_log_prob", )
paddle.einsum("ijk,jk->ik", Tensor([0, 4, 5],"float64"), Tensor([4, 5],"float64"), )
paddle.Tensor.tile(Tensor([1, 7, 0],"float32"), list[417,1,1,], )
paddle.nn.functional.fractional_max_pool3d(x=Tensor([0, 3, 7, 7, 7],"float16"), output_size=list[3,3,3,], random_u=0.3, )
paddle.Tensor.__sub__(Tensor([3, 6, 3, 0, 2, 5],"float64"), Tensor([3, 6, 3, 0, 2, 5],"float64"), )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 1, 64],"float16"), Tensor([1, 2048, 1, 0],"float16"), Tensor([1, 2048, 1, 64],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), is_causal=False, )
paddle.nn.functional.conv1d(Tensor([4, 0, 6],"float32"), Tensor([8, 6, 3],"float32"), bias=Tensor([8],"float32"), padding=2, stride=list[1,], dilation=list[1,], groups=1, data_format="NLC", )
paddle.nn.functional.pad(Tensor([0, 128, 94, 70],"float32"), pad=list[1,1,1,1,], mode="constant", value=0.0, data_format="NCHW", name=None, )
paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 0, 3, 1, 3],"float32"), Tensor([13, 0, 3, 3, 8],"float32"), )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[230,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([1, 2048, 16, 16],"float32"), Tensor([2048, 512, 3, 0],"float32"), padding=0, stride=2, groups=4, )
paddle.nn.functional.fractional_max_pool3d(x=Tensor([2, 0, 7, 7, 7],"float16"), output_size=list[3,3,3,], random_u=0.3, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 64, 64, 64],"float32"), Tensor([64, 0, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=64, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 0, 4, 4],"float32"), weight=Tensor([1, 3, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=list[2,2,1,], padding=1, dilation=2, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 0, 96],"float16"), Tensor([2, 101, 8, 96],"float16"), Tensor([2, 101, 8, 96],"float16"), attn_mask=None, is_causal=False, )
paddle.slice(Tensor([0, 2, 104],"float32"), list[0,1,2,], list[0,0,16,], list[16,2,32,], )
paddle.einsum("...ij,...i->j...", Tensor([10, 11],"float64"), Tensor([3, 0, 5, 10],"float64"), )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([0, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 8, 0, 8],"float64"), kernel_size=list[3,3,3,], stride=2, padding=1, exclusive=True, )
paddle.nn.functional.fractional_max_pool3d(Tensor([2, 0, 7, 7, 7],"float32"), output_size=list[3,3,3,], kernel_size=list[2,2,2,], random_u=0.6, return_mask=False, name=None, )
paddle.Tensor.tile(Tensor([1, 8, 0],"float32"), list[636,1,1,], )
paddle.nn.functional.maxout(Tensor([100, 4, 3, 0],"float64"), 2, 1, None, )
paddle.nn.functional.pad(Tensor([0, 19780, 3],"float32"), list[1,0,], value=2, mode="constant", data_format="NCL", )
paddle.Tensor.tile(Tensor([2279, 0, 1],"float32"), list[1,238,1,], )
paddle.einsum("ij,k->ijk", Tensor([1, 128],"float32"), Tensor([0],"float32"), )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 0, 1, 64],"float16"), Tensor([2, 0, 1, 64],"float16"), Tensor([2, 0, 1, 64],"float16"), attn_mask=Tensor([2, 0, 1, 100],"float16"), is_causal=False, )
paddle.einsum("i,j->ii", Tensor([0],"float64"), Tensor([0],"float64"), )
paddle.nn.functional.avg_pool3d(Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
paddle.nn.functional.pad(Tensor([3, 0, 5, 6],"complex128"), pad=list[1,1,1,1,], mode="constant", value=100, data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[148,1,1,], )
paddle.nn.functional.pad(Tensor([0, 2, 3, 4],"float64"), pad=list[2,1,2,1,], mode="replicate", value=0.0, data_format="NCHW", name=None, )
paddle.slice(Tensor([0, 9, 16],"float32"), axes=list[1,], starts=list[2,], ends=list[9,], )
paddle.nn.functional.hinge_embedding_loss(Tensor([0, 3, 3, 4],"float64"), Tensor([0, 3, 3, 4],"float64"), reduction="mean", margin=-4.0, name=None, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 6, 0, 2],"float64"), weight=Tensor([6, 1, 3, 3],"float64"), bias=Tensor([3],"float64"), stride=1, padding=list[1,0,], groups=3, dilation=1, )
paddle.Tensor.tile(Tensor([10105, 0],"float32"), list[1,5,], )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 0, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )
paddle.nn.functional.conv3d_transpose(Tensor([4, 8, 8, 8, 4],"float32"), Tensor([4, 0, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[list[0,0,],list[1,2,],list[1,2,],list[2,1,],list[0,0,],], stride=1, dilation=1, groups=2, data_format="NDHWC", )
paddle.nn.functional.max_pool1d(x=Tensor([2, 0, 8],"float64"), kernel_size=2, stride=1, padding=list[1,1,], )
paddle.nn.functional.pad(Tensor([3, 1, 40, 0, 3],"float32"), pad=list[2,2,0,0,0,0,], data_format="NCDHW", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[135,1,1,], )
paddle.nn.functional.conv1d_transpose(x=Tensor([0, 3, 8],"float64"), weight=Tensor([3, 2, 3],"float64"), bias=Tensor([2],"float64"), stride=1, padding=list[4,4,], output_padding=0, groups=1, dilation=1, output_size=None, data_format="NCL", )
paddle.Tensor.__rpow__(Tensor([0, 5, 2],"float64"), 2, )
paddle.Tensor.tile(Tensor([2, 0, 7, 14],"float32"), list[1,4,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([2, 2, 0, 3],"float64"), Tensor([2, 2, 1, 1],"float64"), groups=1, padding="VALID", )
paddle.nn.functional.pad(Tensor([2, 0, 4, 4, 4],"float64"), list[1,1,1,1,1,1,], mode="replicate", data_format="NCDHW", )
paddle.Tensor.isnan(Tensor([3, 6, 0, 4, 2, 5],"float64"), )
paddle.nn.functional.conv1d_transpose(Tensor([2, 3, 2],"float32"), Tensor([3, 2, 0],"float32"), bias=Tensor([2],"float32"), output_size=None, output_padding=0, padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.pad(Tensor([0, 3, 4, 4, 4],"float64"), list[1,1,1,1,1,1,], mode="circular", data_format="NCDHW", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 16, 16, 4],"float32"), Tensor([4, 3, 3, 0],"float32"), Tensor([6],"float32"), output_size=None, padding=0, stride=1, dilation=1, groups=2, data_format="NHWC", )
paddle.nn.functional.margin_ranking_loss(Tensor([10, 0],"float64"), Tensor([10, 0],"float64"), Tensor([10, 0],"float64"), 0.0, "mean", None, )
paddle.nn.functional.conv2d(Tensor([1, 128, 0, 256],"float32"), Tensor([3, 128, 0, 1],"float32"), padding=0, groups=1, )
paddle.nn.functional.conv1d_transpose(Tensor([2, 3, 2],"float32"), Tensor([3, 0, 3],"float32"), bias=Tensor([2],"float32"), output_size=None, output_padding=0, padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.maxout(Tensor([0, 6, 5, 4],"float64"), 2, -1, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 0, 4, 128],"float16"), Tensor([1, 0, 4, 128],"float16"), Tensor([1, 0, 4, 128],"float16"), attn_mask=Tensor([1, 0, 2048, 2048],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.Tensor.tile(Tensor([1, 4, 0],"float32"), list[841,1,1,], )
paddle.nn.functional.pad(Tensor([0, 3, 210, 156],"float64"), pad=list[1,1,1,1,], mode="replicate", value=0.0, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([0, 16, 16, 64],"float32"), tuple(0,0,0,0,), data_format="NHWC", )
paddle.slice(Tensor([16, 0, 104],"float32"), list[0,1,2,], list[0,0,16,], list[16,1,32,], )
paddle.vision.ops.generate_proposals(Tensor([1, 3, 0, 14],"float32"), Tensor([1, 12, 10, 14],"float32"), Tensor([1, 2],"float32"), Tensor([420, 4],"float32"), Tensor([420, 4],"float32"), pre_nms_top_n=2000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.sequence_mask(Tensor([0],"int64"), 10, Dtype(float64), None, )
paddle.nn.functional.avg_pool2d(Tensor([0, 1, 4, 32],"float32"), tuple(2,1,), stride=tuple(2,1,), ceil_mode=True, )
paddle.nn.functional.max_pool3d(Tensor([0, 3, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding=0, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[472,1,1,], )
paddle.nn.functional.avg_pool2d(Tensor([1, 0, 20, 20],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.softmax(Tensor([2, 0, 100],"float32"), axis=-1, dtype=Dtype(float32), )
paddle.matmul(Tensor([1],"float32"), Tensor([0],"float32"), )
paddle.nn.quant.weight_only_linear(Tensor([2, 1, 64],"float16"), weight=Tensor([0, 64],"int8"), bias=Tensor([192],"float16"), weight_scale=Tensor([192],"float16"), weight_dtype="int8", )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float64"), weight=Tensor([1, 3, 0, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=list[2,2,1,], padding=tuple(1,2,2,), dilation=tuple(2,2,2,), )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 0, 2],"float32"), Tensor([3, 10, 27, 27],"float32"), bias=Tensor([10],"float32"), padding=0, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([1, 3, 32, 0, 271],"float32"), tuple(0,1,0,0,0,0,), data_format="NCDHW", )
paddle.meshgrid(list[Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),], )
paddle.nn.functional.avg_pool3d(Tensor([2, 3, 8, 0, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,2,1,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 0, 128],"float16"), Tensor([1, 2048, 4, 128],"float16"), Tensor([1, 2048, 4, 128],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.softmax(Tensor([2, 3, 0],"float64"), 0, name=None, )
paddle.roll(Tensor([4, 0, 4, 4],"complex128"), Tensor([2],"int64"), tuple(1,3,), name=None, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 1, 2, 40],"float16"), Tensor([1, 1, 2, 40],"float16"), Tensor([0, 1, 2, 40],"float16"), attn_mask=Tensor([1, 2, 1, 1],"float16"), )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[109,1,1,], )
paddle.nn.functional.multi_margin_loss(input=Tensor([5, 0],"float64"), label=Tensor([5],"int64"), p=1, margin=1.0, weight=None, reduction="mean", )
paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), rtol=1e-05, atol=1e-08, equal_nan=False, name="test_1", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[201,1,1,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[1685,1,1,], )
paddle.Tensor.tile(Tensor([19125, 0],"float32"), list[1,53,], )
paddle.Tensor.tile(Tensor([0, 4],"float32"), list[48,1,1,], )
paddle.Tensor.nonzero(Tensor([0, 10],"bool"), )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([0, 512, 128, 128],"float32"), output_size=tuple(6,6,), data_format="NCHW", name=None, )
paddle.nn.functional.max_pool3d(Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,2,1,], return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.nn.functional.conv2d(Tensor([2, 192, 4, 4],"float32"), Tensor([384, 192, 1, 0],"float32"), None, list[2,2,], 0, list[1,1,], 1, "NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([1, 2048, 16, 16],"float32"), Tensor([2048, 512, 0, 3],"float32"), padding=0, stride=2, groups=4, )
paddle.nn.functional.softmax(Tensor([0, 3, 2, 5, 5],"float16"), )
paddle.nn.functional.avg_pool2d(Tensor([2, 0, 25, 25],"float32"), kernel_size=3, stride=1, padding=1, ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([0, 3, 5, 7, 7],"float32"), list[2,3,5,], )
paddle.Tensor.tile(Tensor([1, 8, 0],"float32"), list[539,1,1,], )
paddle.Tensor.tile(Tensor([1360, 0],"float32"), list[1,33,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[145,1,1,], )
paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([52, 0, 5, 1, 8],"float32"), Tensor([52, 0, 5, 7, 8],"float32"), )
paddle.Tensor.imag(Tensor([100000, 0],"complex128"), )
paddle.nn.functional.conv1d_transpose(Tensor([2, 3, 2],"float32"), Tensor([3, 0, 3],"float32"), bias=Tensor([3],"float32"), output_size=None, output_padding=0, padding=list[1,], stride=list[1,], dilation=list[1,], groups=3, data_format="NCL", )
paddle.nonzero(x=Tensor([0, 4, 7],"float16"), )
paddle.nn.functional.max_pool3d(x=Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], ceil_mode=True, )
paddle.nn.functional.conv1d_transpose(Tensor([2, 3, 0],"float32"), Tensor([3, 1, 3],"float32"), bias=Tensor([1],"float32"), output_size=None, output_padding=0, padding=1, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 8, 8, 8],"float32"), Tensor([3, 0, 3, 3, 3],"float32"), bias=Tensor([6],"float32"), padding="valid", output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=3, output_size=None, data_format="NCDHW", )
paddle.einsum("se,sec->sec", Tensor([10, 60],"float32"), Tensor([10, 60, 0],"float32"), )
paddle.nn.functional.conv3d_transpose(Tensor([4, 8, 8, 8, 4],"float32"), Tensor([4, 0, 3, 3, 3],"float32"), Tensor([4],"float32"), output_size=None, padding="valid", stride=tuple(1,2,1,), dilation=1, groups=4, data_format="NDHWC", )
paddle.nn.functional.kl_div(Tensor([5, 0],"float64"), Tensor([5, 0],"float64"), "none", False, )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([0, 3, 8, 32, 32],"float64"), output_size=3, data_format="NCDHW", name=None, )
paddle.nn.quant.weight_quantize(Tensor([64, 0],"float16"), algo="weight_only_int8", arch=70, group_size=-1, )
paddle.gammaln(Tensor([2, 3, 4, 0],"float64"), )
paddle.nn.functional.conv1d(Tensor([16, 80, 25500],"float32"), Tensor([128, 80, 0],"float32"), bias=None, padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.einsum("nbka,ahc->nbkhc", Tensor([1, 3, 4, 2],"float32"), Tensor([2, 2, 0],"float32"), )
paddle.nn.functional.conv2d(Tensor([2, 192, 4, 0],"float32"), Tensor([48, 192, 1, 0],"float32"), None, list[1,1,], 0, list[1,1,], 1, "NCHW", )
paddle.nn.functional.log_softmax(x=Tensor([2, 0, 1],"float64"), axis=0, )
paddle.nn.functional.conv2d_transpose(Tensor([0, 6, 16, 16],"float32"), Tensor([6, 8, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[1,2,], dilation=list[2,2,], groups=1, output_size=list[20,36,], data_format="NCHW", )
paddle.nanmedian(Tensor([2, 3, 0, 5],"float32"), axis=tuple(1,2,), keepdim=False, mode="min", )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 3, 0, 4],"float64"), output_size=list[3,3,], )
paddle.nn.functional.conv2d(Tensor([1, 1024, 128, 0],"float32"), Tensor([12, 256, 1, 0],"float32"), padding=0, groups=4, )
paddle.vision.ops.generate_proposals(Tensor([1, 9, 12, 9],"float32"), Tensor([1, 36, 12, 9],"float32"), Tensor([1, 2],"float32"), Tensor([972, 0],"float32"), Tensor([972, 4],"float32"), pre_nms_top_n=4000, post_nms_top_n=4000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.mse_loss(Tensor([3, 3, 10, 0],"float32"), Tensor([3, 3, 10, 0],"float32"), "none", )
paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 10],"int32"), bias=Tensor([10],"float16"), dequant_scales=Tensor([0],"float32"), act_method="gelu", compute_dtype="fp16", )
paddle.einsum("ij,jk", Tensor([4, 10],"float64"), Tensor([10, 0],"float64"), )
paddle.nn.functional.softmax(Tensor([1024, 6, 0, 49],"float32"), -1, name=None, )
paddle.nn.functional.softmax(Tensor([13, 4, 0, 7],"float32"), axis=-1, dtype="float32", )
paddle.vision.ops.generate_proposals(Tensor([1, 0, 16, 16],"float32"), Tensor([1, 16, 16, 16],"float32"), Tensor([1, 2],"float32"), Tensor([16, 16, 4, 4],"float32"), Tensor([16, 16, 4, 4],"float32"), pre_nms_top_n=12000, post_nms_top_n=5000, nms_thresh=0.7, min_size=3.0, eta=1.0, pixel_offset=True, return_rois_num=True, )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 0],"float32"), weight=Tensor([3, 1, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=1, dilation=1, )
paddle.nn.functional.pad(Tensor([1, 16, 0, 28, 192],"float16"), tuple(0,0,0,4,0,0,), data_format="NDHWC", )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([0, 3, 8, 32, 32],"float64"), output_size=list[1,3,2,], data_format="NCDHW", name=None, )
paddle.slice(Tensor([1024, 0, 1024],"float16"), axes=list[0,], starts=list[256,], ends=list[512,], )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[827,1,1,], )
paddle.subtract(Tensor([0],"float64"), Tensor([0],"float64"), )
paddle.nn.functional.conv3d_transpose(x=Tensor([0, 3, 2, 2, 2],"float32"), weight=Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=1, dilation=1, )
paddle.nn.functional.maxout(x=Tensor([0, 4, 3, 3],"float64"), groups=2, )
paddle.nn.functional.conv2d_transpose(Tensor([20, 256, 19, 34],"float32"), Tensor([256, 1, 0, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=256, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d(Tensor([8, 128, 256, 256],"float32"), Tensor([128, 128, 0, 3],"float32"), bias=None, stride=1, padding=1, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 2],"float32"), Tensor([3, 0, 27, 27],"float32"), bias=Tensor([10],"float32"), padding=0, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[744,1,1,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[171,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([4, 16, 16, 4],"float32"), Tensor([4, 3, 3, 0],"float32"), Tensor([6],"float32"), output_size=None, padding=list[list[0,0,],list[1,2,],list[2,1,],list[0,0,],], stride=1, dilation=1, groups=2, data_format="NHWC", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[324,1,1,], )
paddle.nn.functional.l1_loss(Tensor([16, 0, 1025],"float32"), Tensor([16, 0, 1025],"float32"), )
paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([52, 4, 0, 1, 8],"float32"), Tensor([52, 4, 0, 7, 8],"float32"), )
paddle.Tensor.lgamma(Tensor([4, 0],"float64"), )
paddle.Tensor.fill_diagonal_(Tensor([2, 2, 0],"float32"), 1, 0, False, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 0, 4],"float32"), weight=Tensor([1, 3, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=2, padding=0, )
paddle.nn.functional.pixel_unshuffle(Tensor([2, 1, 12, 0],"float64"), 3, "NCHW", None, )
paddle.nanquantile(Tensor([0],"float32"), list[0.55,0.7,], 0, )
paddle.Tensor.digamma(Tensor([5, 7, 0],"float64"), )
paddle.logcumsumexp(Tensor([0, 12],"float16"), dtype="float16", axis=1, )
paddle.nanmedian(Tensor([2, 3, 4, 0],"float32"), axis=2, keepdim=False, mode="min", )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 2],"float64"), weight=Tensor([3, 0, 3],"float64"), bias=Tensor([1],"float64"), stride=2, padding=list[1,], output_padding=0, groups=1, dilation=1, output_size=None, data_format="NCL", name=None, )
paddle.nn.functional.conv3d_transpose(Tensor([4, 4, 0, 8, 8],"float32"), Tensor([4, 3, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[list[0,0,],list[0,0,],list[1,1,],list[1,1,],list[2,2,],], stride=1, dilation=1, groups=2, data_format="NCDHW", )
paddle.nn.functional.max_pool2d(Tensor([0, 128, 55, 55],"float32"), kernel_size=3, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.roll(Tensor([0, 5, 4, 4],"complex128"), Tensor([2],"int64"), tuple(0,3,), name=None, )
paddle.nonzero(x=Tensor([0, 3],"float32"), as_tuple=False, )
paddle.nn.functional.conv2d(Tensor([1, 128, 256, 0],"float32"), Tensor([128, 128, 3, 3],"float32"), padding=1, groups=1, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 256, 0, 54],"float32"), Tensor([256, 128, 4, 4],"float32"), bias=None, padding=0, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.quant.weight_only_linear(Tensor([1, 32, 64],"float16"), Tensor([128, 64],"int8"), bias=Tensor([256],"float16"), weight_scale=Tensor([0],"float16"), weight_dtype="int4", group_size=-1, )
paddle.nn.functional.max_pool2d(Tensor([0, 128, 40, 40],"float16"), 3, stride=1, padding=1, data_format="NCHW", )
paddle.einsum("iox,ojx->ijx", Tensor([2, 0, 1],"complex64"), Tensor([3, 0, 1],"complex64"), )
paddle.einsum("nbka,ahc->nbkhc", Tensor([1, 3, 5, 6],"float16"), Tensor([6, 2, 0],"float16"), )
paddle.nn.functional.avg_pool2d(Tensor([0, 3, 32, 32],"float64"), kernel_size=list[3,3,], stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([3, 4, 5, 6, 0],"complex64"), pad=list[1,1,1,1,1,1,], mode="constant", value=100, data_format="NCDHW", name=None, )
paddle.nanmedian(Tensor([2, 3, 0, 5],"float32"), axis=0, keepdim=False, )
paddle.nn.functional.max_pool2d(Tensor([1, 0, 224, 224],"float32"), kernel_size=2, )
paddle.Tensor.tile(Tensor([1, 7, 0],"float32"), list[1003,1,1,], )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([0, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 0, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )
paddle.nn.functional.pad(Tensor([1, 16, 14, 0, 384],"float32"), tuple(0,1,0,0,0,0,), data_format="NDHWC", )
paddle.nn.functional.softmax(Tensor([0, 4, 49, 49],"float32"), -1, name=None, )
paddle.nn.functional.pad(Tensor([0, 1, 128, 128],"float32"), list[2,1,2,1,], )
paddle.nn.functional.local_response_norm(x=Tensor([0, 3, 40, 40],"float32"), size=5, data_format="NCHW", )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 0, 8, 32, 32],"float64"), output_size=list[1,3,2,], data_format="NCDHW", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 6, 0, 2],"float32"), Tensor([6, 1, 3, 3],"float32"), bias=None, padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=3, output_size=None, data_format="NCHW", )
paddle.kthvalue(Tensor([0, 30, 250],"float64"), 244, -1, )
paddle.nn.functional.fractional_max_pool3d(x=Tensor([2, 0, 7, 7, 7],"float32"), output_size=list[None,3,3,], random_u=0.6, )
paddle.nn.functional.channel_shuffle(Tensor([2, 0, 4, 4],"float64"), 3, "NCHW", )
paddle.nn.functional.conv2d(Tensor([8, 128, 256, 0],"float32"), Tensor([128, 128, 3, 3],"float32"), bias=None, stride=1, padding=1, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float64"), Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([0],"float64"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nanmedian(Tensor([2, 0, 4, 5],"float32"), axis=list[0,2,1,3,], keepdim=False, )
paddle.Tensor.tile(Tensor([2, 0, 38, 64, 1],"float16"), list[1,1,1,1,2,], )
paddle.nn.functional.conv1d_transpose(Tensor([2, 3, 2],"float32"), Tensor([3, 1, 3],"float32"), bias=Tensor([0],"float32"), output_size=None, output_padding=0, padding=list[1,], stride=list[2,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 0],"float32"), Tensor([3, 10, 27, 27],"float32"), bias=Tensor([10],"float32"), padding=0, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 160, 16, 0],"float16"), output_size=2, data_format="NCHW", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 2, 3, 3],"float64"), Tensor([2, 2, 1, 0],"float64"), groups=1, padding="SAME", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[629,1,1,], )
paddle.nn.functional.lp_pool2d(Tensor([0, 3, 32, 32],"float32"), 2, kernel_size=5, stride=3, ceil_mode=True, )
paddle.nn.functional.pad(Tensor([3, 4, 5, 0, 7],"complex128"), pad=list[1,2,2,1,1,0,], mode="constant", value=100, data_format="NCDHW", name=None, )
paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 3, 1, 0],"float32"), Tensor([13, 4, 3, 3, 0],"float32"), )
paddle.nn.functional.l1_loss(Tensor([0, 10, 5],"float32"), Tensor([0, 10, 5],"float32"), "mean", name=None, )
paddle.Tensor.tile(Tensor([100, 0],"float32"), list[64,1,1,], )
paddle.nn.functional.avg_pool3d(x=Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=1, exclusive=True, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 0, 4],"float64"), weight=Tensor([1, 3, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=2, padding=0, )
paddle.nn.functional.pad(x=Tensor([1, 1, 1, 2, 0],"float64"), pad=list[0,0,1,1,0,0,], mode="constant", value=0, data_format="NCDHW", )
paddle.nn.functional.conv1d(Tensor([2, 3, 4],"float64"), Tensor([1, 3, 0],"float64"), bias=Tensor([1],"float64"), padding=1, stride=list[1,], dilation=list[2,], groups=1, data_format="NCL", )
paddle.nn.functional.pad(Tensor([3, 1, 0, 1600, 3],"float32"), pad=list[2,2,0,0,0,0,], data_format="NCDHW", )
paddle.nn.functional.max_pool1d(x=Tensor([2, 3, 0],"float64"), kernel_size=2, stride=2, padding=1, )
paddle.Tensor.tile(Tensor([1, 5, 0],"float32"), list[378,1,1,], )
paddle.Tensor.isnan(Tensor([3, 6, 3, 0, 2, 5],"float64"), )
paddle.nn.functional.conv2d(Tensor([1, 3, 0, 224],"float32"), Tensor([3, 3, 3, 3],"float32"), Tensor([3],"float32"), list[4,3,], 0, list[1,1,], 1, "NCHW", )
paddle.nn.functional.conv1d(x=Tensor([2, 3, 4],"float32"), weight=Tensor([1, 3, 3],"float32"), bias=Tensor([0],"float32"), stride=list[1,], padding=1, dilation=2, )
paddle.Tensor.tile(Tensor([1, 5, 0],"float32"), list[518,1,1,], )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 3, 0, 4],"float64"), output_size=list[2,3,], )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 8, 8],"float64"), weight=Tensor([3, 1, 0, 5],"float64"), bias=Tensor([1],"float64"), stride=1, padding="sAmE", output_padding=0, dilation=1, groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.selu(x=Tensor([0, 3, 3],"float64"), )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float32"), Tensor([3, 2, 3, 0, 3],"float32"), bias=Tensor([2],"float32"), padding=0, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float64"), weight=Tensor([1, 3, 3, 3, 3],"float64"), bias=Tensor([0],"float64"), stride=list[2,2,1,], padding=1, dilation=tuple(2,2,2,), )
paddle.nn.functional.sequence_mask(Tensor([0, 3],"int64"), maxlen=Tensor([1],"int32"), dtype=VarType(int64), )
paddle.vision.ops.generate_proposals(Tensor([2, 3, 4, 4],"float32"), Tensor([2, 12, 4, 0],"float32"), Tensor([2, 3],"float32"), Tensor([4, 4, 3, 4],"float32"), Tensor([4, 4, 3, 4],"float32"), pre_nms_top_n=10, post_nms_top_n=5, return_rois_num=True, )
paddle.nn.functional.lp_pool2d(Tensor([2, 32, 32, 0],"float32"), norm_type=2.0, kernel_size=2, stride=list[2,2,], padding=0, ceil_mode=False, data_format="NHWC", name=None, )
paddle.nn.functional.pad(x=Tensor([1, 0, 1, 2, 3],"float64"), pad=tuple(0,1,1,1,2,0,), mode="circular", value=0, data_format="NCDHW", )
paddle.gammaln(Tensor([2, 0, 4, 5],"float64"), )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float32"), weight=Tensor([3, 1, 3, 3],"float32"), bias=Tensor([0],"float32"), stride=1, padding=1, dilation=1, )
paddle.nanmedian(Tensor([0, 3, 4, 5],"float32"), axis=0, keepdim=False, )
paddle.logcumsumexp(Tensor([0, 4],"float32"), )
paddle.nn.functional.avg_pool2d(Tensor([2, 3, 0, 4],"float64"), kernel_size=list[3,3,], stride=list[3,3,], padding=list[0,0,0,0,], ceil_mode=True, exclusive=False, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.layer_norm(Tensor([0, 6, 6, 3],"float32"), list[6,6,3,], weight=Tensor([108],"float32"), bias=Tensor([108],"float32"), epsilon=1e-05, )
paddle.nn.functional.conv2d(Tensor([2, 192, 4, 0],"float32"), Tensor([96, 192, 1, 0],"float32"), None, list[1,1,], 0, list[1,1,], 1, "NCHW", )
paddle.nn.functional.avg_pool3d(x=Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[0,0,0,], exclusive=False, )
paddle.nn.functional.pad(Tensor([0, 1, 2, 1],"float32"), pad=list[0,1,0,0,], mode="constant", value=-1000000.0, data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([1, 3, 0, 245, 224],"float32"), tuple(0,0,0,3,0,0,), data_format="NCDHW", )
paddle.searchsorted(Tensor([2],"float64"), Tensor([0],"float64"), right=True, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 6, 2, 2],"float32"), Tensor([6, 0, 3, 3],"float32"), bias=None, padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=3, output_size=None, data_format="NCHW", )
paddle.nn.functional.zeropad2d(Tensor([4, 3, 0, 224],"float32"), list[2,2,2,2,], )
paddle.slice(Tensor([0, 2, 104],"float32"), list[0,1,2,], list[0,0,56,], list[16,2,72,], )
paddle.nn.functional.max_pool3d(Tensor([1, 0, 6, 33, 33],"float32"), kernel_size=5, stride=5, padding=0, ceil_mode=True, return_mask=True, )
paddle.Tensor.digamma(Tensor([0],"float32"), )
paddle.einsum("nbqa,ahc->nbqhc", Tensor([1, 3, 5, 6],"float32"), Tensor([6, 2, 0],"float32"), )
paddle.nn.functional.conv2d_transpose(Tensor([4, 3, 16, 16],"float32"), Tensor([3, 5, 0, 3],"float32"), None, output_size=None, padding=0, stride=1, dilation=1, groups=1, data_format="NCHW", )
paddle.einsum("ij,ij->ij", Tensor([0, 5],"float64"), Tensor([0, 5],"float64"), )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float32"), list[1,40,], )
paddle.nn.functional.pad(Tensor([1, 16, 0, 17, 384],"float32"), tuple(0,4,0,0,0,0,), data_format="NDHWC", )
paddle.nn.functional.pad(Tensor([3, 1, 0, 40, 40],"float32"), pad=list[0,0,0,0,2,2,], data_format="NCDHW", )
paddle.isin(Tensor([4, 0],"bfloat16"), Tensor([2, 0],"bfloat16"), False, False, )
paddle.nn.functional.conv3d_transpose(Tensor([4, 8, 8, 8, 3],"float32"), Tensor([3, 5, 0, 3, 3],"float32"), Tensor([5],"float32"), output_size=None, padding=0, stride=1, dilation=1, groups=1, data_format="NDHWC", )
paddle.einsum("ij, j", Tensor([0, 11],"float64"), Tensor([11],"float64"), )
paddle.einsum("ij,ij->ij", Tensor([4, 0],"float64"), Tensor([4, 0],"float64"), )
paddle.nn.functional.max_pool3d(Tensor([0, 6, 9, 6, 3],"float32"), list[5,5,5,], stride=list[1,1,1,], padding=list[0,0,0,], data_format="NDHWC", )
paddle.nanmedian(Tensor([2, 3, 4, 0],"float32"), axis=tuple(1,2,3,), keepdim=False, )
paddle.nn.functional.max_pool2d(Tensor([2, 0, 32, 32],"float32"), kernel_size=list[2,2,], stride=None, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.avg_pool3d(Tensor([2, 0, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding="SAME", )
paddle.nn.functional.fractional_max_pool2d(Tensor([2, 0, 7, 7],"float32"), output_size=list[3,3,], kernel_size=list[2,2,], random_u=0.6, return_mask=False, name=None, )
paddle.nn.functional.conv1d_transpose(x=Tensor([0, 3, 2],"float32"), weight=Tensor([3, 1, 3],"float32"), bias=Tensor([1],"float32"), stride=2, padding=list[1,], groups=1, dilation=1, )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 5, 7, 0],"float32"), output_size=list[None,3,None,], )
paddle.nn.functional.max_pool3d(Tensor([1, 3, 4, 4, 0],"float64"), kernel_size=2, stride=2, padding=0, return_mask=True, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.nn.functional.pad(Tensor([1, 16, 18, 0, 384],"float16"), tuple(0,0,0,3,0,0,), data_format="NDHWC", )
paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), rtol=1e-05, atol=1e-08, equal_nan=False, name="test_5", )
paddle.nn.functional.max_pool3d(x=Tensor([2, 3, 0, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=list[1,0,0,], )
paddle.nn.functional.pad(Tensor([0, 64, 16, 16],"float32"), list[0,1,0,1,], value=0, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 16, 16, 4],"float32"), Tensor([4, 0, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[list[0,0,],list[1,2,],list[2,1,],list[0,0,],], stride=1, dilation=1, groups=2, data_format="NHWC", )
paddle.nn.functional.conv1d(x=Tensor([2, 3, 4],"float32"), weight=Tensor([1, 3, 0],"float32"), bias=Tensor([1],"float32"), stride=1, padding=0, )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 0, 2, 2],"float64"), weight=Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,1,], dilation=2, )
paddle.einsum("bind,bjnd->bnij", Tensor([13, 0, 4, 4],"float32"), Tensor([13, 0, 4, 4],"float32"), )
paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([0],"float32"), shift=None, smooth=None, quant_scale=0.0010981468949466944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.nn.functional.hinge_embedding_loss(Tensor([4, 3, 3, 0],"float64"), Tensor([4, 3, 3, 0],"float64"), reduction="sum", margin=-4.0, name=None, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float64"), weight=Tensor([3, 0, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=0, dilation=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.max_pool2d(Tensor([2, 0, 32, 32],"float32"), kernel_size=2, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.meshgrid(Tensor([2],"float64"), Tensor([0],"float64"), Tensor([6],"float64"), )
paddle.slice(Tensor([0, 2, 104],"float32"), list[0,1,2,], list[0,0,56,], list[2,2,72,], )
paddle.nn.functional.log_softmax(x=Tensor([2, 3, 0],"float64"), axis=2, dtype="float64", )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 5, 7, 0],"float32"), output_size=list[3,3,3,], )
paddle.nn.functional.conv2d_transpose(Tensor([4, 128, 94, 70],"float32"), Tensor([128, 128, 2, 0],"float32"), bias=None, padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 0],"float32"), tuple(9,1,), )
paddle.nn.functional.pad(Tensor([1, 16, 15, 0, 384],"float16"), tuple(0,0,0,6,0,0,), data_format="NDHWC", )
paddle.einsum("...ijk, ...xijk -> ...xjk", Tensor([2, 6, 8, 8],"float32"), Tensor([2, 0, 6, 8, 8],"float32"), )
paddle.vision.ops.generate_proposals(Tensor([1, 15, 40, 60],"float32"), Tensor([0, 60, 40, 60],"float32"), Tensor([1, 2],"float32"), Tensor([36000, 4],"float32"), Tensor([36000, 4],"float32"), pre_nms_top_n=12000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.max_pool3d(x=Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=list[1,0,0,], )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float32"), weight=Tensor([6, 1, 0, 3],"float32"), bias=Tensor([6],"float32"), stride=2, padding=0, groups=3, )
paddle.nn.functional.pad(Tensor([0, 3, 6, 6, 6],"float32"), pad=list[2,2,2,2,2,2,], mode="constant", value=0.0, data_format="NCDHW", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([20, 64, 38, 68],"float32"), Tensor([64, 1, 0, 8],"float32"), bias=None, padding=2, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=64, output_size=None, data_format="NCHW", )
paddle.nn.functional.softmax(Tensor([13, 0, 10, 10],"float32"), axis=-1, dtype=Dtype(float32), )
paddle.nn.functional.conv3d_transpose(Tensor([4, 8, 8, 8, 4],"float32"), Tensor([4, 3, 3, 3, 0],"float32"), None, output_size=None, padding="same", stride=1, dilation=1, groups=2, data_format="NDHWC", )
paddle.Tensor.tile(Tensor([169, 0],"float32"), list[15,1,], )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float32"), weight=Tensor([3, 0, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=1, dilation=1, )
paddle.Tensor.tile(Tensor([2279, 1, 0],"float32"), list[1,238,1,], )
paddle.nn.functional.conv3d_transpose(Tensor([0, 4, 8, 8, 8],"float32"), Tensor([4, 3, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,1,2,2,1,1,], stride=1, dilation=1, groups=2, data_format="NCDHW", )
paddle.nn.functional.prelu(Tensor([1, 2, 0, 4],"float32"), Tensor([1],"float32"), data_format="NCHW", )
paddle.nn.functional.conv2d(Tensor([1, 2048, 0, 16],"float32"), Tensor([12, 512, 0, 1],"float32"), padding=0, groups=4, )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float32"), weight=Tensor([1, 3, 3, 3],"float32"), bias=Tensor([0],"float32"), stride=list[2,1,], padding=1, dilation=tuple(2,2,), )
paddle.Tensor.tile(Tensor([8, 0, 512],"float32"), tuple(1,6,1,), )
paddle.nn.functional.margin_ranking_loss(Tensor([0, 10],"float64"), Tensor([0, 10],"float64"), Tensor([0, 10],"float64"), 0.2, "mean", )
paddle.allclose(Tensor([13, 0],"float32"), Tensor([13, 0],"float32"), rtol=0.0001, atol=0.0001, )
paddle.nn.functional.max_pool1d(Tensor([2, 0, 8],"float64"), 2, 1, 0, False, True, None, )
paddle.nn.functional.conv1d_transpose(Tensor([2, 3, 2],"float32"), Tensor([3, 0, 3],"float32"), bias=Tensor([1],"float32"), output_size=None, output_padding=0, padding=1, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv2d(Tensor([8, 3, 256, 0],"float32"), Tensor([128, 3, 1, 0],"float32"), bias=None, stride=1, padding=0, )
paddle.nanmedian(Tensor([0, 3, 4, 5],"float32"), axis=-1, keepdim=False, mode="min", )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 0],"float32"), weight=Tensor([3, 1, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=1, dilation=1, )
paddle.nn.functional.conv2d(Tensor([1, 128, 0, 256],"float32"), Tensor([128, 128, 3, 3],"float32"), padding=1, groups=1, )
paddle.nn.functional.conv3d_transpose(Tensor([4, 3, 8, 8, 8],"float32"), Tensor([3, 0, 3, 3, 3],"float32"), Tensor([5],"float32"), output_size=None, padding=0, stride=1, dilation=1, groups=1, data_format="NCDHW", )
paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 3, 5, 2, 0],"float16"), Tensor([1, 3, 5, 2, 0],"float16"), )
paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([13, 4, 5, 1, 8],"float32"), Tensor([13, 4, 5, 0, 8],"float32"), )
paddle.Tensor.imag(Tensor([0, 20, 2, 3],"complex64"), )
paddle.allclose(Tensor([0],"float64"), Tensor([0],"float64"), rtol=0.015, atol=0.0, name="test_8", )
paddle.nn.functional.layer_norm(Tensor([0, 10, 60, 70],"float32"), list[60,70,], weight=Tensor([4200],"float32"), bias=Tensor([4200],"float32"), epsilon=1e-05, )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 4, 4, 0],"float64"), output_size=list[3,3,], data_format="NHWC", )
paddle.slice(Tensor([3, 0, 104],"float32"), list[0,1,2,], list[0,0,8,], list[3,1,24,], )
paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([0],"float32"), shift=None, smooth=None, quant_scale=0.0009654839523136616, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.nn.functional.max_pool2d(Tensor([2, 3, 0, 32],"float32"), kernel_size=2, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.einsum("bind,bjnd->bnij", Tensor([0, 2, 4, 4],"float32"), Tensor([0, 2, 4, 4],"float32"), )
paddle.gammaln(Tensor([2, 3, 4, 0],"float32"), )
paddle.nn.functional.l1_loss(Tensor([10, 0, 5],"float32"), Tensor([10, 0, 5],"float32"), "sum", name=None, )
paddle.nn.functional.max_pool3d(Tensor([2, 0, 4, 4, 4],"float64"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], return_mask=False, ceil_mode=True, data_format="NCDHW", name=None, )
paddle.nn.functional.lp_pool2d(Tensor([2, 32, 32, 0],"float32"), 2, kernel_size=2, stride=2, ceil_mode=False, data_format="NHWC", )
paddle.nn.functional.conv2d(Tensor([1, 2048, 128, 128],"float32"), Tensor([24, 256, 1, 0],"float32"), padding=0, groups=8, )
paddle.einsum("bnij,jbnd->ibnd", Tensor([10, 4, 7, 7],"float32"), Tensor([7, 10, 4, 0],"float32"), )
paddle.nn.functional.avg_pool2d(x=Tensor([0, 3, 4, 4],"float64"), kernel_size=list[3,3,], stride=tuple(1,1,), padding=tuple(0,0,), )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 2],"float32"), Tensor([3, 1, 3, 0],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 0],"float32"), list[1,200,], )
paddle.Tensor.tile(Tensor([1929, 1, 0],"float32"), list[1,90,1,], )
paddle.nn.functional.pad(Tensor([1, 0, 32, 224, 297],"float32"), tuple(0,3,0,0,0,0,), data_format="NCDHW", )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 2, 3],"float64"), weight=Tensor([3, 1, 0],"float64"), bias=Tensor([3],"float64"), stride=2, padding=list[1,], output_padding=1, groups=3, dilation=1, output_size=None, data_format="NLC", name=None, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[1328,1,1,], )
paddle.nn.functional.conv3d_transpose(Tensor([2, 8, 8, 8, 6],"float32"), Tensor([6, 0, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NDHWC", )
paddle.nn.functional.selu(Tensor([0, 2],"float64"), 1.0507009873554805, 1.6732632423543772, None, )
paddle.nn.functional.conv1d_transpose(Tensor([1, 128, 112],"float32"), Tensor([128, 0, 8],"float32"), bias=Tensor([64],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.Tensor.tile(Tensor([1, 6, 0],"float32"), list[616,1,1,], )
paddle.nn.functional.pad(Tensor([13, 0, 3],"float32"), tuple(1,0,), data_format="NCL", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[926,1,1,], )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 0, 8],"float32"), Tensor([6, 4, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding="valid", output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=2, output_size=None, data_format="NCDHW", )
paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 3, 5, 2, 0],"float32"), Tensor([1, 3, 4, 2, 0],"float32"), )
paddle.slice(Tensor([0, 2, 104],"float32"), list[0,1,2,], list[0,0,16,], list[2,2,32,], )
paddle.nn.functional.softmax(Tensor([256, 0],"float64"), 1, )
paddle.roll(Tensor([4, 0, 4, 4],"complex128"), Tensor([2],"int64"), tuple(0,3,), name=None, )
paddle.nn.functional.conv2d(Tensor([1, 256, 0, 128],"float32"), Tensor([256, 256, 3, 3],"float32"), padding=1, groups=1, )
paddle.nn.functional.pad(Tensor([10, 0, 16, 128],"float32"), Tensor([4],"int32"), value=-math.inf, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([256, 0, 7, 2048],"float16"), output_size=1, data_format="NHWC", name=None, )
paddle.nonzero(Tensor([1, 2, 0, 28],"float32"), )
paddle.nn.quant.weight_quantize(Tensor([0, 256],"float16"), algo="weight_only_int4", arch=70, group_size=-1, )
paddle.nonzero(x=Tensor([3, 0],"float64"), as_tuple=False, )
paddle.slice(Tensor([16, 0, 104],"float32"), list[0,1,2,], list[0,0,64,], list[16,2,80,], )
paddle.nn.functional.conv2d(Tensor([64, 1, 28, 0],"float32"), Tensor([6, 1, 3, 3],"float32"), bias=None, padding=1, stride=list[1,1,], dilation=list[1,1,], groups=1, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([1, 1024, 0, 2],"float32"), Tensor([1024, 512, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.fill_diagonal_(Tensor([7, 0],"float64"), 4, 0, True, )
paddle.nn.functional.conv2d_transpose(Tensor([0, 1024, 2, 2],"float32"), Tensor([1024, 512, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.softmax(Tensor([16, 0],"float64"), 0, name=None, )
paddle.nn.functional.conv1d_transpose(Tensor([2, 2, 3],"float32"), Tensor([3, 0, 3],"float32"), bias=Tensor([3],"float32"), output_size=None, output_padding=1, padding=list[1,], stride=list[2,], dilation=list[1,], groups=3, data_format="NLC", )
paddle.nn.functional.max_pool1d(Tensor([1, 0, 16],"float32"), kernel_size=2, stride=2, return_mask=True, )
paddle.nn.functional.max_pool2d(x=Tensor([2, 0, 32, 32],"float64"), kernel_size=list[3,3,], stride=list[1,1,], ceil_mode=True, )
paddle.nn.functional.selu(x=Tensor([2, 0],"float64"), )
paddle.nn.functional.pad(Tensor([3, 1, 3, 3, 0],"float32"), pad=list[0,0,0,0,2,2,], data_format="NCDHW", )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 0],"float32"), weight=Tensor([3, 1, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=list[1,0,], dilation=1, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[115,1,1,], )
paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 1, 0, 50],"float16"), scale=0.125, )
paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 0],"bfloat16"), Tensor([4, 3, 0],"float16"), )
paddle.slice(Tensor([2, 4, 0],"float32"), axes=list[0,], starts=list[1,], ends=list[2,], )
paddle.einsum("...,...->...", Tensor([5, 5, 0],"float64"), Tensor([5, 5, 0],"float64"), )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 0, 2],"float32"), Tensor([3, 1, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.isnan(Tensor([0, 3],"float32"), )
paddle.nn.functional.l1_loss(Tensor([10, 0, 5],"float32"), Tensor([10, 0, 5],"float32"), "mean", name=None, )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 0],"float32"), weight=Tensor([1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=2, padding=1, )
paddle.Tensor.tile(Tensor([14877, 1, 0],"float32"), list[1,420,1,], )
paddle.nn.functional.group_norm(Tensor([0, 4, 3, 2],"float64"), num_groups=2, weight=Tensor([4],"float64"), bias=Tensor([4],"float64"), data_format="NCHW", )
paddle.nn.functional.conv1d_transpose(Tensor([2, 3, 2],"float64"), Tensor([3, 1, 0],"float64"), bias=Tensor([1],"float64"), output_size=None, output_padding=0, padding=list[1,], stride=list[2,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.avg_pool2d(Tensor([2, 0, 4, 4],"float64"), kernel_size=list[3,3,], stride=list[3,3,], padding=list[0,0,0,0,], ceil_mode=True, exclusive=False, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.conv3d_transpose(Tensor([4, 8, 8, 8, 4],"float32"), Tensor([4, 0, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=0, stride=1, dilation=1, groups=2, data_format="NDHWC", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[1312,1,1,], )
paddle.Tensor.tile(Tensor([10105, 0],"float32"), list[1,15,], )
paddle.nn.functional.avg_pool2d(Tensor([2, 0, 32, 32],"float32"), kernel_size=2, stride=2, padding=0, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 0],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )
paddle.nn.functional.pad(Tensor([12, 32, 32, 0],"float32"), tuple(0,0,0,0,), data_format="NHWC", )
paddle.nn.functional.avg_pool2d(Tensor([0, 3, 32, 32],"float32"), kernel_size=2, stride=2, )
paddle.slice(Tensor([3, 0, 104],"float32"), list[0,1,2,], list[0,0,64,], list[3,1,80,], )
paddle.nn.functional.avg_pool1d(x=Tensor([0, 3, 8],"float64"), kernel_size=2, stride=2, padding=0, )
paddle.nn.functional.square_error_cost(Tensor([0, 100, 100],"float32"), Tensor([0, 100, 100],"float32"), )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([0, 3, 5, 7, 7],"float32"), output_size=list[3,3,3,], )
paddle.subtract(Tensor([0, 3],"complex128"), Tensor([0, 3],"float64"), name="Normal_log_prob", )
paddle.einsum("blqd,bmdk->blqk", Tensor([13, 5, 1, 0],"float32"), Tensor([13, 1, 1, 0],"float32"), )
paddle.vision.ops.generate_proposals(Tensor([1, 3, 10, 14],"float32"), Tensor([1, 0, 10, 14],"float32"), Tensor([1, 2],"float32"), Tensor([420, 4],"float32"), Tensor([420, 4],"float32"), pre_nms_top_n=2000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.max_pool2d(x=Tensor([2, 0, 32, 32],"float64"), kernel_size=list[3,3,], )
paddle.nn.functional.maxout(Tensor([100, 0, 3, 3],"float32"), 2, 1, None, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 4, 0, 3],"float64"), output_size=list[3,3,], data_format="NHWC", name=None, )
paddle.nonzero(Tensor([10, 2, 0, 28],"float32"), )
paddle.nn.functional.avg_pool1d(Tensor([16, 0, 120],"float32"), 25, 1, 0, True, False, None, )
paddle.einsum("bnij,bjnd->bind", Tensor([0, 4, 2, 4],"float32"), Tensor([0, 4, 4, 4],"float32"), )
paddle.nn.functional.conv1d_transpose(Tensor([2, 3, 2],"float32"), Tensor([3, 0, 3],"float32"), bias=Tensor([1],"float32"), output_size=None, output_padding=0, padding=list[1,], stride=list[2,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.Tensor.tile(Tensor([1, 6, 0],"float32"), list[710,1,1,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[230,1,1,], )
paddle.einsum("bnij,bjnd->bind", Tensor([13, 4, 2, 4],"float32"), Tensor([13, 4, 4, 0],"float32"), )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float64"), Tensor([3, 0, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[2,2,2,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.avg_pool3d(Tensor([0, 3, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding=list[0,0,0,0,0,0,], divisor_override=8, )
paddle.vision.ops.prior_box(Tensor([4, 48, 0, 40],"float32"), Tensor([4, 3, 0, 640],"float32"), list[32.0,48.0,64.0,80.0,96.0,128.0,], list[], list[1.0,], list[0.1,0.1,0.2,0.2,], False, False, list[16.0,16.0,], 0.5, False, None, )
paddle.nn.functional.conv1d_transpose(Tensor([1, 256, 28],"float32"), Tensor([256, 128, 0],"float32"), bias=Tensor([128],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[1388,1,1,], )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[502,1,1,], )
paddle.nn.functional.lp_pool2d(Tensor([0, 3, 32, 32],"float16"), 2, kernel_size=3, stride=2, ceil_mode=False, )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[231,1,1,], )
paddle.roll(Tensor([4, 5, 4, 0],"complex128"), Tensor([2],"int64"), tuple(0,3,), name=None, )
paddle.vision.ops.generate_proposals(Tensor([2, 3, 4, 4],"float32"), Tensor([2, 12, 0, 4],"float32"), Tensor([2, 3],"float32"), Tensor([4, 4, 3, 4],"float32"), Tensor([4, 4, 3, 4],"float32"), pre_nms_top_n=10, post_nms_top_n=5, return_rois_num=True, )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 0, 32, 32],"float64"), output_size=list[2,3,3,], )
paddle.nn.functional.conv2d_transpose(Tensor([2, 2, 3, 3],"float64"), Tensor([2, 2, 0, 1],"float64"), groups=1, padding="VALID", )
paddle.meshgrid(list[Tensor([10],"float64"),Tensor([10],"float64"),Tensor([0],"float64"),Tensor([10],"float64"),Tensor([10],"float64"),], )
paddle.Tensor.tile(Tensor([4, 0, 512],"float32"), tuple(1,2,1,), )
paddle.nn.functional.pad(Tensor([0, 3, 32, 242, 224],"float32"), tuple(0,0,0,2,0,0,), data_format="NCDHW", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 0],"float32"), Tensor([6, 8, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[1,2,1,], dilation=list[2,2,2,], groups=1, output_size=None, data_format="NCDHW", )
paddle.logaddexp(Tensor([10, 0, 300],"float32"), Tensor([10, 0, 300],"float32"), )
paddle.slice(Tensor([0, 3, 8],"float32"), axes=list[1,], starts=list[1,], ends=list[2,], )
paddle.Tensor.tile(Tensor([1156, 0],"float32"), list[4,1,1,], )
paddle.einsum("mbnd,mlb->lbnd", Tensor([1, 14, 4, 0],"float32"), Tensor([1, 8, 14],"float32"), )
paddle.nn.functional.conv2d_transpose(Tensor([4, 16, 0, 4],"float32"), Tensor([4, 1, 3, 3],"float32"), Tensor([4],"float32"), output_size=None, padding="valid", stride=tuple(1,2,), dilation=1, groups=4, data_format="NHWC", )
paddle.Tensor.__getitem__(Tensor([3, 3, 0],"float32"), slice(None,None,-1), )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float64"), weight=Tensor([3, 0, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,], dilation=1, )
paddle.nn.functional.pad(Tensor([1, 0, 3],"float32"), pad=list[1,2,], mode="constant", value=0.0, data_format="NCL", name=None, )
paddle.Tensor.tile(Tensor([1, 33, 0],"float32"), list[32,1,1,], )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 1024, 32, 0],"bfloat16"), Tensor([1, 1024, 32, 0],"bfloat16"), Tensor([1, 1024, 32, 0],"bfloat16"), attn_mask=None, is_causal=True, )
paddle.nn.functional.pad(Tensor([0, 7, 16],"float32"), pad=list[1,1,], data_format="NLC", )
paddle.nn.functional.softmax(x=Tensor([13, 4, 0, 12],"float32"), axis=-1, )
paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 0, 508],"float64"), Tensor([508, 130],"float64"), Tensor([130, 508],"float64"), Tensor([130],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )
paddle.nn.functional.pad(Tensor([0, 4, 5, 6, 7],"complex64"), pad=list[1,1,1,1,1,1,], mode="constant", value=100, data_format="NCDHW", name=None, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([0, 160, 16, 32],"float16"), output_size=2, data_format="NCHW", name=None, )
paddle.masked_fill(Tensor([0],"float16"), Tensor([0],"bool"), Tensor([0],"float16"), )
paddle.nn.functional.hinge_embedding_loss(Tensor([3, 0],"float64"), Tensor([3, 0],"float64"), reduction="none", margin=1.0, name=None, )
paddle.slice(Tensor([2, 0, 2],"float64"), axes=list[0,], starts=list[1,], ends=list[2,], )
paddle.nn.functional.pad(Tensor([3, 4, 0, 6, 7],"complex128"), pad=list[1,1,1,1,1,1,], mode="constant", value=100, data_format="NCDHW", name=None, )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[504,1,1,], )
paddle.Tensor.tile(Tensor([1, 5, 0],"float32"), list[1268,1,1,], )
paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 3, 0, 3],"float32"), Tensor([13, 4, 3, 3, 8],"float32"), )
paddle.nn.functional.fractional_max_pool2d(Tensor([2, 0, 7, 7],"float32"), output_size=list[3,3,], kernel_size=2, random_u=0.6, return_mask=False, name=None, )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 3, 0, 7],"float32"), output_size=list[2,5,], )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 0, 5, 7, 7],"float32"), output_size=list[None,3,None,], data_format="NCDHW", name=None, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([0, 4, 4, 3],"float64"), output_size=3, data_format="NHWC", name=None, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([4, 134, 0, 128],"float16"), Tensor([4, 134, 0, 128],"float16"), Tensor([4, 134, 0, 128],"float16"), attn_mask=Tensor([4, 1, 0, 134],"float16"), dropout_p=0.0, training=True, is_causal=False, )
paddle.nanmedian(Tensor([2, 3, 0, 5],"float32"), axis=list[0,3,], keepdim=False, )
paddle.einsum("...jk, ...kl->...jl", Tensor([3, 10, 0],"float64"), Tensor([3, 3, 0],"float64"), )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float32"), weight=Tensor([3, 0, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=1, output_padding=0, dilation=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv3d_transpose(Tensor([0, 3, 8, 8, 8],"float32"), Tensor([3, 5, 3, 3, 3],"float32"), Tensor([5],"float32"), output_size=None, padding=0, stride=1, dilation=1, groups=1, data_format="NCDHW", )
paddle.nn.functional.log_softmax(Tensor([2, 3, 0, 5],"float32"), -1, None, )
paddle.cartesian_prod(list[Tensor([2],"complex128"),Tensor([1],"complex128"),Tensor([0],"complex128"),], )
paddle.nanmedian(Tensor([0, 3, 4, 5],"float32"), axis=tuple(1,2,), keepdim=False, )
paddle.nn.functional.conv2d(Tensor([16, 3, 268, 268],"float32"), weight=Tensor([3, 1, 13, 0],"float32"), groups=3, )
paddle.nn.functional.conv1d(Tensor([2, 3, 4],"float64"), Tensor([1, 3, 3],"float64"), bias=Tensor([0],"float64"), padding=1, stride=list[1,], dilation=list[2,], groups=1, data_format="NCL", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[134,1,1,], )
paddle.nn.functional.scaled_dot_product_attention(Tensor([4, 134, 4, 0],"float16"), Tensor([4, 134, 4, 0],"float16"), Tensor([4, 134, 4, 0],"float16"), attn_mask=Tensor([4, 1, 134, 0],"float16"), dropout_p=0.0, training=True, is_causal=False, )
paddle.vision.ops.generate_proposals(Tensor([1, 9, 10, 8],"float32"), Tensor([1, 36, 10, 8],"float32"), Tensor([1, 2],"float32"), Tensor([720, 0],"float32"), Tensor([720, 4],"float32"), pre_nms_top_n=4000, post_nms_top_n=4000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), tuple(1,14,1,), )
paddle.nn.functional.avg_pool3d(Tensor([0, 3, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
paddle.nn.functional.selu(x=Tensor([2, 0],"float32"), )
paddle.nn.functional.pad(Tensor([12, 0, 16, 64],"float32"), tuple(0,0,0,0,), data_format="NHWC", )
paddle.nn.functional.pixel_shuffle(Tensor([4, 81, 4, 0],"float64"), 3, "NCHW", None, )
paddle.Tensor.tile(Tensor([1, 6, 0],"float32"), list[738,1,1,], )
paddle.Tensor.tile(Tensor([16660, 0],"float32"), list[1,52,], )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 8, 96],"float16"), Tensor([2, 101, 8, 96],"float16"), Tensor([0, 101, 8, 96],"float16"), attn_mask=None, is_causal=False, )
paddle.nn.functional.softmax(Tensor([2, 3, 0],"float32"), axis=0, dtype="float64", )
paddle.nn.functional.conv2d_transpose(Tensor([0, 128, 128, 128],"float32"), Tensor([128, 64, 3, 3],"float32"), bias=Tensor([64],"float32"), padding=1, output_padding=1, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float64"), Tensor([3, 1, 3, 0, 3],"float64"), bias=Tensor([1],"float64"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[2,2,2,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 0],"float32"), weight=Tensor([3, 1, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=0, dilation=1, )
paddle.nn.functional.max_pool3d(x=Tensor([2, 3, 8, 0, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([2, 2, 2, 6],"float64"), Tensor([6, 1, 3, 0],"float64"), bias=Tensor([3],"float64"), padding=list[1,0,], output_padding=1, stride=list[2,2,], dilation=list[1,1,], groups=3, output_size=None, data_format="NHWC", )
paddle.nn.functional.conv2d(Tensor([1, 1, 0, 32],"float32"), Tensor([6, 1, 3, 3],"float32"), bias=Tensor([6],"float32"), padding=1, stride=list[1,1,], dilation=list[1,1,], groups=1, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[744,1,1,], )
paddle.einsum("ij,j", Tensor([0, 1],"float64"), Tensor([1],"float64"), )
paddle.nn.functional.l1_loss(Tensor([20, 500, 0],"float16"), Tensor([20, 500, 0],"float32"), reduction="sum", )
paddle.nn.functional.max_pool3d(Tensor([2, 3, 8, 0, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,1,1,], return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[95,1,1,], )
paddle.nn.functional.pad(Tensor([0, 3, 28, 24],"float32"), pad=list[1,1,1,1,], mode="reflect", value=0.0, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([2, 0, 4, 4, 4],"float64"), list[1,1,1,1,1,1,], mode="circular", data_format="NCDHW", )
paddle.nn.functional.conv2d(Tensor([8, 128, 255, 255],"float32"), Tensor([256, 128, 1, 0],"float32"), bias=None, stride=2, padding=0, )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 8],"float64"), weight=Tensor([3, 0, 3],"float64"), bias=Tensor([2],"float64"), stride=1, padding=list[list[0,0,],list[0,0,],list[3,4,],], output_padding=0, groups=1, dilation=1, output_size=None, data_format="NCL", )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float64"), weight=Tensor([1, 3, 3, 0, 3],"float64"), bias=Tensor([1],"float64"), stride=list[2,2,1,], padding=tuple(1,2,2,), dilation=tuple(2,2,2,), )
paddle.nn.functional.pad(Tensor([1, 0, 3],"float32"), list[1,0,], value=0, mode="constant", data_format="NCL", )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[182,1,1,], )
paddle.nn.functional.max_pool1d(Tensor([1, 0, 16],"float64"), 2, 2, 0, True, False, None, )
paddle.Tensor.tile(Tensor([1, 157920, 0],"float32"), list[4,1,1,], )
paddle.index_fill(Tensor([0],"int64"), Tensor([0],"int64"), 0, 6, )
paddle.nn.functional.square_error_cost(Tensor([3, 2, 1, 0],"float64"), label=Tensor([3, 2, 1, 0],"float64"), )
paddle.nn.functional.avg_pool3d(Tensor([2, 0, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding=0, divisor_override=8, )
paddle.nn.functional.selu(Tensor([3, 0, 3],"float64"), 1.0507009873554805, 0, None, )
paddle.nn.functional.conv3d_transpose(Tensor([4, 8, 8, 8, 4],"float32"), Tensor([4, 1, 0, 3, 3],"float32"), Tensor([4],"float32"), output_size=None, padding="valid", stride=tuple(1,2,1,), dilation=1, groups=4, data_format="NDHWC", )
paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([0],"float32"), shift=None, smooth=None, quant_scale=0.0010831302497535944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.slice(Tensor([2, 0, 104],"float32"), list[0,1,2,], list[0,0,56,], list[2,2,72,], )
paddle.nn.functional.avg_pool1d(x=Tensor([2, 0, 8],"float64"), kernel_size=2, stride=1, padding=1, exclusive=False, )
paddle.nn.functional.kl_div(input=Tensor([0, 128, 128],"float32"), label=Tensor([0, 128, 128],"float32"), reduction="batchmean", )
paddle.nn.functional.conv2d_transpose(Tensor([2, 128, 124, 108],"float32"), Tensor([128, 128, 0, 2],"float32"), bias=None, padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[107,1,1,], )
paddle.nn.functional.max_pool1d(Tensor([2, 3, 0],"float64"), 2, list[1,], 1, False, False, None, )
paddle.nn.functional.max_pool1d(Tensor([2, 3, 0],"float32"), kernel_size=2, stride=2, padding=0, return_mask=True, )
paddle.slice(Tensor([0, 1, 104],"float32"), list[0,1,2,], list[0,0,72,], list[13,1,88,], )
paddle.einsum("nbqa,ahc->nbqhc", Tensor([1, 3, 0, 6],"float32"), Tensor([6, 2, 4],"float32"), )
paddle.nn.functional.pixel_shuffle(Tensor([2, 9, 0, 4],"float64"), 3, "NCHW", )
paddle.nn.functional.max_pool2d(x=Tensor([0, 2, 24, 24],"float32"), kernel_size=2, stride=2, padding=0, )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float32"), weight=Tensor([1, 3, 0, 3],"float32"), bias=Tensor([1],"float32"), stride=list[2,1,], padding=tuple(1,2,), dilation=tuple(2,2,), )
paddle.nn.functional.temporal_shift(x=Tensor([4, 0, 3, 3],"float64"), seg_num=4, )
paddle.nn.functional.softmax(Tensor([14, 4, 0, 7],"float32"), axis=3, )
paddle.incubate.nn.functional.fused_feedforward(Tensor([0, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, )
paddle.nn.functional.softmax(Tensor([4, 0, 81, 94, 311],"float32"), axis=2, )
paddle.Tensor.__sub__(Tensor([0, 3, 10, 10, 1],"float32"), Tensor([0, 3, 10, 10, 1],"float32"), )
paddle.nn.functional.temporal_shift(Tensor([240, 0, 14, 14],"float16"), 8, 0.125, data_format="NCHW", )
paddle.nn.functional.scaled_dot_product_attention(query=Tensor([2, 64, 12, 64],"float16"), key=Tensor([2, 64, 12, 64],"float16"), value=Tensor([2, 64, 12, 0],"float16"), is_causal=True, )
paddle.nn.quant.weight_quantize(Tensor([0, 256],"float16"), algo="weight_only_int8", arch=80, group_size=-1, )
paddle.nn.functional.max_pool2d(Tensor([2, 8, 16, 0],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=True, data_format="NCHW", name=None, )
paddle.nn.functional.max_pool2d(Tensor([0, 24, 384, 384],"float32"), kernel_size=2, stride=1, padding="SAME", return_mask=False, ceil_mode=True, data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([1, 16, 16, 0, 384],"float16"), tuple(0,0,0,5,0,0,), data_format="NDHWC", )
paddle.nn.functional.avg_pool2d(Tensor([2, 0, 4, 4],"float64"), kernel_size=list[3,3,], stride=list[3,3,], padding=list[0,0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 0, 8, 32, 32],"float64"), output_size=list[2,2,2,], data_format="NCDHW", name=None, )
paddle.Tensor.__rpow__(Tensor([2, 2, 0],"float32"), 3, )
paddle.Tensor.tile(Tensor([96, 3, 0, 4, 4],"float32"), list[1,1,2,1,1,], )
paddle.nn.functional.conv2d(Tensor([3, 16, 2, 0],"float32"), Tensor([16, 16, 3, 0],"float32"), None, list[2,2,], 1, list[1,1,], 1, "NCHW", )
paddle.nn.functional.log_softmax(Tensor([100000, 0, 3],"float32"), )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 0, 10],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
paddle.vision.ops.yolo_box(Tensor([2, 16, 8, 0],"float32"), img_size=Tensor([2, 2],"int32"), anchors=list[10,13,16,30,], class_num=2, conf_thresh=0.01, downsample_ratio=8, clip_bbox=True, scale_x_y=1.0, iou_aware=True, iou_aware_factor=0.5, )
paddle.Tensor.tile(Tensor([1000, 1, 0],"float32"), list[1,4,1,], )
paddle.frac(Tensor([0, 3],"int32"), )
paddle.nn.functional.pad(Tensor([1, 3, 32, 242, 0],"float32"), tuple(0,0,0,2,0,0,), data_format="NCDHW", )
paddle.Tensor.tile(Tensor([24565, 0],"float32"), list[1,418,], )
paddle.nn.functional.avg_pool2d(Tensor([3, 0, 44, 40],"float32"), kernel_size=tuple(5,1,), stride=1, )
paddle.nn.functional.max_pool2d(Tensor([1, 0, 32, 32],"float32"), kernel_size=5, stride=1, padding=2, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.conv2d(Tensor([8, 128, 0, 257],"float32"), Tensor([256, 128, 3, 3],"float32"), bias=None, stride=2, padding=0, )
paddle.vision.ops.prior_box(Tensor([4, 48, 40, 0],"float32"), Tensor([4, 3, 640, 0],"float32"), list[32.0,48.0,64.0,80.0,96.0,128.0,], list[], list[1.0,], list[0.1,0.1,0.2,0.2,], False, False, list[16.0,16.0,], 0.5, False, None, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 1, 2, 0],"float16"), Tensor([1, 1, 2, 0],"float16"), Tensor([1, 1, 2, 0],"float16"), attn_mask=Tensor([1, 2, 1, 0],"float16"), )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 0],"float32"), weight=Tensor([1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=2, padding=0, )
paddle.slice(Tensor([21, 0, 4],"float32"), axes=list[2,], starts=list[3,], ends=list[4,], )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 1, 0],"float16"), Tensor([1, 2048, 1, 0],"float16"), Tensor([1, 2048, 1, 0],"float16"), attn_mask=Tensor([1, 1, 2048, 0],"float16"), is_causal=True, )
paddle.nn.functional.conv2d_transpose(Tensor([1, 4096, 16, 0],"float32"), Tensor([4096, 512, 3, 3],"float32"), padding=0, stride=2, groups=8, )
paddle.nn.functional.conv2d(Tensor([8, 128, 0, 256],"float32"), Tensor([128, 128, 0, 3],"float32"), bias=None, stride=1, padding=1, )
paddle.nn.functional.max_pool3d(x=Tensor([8, 64, 4, 112, 0],"float32"), kernel_size=list[1,3,3,], stride=list[1,2,2,], padding=list[0,1,1,], data_format="NCDHW", )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 2048, 0, 64],"float32"), output_size=tuple(1,1,), data_format="NCHW", name=None, )
paddle.nn.functional.flashmask_attention(Tensor([1, 2048, 8, 96],"float16"), Tensor([1, 2048, 8, 96],"float16"), Tensor([1, 2048, 0, 96],"float16"), startend_row_indices=Tensor([1, 1, 2048, 1],"int32"), causal=True, )
paddle.einsum("lbnd,mlb->mbnd", Tensor([8, 14, 4, 8],"float32"), Tensor([0, 8, 14],"float32"), )
paddle.nn.functional.conv2d(Tensor([8, 128, 255, 0],"float32"), Tensor([256, 128, 1, 1],"float32"), bias=None, stride=2, padding=0, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 2],"float64"), Tensor([3, 1, 3, 3],"float64"), bias=Tensor([0],"float64"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[2,2,], groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 0],"float32"), tuple(8550,1,), )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 0],"float32"), Tensor([3, 1, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.avg_pool2d(Tensor([1, 0, 40, 60],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.l1_loss(Tensor([10, 10, 0],"float32"), Tensor([10, 10, 0],"float32"), reduction="sum", )
paddle.nn.functional.conv1d_transpose(Tensor([2, 3, 0],"float32"), Tensor([3, 1, 3],"float32"), bias=Tensor([1],"float32"), output_size=None, output_padding=0, padding=list[1,], stride=list[1,], dilation=list[2,], groups=1, data_format="NCL", )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 0, 32],"float64"), output_size=list[2,3,3,], )
paddle.reverse(Tensor([13, 0],"int64"), list[1,], )
paddle.cumsum(Tensor([10, 0],"float16"), dtype="float16", )
paddle.nn.functional.max_pool2d(Tensor([0, 64, 288, 399],"float32"), kernel_size=3, stride=2, padding=1, )
paddle.nonzero(Tensor([3, 0, 2, 2],"float64"), True, )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 2],"float64"), weight=Tensor([3, 2, 3, 3, 0],"float64"), bias=Tensor([1],"float64"), stride=1, padding=0, dilation=1, )
paddle.Tensor.tile(Tensor([1, 9, 0],"float32"), list[367,1,1,], )
paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 0],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0014022786635905504, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.Tensor.__sub__(Tensor([1, 1, 0, 1],"float32"), Tensor([1, 1, 0, 1],"float32"), )
paddle.nn.functional.pad(Tensor([0, 7485, 3],"float32"), list[1,0,], value=2, mode="constant", data_format="NCL", )
paddle.Tensor.tile(Tensor([16660, 0],"float32"), list[1,88,], )
paddle.nn.functional.pad(Tensor([1, 0, 3],"float32"), list[1,0,], value=7, mode="constant", data_format="NCL", )
paddle.nn.functional.conv2d(Tensor([4, 16, 0, 3],"float32"), Tensor([5, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[1,1,2,2,], stride=1, dilation=2, groups=1, data_format="NHWC", )
paddle.nn.functional.max_unpool3d(Tensor([1, 0, 2, 2, 3],"float64"), Tensor([1, 0, 2, 2, 3],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCDHW", output_size=None, name=None, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 0, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[1,2,1,], dilation=list[2,2,2,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 2],"float32"), weight=Tensor([3, 2, 0],"float32"), bias=Tensor([1],"float32"), stride=1, padding=0, dilation=1, )
paddle.nn.functional.softmax(Tensor([128, 0, 16, 49, 49],"float32"), -1, name=None, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 0, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
paddle.nn.functional.gelu(Tensor([16, 16, 0, 64],"float16"), approximate=True, )
paddle.nonzero(x=Tensor([12, 0],"bfloat16"), )
paddle.nn.quant.weight_only_linear(Tensor([100, 512],"float16"), weight=Tensor([0, 512],"int8"), weight_scale=Tensor([512],"float16"), weight_dtype="int8", )
paddle.nn.functional.conv2d(Tensor([1, 3, 224, 224],"float32"), Tensor([3, 3, 3, 0],"float32"), Tensor([3],"float32"), list[4,3,], 0, list[1,1,], 1, "NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 3, 2],"float64"), Tensor([2, 2, 0, 1],"float64"), groups=1, padding=list[1,1,], data_format="NHWC", )
paddle.nn.functional.pixel_shuffle(Tensor([2, 0, 4, 4],"float32"), upscale_factor=3, )
paddle.logcumsumexp(Tensor([10, 0],"float16"), dtype="float16", axis=1, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 512, 32, 0],"float32"), Tensor([512, 1, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=512, output_size=None, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([1, 0, 2, 2, 3],"float64"), pad=list[1,1,1,0,1,0,], mode="reflect", value=0.0, data_format="NCDHW", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 0, 16],"float32"), Tensor([6, 8, 3, 3],"float32"), bias=Tensor([8],"float32"), padding="valid", output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.strided_slice(x=Tensor([5, 8, 6, 4, 2, 0],"float64"), axes=list[1,2,5,], starts=list[6,5,4,], ends=list[2,0,1,], strides=list[-1,-2,-3,], )
paddle.nn.functional.conv3d_transpose(Tensor([4, 3, 8, 8, 8],"float32"), Tensor([3, 5, 3, 3, 0],"float32"), Tensor([5],"float32"), output_size=None, padding=0, stride=1, dilation=1, groups=1, data_format="NCDHW", )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 2, 2, 6],"float64"), weight=Tensor([6, 1, 3, 0],"float64"), bias=Tensor([3],"float64"), stride=1, padding=list[1,0,], output_padding=0, dilation=1, groups=3, output_size=None, data_format="NHWC", )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 1024, 8, 16],"bfloat16"), Tensor([0, 1024, 2, 16],"bfloat16"), Tensor([1, 1024, 2, 16],"bfloat16"), attn_mask=None, is_causal=True, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 0, 2],"float32"), Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=1, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[662,1,1,], )
paddle.nn.functional.pad(Tensor([1, 16, 14, 15, 0],"float32"), tuple(0,6,0,0,0,0,), data_format="NDHWC", )
paddle.nn.functional.pixel_unshuffle(Tensor([2, 12, 0, 1],"float64"), 3, "NHWC", )
paddle.nn.functional.sequence_mask(Tensor([2, 0],"int64"), maxlen=Tensor([1],"int32"), dtype=VarType(float64), )
paddle.Tensor.tile(Tensor([8, 1, 0, 2],"float32"), list[1,5,4,1,], )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 3, 4, 0],"float32"), output_size=list[3,3,], data_format="NCHW", name=None, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 0, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )
paddle.nanmedian(Tensor([0, 5],"float64"), axis=1, )
paddle.nn.functional.conv2d_transpose(Tensor([1, 256, 0, 14],"float32"), Tensor([256, 256, 2, 2],"float32"), bias=Tensor([256],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv1d_transpose(Tensor([2, 3, 2],"float64"), Tensor([3, 0, 3],"float64"), bias=Tensor([1],"float64"), output_size=None, output_padding=0, padding=1, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.Tensor.frexp(Tensor([4, 0, 2],"float32"), )
paddle.nn.functional.max_pool1d(x=Tensor([0, 3, 8],"float64"), kernel_size=2, stride=1, padding=list[1,1,], )
paddle.nn.functional.max_pool2d(Tensor([2, 244, 244, 0],"float32"), kernel_size=list[5,3,], stride=list[1,2,], padding=tuple(2,1,), )
paddle.nn.functional.pad(Tensor([1, 16, 14, 15, 0],"float16"), tuple(0,6,0,0,0,0,), data_format="NDHWC", )
paddle.nn.functional.conv2d_transpose(Tensor([0, 1024, 128, 128],"float32"), Tensor([1024, 128, 3, 3],"float32"), padding=0, stride=2, groups=4, )
paddle.nn.functional.lp_pool2d(Tensor([2, 0, 32, 32],"float32"), -math.inf, kernel_size=2, stride=2, ceil_mode=False, )
paddle.nn.functional.l1_loss(Tensor([1, 0],"float32"), Tensor([1, 0],"float32"), reduction="sum", )
paddle.nn.functional.pad(Tensor([1, 16, 0, 14, 384],"float16"), tuple(0,0,0,1,0,0,), data_format="NDHWC", )
paddle.Tensor.tile(Tensor([1, 4, 0],"float32"), list[420,1,1,], )
paddle.nn.functional.conv2d(Tensor([16, 3, 0, 268],"float32"), weight=Tensor([3, 1, 0, 13],"float32"), groups=3, )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 0, 5, 7, 7],"float32"), output_size=5, data_format="NCDHW", name=None, )
paddle.nn.functional.conv1d(Tensor([1, 256, 0],"float32"), Tensor([256, 256, 7],"float32"), bias=Tensor([256],"float32"), padding=15, stride=list[1,], dilation=list[5,], groups=1, data_format="NCL", )
paddle.Tensor.tile(Tensor([1, 4, 0],"float32"), list[472,1,1,], )
paddle.nn.functional.conv1d(Tensor([13, 32, 0],"float32"), Tensor([32, 32, 0],"float32"), bias=None, padding=0, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([0, 3, 7, 7],"float32"), output_size=list[2,5,], )
paddle.nn.functional.pad(Tensor([3, 1, 3, 1600, 0],"float32"), pad=list[2,2,0,0,0,0,], data_format="NCDHW", )
paddle.nn.functional.conv2d(Tensor([4, 3, 0, 16],"float32"), Tensor([5, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[0,0,],list[1,1,],list[2,2,],], stride=1, dilation=1, groups=1, data_format="NCHW", )
paddle.nn.functional.pixel_shuffle(x=Tensor([2, 9, 4, 0],"float64"), upscale_factor=3, data_format="NCHW", )
paddle.einsum("sec,sm->ecm", Tensor([2, 60, 0],"float32"), Tensor([2, 64],"float32"), )
paddle.nn.functional.max_pool3d(Tensor([0, 8, 8, 8, 3],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], return_mask=False, ceil_mode=False, data_format="NDHWC", name=None, )
paddle.einsum("ijk, ik->jk", Tensor([4, 0, 5],"float64"), Tensor([4, 5],"float64"), )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 0, 2],"float32"), Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[2,2,2,], groups=1, output_size=None, data_format="NCDHW", )
paddle.einsum("ijbs,ibns->bnij", Tensor([7, 0, 14, 2],"float32"), Tensor([7, 14, 4, 2],"float32"), )
paddle.nn.functional.avg_pool2d(x=Tensor([2, 0, 4, 4],"float64"), kernel_size=list[3,3,], stride=tuple(1,1,), padding=tuple(0,0,), )
paddle.nn.functional.conv2d_transpose(Tensor([1, 128, 128, 0],"float32"), Tensor([128, 64, 3, 3],"float32"), bias=Tensor([64],"float32"), padding=1, output_padding=1, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([0, 3, 8, 32, 32],"float64"), output_size=list[1,1,1,], data_format="NCDHW", name=None, )
paddle.nn.functional.temporal_shift(Tensor([128, 1024, 14, 0],"float32"), 8, 0.125, )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 2],"float64"), weight=Tensor([3, 1, 3],"float64"), bias=Tensor([0],"float64"), stride=1, padding=1, dilation=1, )
paddle.nn.functional.max_pool3d(x=Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=1, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 1, 64],"float16"), Tensor([1, 2048, 0, 64],"float16"), Tensor([1, 2048, 1, 64],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), is_causal=True, )
paddle.nn.functional.softmax(Tensor([16, 0],"float64"), 1, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 64, 32, 32],"float32"), Tensor([64, 1, 8, 0],"float32"), bias=None, padding=2, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=64, output_size=None, data_format="NCHW", )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 0, 16],"float16"), Tensor([1, 2048, 2, 16],"float16"), Tensor([1, 2048, 2, 16],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), is_causal=True, )
paddle.nn.functional.hinge_embedding_loss(Tensor([0, 10, 5],"float64"), Tensor([0, 10, 5],"float64"), )
paddle.nn.functional.max_pool1d(Tensor([0, 3, 16],"float64"), 2, 2, 0, True, False, None, )
paddle.nn.functional.mse_loss(Tensor([0, 10, 10],"float32"), Tensor([0, 10, 10],"float32"), "mean", )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 32, 0],"float64"), output_size=list[2,2,2,], data_format="NCDHW", name=None, )
paddle.nn.functional.hinge_embedding_loss(Tensor([4, 0, 3],"float64"), Tensor([4, 0, 3],"float64"), reduction="mean", margin=-4.0, name=None, )
paddle.nn.functional.max_pool1d(x=Tensor([2, 0, 8],"float64"), kernel_size=2, stride=1, padding=1, )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([0, 3, 8, 32, 32],"float64"), output_size=list[2,3,3,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[331,1,1,], )
paddle.vision.ops.generate_proposals(Tensor([1, 15, 40, 60],"float32"), Tensor([1, 60, 40, 60],"float32"), Tensor([1, 2],"float32"), Tensor([36000, 0],"float32"), Tensor([36000, 4],"float32"), pre_nms_top_n=12000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 0, 4, 4],"float32"), weight=Tensor([1, 3, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=list[2,2,1,], padding=tuple(1,2,2,), dilation=tuple(2,2,2,), )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 0],"float32"), weight=Tensor([3, 2, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=0, dilation=1, )
paddle.nn.functional.conv3d_transpose(Tensor([4, 4, 0, 8, 8],"float32"), Tensor([4, 3, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,1,2,2,1,1,], stride=1, dilation=1, groups=2, data_format="NCDHW", )
paddle.nn.functional.sequence_mask(Tensor([2, 2, 3, 3, 0],"float64"), maxlen=5, dtype=type(numpy.int32), )
paddle.Tensor.tile(Tensor([19125, 0],"float32"), list[1,56,], )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 0, 5, 7, 7],"float32"), output_size=list[2,3,5,], )
paddle.nn.functional.mse_loss(Tensor([3, 3, 10, 0],"float32"), Tensor([3, 3, 10, 0],"float32"), "mean", )
paddle.slice(Tensor([16, 0, 104],"float32"), list[0,1,2,], list[0,0,40,], list[16,2,56,], )
paddle.nn.functional.pad(Tensor([0, 3, 4, 4],"float32"), list[1,1,1,1,], mode="replicate", data_format="NCHW", )
paddle.allclose(tuple(Tensor([0, 7, 16],"float32"),Tensor([0, 7, 16],"float32"),), tuple(Tensor([0, 7, 16],"float32"),Tensor([0, 7, 16],"float32"),), rtol=0.0001, atol=0.0001, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 4, 16, 16],"float32"), Tensor([4, 0, 3, 3],"float32"), Tensor([4],"float32"), output_size=list[18,34,], padding="valid", stride=tuple(1,2,), dilation=1, groups=1, data_format="NCHW", )
paddle.nn.functional.max_pool3d(x=Tensor([8, 0, 4, 56, 56],"float32"), kernel_size=list[1,1,1,], stride=list[1,1,1,], padding=list[0,0,0,], data_format="NCDHW", )
paddle.einsum("blkd,bldq->blkq", Tensor([52, 5, 0, 1],"float32"), Tensor([52, 5, 0, 3],"float32"), )
paddle.roll(Tensor([4, 5, 0, 4],"complex128"), Tensor([2],"int64"), tuple(0,3,), name=None, )
paddle.allclose(x=Tensor([0],"float64"), y=Tensor([0],"float64"), rtol=0.01, atol=0.01, equal_nan=False, )
paddle.nn.functional.pad(Tensor([1, 0, 3],"float32"), pad=list[2,2,], mode="replicate", value=0.0, data_format="NCL", name=None, )
paddle.cummax(Tensor([100, 0],"float32"), axis=-2, dtype="int32", )
paddle.nn.functional.square_error_cost(Tensor([0, 2, 1, 2],"float64"), label=Tensor([0, 2, 1, 2],"float64"), )
paddle.slice(Tensor([0, 8, 24],"float32"), axes=list[1,], starts=list[7,], ends=list[8,], )
paddle.nn.functional.pad(x=Tensor([0, 2, 1, 2],"float64"), pad=list[1,1,2,3,], mode="constant", value=2.0, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([0, 1, 3, 2],"float32"), pad=list[0,1,0,0,], mode="constant", value=-1000000.0, data_format="NCHW", name=None, )
paddle.vision.ops.generate_proposals(Tensor([1, 0, 40, 60],"float32"), Tensor([1, 60, 40, 60],"float32"), Tensor([1, 2],"float32"), Tensor([36000, 4],"float32"), Tensor([36000, 4],"float32"), pre_nms_top_n=12000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.conv1d(x=Tensor([2, 3, 4],"float32"), weight=Tensor([1, 3, 3],"float32"), bias=Tensor([0],"float32"), stride=2, padding=0, )
paddle.nn.functional.pad(Tensor([13, 7, 0],"float32"), pad=list[1,1,], data_format="NLC", )
paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), atol=0.5, )
paddle.nn.functional.max_pool3d(x=Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], )
paddle.nn.functional.pad(Tensor([0, 2, 3],"float32"), pad=list[1,2,], mode="constant", value=0.0, data_format="NCL", name=None, )
paddle.Tensor.tile(Tensor([5112, 0, 1],"float32"), list[1,188,1,], )
paddle.nn.functional.layer_norm(Tensor([0, 209, 384],"float32"), 384, weight=Tensor([384],"float32"), bias=Tensor([384],"float32"), epsilon=1e-05, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 0],"float64"), weight=Tensor([1, 3, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=list[2,2,1,], padding=tuple(1,2,2,), dilation=tuple(2,2,2,), )
paddle.isin(Tensor([0, 64],"bfloat16"), Tensor([0, 256],"bfloat16"), False, True, )
paddle.nn.functional.fractional_max_pool3d(x=Tensor([2, 3, 0, 7, 7],"float32"), output_size=list[None,3,3,], random_u=0.6, )
paddle.nn.functional.conv2d(Tensor([1, 128, 12, 20],"float32"), Tensor([128, 128, 3, 0],"float32"), None, list[1,1,], 1, list[1,1,], 1, )
paddle.nn.functional.avg_pool2d(Tensor([2, 0, 32, 32],"float64"), kernel_size=list[3,3,], stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nanmedian(Tensor([0, 3, 4, 5],"float32"), axis=list[0,-1,], keepdim=False, )
paddle.nn.functional.hinge_embedding_loss(Tensor([0, 3, 3],"float64"), Tensor([0, 3, 3],"float64"), reduction="mean", margin=-4.0, name=None, )
paddle.nn.functional.max_pool2d(Tensor([0, 3, 32, 32],"float64"), kernel_size=list[3,3,], stride=list[1,1,], padding=list[1,1,], return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.gammaln(Tensor([2, 3, 0, 5],"float32"), )
paddle.nn.functional.conv2d(Tensor([1, 128, 256, 256],"float32"), Tensor([128, 128, 0, 3],"float32"), padding=1, groups=1, )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 5, 7, 0],"float32"), output_size=list[3,3,3,], data_format="NDHWC", )
paddle.nn.functional.conv2d(Tensor([8, 128, 257, 257],"float32"), Tensor([256, 128, 0, 3],"float32"), bias=None, stride=2, padding=0, )
paddle.Tensor.tile(Tensor([1, 0],"float32"), tuple(11,1,), )
paddle.nn.functional.avg_pool2d(x=Tensor([0, 3, 4, 4],"float64"), kernel_size=list[3,3,], stride=list[3,3,], padding=list[0,0,0,0,], ceil_mode=False, exclusive=False, )
paddle.einsum("i...j, i...j->...", Tensor([0, 2],"float32"), Tensor([0, 2],"float32"), )
paddle.nn.functional.avg_pool3d(x=Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], )
paddle.nn.functional.softmax(Tensor([2, 0, 4],"float32"), axis=-1, dtype="float32", )
paddle.nn.functional.margin_ranking_loss(Tensor([0, 10],"float64"), Tensor([0, 10],"float64"), Tensor([0, 10],"float64"), 0.0, "mean", )
paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 0],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0010981468949466944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 0],"float16"), bias=Tensor([512],"float16"), act_method="gelu", compute_dtype="default", )
paddle.allclose(tuple(Tensor([0, 20, 32],"float32"),), tuple(Tensor([0, 20, 32],"float32"),), rtol=0.0001, atol=0.0001, )
paddle.nn.functional.avg_pool2d(Tensor([0, 1, 40, 44],"float32"), kernel_size=tuple(1,5,), stride=1, )
paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([59, 64],"float16"), )
paddle.nn.functional.pad(Tensor([0, 16, 15, 14, 384],"float16"), tuple(0,0,0,1,0,0,), data_format="NDHWC", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[569,1,1,], )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float64"), weight=Tensor([1, 3, 3, 3],"float64"), bias=Tensor([0],"float64"), stride=list[2,1,], padding=list[1,2,], dilation=tuple(2,2,), )
paddle.strided_slice(x=Tensor([5, 8, 6, 4, 0, 6],"float64"), axes=list[1,2,5,], starts=list[6,5,4,], ends=list[2,0,1,], strides=list[-1,-2,-3,], )
paddle.nn.functional.selu(x=Tensor([0, 3, 3],"float64"), alpha=0, scale=1.0507009873554805, )
paddle.nn.functional.conv1d(Tensor([16, 64, 25500],"float32"), Tensor([128, 64, 0],"float32"), bias=Tensor([128],"float32"), padding=2, stride=list[1,], dilation=list[2,], groups=1, data_format="NCL", )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[314,1,1,], )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 0],"float16"), Tensor([2, 100, 1, 0],"float16"), Tensor([2, 100, 1, 0],"float16"), attn_mask=Tensor([2, 1, 1, 0],"float16"), is_causal=False, )
paddle.nn.functional.max_pool3d(Tensor([0, 32, 32, 32, 3],"float32"), kernel_size=2, stride=2, padding=0, data_format="NDHWC", return_mask=False, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 0, 2],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,], dilation=2, output_size=None, data_format="NCHW", )
paddle.nn.functional.max_pool2d(x=Tensor([0, 3, 32, 32],"float64"), kernel_size=list[3,3,], stride=list[1,1,], )
paddle.einsum("i...j, i...j->...", Tensor([2, 0],"float32"), Tensor([2, 0],"float32"), )
paddle.nn.functional.pad(Tensor([1024, 1, 0, 129],"float32"), list[1,1,1,1,], )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 0, 64],"float16"), Tensor([1, 2048, 1, 64],"float16"), Tensor([1, 2048, 1, 64],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), is_causal=True, )
paddle.nn.functional.pad(Tensor([1, 16, 0, 17, 384],"float16"), tuple(0,4,0,0,0,0,), data_format="NDHWC", )
paddle.Tensor.tile(Tensor([4, 1, 0],"float32"), tuple(1,9,1,), )
paddle.allclose(Tensor([1, 5, 0],"float32"), Tensor([1, 5, 0],"float32"), atol=1e-05, )
paddle.Tensor.tile(Tensor([4562, 1, 0],"float32"), list[1,135,1,], )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 0, 7, 7],"float32"), output_size=5, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 512, 0, 7],"float32"), output_size=tuple(7,7,), data_format="NCHW", name=None, )
paddle.nn.functional.margin_ranking_loss(Tensor([0, 2],"float64"), Tensor([0, 2],"float64"), Tensor([0, 2],"float64"), 0.0, "mean", None, )
paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 512],"int32"), bias=Tensor([512],"float32"), dequant_scales=Tensor([512],"float32"), shift=Tensor([0],"float32"), smooth=Tensor([512],"float32"), act_method="gelu", compute_dtype="fp32", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.Tensor.tile(Tensor([4, 1, 0],"float32"), tuple(1,5,1,), )
paddle.gammaln(Tensor([2, 0, 4, 5],"float32"), )
paddle.vision.ops.prior_box(Tensor([4, 48, 0, 80],"float32"), Tensor([4, 3, 640, 640],"float32"), list[16.0,24.0,], list[], list[1.0,], list[0.1,0.1,0.2,0.2,], False, False, list[8.0,8.0,], 0.5, False, None, )
paddle.nn.functional.conv2d_transpose(Tensor([1, 4096, 16, 16],"float32"), Tensor([4096, 0, 3, 3],"float32"), padding=0, stride=2, groups=8, )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[284,1,1,], )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float32"), weight=Tensor([1, 3, 0, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=0, )
paddle.nn.functional.pixel_shuffle(x=Tensor([2, 4, 0, 9],"float64"), upscale_factor=3, data_format="NHWC", )
paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([13, 4, 0, 1, 8],"float32"), Tensor([13, 4, 0, 7, 8],"float32"), )
paddle.nn.functional.max_pool3d(Tensor([8, 64, 16, 112, 0],"float32"), kernel_size=tuple(3,3,3,), stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), atol=0.001, )
paddle.einsum("td,dnh->tnh", Tensor([0, 32],"float32"), Tensor([32, 4, 4],"float32"), )
paddle.nn.functional.pad(Tensor([1, 2, 3, 0, 5],"complex64"), pad=list[1,2,1,1,3,4,], mode="constant", value=100, data_format="NCDHW", )
paddle.nn.functional.conv2d_transpose(x=Tensor([0, 3, 2, 2],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=0, dilation=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.sequence_mask(Tensor([2, 0],"int64"), maxlen=10, dtype=VarType(int64), )
paddle.nn.functional.pad(Tensor([2, 0, 4],"float64"), list[1,1,], mode="replicate", data_format="NCL", )
paddle.einsum("bij,bjk->bik", Tensor([3, 4, 0],"float64"), Tensor([3, 5, 0],"float64"), )
paddle.slice(Tensor([1024, 2, 0],"float16"), axes=list[0,], starts=list[512,], ends=list[768,], )
paddle.einsum("bind,bjnd->bnij", Tensor([0, 2, 4, 4],"float32"), Tensor([0, 4, 4, 4],"float32"), )
paddle.nn.functional.pad(Tensor([3, 0, 5],"complex64"), pad=list[1,2,], mode="circular", value=0.0, data_format="NCL", name=None, )
paddle.slice(Tensor([0, 1, 104],"float32"), list[0,1,2,], list[0,0,8,], list[13,1,24,], )
paddle.nn.functional.gelu(Tensor([2, 0],"float16"), approximate=True, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 64, 32, 32],"float32"), Tensor([64, 0, 8, 8],"float32"), bias=None, padding=2, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=64, output_size=None, data_format="NCHW", )
paddle.nn.functional.lp_pool2d(Tensor([0, 3, 32, 32],"float16"), norm_type=2.0, kernel_size=3, stride=2, padding=0, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([1, 0, 18, 14, 384],"float32"), tuple(0,0,0,3,0,0,), data_format="NDHWC", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[358,1,1,], )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 0, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[166,1,1,], )
paddle.Tensor.tile(Tensor([8, 1, 0],"float32"), tuple(1,11,1,), )
paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 256],"float16"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float16"), residual=Tensor([16, 256],"float16"), residual_alpha=0.69204696, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 3, 0, 7],"float32"), output_size=list[None,3,], data_format="NCHW", name=None, )
paddle.nn.functional.conv1d_transpose(Tensor([0, 6, 16],"float32"), Tensor([6, 8, 3],"float32"), bias=Tensor([8],"float32"), output_size=None, output_padding=0, padding=list[1,2,], stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[197,1,1,], )
paddle.Tensor.tile(Tensor([1, 6, 0],"float32"), list[492,1,1,], )
paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 0, 1, 3],"float32"), Tensor([13, 4, 0, 3, 8],"float32"), )
paddle.nn.functional.conv1d_transpose(x=Tensor([0, 3, 2],"float64"), weight=Tensor([3, 2, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=0, dilation=1, )
paddle.nn.functional.pad(Tensor([1, 16, 14, 16, 0],"float32"), tuple(0,5,0,0,0,0,), data_format="NDHWC", )
paddle.nn.quant.weight_quantize(Tensor([64, 0],"float16"), algo="weight_only_int8", arch=80, group_size=-1, )
paddle.nn.functional.pad(Tensor([1024, 1, 129, 0],"float32"), list[1,1,1,1,], )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 0, 8, 32, 32],"float64"), output_size=tuple(3,3,3,), data_format="NDHWC", name=None, )
paddle.nn.functional.max_pool2d(Tensor([1, 0, 384, 384],"float32"), kernel_size=2, stride=1, padding="SAME", return_mask=False, ceil_mode=True, data_format="NCHW", name=None, )
paddle.Tensor.fill_diagonal_(Tensor([0, 2, 2],"float64"), 1, 0, False, )
paddle.logcumsumexp(Tensor([10, 0],"float16"), dtype="float16", axis=None, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 8, 8],"float64"), weight=Tensor([3, 1, 5, 0],"float64"), bias=Tensor([1],"float64"), stride=1, padding="sAmE", output_padding=0, dilation=1, groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[148,1,1,], )
paddle.nn.functional.max_pool1d(Tensor([0, 3, 8],"float64"), 2, list[1,], 1, False, False, None, )
paddle.slice(Tensor([2, 4, 0],"float64"), axes=list[0,], starts=list[1,], ends=list[2,], )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[114,1,1,], )
paddle.nn.functional.conv2d(Tensor([1, 128, 0, 256],"float32"), Tensor([128, 128, 0, 3],"float32"), padding=1, groups=1, )
paddle.nn.functional.pad(Tensor([0, 16, 16, 14, 384],"float32"), tuple(0,0,0,5,0,0,), data_format="NDHWC", )
paddle.Tensor.tile(Tensor([4115, 0, 1],"float32"), list[1,143,1,], )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 0, 1, 1, 1],"float32"), bias=Tensor([8],"float32"), padding=tuple(2,3,1,), output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.pad(Tensor([30, 0, 16, 112],"float32"), list[0,1,0,1,], value=-math.inf, )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[176,1,1,], )
paddle.nanmedian(Tensor([4, 0],"float64"), axis=1, )
paddle.nn.functional.selu(Tensor([0, 3, 3],"float64"), 1.0507009873554805, 0, None, )
paddle.nn.functional.pad(Tensor([1048576, 0, 1, 1],"float32"), list[0,0,0,0,], )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float64"), weight=Tensor([3, 1, 3, 0],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,], dilation=2, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 6, 2, 2],"float32"), weight=Tensor([6, 0, 3, 3],"float32"), bias=Tensor([3],"float32"), stride=1, padding=list[1,0,], groups=3, dilation=1, )
paddle.nanmedian(Tensor([2, 3, 4, 0],"float32"), axis=list[0,-1,], keepdim=False, mode="min", )
paddle.nn.functional.conv1d(Tensor([2, 3, 4],"float64"), Tensor([1, 3, 0],"float64"), bias=Tensor([1],"float64"), padding=list[1,], stride=list[1,], dilation=list[2,], groups=1, data_format="NCL", )
paddle.Tensor.__getitem__(Tensor([3, 0, 3],"float32"), tuple(slice(None,None,-1),slice(None,1,None),slice(None,-1,None),), )
paddle.Tensor.tile(Tensor([16660, 0],"float32"), list[1,92,], )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([512, 256, 0, 25],"float16"), output_size=list[1,25,], data_format="NCHW", name=None, )
paddle.nn.functional.prelu(Tensor([1, 0, 3, 4],"float32"), Tensor([1],"float32"), )
paddle.nn.functional.l1_loss(Tensor([10, 10, 0],"float32"), Tensor([10, 10, 0],"float32"), "mean", name=None, )
paddle.nn.functional.temporal_shift(Tensor([0, 256, 28, 28],"float32"), 16, 0.0625, )
paddle.nn.functional.pad(Tensor([1, 19780, 0],"float32"), list[1,0,], value=2, mode="constant", data_format="NCL", )
paddle.subtract(Tensor([16, 96, 0],"float32"), Tensor([16, 96, 0],"float32"), )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 0, 4, 80],"float16"), list[1,40,], )
paddle.einsum("iox,ojx->ijx", Tensor([2, 3, 1],"complex64"), Tensor([3, 0, 1],"complex64"), )
paddle.nn.functional.max_pool3d(Tensor([0, 4, 4, 4, 4],"float32"), list[3,3,3,], stride=1, padding=list[0,0,0,], data_format="NDHWC", )
paddle.nn.functional.avg_pool2d(Tensor([0, 3, 4, 4],"float64"), kernel_size=list[3,3,], stride=tuple(1,1,), padding=list[0,0,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.softmax(Tensor([128, 1, 0, 49, 49],"float32"), -1, name=None, )
paddle.Tensor.__sub__(Tensor([1, 0, 10285],"float32"), Tensor([1, 0, 10285],"float32"), )
paddle.nn.functional.conv1d(Tensor([16, 64, 25500],"float32"), Tensor([128, 64, 0],"float32"), bias=Tensor([128],"float32"), padding=128, stride=list[1,], dilation=list[128,], groups=1, data_format="NCL", )
paddle.nn.functional.pad(Tensor([0, 3, 32, 224, 297],"float32"), tuple(0,3,0,0,0,0,), data_format="NCDHW", )
paddle.nn.functional.pad(Tensor([1, 16, 14, 17, 0],"float32"), tuple(0,4,0,0,0,0,), data_format="NDHWC", )
paddle.searchsorted(Tensor([0],"float64"), Tensor([0],"float64"), right=True, )
paddle.nn.functional.max_pool2d(Tensor([1, 0, 224, 224],"float32"), kernel_size=1, )
paddle.einsum("k...,jk", Tensor([2, 0, 5, 3],"float64"), Tensor([2, 2],"float64"), )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 3, 4, 0],"float64"), output_size=list[2,3,], )
paddle.nn.functional.conv2d_transpose(Tensor([4, 256, 47, 35],"float32"), Tensor([256, 128, 0, 4],"float32"), bias=None, padding=0, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.l1_loss(Tensor([1, 3, 0, 256],"float32"), Tensor([1, 3, 0, 256],"float32"), "mean", name=None, )
paddle.einsum("i,d->id", Tensor([14],"float32"), Tensor([0],"float32"), )
paddle.nanmedian(Tensor([2, 0, 4, 5],"float32"), axis=list[0,-1,], keepdim=False, mode="min", )
paddle.nn.functional.pad(Tensor([4, 0, 94, 70],"float32"), pad=list[1,1,1,1,], mode="constant", value=0.0, data_format="NCHW", name=None, )
paddle.einsum("abcd,dfg->abcfg", Tensor([0, 4, 5, 3],"float64"), Tensor([3, 4, 5],"float64"), )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 0, 7, 7],"float32"), output_size=list[2,3,5,], )
paddle.nn.functional.pixel_shuffle(Tensor([2, 0, 4, 9],"float64"), 3, "NHWC", )
paddle.Tensor.isnan(Tensor([3, 0, 3],"float32"), )
paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 0, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, )
paddle.nanmedian(Tensor([0, 3, 4, 5],"float32"), axis=-2, keepdim=False, )
paddle.nn.functional.avg_pool2d(Tensor([0, 3, 4, 4],"float64"), kernel_size=list[3,3,], stride=list[1,1,], padding=list[1,1,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([1, 256, 128, 128],"float32"), Tensor([256, 0, 3, 3],"float32"), padding=0, stride=2, groups=1, )
paddle.nn.functional.pad(Tensor([3, 0, 5, 6, 7],"complex128"), pad=list[1,2,2,1,1,0,], mode="replicate", value=0.0, data_format="NCDHW", name=None, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 6, 2, 0],"float32"), weight=Tensor([6, 1, 3, 3],"float32"), bias=Tensor([3],"float32"), stride=1, padding=list[1,0,], groups=3, dilation=1, )
paddle.nn.functional.max_pool2d(Tensor([1, 1, 0, 5],"float32"), kernel_size=2, stride=2, return_mask=True, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[873,1,1,], )
paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 256],"float32"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float32"), residual=Tensor([16, 256],"float32"), residual_alpha=0.69204696, )
paddle.einsum("blqd,bmdk->blqk", Tensor([52, 5, 1, 1],"float32"), Tensor([52, 1, 0, 1],"float32"), )
paddle.nn.functional.avg_pool3d(x=Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=1, exclusive=True, )
paddle.nn.functional.max_pool2d(Tensor([0, 128, 40, 40],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.slice(Tensor([0, 1, 104],"float32"), list[0,1,2,], list[0,0,56,], list[13,1,72,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[92,1,1,], )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 0],"float64"), weight=Tensor([1, 3, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=2, padding=1, )
paddle.nn.functional.avg_pool1d(x=Tensor([2, 0, 8],"float32"), kernel_size=2, stride=2, padding=0, )
paddle.slice(Tensor([21, 0, 4],"float32"), axes=list[2,], starts=list[2,], ends=list[3,], )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([1, 3, 0, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=list[2,2,1,], padding=list[1,2,2,], dilation=tuple(2,2,2,), )
paddle.nn.functional.pad(Tensor([3, 4, 5, 0],"complex64"), pad=list[1,1,1,1,], mode="constant", value=100, data_format="NCHW", name=None, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 0, 28, 28],"float32"), output_size=7, data_format="NCHW", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 3, 0, 7],"float32"), Tensor([3, 6, 5, 5],"float32"), bias=Tensor([6],"float32"), padding=2, output_padding=list[1,1,], stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv1d_transpose(x=Tensor([0, 3, 5],"float64"), weight=Tensor([3, 2, 3],"float64"), bias=Tensor([2],"float64"), stride=1, padding="SaME", output_padding=0, groups=1, dilation=1, output_size=None, data_format="NCL", )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 8, 8],"float64"), weight=Tensor([3, 0, 5, 5],"float64"), bias=Tensor([1],"float64"), stride=1, padding="sAmE", output_padding=0, dilation=1, groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1914, 2048, 0, 7],"float16"), output_size=1, )
paddle.nn.functional.max_pool3d(x=Tensor([2, 3, 0, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=2, padding=1, )
paddle.slice(Tensor([0, 2, 104],"float32"), list[0,1,2,], list[0,0,8,], list[16,2,24,], )
paddle.nn.functional.conv2d(Tensor([1, 2048, 128, 128],"float32"), Tensor([2048, 256, 0, 3],"float32"), padding=1, groups=8, )
paddle.nn.functional.conv3d_transpose(Tensor([0, 6, 8, 8, 8],"float32"), Tensor([6, 8, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=list[1,2,2,3,2,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.take(Tensor([0, 4],"float64"), Tensor([5, 8],"int64"), mode="wrap", )
paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([0],"float32"), shift=None, smooth=None, quant_scale=0.0010831302497535944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.nn.functional.pad(Tensor([0, 20524, 3],"float32"), list[1,0,], value=1, mode="constant", data_format="NCL", )
paddle.nn.functional.max_pool2d(Tensor([2, 4, 0, 40],"float64"), kernel_size=tuple(2,4,), stride=None, padding=0, return_mask=True, ceil_mode=False, data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([1001, 0, 80],"float32"), list[1,2,1,], )
paddle.nn.functional.mse_loss(Tensor([0, 10],"float32"), Tensor([0, 10],"float32"), "sum", )
paddle.nn.functional.max_pool3d(Tensor([2, 0, 32, 32, 32],"float32"), kernel_size=2, stride=None, padding=0, return_mask=False, ceil_mode=True, data_format="NCDHW", name=None, )
paddle.allclose(Tensor([1, 0, 32],"float32"), Tensor([1, 0, 32],"float32"), atol=1e-05, )
paddle.nn.functional.conv2d(Tensor([2, 48, 4, 4],"float32"), Tensor([48, 48, 0, 3],"float32"), None, list[1,1,], 1, list[1,1,], 1, "NCHW", )
paddle.Tensor.tile(Tensor([100, 0],"float32"), list[2,1,1,], )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 0, 2, 2],"float32"), weight=Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=list[1,0,1,], dilation=2, )
paddle.reverse(Tensor([0, 12, 32],"float64"), axis=1, )
paddle.Tensor.tile(Tensor([1, 0, 1],"float32"), list[1,100,1,], )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 0, 4],"float32"), weight=Tensor([1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=list[2,1,], padding=tuple(1,2,), dilation=tuple(2,2,), )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 0, 4],"float64"), weight=Tensor([1, 3, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=list[2,2,1,], padding=tuple(1,2,2,), dilation=tuple(2,2,2,), )
paddle.nn.functional.conv3d(Tensor([4, 6, 8, 0, 8],"float32"), Tensor([8, 3, 3, 3, 3],"float32"), Tensor([8],"float32"), padding="same", stride=1, dilation=1, groups=2, data_format="NCDHW", )
paddle.nn.functional.conv3d_transpose(Tensor([4, 8, 8, 8, 4],"float32"), Tensor([4, 1, 3, 0, 3],"float32"), Tensor([4],"float32"), output_size=None, padding="valid", stride=tuple(1,2,1,), dilation=1, groups=4, data_format="NDHWC", )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 128, 0, 32],"float32"), output_size=tuple(1,1,), data_format="NCHW", name=None, )
paddle.index_fill(Tensor([0, 40],"float32"), Tensor([2],"int64"), 1, -1, )
paddle.nn.functional.max_pool3d(Tensor([2, 3, 0, 32, 32],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, )
paddle.nn.functional.sequence_mask(Tensor([0, 3],"int64"), maxlen=10, dtype=VarType(bool), )
paddle.nn.functional.pad(Tensor([1, 2, 3, 4, 0],"complex64"), pad=list[1,2,1,1,3,4,], mode="constant", value=100, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[897,1,1,], )
paddle.einsum("...ijk, ...xijk -> ...xjk", Tensor([6, 28, 28],"float32"), Tensor([0, 6, 28, 28],"float32"), )
paddle.nn.functional.zeropad2d(Tensor([4, 3, 224, 0],"float32"), list[2,2,2,2,], )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 0, 4],"float64"), weight=Tensor([1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=list[2,1,], padding=1, dilation=tuple(2,2,), )
paddle.nn.functional.lp_pool1d(Tensor([2, 3, 0],"float32"), norm_type=4, kernel_size=3, stride=2, padding=list[1,], )
paddle.Tensor.__rpow__(Tensor([0],"float64"), 2, )
paddle.nn.functional.pad(Tensor([0, 25500, 1],"float32"), pad=list[256,256,], mode="reflect", data_format="NLC", )
paddle.einsum("xy,yz->xz", Tensor([4, 0],"complex64"), Tensor([4, 0],"complex64"), )
paddle.Tensor.__getitem__(Tensor([3, 3, 0],"float32"), tuple(slice(None,None,-1),slice(None,1,None),slice(None,-1,None),), )
paddle.Tensor.tile(Tensor([8, 1, 0],"float32"), tuple(1,3,1,), )
paddle.slice(Tensor([8, 2, 0, 100],"float32"), axes=list[1,], starts=list[1,], ends=list[2,], )
paddle.nn.functional.conv3d(Tensor([4, 6, 0, 8, 8],"float32"), Tensor([12, 1, 0, 3, 3],"float32"), None, padding="valid", stride=1, dilation=1, groups=6, data_format="NCDHW", )
paddle.einsum("ibnd,jbnd->bnij", Tensor([7, 10, 0, 8],"float32"), Tensor([7, 10, 0, 8],"float32"), )
paddle.nn.functional.pad(Tensor([0, 3, 686, 1024],"float32"), tuple(0,0,0,338,), )
paddle.nn.functional.softmax(Tensor([2, 3, 4, 0],"float32"), axis=0, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 4, 0, 16],"float32"), Tensor([4, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,1,2,2,], stride=1, dilation=1, groups=2, data_format="NCHW", )
paddle.nn.functional.conv1d(Tensor([1, 128, 0],"float32"), Tensor([128, 128, 3],"float32"), bias=Tensor([128],"float32"), padding=1, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.einsum("td,dnh->tnh", Tensor([14, 32],"float32"), Tensor([32, 0, 4],"float32"), )
paddle.nn.functional.pad(Tensor([1, 1, 0, 111],"float32"), pad=list[0,1,0,0,], mode="constant", value=-10000.0, data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[835,1,1,], )
paddle.nn.functional.pad(Tensor([3, 4, 0, 6, 7],"complex128"), pad=list[1,2,2,1,1,0,], mode="constant", value=100, data_format="NCDHW", name=None, )
paddle.einsum("xy,yz->xz", Tensor([4, 4],"float32"), Tensor([4, 0],"float32"), )
paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 1, 64],"float16"), Tensor([64],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 0],"float64"), Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[2,2,2,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.poisson_nll_loss(Tensor([10, 0],"float32"), Tensor([10, 0],"float32"), log_input=True, full=False, epsilon=1e-08, reduction="sum", name=None, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[70,1,1,], )
paddle.nn.functional.pad(Tensor([10, 0, 16, 128],"float32"), Tensor([4],"int32"), value=0, )
paddle.nn.functional.conv2d_transpose(Tensor([0, 3, 2, 2],"float32"), Tensor([3, 1, 3, 3],"float32"), bias=None, padding=1, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 0, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[1,2,1,], dilation=list[2,2,2,], groups=1, output_size=list[12,19,12,], data_format="NCDHW", )
paddle.nn.functional.conv2d(Tensor([1, 1, 101, 165],"float32"), Tensor([64, 1, 0, 7],"float32"), None, list[2,2,], 0, list[1,1,], 1, )
paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([101, 64],"float16"), )
paddle.nn.functional.pad(Tensor([1, 20524, 0],"float32"), list[1,0,], value=1, mode="constant", data_format="NCL", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float64"), Tensor([3, 0, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.Tensor.tile(Tensor([1, 4, 0],"float32"), list[452,1,1,], )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float32"), weight=Tensor([3, 1, 3, 3],"float32"), bias=Tensor([0],"float32"), stride=1, padding=list[1,0,], dilation=1, )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 7, 0],"float16"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 128, 0, 32],"float32"), output_size=tuple(2,2,), data_format="NCHW", name=None, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 0, 64],"float16"), Tensor([1, 2048, 1, 64],"float16"), Tensor([1, 2048, 1, 64],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), is_causal=False, )
paddle.isin(Tensor([0, 64],"bfloat16"), Tensor([4, 256],"bfloat16"), False, False, )
paddle.nanquantile(Tensor([4, 7, 0],"float64"), q=0.75, axis=list[0,2,], )
paddle.einsum("ijk, jil -> kl", Tensor([3, 4, 5],"float64"), Tensor([4, 3, 0],"float64"), )
paddle.nn.functional.max_pool1d(x=Tensor([0, 3, 8],"float64"), kernel_size=list[3,], stride=1, padding=1, )
paddle.nn.functional.margin_ranking_loss(Tensor([0, 10],"float64"), Tensor([0, 10],"float64"), Tensor([0, 10],"float64"), 0.0, "sum", None, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 64, 248, 216],"float32"), Tensor([64, 128, 1, 0],"float32"), bias=None, padding=0, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.fractional_max_pool3d(x=Tensor([2, 3, 7, 7, 0],"float32"), output_size=list[3,3,None,], random_u=0.6, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[228,1,1,], )
paddle.nn.functional.max_unpool3d(Tensor([1, 3, 0, 2, 3],"float64"), Tensor([1, 3, 0, 2, 3],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCDHW", output_size=None, name=None, )
paddle.slice(Tensor([0, 1, 104],"float32"), list[0,1,2,], list[0,0,56,], list[16,1,72,], )
paddle.reverse(Tensor([2, 0, 3],"float32"), axis=0, )
paddle.nn.functional.pad(Tensor([3, 0, 5],"complex128"), pad=list[1,2,], mode="circular", value=0.0, data_format="NCL", name=None, )
paddle.nn.functional.pad(Tensor([0, 3, 140, 240],"float64"), pad=list[0,0,40,40,], mode="constant", value=0.0, data_format="NCHW", )
paddle.nn.functional.avg_pool1d(x=Tensor([2, 0, 8],"float64"), kernel_size=2, stride=2, padding=0, )
paddle.nn.functional.max_pool3d(Tensor([2, 0, 8, 8, 8],"float32"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.nn.functional.poisson_nll_loss(Tensor([0, 2],"float32"), Tensor([0, 2],"float32"), log_input=True, full=True, epsilon=1e-08, reduction="mean", name=None, )
paddle.nn.functional.prelu(Tensor([1, 2, 3, 0],"float32"), Tensor([2],"float32"), )
paddle.einsum("b h i j, b h j d -> b h i d", Tensor([0, 8, 1, 109],"float32"), Tensor([0, 8, 109, 64],"float32"), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[1,100,1,], )
paddle.nn.functional.avg_pool2d(Tensor([64, 0, 3, 80],"float32"), kernel_size=list[3,2,], stride=list[3,2,], padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.conv2d(Tensor([8, 128, 257, 0],"float32"), Tensor([256, 128, 3, 0],"float32"), bias=None, stride=2, padding=0, )
paddle.nn.functional.max_pool2d(Tensor([2, 0, 27, 27],"float32"), kernel_size=3, stride=1, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.temporal_shift(x=Tensor([2, 0, 3, 3],"float64"), seg_num=2, shift_ratio=0.4, data_format="NHWC", )
paddle.nn.functional.pad(Tensor([0, 16, 14, 19, 384],"float16"), tuple(0,2,0,0,0,0,), data_format="NDHWC", )
paddle.Tensor.frexp(Tensor([0, 12],"float32"), )
paddle.nn.functional.max_pool2d(Tensor([1, 0, 40, 40],"float32"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([4, 1, 0],"float32"), tuple(1,2,1,), )
paddle.nn.functional.group_norm(Tensor([0, 1024, 12, 32],"float32"), num_groups=32, epsilon=1e-05, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), )
paddle.nn.functional.max_unpool3d(Tensor([1, 0, 2, 2, 3],"int64"), Tensor([1, 0, 2, 2, 3],"int32"), kernel_size=2, stride=2, output_size=list[1,3,4,4,6,], )
paddle.nn.functional.pad(Tensor([0, 1, 2, 3],"float64"), pad=list[1,0,1,2,], mode="constant", value=0.0, data_format="NCHW", name=None, )
paddle.nn.functional.lp_pool1d(Tensor([2, 0, 32],"float16"), 5.0, 5, 3, 0, False, "NCL", None, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 100, 0, 64],"float16"), Tensor([2, 100, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 100],"float16"), is_causal=False, )
paddle.einsum("ibm,hm->ibh", Tensor([0, 14, 32],"float32"), Tensor([32, 32],"float32"), )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 4, 128],"float16"), Tensor([1, 2048, 0, 128],"float16"), Tensor([1, 2048, 4, 128],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), dropout_p=0.0, training=True, is_causal=False, )
paddle.slice(Tensor([3, 0],"float32"), axes=list[0,], starts=list[2,], ends=list[3,], )
paddle.nn.functional.conv2d(Tensor([1, 128, 12, 0],"float32"), Tensor([128, 128, 3, 3],"float32"), None, list[1,1,], 1, list[1,1,], 1, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 64, 0, 32],"float32"), Tensor([64, 1, 8, 8],"float32"), bias=None, padding=2, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=64, output_size=None, data_format="NCHW", )
paddle.roll(Tensor([4, 5, 4, 0],"float64"), Tensor([4],"int64"), list[0,1,2,3,], name=None, )
paddle.Tensor.trunc(Tensor([0, 28],"float32"), )
paddle.nn.functional.mse_loss(Tensor([10, 0],"float32"), Tensor([10, 0],"float32"), "mean", )
paddle.einsum("b h i d, b h j d -> b h i j", Tensor([1, 0, 1, 64],"float32"), Tensor([1, 0, 109, 64],"float32"), )
paddle.nn.functional.softmax(Tensor([2, 3, 0],"float32"), axis=1, dtype="float32", )
paddle.nn.functional.avg_pool2d(Tensor([1, 1024, 0, 54],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.quant.weight_quantize(Tensor([64, 0],"float16"), algo="weight_only_int4", arch=70, group_size=-1, )
paddle.nn.functional.avg_pool2d(Tensor([0, 128, 64, 64],"float32"), kernel_size=1, stride=1, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.conv1d(Tensor([2, 3, 0],"float32"), Tensor([6, 1, 3],"float32"), bias=Tensor([6],"float32"), padding=0, stride=list[2,], dilation=list[1,], groups=3, data_format="NCL", )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([0, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )
paddle.allclose(Tensor([0, 16],"float32"), Tensor([0, 16],"float32"), atol=1e-05, rtol=1e-05, )
paddle.nn.functional.conv1d_transpose(Tensor([4, 6, 16],"float32"), Tensor([6, 8, 0],"float32"), bias=Tensor([8],"float32"), output_size=None, output_padding=0, padding="valid", stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv2d(Tensor([1, 1024, 0, 32],"float32"), Tensor([256, 1024, 1, 1],"float32"), None, list[1,1,], 0, list[1,1,], 1, )
paddle.nn.functional.conv2d(Tensor([1, 3, 224, 0],"float32"), Tensor([3, 3, 3, 3],"float32"), Tensor([3],"float32"), list[3,2,], 0, list[1,1,], 1, "NCHW", )
paddle.nn.functional.zeropad2d(Tensor([4, 3, 224, 0],"float64"), list[2,2,2,2,], )
paddle.nn.functional.conv3d(Tensor([4, 8, 8, 0, 3],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[1,1,],list[2,2,],list[1,1,],list[0,0,],], stride=1, dilation=1, groups=1, data_format="NDHWC", )
paddle.nn.functional.conv2d(Tensor([8, 128, 0, 257],"float32"), Tensor([256, 128, 0, 3],"float32"), bias=None, stride=2, padding=0, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[274,1,1,], )
paddle.Tensor.isnan(Tensor([2, 0],"float64"), )
paddle.nn.functional.l1_loss(Tensor([10, 499, 0],"float32"), Tensor([10, 499, 0],"float32"), "sum", name=None, )
paddle.nn.functional.max_unpool3d(Tensor([1, 0, 2, 2, 3],"float32"), Tensor([1, 0, 2, 2, 3],"int64"), kernel_size=2, stride=2, output_size=list[1,3,4,4,6,], )
paddle.Tensor.tile(Tensor([2, 2, 1, 0, 1],"float16"), list[1,1,1,1,2,], )
paddle.vision.ops.generate_proposals(Tensor([1, 9, 12, 9],"float32"), Tensor([1, 0, 12, 9],"float32"), Tensor([1, 2],"float32"), Tensor([972, 4],"float32"), Tensor([972, 4],"float32"), pre_nms_top_n=4000, post_nms_top_n=4000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.allclose(Tensor([0],"float64"), Tensor([0],"float64"), atol=0.0001, rtol=0.0001, )
paddle.nn.functional.avg_pool3d(Tensor([2, 3, 8, 0, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
paddle.nn.functional.max_pool3d(Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=list[1,0,0,], return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.nn.functional.pad(Tensor([1, 2, 3, 0, 5],"complex128"), pad=list[1,2,1,1,3,4,], mode="constant", value=100, )
paddle.einsum("ij,j", Tensor([1, 1],"float64"), Tensor([0],"float64"), )
paddle.nn.functional.max_unpool2d(Tensor([1, 0, 2, 2],"float32"), Tensor([1, 0, 2, 2],"int32"), kernel_size=2, stride=2, output_size=tuple(5,5,), )
paddle.nn.functional.pad(Tensor([1, 0, 14, 16, 384],"float32"), tuple(0,5,0,0,0,0,), data_format="NDHWC", )
paddle.nn.functional.pad(Tensor([0, 16, 18, 14, 384],"float32"), tuple(0,0,0,3,0,0,), data_format="NDHWC", )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([0, 3, 4, 4],"float32"), output_size=list[3,3,], )
paddle.nn.functional.conv2d(Tensor([4, 6, 16, 16],"float32"), Tensor([8, 3, 3, 0],"float32"), Tensor([8],"float32"), padding="same", stride=1, dilation=1, groups=2, data_format="NCHW", )
paddle.nn.functional.max_pool2d(Tensor([2, 3, 0, 32],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[593,1,1,], )
paddle.nn.functional.lp_pool2d(Tensor([2, 0, 32, 32],"float16"), 2, kernel_size=3, stride=2, ceil_mode=False, )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 0, 32],"float64"), output_size=list[1,1,1,], data_format="NCDHW", name=None, )
paddle.nn.quant.weight_quantize(Tensor([0, 768],"float16"), algo="weight_only_int8", )
paddle.nn.functional.conv2d_transpose(Tensor([2, 2, 2, 6],"float32"), Tensor([6, 1, 3, 0],"float32"), bias=Tensor([3],"float32"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=3, output_size=None, data_format="NHWC", )
paddle.vision.ops.yolo_box(Tensor([2, 14, 0, 8],"float32"), img_size=Tensor([2, 2],"int32"), anchors=list[10,13,16,30,], class_num=2, conf_thresh=0.01, downsample_ratio=8, clip_bbox=True, scale_x_y=1.0, )
paddle.Tensor.lgamma(Tensor([5, 7, 8, 0],"float64"), )
paddle.nanmedian(Tensor([2, 0],"float64"), axis=1, keepdim=False, )
paddle.nn.functional.log_softmax(Tensor([2, 3, 4, 0],"float32"), 1, None, )
paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 3, 0, 2, 4],"float32"), Tensor([1, 3, 0, 2, 4],"float32"), )
paddle.slice(Tensor([3, 0, 4],"float32"), axes=list[0,], starts=list[2,], ends=list[3,], )
paddle.nn.functional.max_pool3d(x=Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=2, padding=1, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 2, 3, 3],"float64"), Tensor([2, 2, 1, 0],"float64"), groups=1, padding="VALID", )
paddle.nn.functional.avg_pool2d(Tensor([0, 104, 28, 28],"float32"), kernel_size=3, stride=2, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.max_pool2d(Tensor([2, 0, 32, 32],"float32"), kernel_size=2, stride=2, padding=1, ceil_mode=False, )
paddle.nn.functional.max_pool2d(Tensor([1, 24, 0, 40],"float32"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 1024, 8, 16],"bfloat16"), Tensor([1, 1024, 2, 16],"bfloat16"), Tensor([1, 1024, 0, 16],"bfloat16"), attn_mask=None, is_causal=True, )
paddle.Tensor.imag(Tensor([0, 784],"complex64"), )
paddle.nn.functional.max_pool3d(x=Tensor([0, 8, 32, 112, 112],"float32"), kernel_size=list[1,3,3,], stride=list[1,2,2,], padding=list[0,1,1,], data_format="NCDHW", )
paddle.nn.functional.avg_pool2d(Tensor([32, 0, 56, 56],"float32"), kernel_size=2, stride=2, padding="SAME", ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.einsum("xy,yz->xz", Tensor([0, 4],"complex64"), Tensor([4, 4],"complex64"), )
paddle.subtract(x=Tensor([0, 2],"float32"), y=Tensor([0, 2],"float32"), )
paddle.einsum("ij->j", Tensor([4, 0],"float64"), )
paddle.masked_fill(Tensor([120],"float32"), Tensor([0, 120],"bool"), Tensor([1],"float32"), )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 0],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,], dilation=1, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 5, 0],"float32"), list[536,1,1,], )
paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), )
paddle.nn.functional.pad(Tensor([1, 1, 2, 3, 0],"float64"), pad=list[1,0,1,2,1,0,], mode="constant", value=0.0, data_format="NCDHW", name=None, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 0, 32, 32],"float32"), output_size=tuple(3,3,), data_format="NCHW", name=None, )
paddle.slice(Tensor([11, 0, 4],"float32"), axes=list[2,], starts=list[1,], ends=list[2,], )
paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([0],"float32"), shift=None, smooth=None, quant_scale=0.0014022786635905504, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.logcumsumexp(Tensor([3, 0],"float32"), )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float32"), weight=Tensor([1, 3, 3, 0],"float32"), bias=Tensor([1],"float32"), stride=list[2,1,], padding=1, dilation=2, )
paddle.nn.functional.prelu(Tensor([0, 2, 3, 4],"float32"), Tensor([2],"float32"), data_format="NCHW", )
paddle.nn.functional.margin_ranking_loss(Tensor([0, 10],"float64"), Tensor([0, 10],"float64"), Tensor([0, 10],"float64"), 0.2, "none", None, )
paddle.nn.functional.max_pool2d(Tensor([1, 0, 10, 10],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.einsum("ijkl, lmn->ijn", Tensor([2, 4, 5, 3],"float64"), Tensor([3, 2, 0],"float64"), )
paddle.nn.functional.softmax(Tensor([10, 8, 0, 89],"float32"), axis=-1, )
paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 3, 1, 3],"float32"), Tensor([13, 4, 3, 3, 0],"float32"), )
paddle.Tensor.tile(Tensor([10285, 0],"float32"), list[1,95,], )
paddle.nn.functional.pad(Tensor([1, 0, 2, 3],"float64"), pad=list[1,0,1,2,], mode="constant", value=0.0, data_format="NCHW", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([0, 6, 16, 16],"float32"), Tensor([6, 8, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=list[list[0,0,],list[0,0,],list[1,2,],list[2,1,],], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.max_pool3d(Tensor([1, 3, 4, 0, 6],"float64"), kernel_size=2, stride=2, padding=0, return_mask=True, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.nn.functional.max_unpool2d(Tensor([1, 1, 2, 0],"float32"), Tensor([1, 1, 2, 0],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCHW", output_size=None, name=None, )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 3, 0, 4],"float32"), output_size=list[3,3,], )
paddle.Tensor.tile(Tensor([1, 8, 0],"float32"), list[768,1,1,], )
paddle.nn.functional.margin_ranking_loss(Tensor([0, 2],"float64"), other=Tensor([0, 2],"float64"), label=Tensor([0, 2],"float64"), margin=0.0, reduction="mean", name=None, )
paddle.nn.functional.l1_loss(Tensor([2, 0, 32, 32],"float32"), Tensor([2, 0, 32, 32],"float32"), "mean", name=None, )
paddle.nn.functional.softmax(Tensor([2, 3, 0, 5],"float32"), 0, name=None, )
paddle.allclose(x=Tensor([2, 2, 2, 0],"float64"), y=Tensor([2, 2, 2, 0],"float64"), rtol=-3.0, atol=-2.0, )
paddle.nn.quant.weight_only_linear(Tensor([2, 1, 512],"float16"), weight=Tensor([0, 512],"int8"), bias=Tensor([1024],"float16"), weight_scale=Tensor([1024],"float16"), weight_dtype="int8", )
paddle.nn.functional.conv3d_transpose(Tensor([4, 4, 8, 8, 0],"float32"), Tensor([4, 3, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,1,2,2,1,1,], stride=1, dilation=1, groups=2, data_format="NCDHW", )
paddle.nn.functional.conv3d(Tensor([4, 8, 8, 8, 3],"float32"), Tensor([5, 3, 3, 3, 0],"float32"), Tensor([5],"float32"), padding=0, stride=1, dilation=1, groups=1, data_format="NDHWC", )
paddle.nn.functional.max_pool2d(Tensor([256, 112, 0, 64],"float16"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NHWC", name=None, )
paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([13, 4, 5, 0, 8],"float32"), Tensor([13, 4, 5, 7, 8],"float32"), )
paddle.isin(Tensor([4, 0],"bfloat16"), Tensor([2, 3],"bfloat16"), False, True, )
paddle.nn.functional.avg_pool2d(Tensor([64, 0, 3, 80],"float16"), kernel_size=list[3,2,], stride=list[3,2,], padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([1, 0, 32, 224, 258],"float32"), tuple(0,2,0,0,0,0,), data_format="NCDHW", )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 8, 96],"float16"), Tensor([2, 101, 8, 96],"float16"), Tensor([2, 101, 8, 0],"float16"), attn_mask=None, is_causal=False, )
paddle.Tensor.lgamma(Tensor([0, 7, 8],"float64"), )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 4, 0],"float32"), list[1,40,], )
paddle.nn.functional.max_pool2d(Tensor([0, 256, 20, 27],"float32"), 1, stride=2, )
paddle.einsum("bnij,bjnd->bind", Tensor([13, 4, 2, 0],"float32"), Tensor([13, 2, 4, 0],"float32"), )
paddle.nn.functional.conv2d_transpose(Tensor([2, 6, 2, 0],"float32"), Tensor([6, 1, 3, 3],"float32"), bias=None, padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=3, output_size=None, data_format="NCHW", )
paddle.Tensor.__getitem__(Tensor([0, 3, 3],"float32"), tuple(slice(None,None,-1),slice(None,None,-1),slice(None,None,-1),), )
paddle.nn.quant.weight_only_linear(Tensor([131, 768],"float16"), weight=Tensor([0, 768],"int8"), bias=None, weight_scale=Tensor([2304],"float16"), weight_dtype="int8", )
paddle.nn.quant.weight_only_linear(Tensor([1, 32, 128],"float16"), Tensor([0, 128],"int8"), bias=Tensor([288],"float16"), weight_scale=Tensor([288],"float16"), weight_dtype="int8", group_size=-1, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float32"), weight=Tensor([3, 1, 3, 0],"float32"), bias=Tensor([1],"float32"), stride=1, padding=list[1,0,], dilation=2, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[270,1,1,], )
paddle.nn.functional.max_pool3d(x=Tensor([2, 3, 8, 0, 8],"float64"), kernel_size=list[3,3,3,], stride=2, padding=1, )
paddle.einsum("i , j -> i j", Tensor([0],"float32"), Tensor([2],"float32"), )
paddle.nn.functional.conv1d_transpose(x=Tensor([0, 3, 2],"float64"), weight=Tensor([3, 1, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,], output_padding=0, groups=1, dilation=2, output_size=None, data_format="NCL", name=None, )
paddle.nanmedian(Tensor([2, 3, 4, 0],"float32"), axis=-1, keepdim=False, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float64"), weight=Tensor([1, 3, 0, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=2, padding=0, )
paddle.Tensor.frexp(Tensor([10, 0],"float64"), )
paddle.quantile(Tensor([4, 0, 6],"float64"), q=0.1, axis=list[1,2,], keepdim=True, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 4, 4],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([0],"float64"), stride=1, padding="vaLiD", output_padding=0, dilation=1, groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv3d_transpose(x=Tensor([0, 3, 2, 2, 2],"float64"), weight=Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,1,], dilation=2, )
paddle.slice(Tensor([2, 0, 104],"float32"), list[0,1,2,], list[0,0,72,], list[2,2,88,], )
paddle.Tensor.flip(Tensor([4, 0],"float32"), 1, )
paddle.nn.functional.l1_loss(Tensor([0, 10, 5],"float32"), Tensor([0, 10, 5],"float32"), "sum", name=None, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[724,1,1,], )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float64"), weight=Tensor([1, 3, 3, 3, 0],"float64"), bias=Tensor([1],"float64"), stride=list[2,2,1,], padding=1, dilation=2, )
paddle.nn.functional.conv2d(Tensor([3, 8, 4, 0],"float32"), Tensor([8, 8, 3, 3],"float32"), None, list[2,2,], 1, list[1,1,], 1, "NCHW", )
paddle.Tensor.tile(Tensor([8, 1, 0, 2],"float32"), list[1,3,4,1,], )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 0, 2],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=1, dilation=1, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[710,1,1,], )
paddle.nn.functional.conv1d_transpose(Tensor([2, 3, 2],"float32"), Tensor([3, 1, 0],"float32"), bias=Tensor([1],"float32"), output_size=None, output_padding=0, padding=list[1,], stride=list[2,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nanmedian(Tensor([2, 3, 4, 0],"float32"), axis=-2, keepdim=False, mode="min", )
paddle.nn.functional.conv2d_transpose(x=Tensor([0, 6, 2, 2],"float32"), weight=Tensor([6, 1, 3, 3],"float32"), bias=Tensor([3],"float32"), stride=1, padding=list[1,0,], groups=3, dilation=1, )
paddle.nn.functional.pad(Tensor([12, 1, 0, 16],"float32"), list[2,1,2,1,], )
paddle.nn.functional.avg_pool3d(Tensor([2, 0, 8, 8, 8],"float32"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCDHW", name=None, )
paddle.nn.functional.softmax(Tensor([0, 7, 99],"float32"), )
paddle.nn.functional.fractional_max_pool3d(Tensor([0, 3, 7, 7, 7],"float32"), output_size=list[3,3,3,], kernel_size=2, random_u=0.6, return_mask=False, name=None, )
paddle.nn.functional.conv1d(Tensor([13, 1, 0],"float32"), Tensor([32, 1, 0],"float32"), bias=None, padding=0, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.Tensor.digamma(Tensor([5, 0, 8, 10],"float64"), )
paddle.nn.functional.selu(x=Tensor([3, 0, 3],"float64"), )
paddle.nn.functional.poisson_nll_loss(Tensor([0, 2],"float16"), Tensor([0, 2],"float16"), log_input=True, full=False, epsilon=1e-08, reduction="mean", name=None, )
paddle.Tensor.tile(Tensor([12240, 0],"float32"), list[1,37,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[1082,1,1,], )
paddle.nn.functional.conv1d_transpose(Tensor([0, 3, 2],"float32"), Tensor([3, 1, 3],"float32"), bias=Tensor([1],"float32"), output_size=None, output_padding=0, padding=1, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv2d_transpose(Tensor([1, 1024, 0, 128],"float32"), Tensor([1024, 128, 3, 3],"float32"), padding=0, stride=2, groups=4, )
paddle.nn.functional.conv2d_transpose(Tensor([0, 4, 16, 16],"float32"), Tensor([4, 4, 3, 3],"float32"), Tensor([4],"float32"), output_size=list[18,34,], padding="valid", stride=tuple(1,2,), dilation=1, groups=1, data_format="NCHW", )
paddle.nn.functional.conv2d(Tensor([4, 6, 0, 16],"float32"), Tensor([12, 1, 0, 3],"float32"), None, padding="valid", stride=1, dilation=1, groups=6, data_format="NCHW", )
paddle.nn.functional.conv1d(Tensor([13, 7, 32],"float32"), Tensor([32, 1, 0],"float32"), bias=None, padding=1, stride=list[1,], dilation=list[1,], groups=32, data_format="NLC", )
paddle.nn.functional.conv2d(Tensor([2, 3, 37, 37],"float32"), Tensor([64, 3, 7, 0],"float32"), None, list[2,2,], 0, list[1,1,], 1, "NCHW", )
paddle.nn.functional.avg_pool3d(Tensor([0, 3, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding=0, divisor_override=8, )
paddle.Tensor.__sub__(Tensor([3, 6, 3, 1, 0, 5],"float64"), Tensor([3, 6, 3, 1, 0, 5],"float64"), )
paddle.nn.functional.conv2d_transpose(Tensor([4, 16, 16, 4],"float32"), Tensor([4, 0, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding="same", stride=1, dilation=1, groups=2, data_format="NHWC", )
paddle.Tensor.__getitem__(Tensor([0, 3, 3],"float32"), tuple(slice(None,-1,None),slice(None,None,-1),slice(-1,None,None),), )
paddle.Tensor.tile(Tensor([13, 0, 1],"float32"), repeat_times=list[1,12,1,], )
paddle.nn.functional.conv3d(Tensor([4, 8, 8, 0, 3],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[1,1,2,2,3,3,], stride=1, dilation=2, groups=1, data_format="NDHWC", )
paddle.nn.functional.conv3d_transpose(Tensor([4, 8, 8, 8, 3],"float32"), Tensor([3, 5, 3, 3, 0],"float32"), Tensor([5],"float32"), output_size=None, padding=0, stride=1, dilation=1, groups=1, data_format="NDHWC", )
paddle.nn.functional.avg_pool3d(x=Tensor([0, 2048, 4, 7, 7],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", )
paddle.nn.functional.conv1d(Tensor([16, 80, 89],"float32"), Tensor([80, 80, 0],"float32"), bias=None, padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.temporal_shift(x=Tensor([0, 4, 2, 2],"float32"), seg_num=2, shift_ratio=0.2, )
paddle.Tensor.tile(Tensor([1, 0, 768],"float32"), list[7,1,1,], )
paddle.nn.functional.conv2d(Tensor([8, 256, 128, 128],"float32"), Tensor([256, 256, 0, 3],"float32"), bias=None, stride=1, padding=1, )
paddle.nn.functional.softmax(Tensor([2, 3, 0, 5, 5],"float16"), )
paddle.nn.functional.group_norm(Tensor([2, 3, 2, 0, 4],"float64"), num_groups=2, weight=Tensor([4],"float64"), bias=Tensor([4],"float64"), data_format="NDHWC", )
paddle.nn.functional.avg_pool2d(x=Tensor([2, 0, 32, 32],"float64"), kernel_size=list[3,3,], )
paddle.allclose(Tensor([0, 2],"float32"), Tensor([0, 2],"float32"), rtol=0.0001, atol=0.0001, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 0],"float32"), Tensor([3, 1, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[2,2,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 1024, 8, 16],"bfloat16"), Tensor([1, 0, 2, 16],"bfloat16"), Tensor([1, 1024, 2, 16],"bfloat16"), attn_mask=None, is_causal=True, )
paddle.nn.functional.conv1d(Tensor([4, 6, 16],"float32"), Tensor([512, 6, 0],"float32"), bias=Tensor([512],"float32"), padding=list[1,2,], stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 0, 4, 3],"float64"), output_size=3, data_format="NHWC", name=None, )
paddle.nn.functional.conv2d(Tensor([1024, 1, 258, 0],"float32"), Tensor([1, 1, 4, 0],"float32"), )
paddle.Tensor.tile(Tensor([10105, 0],"float32"), list[1,14,], )
paddle.nn.functional.conv2d_transpose(Tensor([2, 2, 0, 3],"float64"), Tensor([2, 2, 0, 1],"float64"), groups=1, padding=list[1,0,0,1,], )
paddle.nn.functional.pad(Tensor([0, 4, 5, 6, 7],"complex64"), pad=list[1,2,2,1,1,0,], mode="circular", value=0.0, data_format="NCDHW", name=None, )
paddle.nn.functional.pad(Tensor([1, 1, 10, 0],"float32"), pad=list[0,1,0,0,], mode="constant", value=-10000.0, data_format="NCHW", name=None, )
paddle.nn.functional.mse_loss(Tensor([16, 0, 2],"float32"), Tensor([16, 0, 2],"float32"), reduction="none", )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([0, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )
paddle.Tensor.__pow__(Tensor([10, 0],"float32"), Tensor([10, 0],"float32"), )
paddle.nn.functional.avg_pool2d(Tensor([0, 256, 56, 56],"float32"), kernel_size=2, stride=2, padding="SAME", ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.mse_loss(input=Tensor([2, 0],"float32"), label=Tensor([2, 0],"float32"), )
paddle.einsum("binh,tnh->bnit", Tensor([0, 2, 4, 4],"float32"), Tensor([4, 4, 4],"float32"), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[224,1,1,], )
paddle.vision.ops.generate_proposals(Tensor([0, 15, 42, 63],"float32"), Tensor([1, 60, 42, 63],"float32"), Tensor([1, 2],"float32"), Tensor([39690, 4],"float32"), Tensor([39690, 4],"float32"), pre_nms_top_n=12000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.max_pool3d(Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,0,0,], return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.nn.functional.conv1d(Tensor([2, 3, 4],"float32"), Tensor([1, 3, 3],"float32"), bias=Tensor([0],"float32"), padding=tuple(1,), stride=list[1,], dilation=list[2,], groups=1, data_format="NCL", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[1038,1,1,], )
paddle.nn.functional.conv1d_transpose(Tensor([2, 3, 2],"float32"), Tensor([3, 0, 3],"float32"), bias=Tensor([1],"float32"), output_size=None, output_padding=0, padding=list[1,], stride=list[1,], dilation=list[2,], groups=1, data_format="NCL", )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 2],"float32"), weight=Tensor([3, 0, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=list[1,], groups=3, dilation=1, )
paddle.einsum("a...d,...cb->...abcd", Tensor([10, 3, 2, 3, 4],"float64"), Tensor([12, 0],"float64"), )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 0],"float64"), weight=Tensor([1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=2, padding=0, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 0],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=False, )
paddle.nn.functional.kl_div(Tensor([40, 0, 50],"float32"), Tensor([40, 0, 50],"float32"), "none", False, )
paddle.nn.functional.conv1d(Tensor([1, 128, 0],"float32"), Tensor([128, 128, 11],"float32"), bias=Tensor([128],"float32"), padding=25, stride=list[1,], dilation=list[5,], groups=1, data_format="NCL", )
paddle.nn.quant.weight_only_linear(Tensor([0, 512],"float16"), weight=Tensor([1024, 512],"int8"), bias=Tensor([1024],"float16"), weight_scale=Tensor([1024],"float16"), weight_dtype="int8", )
paddle.nn.functional.avg_pool1d(x=Tensor([0, 3, 8],"float64"), kernel_size=3, stride=4, padding=0, )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[45,1,1,], )
paddle.nn.functional.pad(Tensor([0, 16, 14, 14, 384],"float32"), tuple(0,0,0,0,0,0,), data_format="NDHWC", )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[1685,1,1,], )
paddle.index_fill(Tensor([4],"int64"), Tensor([0],"int64"), 0, 2, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 4, 4],"float64"), weight=Tensor([3, 1, 3, 0],"float64"), bias=Tensor([1],"float64"), stride=1, padding="vaLiD", output_padding=0, dilation=1, groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.pixel_unshuffle(Tensor([2, 1, 12, 0],"float16"), 3, "NCHW", )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([0, 3, 8, 32, 32],"float32"), output_size=list[3,3,3,], )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 8, 0],"float16"), Tensor([2, 101, 8, 0],"float16"), Tensor([2, 101, 8, 0],"float16"), attn_mask=None, is_causal=False, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 2],"float32"), Tensor([3, 1, 3, 0],"float32"), bias=None, padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[2,2,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 0, 2, 2],"float64"), weight=Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,1,], dilation=1, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 0],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=1, output_padding=0, dilation=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([30, 64, 0, 128],"float32"), list[0,1,0,1,], value=-math.inf, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float32"), weight=Tensor([3, 1, 3, 3],"float32"), bias=Tensor([0],"float32"), stride=1, padding=list[1,0,], dilation=2, )
paddle.Tensor.tile(Tensor([1, 0, 768],"float32"), list[16,1,1,], )
paddle.einsum("nbka,ahc->nbkhc", Tensor([1, 0, 5, 6],"float16"), Tensor([6, 2, 4],"float16"), )
paddle.nn.functional.conv2d_transpose(Tensor([1, 1024, 2, 2],"float32"), Tensor([1024, 512, 4, 0],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 4, 128],"float16"), Tensor([1, 2048, 4, 128],"float16"), Tensor([1, 2048, 4, 0],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([0, 3, 4, 4],"float64"), output_size=list[3,3,], )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 0],"float64"), weight=Tensor([3, 2, 3],"float64"), bias=Tensor([2],"float64"), stride=1, padding="SaME", output_padding=0, groups=1, dilation=1, output_size=None, data_format="NCL", )
paddle.nn.functional.pad(Tensor([1, 0, 3, 4, 5],"complex128"), pad=list[1,2,1,1,3,4,], mode="constant", value=100, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 16],"float32"), Tensor([6, 8, 3, 0],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[1,2,], dilation=list[2,2,], groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.fill_diagonal_(Tensor([3, 0],"float64"), 1, offset=0, wrap=True, )
paddle.Tensor.fill_diagonal_(Tensor([0, 3],"bool"), 0, offset=0, wrap=True, )
paddle.nn.functional.poisson_nll_loss(Tensor([0, 2],"float32"), Tensor([0, 2],"float32"), log_input=True, full=False, epsilon=1e-08, reduction="sum", name=None, )
paddle.nn.functional.pad(Tensor([1, 0, 18, 14, 384],"float16"), tuple(0,0,0,3,0,0,), data_format="NDHWC", )
paddle.nn.functional.avg_pool3d(x=Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,1,1,], exclusive=True, )
paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([0, 4, 3, 1, 3],"float32"), Tensor([0, 4, 3, 3, 8],"float32"), )
paddle.nn.functional.pad(Tensor([0, 2, 2, 2, 2],"float64"), pad=list[1,1,1,0,1,0,], mode="reflect", value=0.0, data_format="NDHWC", name=None, )
paddle.nn.functional.poisson_nll_loss(Tensor([10, 0],"float16"), Tensor([10, 0],"float16"), log_input=True, full=False, epsilon=1e-08, reduction="mean", )
paddle.nn.functional.conv2d(Tensor([1, 256, 0, 128],"float32"), Tensor([3, 256, 1, 1],"float32"), padding=0, groups=1, )
paddle.nn.functional.softmax(x=Tensor([2, 3, 0],"float64"), axis=2, )
paddle.nn.functional.conv2d(Tensor([1, 256, 128, 128],"float32"), Tensor([3, 256, 0, 1],"float32"), padding=0, groups=1, )
paddle.nn.functional.pad(Tensor([1, 16, 0, 14, 384],"float32"), tuple(0,0,0,6,0,0,), data_format="NDHWC", )
paddle.nn.functional.poisson_nll_loss(Tensor([4, 0, 2],"bfloat16"), Tensor([4, 0, 2],"float32"), )
paddle.kthvalue(Tensor([0, 200, 40],"float32"), k=1, axis=1, )
paddle.nn.functional.pad(Tensor([0, 4, 5, 6],"complex128"), pad=list[1,2,2,1,], mode="circular", value=0.0, data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[172,1,1,], )
paddle.nn.functional.conv1d(Tensor([1, 128, 112],"float32"), Tensor([128, 128, 0],"float32"), bias=Tensor([128],"float32"), padding=15, stride=list[1,], dilation=list[3,], groups=1, data_format="NCL", )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[124,1,1,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[175,1,1,], )
paddle.slice(Tensor([0, 2, 104],"float32"), list[0,1,2,], list[0,0,40,], list[2,2,56,], )
paddle.nn.functional.pad(x=Tensor([3, 2, 0, 2],"float64"), pad=list[1,1,2,3,], mode="constant", value=2.0, data_format="NCHW", )
paddle.nn.functional.log_softmax(x=Tensor([2, 3, 0],"float64"), )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[556,1,1,], )
paddle.einsum("bmtd,mdhr->bmhtr", Tensor([13, 2, 16, 0],"float32"), Tensor([2, 16, 4, 0],"float32"), )
paddle.nn.functional.scaled_dot_product_attention(Tensor([4, 134, 4, 128],"float16"), Tensor([4, 134, 4, 128],"float16"), Tensor([4, 134, 0, 128],"float16"), attn_mask=Tensor([4, 1, 134, 134],"float16"), dropout_p=0.0, training=True, is_causal=False, )
paddle.nn.functional.max_pool3d(Tensor([2, 3, 0, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,0,0,], return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.einsum("ijbs,ibns->bnij", Tensor([7, 7, 14, 2],"float32"), Tensor([7, 14, 0, 2],"float32"), )
paddle.nn.functional.margin_ranking_loss(Tensor([0, 10],"float64"), Tensor([0, 10],"float64"), Tensor([0, 10],"float64"), 0.2, "none", )
paddle.nn.functional.pad(Tensor([1, 0, 31, 28, 192],"float32"), tuple(0,0,0,4,0,0,), data_format="NDHWC", )
paddle.Tensor.fill_diagonal_(Tensor([0, 3],"float32"), 1, offset=0, wrap=True, )
paddle.nn.functional.pixel_shuffle(Tensor([4, 0, 128, 128],"float16"), 2, "NCHW", None, )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([0, 3, 5, 7, 7],"float32"), output_size=list[2,3,5,], data_format="NCDHW", name=None, )
paddle.nn.functional.avg_pool3d(x=Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], )
paddle.Tensor.tile(Tensor([1, 4, 0],"float32"), list[1799,1,1,], )
paddle.nn.functional.softmax(Tensor([2, 0, 4],"float32"), axis=-3, dtype=None, name=None, )
paddle.einsum("...ij,...jk->...ik", Tensor([0, 5],"float64"), Tensor([5, 5],"float64"), )
paddle.nn.functional.conv1d(Tensor([1, 1280, 3000],"float32"), Tensor([1280, 1280, 0],"float32"), bias=Tensor([1280],"float32"), padding=1, stride=list[2,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.meshgrid(Tensor([0],"float32"), Tensor([0],"float32"), Tensor([0],"float32"), )
paddle.nn.functional.max_pool3d(x=Tensor([0, 3, 8, 8, 8],"float32"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], )
paddle.nn.functional.conv2d_transpose(Tensor([2, 6, 2, 0],"float32"), Tensor([6, 1, 3, 3],"float32"), bias=Tensor([3],"float32"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=3, output_size=None, data_format="NCHW", )
paddle.strided_slice(x=Tensor([0, 8, 6, 4, 2, 6],"float64"), axes=list[1,2,5,], starts=list[6,5,4,], ends=list[2,0,1,], strides=list[-1,-2,-3,], )
paddle.incubate.nn.functional.fused_bias_act(Tensor([100, 0],"float16"), None, act_method="swiglu", )
paddle.nn.functional.avg_pool2d(Tensor([0, 1536, 7, 7],"float32"), kernel_size=7, stride=1, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.maxout(Tensor([2, 6, 5, 0],"float64"), 2, 1, )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[324,1,1,], )
paddle.nn.functional.conv1d(Tensor([1, 128, 112],"float32"), Tensor([128, 128, 0],"float32"), bias=Tensor([128],"float32"), padding=5, stride=list[1,], dilation=list[5,], groups=1, data_format="NCL", )
paddle.logcumsumexp(Tensor([10, 0],"float32"), dtype="float32", axis=None, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 8, 96],"float16"), Tensor([2, 100, 8, 96],"float16"), Tensor([2, 0, 8, 96],"float16"), attn_mask=None, is_causal=False, )
paddle.vision.ops.generate_proposals(Tensor([1, 15, 40, 60],"float32"), Tensor([1, 60, 40, 60],"float32"), Tensor([1, 2],"float32"), Tensor([0, 4],"float32"), Tensor([36000, 4],"float32"), pre_nms_top_n=12000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.allclose(Tensor([0, 8],"float32"), Tensor([0, 8],"float32"), atol=1e-06, rtol=1e-06, )
paddle.nn.functional.softmax(Tensor([128, 0, 16, 49, 49],"float16"), -1, name=None, )
paddle.nn.functional.max_pool2d(Tensor([2, 0, 32, 32],"float64"), kernel_size=list[3,3,], stride=list[1,1,], padding=list[0,0,0,0,], return_mask=False, ceil_mode=True, data_format="NCHW", name=None, )
paddle.Tensor.__rpow__(Tensor([0, 5, 2],"float32"), 2, )
paddle.incubate.softmax_mask_fuse(x=Tensor([0, 8, 8, 32],"float16"), mask=Tensor([0, 1, 8, 32],"float16"), )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 3, 0, 4],"float64"), output_size=list[1,1,], )
paddle.nn.functional.pad(Tensor([2, 0, 4, 4],"float32"), list[1,1,1,1,], mode="reflect", data_format="NCHW", )
paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 0],"int32"), bias=Tensor([10],"float32"), dequant_scales=Tensor([10],"float32"), act_method="gelu", compute_dtype="fp32", )
paddle.slice(Tensor([0, 2, 104],"float32"), list[0,1,2,], list[0,0,88,], list[16,2,104,], )
paddle.nn.functional.pad(Tensor([2, 0, 4],"float32"), list[1,1,], mode="circular", data_format="NCL", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float32"), Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([0],"float32"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.max_pool3d(x=Tensor([8, 0, 32, 112, 112],"float32"), kernel_size=list[1,3,3,], stride=list[1,2,2,], padding=list[0,1,1,], data_format="NCDHW", )
paddle.nn.functional.conv2d_transpose(Tensor([0, 2, 3, 3],"float64"), Tensor([2, 2, 1, 1],"float64"), groups=1, padding="VALID", )
paddle.nn.functional.pad(Tensor([3, 4, 5, 6, 0],"complex128"), pad=list[1,1,1,1,1,1,], mode="constant", value=100, data_format="NCDHW", name=None, )
paddle.nn.functional.max_pool2d(Tensor([2, 3, 32, 0],"float32"), kernel_size=2, stride=None, padding="SAME", return_mask=True, )
paddle.nn.functional.multi_margin_loss(input=Tensor([5, 0],"float64"), label=Tensor([5],"int64"), p=1, margin=1.0, weight=None, reduction="none", )
paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 0, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, Tensor([4],"float32"), Tensor([4],"float32"), dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )
paddle.nn.functional.max_pool1d(x=Tensor([0, 3, 8],"float64"), kernel_size=1, stride=1, padding=0, )
paddle.nn.functional.pad(Tensor([0, 3, 32, 245, 224],"float32"), tuple(0,0,0,3,0,0,), data_format="NCDHW", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[1155,1,1,], )
paddle.slice(Tensor([0, 2, 104],"float32"), list[0,1,2,], list[0,0,64,], list[2,2,80,], )
paddle.nn.functional.pad(Tensor([1, 3, 32, 0, 224],"float32"), tuple(0,0,0,2,0,0,), data_format="NCDHW", )
paddle.take(Tensor([3, 0],"float64"), Tensor([2, 3],"int64"), mode="raise", )
paddle.Tensor.tile(Tensor([1, 1, 28, 0],"float32"), list[1,3,1,1,], )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 160, 0, 32],"float16"), output_size=4, data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([3, 4, 5, 0, 7],"complex128"), pad=list[1,1,1,1,1,1,], mode="constant", value=100, data_format="NCDHW", name=None, )
paddle.nn.functional.pixel_shuffle(x=Tensor([2, 9, 0, 4],"float64"), upscale_factor=3, data_format="NCHW", )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 32, 0],"float32"), output_size=list[3,3,3,], data_format="NCDHW", name=None, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[455,1,1,], )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 0, 8, 32, 32],"float64"), output_size=tuple(3,3,3,), data_format="NCDHW", )
paddle.nn.functional.conv2d(Tensor([2, 3, 0, 37],"float32"), Tensor([64, 3, 0, 7],"float32"), None, list[2,2,], 0, list[1,1,], 1, "NCHW", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[123,1,1,], )
paddle.nn.functional.conv2d_transpose(x=Tensor([0, 3, 2, 2],"float32"), weight=Tensor([3, 1, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=list[1,0,], dilation=2, )
paddle.einsum("abcd,dfg->abcfg", Tensor([2, 4, 5, 3],"float64"), Tensor([3, 4, 0],"float64"), )
paddle.slice(Tensor([0, 1, 104],"float32"), list[0,1,2,], list[0,0,24,], list[3,1,40,], )
paddle.nn.functional.pad(Tensor([1, 16, 14, 15, 0],"float16"), tuple(0,1,0,0,0,0,), data_format="NDHWC", )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 32, 0],"float64"), output_size=3, )
paddle.Tensor.tile(Tensor([1, 6, 0],"float32"), list[844,1,1,], )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 0, 7, 7],"float32"), output_size=list[None,3,], data_format="NCHW", name=None, )
paddle.nn.functional.avg_pool3d(Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,2,1,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
paddle.nn.functional.avg_pool1d(x=Tensor([2, 0, 8],"float64"), kernel_size=1, stride=1, padding=0, )
paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 0],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0009654839523136616, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.nn.functional.max_unpool3d(Tensor([0, 3, 2, 2, 3],"float64"), Tensor([0, 3, 2, 2, 3],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCDHW", output_size=None, name=None, )
paddle.einsum("ibnd,jbnd->bnij", Tensor([7, 10, 4, 0],"float32"), Tensor([14, 10, 4, 0],"float32"), )
paddle.nn.functional.kl_div(Tensor([0, 20],"float64"), Tensor([0, 20],"float64"), "mean", True, )
paddle.nn.functional.maxout(Tensor([100, 0, 3, 3],"float64"), 2, 1, None, )
paddle.Tensor.tile(Tensor([1, 4, 0],"float32"), list[263,1,1,], )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 8, 8, 0],"float32"), Tensor([3, 2, 3, 3, 3],"float32"), bias=Tensor([6],"float32"), padding="valid", output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=3, output_size=None, data_format="NCDHW", )
paddle.nn.functional.softmax(Tensor([0, 17],"float16"), axis=1, )
paddle.nn.functional.conv3d(Tensor([4, 8, 8, 8, 3],"float32"), Tensor([5, 3, 3, 3, 0],"float32"), Tensor([5],"float32"), padding=list[1,1,2,2,3,3,], stride=1, dilation=2, groups=1, data_format="NDHWC", )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[106,1,1,], )
paddle.nn.functional.conv2d(Tensor([1, 1024, 12, 32],"float32"), Tensor([256, 1024, 0, 1],"float32"), None, list[1,1,], 0, list[1,1,], 1, )
paddle.nn.functional.conv2d(Tensor([3, 8, 0, 4],"float32"), Tensor([8, 8, 0, 3],"float32"), None, list[2,2,], 1, list[1,1,], 1, "NCHW", )
paddle.nn.functional.log_softmax(Tensor([0, 3],"float32"), 1, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[420,1,1,], )
paddle.nanmedian(Tensor([2, 3, 0, 5],"float32"), axis=list[0,-1,], keepdim=False, mode="min", )
paddle.nn.functional.pad(Tensor([1, 3, 0, 240],"float64"), pad=list[0,0,40,40,], mode="constant", value=0.0, data_format="NCHW", )
paddle.einsum("binh,tnh->bnit", Tensor([13, 2, 4, 4],"float32"), Tensor([0, 4, 4],"float32"), )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 5],"float64"), weight=Tensor([3, 2, 0],"float64"), bias=Tensor([2],"float64"), stride=1, padding="vALiD", output_padding=0, groups=1, dilation=1, output_size=None, data_format="NCL", )
paddle.slice(Tensor([0, 1, 104],"float32"), list[0,1,2,], list[0,0,48,], list[16,1,64,], )
paddle.frexp(Tensor([0, 12],"float32"), )
paddle.nn.quant.weight_quantize(Tensor([0, 256],"float16"), algo="weight_only_int8", arch=75, group_size=-1, )
paddle.nn.functional.max_pool2d(Tensor([2, 0, 17, 17],"float32"), kernel_size=tuple(3,3,), stride=tuple(2,2,), padding=tuple(0,0,), return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([1, 4, 0],"float32"), list[300,1,1,], )
paddle.vision.ops.generate_proposals(Tensor([2, 3, 0, 4],"float32"), Tensor([2, 12, 4, 4],"float32"), Tensor([2, 3],"float32"), Tensor([4, 4, 3, 4],"float32"), Tensor([4, 4, 3, 4],"float32"), pre_nms_top_n=10, post_nms_top_n=5, return_rois_num=True, )
paddle.allclose(Tensor([1, 0, 32],"float32"), Tensor([1, 0, 32],"float32"), )
paddle.nn.functional.conv2d_transpose(Tensor([4, 16, 16, 4],"float32"), Tensor([4, 1, 0, 3],"float32"), Tensor([4],"float32"), output_size=None, padding="valid", stride=tuple(1,2,), dilation=1, groups=4, data_format="NHWC", )
paddle.Tensor.tile(Tensor([16121, 0, 1],"float32"), list[1,811,1,], )
paddle.nn.functional.conv1d_transpose(Tensor([0, 6, 16],"float32"), Tensor([6, 4, 3],"float32"), bias=Tensor([8],"float32"), output_size=None, output_padding=0, padding="valid", stride=list[1,], dilation=list[1,], groups=2, data_format="NCL", )
paddle.nn.functional.kl_div(Tensor([0, 20],"float64"), Tensor([0, 20],"float64"), "sum", False, )
paddle.nn.functional.conv1d(x=Tensor([2, 3, 0],"float32"), weight=Tensor([1, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=2, padding=1, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 160, 0, 32],"float32"), output_size=2, data_format="NCHW", name=None, )
paddle.nn.functional.avg_pool2d(Tensor([13, 0, 7, 32],"float32"), tuple(2,1,), stride=tuple(2,1,), ceil_mode=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 8, 96],"float16"), Tensor([2, 100, 8, 96],"float16"), Tensor([0, 100, 8, 96],"float16"), attn_mask=None, is_causal=False, )
paddle.nn.functional.avg_pool2d(Tensor([0, 192, 25, 25],"float32"), kernel_size=3, stride=1, padding=1, ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.conv1d(Tensor([13, 64, 0],"float32"), Tensor([32, 64, 1],"float32"), bias=Tensor([32],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.searchsorted(sorted_sequence=Tensor([7],"float32"), values=Tensor([2, 2, 0],"float32"), right=True, )
paddle.nn.functional.conv2d_transpose(Tensor([1, 512, 16, 16],"float32"), Tensor([512, 512, 0, 3],"float32"), padding=0, stride=2, groups=1, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float64"), weight=Tensor([3, 1, 3, 0],"float64"), bias=Tensor([1],"float64"), stride=1, padding=0, dilation=1, )
paddle.nn.functional.conv3d_transpose(x=Tensor([0, 3, 2, 2, 2],"float64"), weight=Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,1,], dilation=1, )
paddle.nn.functional.softmax(Tensor([0, 100],"float32"), )
paddle.nn.functional.conv2d_transpose(Tensor([0, 256, 64, 64],"float32"), Tensor([256, 128, 3, 3],"float32"), bias=Tensor([128],"float32"), padding=1, output_padding=1, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 0],"float32"), Tensor([3, 1, 3, 3],"float32"), bias=None, padding=1, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([19125, 0],"float32"), list[1,121,], )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[2700,1,1,], )
paddle.nn.functional.conv2d(Tensor([2, 192, 4, 0],"float32"), Tensor([384, 192, 1, 1],"float32"), None, list[2,2,], 0, list[1,1,], 1, "NCHW", )
paddle.nn.functional.conv1d(Tensor([2, 3, 4],"float32"), Tensor([1, 3, 0],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[2,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 512, 32, 32],"float32"), Tensor([512, 0, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=512, output_size=None, data_format="NCHW", )
paddle.nn.functional.log_softmax(Tensor([0, 2],"float32"), )
paddle.Tensor.tile(Tensor([1, 9, 0],"float32"), list[804,1,1,], )
paddle.nn.functional.pad(Tensor([0, 4, 5, 6],"complex128"), pad=list[1,1,1,1,], mode="constant", value=100, data_format="NCHW", name=None, )
paddle.einsum("ji,j", Tensor([10, 0],"float64"), Tensor([10],"float64"), )
paddle.nn.functional.conv2d(Tensor([8, 3, 0, 256],"float32"), Tensor([128, 3, 0, 1],"float32"), bias=None, stride=1, padding=0, )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 3, 0, 4],"float64"), output_size=list[1,4,], )
paddle.nn.functional.conv3d_transpose(Tensor([2, 2, 2, 2, 3],"float64"), Tensor([3, 1, 3, 0, 3],"float64"), bias=Tensor([3],"float64"), padding=list[1,1,1,], output_padding=1, stride=list[2,2,2,], dilation=list[1,1,1,], groups=3, output_size=None, data_format="NDHWC", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 1024, 0, 16],"float32"), Tensor([1024, 1, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1024, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv1d(x=Tensor([2, 3, 4],"float64"), weight=Tensor([6, 1, 0],"float64"), bias=Tensor([6],"float64"), stride=2, padding=0, groups=3, )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 2],"float32"), weight=Tensor([3, 0, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=0, dilation=1, )
paddle.nn.functional.conv2d(Tensor([1, 2048, 16, 16],"float32"), Tensor([12, 512, 1, 0],"float32"), padding=0, groups=4, )
paddle.nanmedian(Tensor([0, 3, 4, 5],"float32"), axis=-1, keepdim=False, )
paddle.nn.quant.weight_quantize(Tensor([0, 256],"float16"), algo="weight_only_int4", arch=75, group_size=-1, )
paddle.einsum("bnij,jbnd->ibnd", Tensor([11, 4, 7, 0],"float32"), Tensor([7, 11, 4, 0],"float32"), )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 0],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )
paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 3, 0, 2, 4],"float16"), Tensor([1, 3, 5, 2, 4],"float16"), )
paddle.nn.functional.pad(Tensor([16, 0, 256, 256],"float32"), list[14,14,14,14,], )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 3, 0, 7],"float32"), list[2,5,], )
paddle.vision.ops.generate_proposals(Tensor([1, 15, 42, 0],"float32"), Tensor([1, 60, 42, 63],"float32"), Tensor([1, 2],"float32"), Tensor([39690, 4],"float32"), Tensor([39690, 4],"float32"), pre_nms_top_n=12000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 0, 128],"float16"), Tensor([1, 2048, 0, 128],"float16"), Tensor([1, 2048, 0, 128],"float16"), attn_mask=Tensor([1, 1, 0, 2048],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.softmax(Tensor([13, 0, 99],"float32"), )
paddle.nn.functional.margin_ranking_loss(Tensor([10, 0],"float64"), Tensor([10, 0],"float64"), Tensor([10, 0],"float64"), 0.2, "none", None, )
paddle.Tensor.subtract(Tensor([0, 3, 4],"float32"), Tensor([0, 3, 4],"float32"), )
paddle.Tensor.fill_diagonal_(Tensor([7, 0],"float32"), 1, offset=0, wrap=False, )
paddle.nn.functional.max_pool3d(Tensor([2, 3, 32, 0, 32],"float32"), kernel_size=2, stride=None, padding=0, return_mask=False, ceil_mode=True, data_format="NCDHW", name=None, )
paddle.Tensor.tile(Tensor([1, 0],"float32"), list[1,40,], )
paddle.nn.functional.softmax(Tensor([0, 256, 1],"float32"), axis=1, )
paddle.Tensor.tile(Tensor([4115, 1, 0],"float32"), list[1,143,1,], )
paddle.Tensor.tile(Tensor([1, 6, 0],"float32"), list[584,1,1,], )
paddle.Tensor.__getitem__(Tensor([0, 3, 3],"float32"), tuple(slice(1,-1,None),slice(0,2,None),slice(None,None,-1),), )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 0, 32, 32],"float64"), output_size=list[1,1,1,], )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 0, 2],"float32"), weight=Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=1, dilation=1, )
paddle.nn.functional.pad(Tensor([0, 3, 6, 6, 6],"float64"), pad=list[2,2,2,2,2,2,], mode="constant", value=0.0, data_format="NCDHW", name=None, )
paddle.nanmedian(Tensor([2, 0, 4, 5],"float32"), axis=-2, keepdim=False, )
paddle.nn.functional.pad(Tensor([4, 3, 6, 0, 6],"float64"), pad=list[2,2,2,2,2,2,], mode="constant", value=0.0, data_format="NCDHW", name=None, )
paddle.Tensor.fill_diagonal_(Tensor([3, 3, 0],"float32"), 1, offset=0, wrap=True, )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[197,1,1,], )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 2],"float64"), weight=Tensor([3, 1, 3],"float64"), bias=Tensor([0],"float64"), stride=1, padding=list[1,], groups=1, dilation=2, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 160, 0, 32],"float32"), output_size=4, data_format="NCHW", name=None, )
paddle.nn.functional.max_pool3d(Tensor([0, 3, 8, 8, 8],"float32"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 2],"float64"), weight=Tensor([3, 1, 3],"float64"), bias=Tensor([0],"float64"), stride=2, padding=list[1,], output_padding=0, groups=1, dilation=1, output_size=None, data_format="NCL", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([1, 128, 128, 128],"float32"), Tensor([128, 3, 0, 4],"float32"), bias=Tensor([3],"float32"), padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.max_pool3d(Tensor([0, 6, 7, 9, 3],"float32"), list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], data_format="NDHWC", )
paddle.nn.functional.pad(Tensor([13, 0, 16],"float32"), pad=list[1,1,], data_format="NLC", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 16],"float32"), Tensor([6, 8, 0, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[2,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.max_pool2d(Tensor([1, 0, 20, 30],"float32"), 1, stride=2, )
paddle.nn.functional.max_pool2d(Tensor([0, 2, 32, 32],"float32"), kernel_size=2, stride=2, padding=0, return_mask=True, ceil_mode=False, data_format="NCHW", name=None, )
paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), atol=1e-05, rtol=1e-05, )
paddle.nn.functional.pad(Tensor([1, 16, 15, 0, 384],"float32"), tuple(0,0,0,6,0,0,), data_format="NDHWC", )
paddle.nonzero(Tensor([0, 3],"float32"), True, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([0, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[1155,1,1,], )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float32"), weight=Tensor([1, 3, 2, 2],"float32"), bias=Tensor([0],"float32"), stride=1, padding=0, )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[660,1,1,], )
paddle.nn.quant.weight_quantize(Tensor([0, 288],"float16"), algo="weight_only_int8", group_size=-1, )
paddle.searchsorted(Tensor([0],"float32"), Tensor([0],"float32"), right=True, )
paddle.nn.functional.max_pool2d(Tensor([256, 0, 112, 64],"float16"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NHWC", name=None, )
paddle.nn.functional.max_pool2d(Tensor([0, 256, 7, 7],"float32"), kernel_size=1, stride=2, padding=0, )
paddle.nn.functional.conv1d_transpose(Tensor([2, 3, 0],"float64"), Tensor([3, 2, 3],"float64"), bias=Tensor([2],"float64"), output_size=None, output_padding=0, padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.hinge_embedding_loss(Tensor([4, 0, 3, 4],"float64"), Tensor([4, 0, 3, 4],"float64"), reduction="none", margin=-4.0, name=None, )
paddle.nn.functional.pad(Tensor([3, 4, 5, 6, 0],"complex64"), pad=list[1,2,2,1,1,0,], mode="constant", value=100, data_format="NCDHW", name=None, )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 7, 0],"float32"), list[2,3,5,], )
paddle.searchsorted(sorted_sequence=Tensor([5],"float32"), values=Tensor([0],"float32"), )
paddle.nn.functional.conv2d(Tensor([8, 128, 256, 0],"float32"), Tensor([128, 128, 3, 0],"float32"), bias=None, stride=1, padding=1, )
paddle.vision.ops.yolo_box(Tensor([2, 14, 8, 0],"float32"), img_size=Tensor([2, 2],"int32"), anchors=list[10,13,16,30,], class_num=2, conf_thresh=0.01, downsample_ratio=8, clip_bbox=True, scale_x_y=1.0, )
paddle.nn.functional.maxout(Tensor([10, 0, 3, 3],"float64"), 3, 1, None, )
paddle.nn.functional.conv2d(Tensor([4, 6, 16, 16],"float32"), Tensor([8, 3, 0, 3],"float32"), Tensor([8],"float32"), padding="same", stride=1, dilation=1, groups=2, data_format="NCHW", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float32"), Tensor([3, 2, 0, 3, 3],"float32"), bias=Tensor([2],"float32"), padding=0, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[330,1,1,], )
paddle.nn.functional.conv3d_transpose(Tensor([2, 2, 2, 2, 3],"float64"), Tensor([3, 0, 3, 3, 3],"float64"), bias=Tensor([3],"float64"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=3, output_size=None, data_format="NDHWC", )
paddle.nn.functional.softmax(Tensor([2, 0],"float32"), axis=0, dtype="float32", )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([0, 128, 32, 32],"float32"), output_size=tuple(3,3,), data_format="NCHW", name=None, )
paddle.einsum("blqd,bmdk->blqk", Tensor([52, 5, 1, 1],"float32"), Tensor([52, 1, 1, 0],"float32"), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[182,1,1,], )
paddle.Tensor.tile(Tensor([96, 3, 1, 4, 0],"float32"), list[1,1,2,1,1,], )
paddle.nn.functional.max_pool1d(Tensor([0, 3, 32],"float32"), kernel_size=2, stride=2, padding=0, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 1024, 0, 16],"bfloat16"), Tensor([1, 1024, 0, 16],"bfloat16"), Tensor([1, 1024, 0, 16],"bfloat16"), attn_mask=None, is_causal=True, )
paddle.incubate.softmax_mask_fuse(x=Tensor([2, 0, 8, 1020],"float16"), mask=Tensor([2, 1, 8, 1020],"float16"), )
paddle.roll(Tensor([4, 5, 0, 4],"float64"), Tensor([4],"int64"), list[0,1,2,3,], name=None, )
paddle.nn.functional.pad(Tensor([1, 0, 2, 2],"float32"), pad=list[2,2,2,2,], mode="replicate", value=0.0, data_format="NCHW", name=None, )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 0, 32, 32],"float64"), output_size=tuple(3,3,3,), data_format="NCDHW", )
paddle.nn.functional.max_pool3d(Tensor([2, 3, 32, 0, 32],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, )
paddle.Tensor.tile(Tensor([4, 2, 64, 0],"float32"), list[1,36,1,1,], )
paddle.nn.functional.conv3d_transpose(x=Tensor([0, 3, 2, 2, 2],"float32"), weight=Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([3],"float32"), stride=1, padding=list[1,0,1,], groups=3, dilation=1, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[345,1,1,], )
paddle.Tensor.__rpow__(Tensor([10, 0],"float64"), 2, )
paddle.nn.functional.avg_pool1d(Tensor([0, 3, 32],"float32"), kernel_size=2, stride=2, padding="SAME", )
paddle.einsum("ijk,jk->i", Tensor([0, 4, 5],"float64"), Tensor([4, 5],"float64"), )
paddle.where(Tensor([0, 10],"bool"), )
paddle.nn.functional.avg_pool3d(Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,0,0,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
paddle.nn.functional.fractional_max_pool2d(x=Tensor([0, 3, 7, 7],"float32"), output_size=list[3,3,], random_u=0.3, )
paddle.nn.functional.conv3d(Tensor([4, 8, 8, 8, 3],"float32"), Tensor([5, 3, 3, 0, 3],"float32"), Tensor([5],"float32"), padding=0, stride=1, dilation=1, groups=1, data_format="NDHWC", )
paddle.nn.functional.conv2d(Tensor([1, 1024, 256, 256],"float32"), Tensor([24, 128, 0, 1],"float32"), padding=0, groups=8, )
paddle.nn.functional.max_pool1d(Tensor([1, 3, 0],"float32"), kernel_size=2, stride=2, return_mask=True, )
paddle.nn.functional.conv2d(Tensor([2, 192, 4, 0],"float32"), Tensor([48, 192, 1, 1],"float32"), None, list[1,1,], 0, list[1,1,], 1, "NCHW", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[1268,1,1,], )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 8, 8, 8],"float32"), Tensor([3, 2, 0, 3, 3],"float32"), bias=Tensor([6],"float32"), padding="valid", output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=3, output_size=None, data_format="NCDHW", )
paddle.nn.functional.pad(Tensor([1, 0, 3],"float32"), list[1,0,], value=5, mode="constant", data_format="NCL", )
paddle.nn.functional.conv1d(x=Tensor([2, 4, 3],"float64"), weight=Tensor([2, 3, 0],"float64"), bias=Tensor([2],"float64"), stride=1, padding=0, data_format="NLC", )
paddle.nn.functional.lp_pool1d(Tensor([0, 3, 32],"float32"), norm_type=4, kernel_size=3, stride=2, padding=list[1,], )
paddle.Tensor.lgamma(Tensor([0, 5],"float64"), )
paddle.nn.functional.conv2d_transpose(Tensor([1, 128, 128, 128],"float32"), Tensor([128, 3, 4, 0],"float32"), bias=Tensor([3],"float32"), padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 0, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )
paddle.meshgrid(list[Tensor([0],"float64"),Tensor([0],"float64"),Tensor([0],"float64"),Tensor([0],"float64"),Tensor([0],"float64"),], )
paddle.nn.quant.weight_quantize(Tensor([0, 64],"float16"), algo="weight_only_int8", )
paddle.nn.functional.softmax(Tensor([0, 8],"float32"), axis=1, )
paddle.nn.functional.log_softmax(Tensor([2, 3, 4, 0],"float32"), -1, "float64", )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 2, 2, 6],"float64"), weight=Tensor([6, 1, 3, 0],"float64"), bias=Tensor([3],"float64"), output_padding=1, stride=2, padding=list[1,0,], groups=3, data_format="NHWC", dilation=1, )
paddle.einsum("bnij,jbnd->ibnd", Tensor([11, 4, 7, 7],"float32"), Tensor([7, 11, 4, 0],"float32"), )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([0, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
paddle.slice(Tensor([1024, 16, 0],"float16"), axes=list[0,], starts=list[512,], ends=list[640,], )
paddle.slice(Tensor([0, 1, 104],"float32"), list[0,1,2,], list[0,0,88,], list[3,1,104,], )
paddle.nn.functional.poisson_nll_loss(Tensor([0, 2],"float32"), Tensor([0, 2],"float32"), log_input=False, full=False, epsilon=1e-08, reduction="mean", )
paddle.Tensor.tile(Tensor([10285, 0],"float32"), list[1,26,], )
paddle.nn.functional.avg_pool2d(Tensor([0, 1, 7, 32],"float32"), tuple(2,1,), stride=tuple(2,1,), ceil_mode=True, )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 0],"float64"), weight=Tensor([3, 2, 3],"float64"), bias=Tensor([2],"float64"), stride=1, padding="vALiD", output_padding=0, groups=1, dilation=1, output_size=None, data_format="NCL", )
paddle.nn.functional.temporal_shift(Tensor([240, 1024, 0, 14],"float32"), 8, 0.125, data_format="NCHW", )
paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 0],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0010831302497535944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 128, 0, 108],"float32"), Tensor([128, 128, 2, 2],"float32"), bias=None, padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 0, 32],"float64"), output_size=3, data_format="NCDHW", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 2],"float32"), Tensor([3, 0, 3, 3],"float32"), bias=None, padding=0, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.max_pool2d(Tensor([2, 0, 16, 64],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=True, data_format="NCHW", name=None, )
paddle.nn.functional.conv2d(Tensor([1, 1024, 256, 0],"float32"), Tensor([24, 128, 1, 0],"float32"), padding=0, groups=8, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float32"), Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([0],"float32"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[2,2,2,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.max_pool2d(Tensor([1, 0, 32, 32],"float32"), kernel_size=9, stride=1, padding=4, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.avg_pool1d(Tensor([0, 3, 8],"float64"), 2, 2, 0, True, False, None, )
paddle.nn.functional.conv3d(Tensor([4, 8, 8, 8, 3],"float32"), Tensor([5, 3, 3, 3, 0],"float32"), Tensor([5],"float32"), padding=list[1,2,1,], stride=1, dilation=1, groups=1, data_format="NDHWC", )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 2, 3],"float64"), weight=Tensor([3, 0, 3],"float64"), bias=Tensor([3],"float64"), stride=2, padding=list[1,], output_padding=1, groups=3, dilation=1, output_size=None, data_format="NLC", name=None, )
paddle.Tensor.tile(Tensor([1, 6, 0],"float32"), list[685,1,1,], )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 0, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=True, )
paddle.nn.functional.conv1d_transpose(Tensor([4, 6, 16],"float32"), Tensor([6, 0, 3],"float32"), bias=Tensor([8],"float32"), output_size=None, output_padding=0, padding=list[2,], stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.fractional_max_pool2d(Tensor([0, 3, 7, 7],"float32"), output_size=5, kernel_size=None, random_u=0.5, return_mask=False, name=None, )
paddle.nn.functional.group_norm(Tensor([2, 0, 2, 4],"float64"), num_groups=2, weight=Tensor([4],"float64"), bias=Tensor([4],"float64"), data_format="NHWC", )
paddle.einsum("blkd,bldq->blkq", Tensor([0, 5, 1, 1],"float32"), Tensor([0, 5, 1, 3],"float32"), )
paddle.nn.quant.weight_quantize(Tensor([128, 0],"float16"), algo="weight_only_int8", arch=70, group_size=-1, )
paddle.nn.functional.conv2d(Tensor([1, 1, 101, 261],"float32"), Tensor([64, 1, 0, 7],"float32"), None, list[2,2,], 0, list[1,1,], 1, )
paddle.nn.functional.avg_pool2d(Tensor([8, 0, 3, 80],"float32"), kernel_size=list[2,2,], stride=list[2,2,], padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 0, 64],"float16"), Tensor([2, 100, 0, 64],"float16"), Tensor([2, 100, 0, 64],"float16"), attn_mask=Tensor([2, 1, 0, 100],"float16"), is_causal=False, )
paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 1, 1, 0],"float32"), Tensor([13, 4, 1, 7, 0],"float32"), )
paddle.allclose(Tensor([2, 0],"float32"), Tensor([2, 0],"float32"), atol=1e-06, )
paddle.nn.functional.conv2d(Tensor([3, 3, 32, 32],"float32"), Tensor([64, 3, 0, 7],"float32"), None, list[2,2,], 3, list[1,1,], 1, "NCHW", )
paddle.nn.functional.conv1d(Tensor([2, 3, 4],"float64"), Tensor([6, 1, 0],"float64"), bias=Tensor([6],"float64"), padding=0, stride=list[2,], dilation=list[1,], groups=3, data_format="NCL", )
paddle.nn.functional.conv2d(Tensor([1, 128, 12, 20],"float32"), Tensor([128, 128, 0, 3],"float32"), None, list[1,1,], 1, list[1,1,], 1, )
paddle.nn.functional.fractional_max_pool3d(x=Tensor([2, 0, 7, 7, 7],"float64"), output_size=list[3,3,3,], random_u=None, )
paddle.nn.functional.conv2d(Tensor([1, 2048, 128, 0],"float32"), Tensor([2048, 256, 3, 0],"float32"), padding=1, groups=8, )
paddle.gammaln(Tensor([2, 3, 0, 5],"float64"), )
paddle.quantile(Tensor([5, 0, 4],"float64"), q=list[0.2,0.67,], axis=list[1,-1,], )
paddle.slice(Tensor([1024, 0, 1024],"float16"), axes=list[0,], starts=list[384,], ends=list[512,], )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 0, 4],"float32"), weight=Tensor([1, 3, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=list[2,2,1,], padding=1, dilation=2, )
paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([13, 0, 5, 1, 8],"float32"), Tensor([13, 0, 5, 7, 8],"float32"), )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([1, 3, 3, 3, 0],"float32"), bias=Tensor([1],"float32"), stride=list[2,2,1,], padding=1, dilation=tuple(2,2,2,), )
paddle.nn.functional.pad(Tensor([1, 16, 14, 0, 384],"float16"), tuple(0,4,0,0,0,0,), data_format="NDHWC", )
paddle.nn.functional.pad(Tensor([1, 0, 14, 15, 384],"float32"), tuple(0,6,0,0,0,0,), data_format="NDHWC", )
paddle.einsum("ij,k->ijk", Tensor([0, 128],"float32"), Tensor([32],"float32"), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[556,1,1,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[704,1,1,], )
paddle.nn.functional.conv3d_transpose(x=Tensor([0, 3, 2, 2, 2],"float32"), weight=Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=list[1,0,1,], dilation=1, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 0, 2],"float32"), weight=Tensor([3, 1, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=1, output_padding=0, dilation=1, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([2450, 1, 0],"float32"), list[1,223,1,], )
paddle.nn.functional.conv2d(Tensor([8, 256, 128, 0],"float32"), Tensor([256, 256, 3, 0],"float32"), bias=None, stride=1, padding=1, )
paddle.nn.functional.conv3d(Tensor([4, 6, 8, 8, 8],"float32"), Tensor([12, 1, 0, 3, 3],"float32"), None, padding="valid", stride=1, dilation=1, groups=6, data_format="NCDHW", )
paddle.nn.functional.pad(Tensor([0, 3, 256, 256],"float32"), list[14,14,14,14,], )
paddle.incubate.nn.functional.fused_feedforward(x=Tensor([0, 2, 2],"float32"), linear1_weight=Tensor([2, 2],"float32"), linear2_weight=Tensor([2, 2],"float32"), activation="gelu", dropout1_rate=0, dropout2_rate=0, )
paddle.subtract(Tensor([16, 0, 2],"float32"), Tensor([16, 0, 2],"float32"), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[314,1,1,], )
paddle.nn.functional.prelu(Tensor([1, 0, 3, 4],"float32"), Tensor([1],"float32"), data_format="NCHW", )
paddle.meshgrid(Tensor([0],"float64"), Tensor([0],"float64"), Tensor([0],"float64"), )
paddle.Tensor.tile(Tensor([1, 6, 0],"float32"), list[629,1,1,], )
paddle.subtract(Tensor([1, 1, 0, 30],"float32"), Tensor([1, 1, 0, 30],"float32"), )
paddle.nn.functional.avg_pool2d(Tensor([1, 1024, 0, 60],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 0, 4, 4],"float64"), weight=Tensor([1, 3, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=list[2,2,1,], padding=1, dilation=2, )
paddle.Tensor.tile(Tensor([1, 5, 0],"float32"), list[4506,1,1,], )
paddle.einsum("..., f -> ... f", Tensor([16],"float32"), Tensor([0],"float32"), )
paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([13, 4, 5, 1, 0],"float32"), Tensor([13, 4, 5, 7, 0],"float32"), )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 0, 4],"float64"), weight=Tensor([1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=2, padding=0, )
paddle.einsum("blq,bhlk->bhlqk", Tensor([13, 5, 1],"float32"), Tensor([13, 4, 5, 0],"float32"), )
paddle.nn.functional.conv2d(Tensor([8, 128, 0, 255],"float32"), Tensor([256, 128, 0, 1],"float32"), bias=None, stride=2, padding=0, )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 0, 7, 10],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 1, 64],"float16"), Tensor([1, 2048, 1, 64],"float16"), Tensor([0, 2048, 1, 64],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), is_causal=False, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[158,1,1,], )
paddle.Tensor.tile(Tensor([1, 8, 0],"float32"), list[650,1,1,], )
paddle.nn.functional.conv3d(Tensor([4, 8, 8, 8, 3],"float32"), Tensor([5, 3, 3, 0, 3],"float32"), Tensor([5],"float32"), padding=list[1,1,2,2,3,3,], stride=1, dilation=2, groups=1, data_format="NDHWC", )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 0, 8],"float64"), weight=Tensor([3, 1, 5, 5],"float64"), bias=Tensor([1],"float64"), stride=1, padding="sAmE", output_padding=0, dilation=1, groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.multi_margin_loss(input=Tensor([5, 0],"float64"), label=Tensor([5],"int64"), p=1, margin=1.0, weight=None, reduction="sum", )
paddle.Tensor.tile(Tensor([1, 1, 0, 64, 2],"float32"), tuple(16,10,1,1,1,), )
paddle.nn.functional.lp_pool2d(Tensor([0, 3, 32, 32],"float32"), -math.inf, kernel_size=2, stride=2, ceil_mode=False, )
paddle.nn.functional.pad(Tensor([3, 0, 5],"complex128"), pad=list[1,2,], mode="constant", value=100, data_format="NCL", name=None, )
paddle.Tensor.tile(Tensor([8, 0, 512],"float32"), tuple(1,4,1,), )
paddle.nn.functional.avg_pool2d(Tensor([0, 512, 3, 80],"float32"), kernel_size=list[3,2,], stride=list[3,2,], padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.hinge_embedding_loss(Tensor([0, 3],"float64"), Tensor([0, 3],"float64"), reduction="mean", margin=1.0, name=None, )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 0, 5, 7, 7],"float32"), output_size=list[3,3,3,], )
paddle.nn.functional.conv2d_transpose(Tensor([1, 256, 14, 14],"float32"), Tensor([256, 256, 0, 2],"float32"), bias=Tensor([256],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.einsum("i->ii", Tensor([0],"float64"), )
paddle.nn.functional.max_pool2d(Tensor([0, 3, 32, 32],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, )
paddle.nn.functional.avg_pool3d(Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,0,0,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
paddle.nn.functional.conv1d(Tensor([1, 128, 112],"float32"), Tensor([128, 128, 0],"float32"), bias=Tensor([128],"float32"), padding=1, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.Tensor.tile(Tensor([1001, 0, 80],"float32"), list[1,4,1,], )
paddle.nn.functional.conv2d(Tensor([1, 1024, 0, 128],"float32"), Tensor([12, 256, 0, 1],"float32"), padding=0, groups=4, )
paddle.nn.functional.margin_ranking_loss(Tensor([0, 10],"float64"), Tensor([0, 10],"float64"), Tensor([0, 10],"float64"), 0.2, "mean", None, )
paddle.nn.functional.conv1d(Tensor([1, 256, 28],"float32"), Tensor([256, 256, 0],"float32"), bias=Tensor([256],"float32"), padding=9, stride=list[1,], dilation=list[3,], groups=1, data_format="NCL", )
paddle.nn.functional.pad(Tensor([4, 0, 6, 6, 6],"float32"), pad=list[2,2,2,2,2,2,], mode="constant", value=0.0, data_format="NCDHW", name=None, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 1024, 8, 16],"bfloat16"), Tensor([1, 1024, 0, 16],"bfloat16"), Tensor([1, 1024, 2, 16],"bfloat16"), attn_mask=None, is_causal=True, )
paddle.einsum("...qk,...kd->...qd", Tensor([0, 4, 3, 1, 2],"float32"), Tensor([0, 4, 3, 2, 8],"float32"), )
paddle.nn.functional.pad(Tensor([0, 4, 5, 6],"complex64"), pad=list[1,2,2,1,], mode="constant", value=100, data_format="NCHW", name=None, )
paddle.nn.functional.max_pool1d(x=Tensor([0, 3, 8],"float64"), kernel_size=2, stride=1, padding=0, ceil_mode=True, )
paddle.nn.functional.max_pool3d(Tensor([2, 3, 0, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=list[1,0,0,], return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.einsum("bind,bjnd->bnij", Tensor([13, 2, 4, 0],"float32"), Tensor([13, 2, 4, 0],"float32"), )
paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 0],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0010831302497535944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.cummax(Tensor([100, 0],"float32"), axis=-2, )
paddle.nn.functional.zeropad2d(Tensor([4, 3, 0, 224],"float64"), list[2,2,2,2,], )
paddle.einsum("i...->...", Tensor([5, 0, 3, 3],"float64"), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[223,1,1,], )
paddle.einsum("ijbs,ibns->bnij", Tensor([7, 7, 14, 0],"float32"), Tensor([7, 14, 4, 0],"float32"), )
paddle.slice(Tensor([16, 0, 104],"float32"), list[0,1,2,], list[0,0,8,], list[16,2,24,], )
paddle.allclose(Tensor([0, 5, 32],"float32"), Tensor([0, 5, 32],"float32"), atol=1e-05, )
paddle.nn.functional.pad(Tensor([1, 16, 16, 14, 0],"float32"), tuple(0,0,0,5,0,0,), data_format="NDHWC", )
paddle.linalg.slogdet(Tensor([0, 3, 3],"float32"), )
paddle.Tensor.tile(Tensor([16660, 0],"float32"), list[1,142,], )
paddle.vision.ops.generate_proposals(Tensor([2, 0, 4, 4],"float32"), Tensor([2, 12, 4, 4],"float32"), Tensor([2, 3],"float32"), Tensor([4, 4, 3, 4],"float32"), Tensor([4, 4, 3, 4],"float32"), pre_nms_top_n=10, post_nms_top_n=5, return_rois_num=True, )
paddle.nn.functional.mse_loss(Tensor([16, 96, 0],"float32"), Tensor([16, 96, 0],"float32"), reduction="none", )
paddle.nn.functional.max_pool2d(Tensor([2, 0, 32, 32],"float64"), kernel_size=list[3,3,], stride=list[1,2,], padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 96, 2, 2],"float32"), Tensor([96, 96, 0, 4],"float32"), bias=Tensor([96],"float32"), padding=0, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[3240,1,1,], )
paddle.slice(Tensor([13, 0, 104],"float32"), list[0,1,2,], list[0,0,88,], list[13,1,104,], )
paddle.Tensor.__setitem__(Tensor([6, 0, 4, 3],"complex128"), list[list[2,3,4,],list[1,2,5,],], 100, )
paddle.einsum("bij,bjk->bik", Tensor([3, 4, 5],"float64"), Tensor([3, 5, 0],"float64"), )
paddle.nn.functional.conv2d_transpose(Tensor([1, 128, 80, 80],"float32"), Tensor([128, 128, 2, 0],"float32"), bias=Tensor([128],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.avg_pool2d(Tensor([64, 0, 28, 28],"float16"), kernel_size=3, stride=2, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.multi_margin_loss(Tensor([5, 0],"float64"), Tensor([5],"int64"), p=1, margin=1.0, weight=None, reduction="sum", name=None, )
paddle.nn.functional.maxout(x=Tensor([0, 2, 2, 6],"float64"), groups=2, axis=3, )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 0, 7, 7],"float32"), output_size=list[3,3,3,], data_format="NDHWC", name=None, )
paddle.nn.functional.pad(Tensor([12, 0, 128, 128],"float32"), list[2,1,2,1,], )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([1, 3, 3, 3, 0],"float32"), bias=Tensor([1],"float32"), stride=1, padding=0, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 6, 2, 2],"float64"), weight=Tensor([6, 1, 3, 0],"float64"), bias=Tensor([3],"float64"), stride=1, padding=list[1,0,], groups=3, dilation=1, )
paddle.nn.functional.conv2d_transpose(Tensor([0, 64, 64, 64],"float32"), Tensor([64, 1, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=64, output_size=None, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([1, 0, 3],"float32"), list[1,0,], value=2, mode="constant", data_format="NCL", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[116,1,1,], )
paddle.einsum("bmtd,mdhr->bmhtr", Tensor([13, 2, 16, 16],"float32"), Tensor([2, 16, 4, 0],"float32"), )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([0, 256, 2, 25],"float32"), output_size=list[1,25,], data_format="NCHW", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([1, 512, 16, 16],"float32"), Tensor([512, 0, 3, 3],"float32"), padding=0, stride=2, groups=1, )
paddle.nn.functional.conv3d_transpose(Tensor([4, 8, 8, 8, 3],"float32"), Tensor([3, 0, 3, 3, 3],"float32"), Tensor([5],"float32"), output_size=None, padding=0, stride=1, dilation=1, groups=1, data_format="NDHWC", )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 160, 11, 0],"float32"), tuple(1,1,), )
paddle.nn.functional.pad(x=Tensor([1, 1, 0, 2, 3],"float64"), pad=tuple(0,1,1,1,2,0,), mode="constant", value=0, data_format="NCDHW", )
paddle.nn.functional.pad(Tensor([2, 48, 0, 8],"float32"), list[0,1,0,1,], value=0, )
paddle.slice(Tensor([0, 51],"float32"), axes=list[1,], starts=list[49,], ends=list[50,], )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 2],"float64"), weight=Tensor([3, 1, 0],"float64"), bias=Tensor([1],"float64"), stride=2, padding=list[1,], groups=1, dilation=1, )
paddle.nn.functional.temporal_shift(x=Tensor([2, 2, 0, 3],"float32"), seg_num=2, )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float64"), weight=Tensor([1, 3, 0, 3],"float64"), bias=Tensor([1],"float64"), stride=2, padding=1, )
paddle.nn.functional.interpolate(x=Tensor([2, 0, 5, 7, 7],"float32"), mode="area", size=list[2,3,5,], )
paddle.einsum("nbtc,nbsc->nbts", Tensor([2, 52, 0, 8],"float32"), Tensor([2, 52, 14, 8],"float32"), )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 8, 3, 0, 3],"float32"), bias=Tensor([8],"float32"), padding=list[list[0,0,],list[0,0,],list[2,3,],list[1,2,],list[2,1,],], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.pad(x=Tensor([0, 1, 1, 2, 3],"float64"), pad=tuple(2,2,1,1,0,0,), mode="reflect", data_format="NCDHW", )
paddle.nn.functional.pad(Tensor([1024, 1, 1, 0],"float32"), list[2,2,2,2,], )
paddle.nn.functional.max_pool1d(x=Tensor([2, 0, 8],"float64"), kernel_size=list[3,], stride=1, padding=1, )
paddle.einsum("ibnd,jbnd->bnij", Tensor([7, 10, 0, 8],"float32"), Tensor([14, 10, 0, 8],"float32"), )
paddle.einsum("ij,ij->", Tensor([0, 2],"float32"), Tensor([0, 2],"float32"), )
paddle.einsum("a...b,b...c,c...a", Tensor([4, 3, 0, 4],"float64"), Tensor([4, 3, 0, 4],"float64"), Tensor([4, 3, 0, 4],"float64"), )
paddle.nn.functional.avg_pool1d(Tensor([0, 3, 32],"float32"), kernel_size=2, stride=2, padding=list[0,], )
paddle.nn.functional.max_unpool1d(Tensor([0, 3, 8],"int64"), Tensor([0, 3, 8],"int32"), kernel_size=2, stride=2, output_size=tuple(1,3,16,), )
paddle.allclose(Tensor([64, 0],"float32"), Tensor([64, 0],"float32"), atol=1e-05, rtol=1e-05, )
paddle.einsum("ak, kn-> an", Tensor([60000, 0],"float32"), Tensor([11, 0],"float32"), )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 6, 0, 2],"float32"), weight=Tensor([6, 1, 3, 3],"float32"), bias=Tensor([3],"float32"), stride=1, padding=list[1,0,], groups=3, dilation=1, )
paddle.nn.functional.conv1d_transpose(Tensor([4, 6, 16],"float32"), Tensor([6, 4, 0],"float32"), bias=Tensor([8],"float32"), output_size=None, output_padding=0, padding="valid", stride=list[1,], dilation=list[1,], groups=2, data_format="NCL", )
paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([0],"float32"), shift=None, smooth=None, quant_scale=0.001479289960116148, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.nn.functional.conv2d(Tensor([3, 16, 0, 2],"float32"), Tensor([16, 16, 3, 3],"float32"), None, list[2,2,], 1, list[1,1,], 1, "NCHW", )
paddle.nn.functional.pad(Tensor([0, 48, 8, 8],"float32"), list[0,1,0,1,], value=0, )
paddle.allclose(x=Tensor([0],"float64"), y=Tensor([0],"float64"), rtol=0.01, atol=0.0, equal_nan=False, )
paddle.einsum("iji->j", Tensor([5, 0, 5],"float64"), )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 16],"float32"), Tensor([6, 0, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=list[1,2,2,1,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.softmax(Tensor([2, 0, 101],"float32"), axis=-1, dtype=Dtype(float32), )
paddle.nn.functional.maxout(Tensor([0, 9, 3, 3],"float64"), 3, 1, None, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 8, 16],"float16"), Tensor([1, 2048, 2, 16],"float16"), Tensor([1, 0, 2, 16],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), is_causal=True, )
paddle.nn.quant.weight_quantize(Tensor([128, 0],"float16"), algo="weight_only_int8", arch=86, group_size=-1, )
paddle.allclose(tuple(Tensor([2, 0, 100],"float32"),), tuple(Tensor([2, 0, 100],"float32"),), rtol=0.0001, atol=0.0001, )
paddle.nn.functional.conv1d(Tensor([13, 32, 7],"float32"), Tensor([32, 16, 0],"float32"), bias=Tensor([32],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=2, data_format="NCL", )
paddle.nn.functional.conv2d_transpose(Tensor([0, 2, 3, 3],"float64"), Tensor([2, 2, 1, 1],"float64"), groups=1, padding=list[1,0,0,1,], )
paddle.nn.functional.pad(Tensor([2, 0, 4, 4],"float64"), list[1,1,1,1,], mode="reflect", data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[827,1,1,], )
paddle.Tensor.tile(Tensor([8, 1, 1, 0],"float32"), list[1,4,4,1,], )
paddle.Tensor.fill_diagonal_(Tensor([3, 0],"float64"), 1, offset=2, wrap=True, )
paddle.nn.functional.softmax(Tensor([13, 4, 0, 10],"float32"), axis=-1, dtype=Dtype(float32), )
paddle.nn.functional.conv2d_transpose(Tensor([2, 6, 2, 2],"float32"), Tensor([6, 0, 3, 3],"float32"), bias=Tensor([3],"float32"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=3, output_size=None, data_format="NCHW", )
paddle.vision.ops.generate_proposals(Tensor([1, 3, 10, 15],"float32"), Tensor([1, 12, 10, 0],"float32"), Tensor([1, 2],"float32"), Tensor([450, 4],"float32"), Tensor([450, 4],"float32"), pre_nms_top_n=2000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.pad(Tensor([3, 0, 5, 6],"complex64"), pad=list[1,2,2,1,], mode="replicate", value=0.0, data_format="NCHW", name=None, )
paddle.nn.functional.conv2d(Tensor([1, 1024, 128, 128],"float32"), Tensor([1024, 256, 3, 0],"float32"), padding=1, groups=4, )
paddle.Tensor.tile(Tensor([5358, 1, 0],"float32"), list[1,180,1,], )
paddle.incubate.nn.functional.fused_feedforward(Tensor([0, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, )
paddle.nn.functional.max_pool3d(x=Tensor([0, 32, 32, 56, 56],"float32"), kernel_size=list[1,1,1,], stride=list[1,1,1,], padding=list[0,0,0,], data_format="NCDHW", )
paddle.Tensor.tile(Tensor([1, 5, 0],"float32"), list[333,1,1,], )
paddle.Tensor.tile(Tensor([1, 4, 0],"float32"), list[276,1,1,], )
paddle.allclose(x=Tensor([2, 2, 2, 0],"float64"), y=Tensor([2, 2, 2, 0],"float64"), )
paddle.nn.functional.conv2d_transpose(Tensor([4, 16, 16, 4],"float32"), Tensor([4, 3, 3, 0],"float32"), Tensor([6],"float32"), output_size=None, padding="same", stride=1, dilation=1, groups=2, data_format="NHWC", )
paddle.Tensor.tile(Tensor([8, 1, 0],"float32"), tuple(1,13,1,), )
paddle.Tensor.tile(Tensor([1, 1, 2, 0, 8],"float32"), list[1,1,1,2,1,], )
paddle.allclose(Tensor([13, 0, 32],"float32"), Tensor([13, 0, 32],"float32"), atol=0.0001, )
paddle.nn.functional.avg_pool3d(x=Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[1,2,3,], padding=list[0,0,0,], exclusive=False, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[738,1,1,], )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 0, 8, 32, 32],"float32"), output_size=list[3,3,3,], data_format="NCDHW", name=None, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float64"), weight=Tensor([1, 3, 0, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=list[2,2,1,], padding=list[1,2,2,], dilation=tuple(2,2,2,), )
paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 0, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=tuple(1,0,0,), exclusive=True, )
paddle.Tensor.tile(Tensor([1, 4, 0],"float32"), list[952,1,1,], )
paddle.nn.functional.log_softmax(x=Tensor([2, 2, 0],"float32"), axis=0, )
paddle.nn.functional.kl_div(Tensor([0, 20],"float64"), Tensor([0, 20],"float32"), )
paddle.nn.functional.conv2d(Tensor([1, 1024, 0, 256],"float32"), Tensor([1024, 128, 0, 3],"float32"), padding=1, groups=8, )
paddle.roll(Tensor([4, 5, 0, 4],"complex128"), Tensor([4],"int64"), list[0,1,2,3,], name=None, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 1, 64],"float16"), Tensor([1, 2048, 1, 64],"float16"), Tensor([1, 2048, 1, 0],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), is_causal=False, )
paddle.nn.functional.conv3d_transpose(Tensor([4, 8, 8, 8, 4],"float32"), Tensor([4, 3, 3, 0, 3],"float32"), Tensor([6],"float32"), output_size=None, padding="valid", stride=tuple(1,2,1,), dilation=tuple(2,1,1,), groups=2, data_format="NDHWC", )
paddle.nn.functional.softmax(Tensor([16, 0],"float64"), 1, name=None, )
paddle.nn.functional.max_pool1d(x=Tensor([0, 3, 8],"float64"), kernel_size=3, stride=4, padding=0, )
paddle.nn.functional.conv3d_transpose(Tensor([0, 4, 8, 8, 8],"float32"), Tensor([4, 3, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,2,1,], stride=1, dilation=1, groups=2, data_format="NCDHW", )
paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([0, 3, 5, 2, 4],"float16"), Tensor([0, 3, 5, 2, 4],"float16"), )
paddle.einsum("nbtc,nbsc->nbts", Tensor([0, 52, 7, 8],"float32"), Tensor([0, 52, 14, 8],"float32"), )
paddle.nn.functional.log_softmax(Tensor([0, 2, 3],"float32"), )
paddle.nn.functional.pad(Tensor([3, 0, 5, 6, 7],"complex64"), pad=list[1,2,2,1,1,0,], mode="circular", value=0.0, data_format="NCDHW", name=None, )
paddle.nn.functional.pad(Tensor([1, 16, 14, 0, 384],"float32"), tuple(0,0,0,0,0,0,), data_format="NDHWC", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[1892,1,1,], )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float64"), Tensor([3, 0, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=1, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 0],"float32"), weight=Tensor([1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=list[2,1,], padding=list[1,2,], dilation=tuple(2,2,), )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([256, 7, 0, 2048],"float16"), output_size=1, data_format="NHWC", name=None, )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[281,1,1,], )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 2],"float64"), weight=Tensor([3, 1, 3, 0, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,1,], dilation=1, )
paddle.nn.functional.max_pool2d(Tensor([0, 3, 32, 32],"float32"), kernel_size=2, stride=None, padding="SAME", return_mask=True, )
paddle.nn.functional.pad(Tensor([0, 2, 3, 4],"float64"), pad=list[2,1,2,1,], mode="replicate", value=0.0, data_format="NHWC", name=None, )
paddle.kthvalue(Tensor([30, 200, 0],"float32"), k=1, axis=1, keepdim=True, )
paddle.nn.functional.pad(Tensor([1, 16, 14, 17, 0],"float16"), tuple(0,4,0,0,0,0,), data_format="NDHWC", )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([0, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 0, 2],"float64"), Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[2,2,2,], groups=1, output_size=None, data_format="NCDHW", )
paddle.vision.ops.prior_box(Tensor([4, 96, 0, 80],"float32"), Tensor([4, 3, 640, 640],"float32"), list[16.0,24.0,], list[], list[1.0,], list[0.1,0.1,0.2,0.2,], False, False, list[8.0,8.0,], 0.5, False, None, )
paddle.slice(Tensor([0, 1, 104],"float32"), list[0,1,2,], list[0,0,32,], list[13,1,48,], )
paddle.nn.functional.max_pool3d(x=Tensor([2, 3, 0, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=1, )
paddle.einsum("i,j", Tensor([0],"float64"), Tensor([10],"float64"), )
paddle.gammaln(Tensor([0, 3, 4, 5],"float64"), )
paddle.einsum("nbtc,nbsc->nbts", Tensor([2, 52, 7, 8],"float32"), Tensor([2, 52, 0, 8],"float32"), )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float64"), weight=Tensor([1, 3, 3, 3, 0],"float64"), bias=Tensor([1],"float64"), stride=list[2,2,1,], padding=1, dilation=list[2,2,2,], )
paddle.nn.functional.max_pool2d(x=Tensor([0, 4, 8, 8],"float32"), kernel_size=2, stride=2, padding=0, )
paddle.matmul(Tensor([10],"float32"), Tensor([0],"float32"), True, True, )
paddle.Tensor.fill_diagonal_(Tensor([0, 3, 3],"float32"), 1, offset=0, wrap=True, )
paddle.nn.functional.max_pool1d(x=Tensor([2, 0, 8],"float32"), kernel_size=2, stride=2, padding=0, )
paddle.nn.functional.hinge_embedding_loss(Tensor([0, 3],"float64"), Tensor([0, 3],"float64"), reduction="none", margin=1.0, name=None, )
paddle.nn.functional.softmax(Tensor([2, 8, 0, 129],"float16"), -1, )
paddle.nn.functional.avg_pool3d(Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,1,1,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([0],"float64"), stride=1, padding=1, dilation=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 6, 2, 0],"float64"), weight=Tensor([6, 1, 3, 3],"float64"), bias=Tensor([3],"float64"), stride=1, padding=list[1,0,], groups=3, dilation=1, )
paddle.nn.functional.pad(Tensor([1024, 1, 256, 0],"float32"), list[1,1,1,1,], )
paddle.nn.functional.lp_pool2d(Tensor([0, 32, 32, 3],"float32"), 2, kernel_size=2, stride=2, ceil_mode=False, data_format="NHWC", )
paddle.nn.functional.pad(Tensor([1, 0, 61, 56, 96],"float32"), tuple(0,0,0,2,0,0,), data_format="NDHWC", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 16],"float32"), Tensor([6, 8, 3, 0],"float32"), bias=Tensor([8],"float32"), padding="valid", output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.sequence_mask(Tensor([2, 0],"int64"), maxlen=-1, dtype=VarType(int64), )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 0, 32, 128],"bfloat16"), Tensor([1, 1024, 32, 128],"bfloat16"), Tensor([1, 1024, 32, 128],"bfloat16"), attn_mask=None, is_causal=True, )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[398,1,1,], )
paddle.nn.functional.selu(Tensor([0, 5, 5, 10],"float64"), 1.5, 2.0, None, )
paddle.nn.functional.log_softmax(x=Tensor([2, 0, 4],"float64"), axis=2, dtype="float32", )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float64"), weight=Tensor([1, 3, 3, 0, 3],"float64"), bias=Tensor([1],"float64"), stride=list[2,2,1,], padding=1, dilation=tuple(2,2,2,), )
paddle.Tensor.lgamma(Tensor([2, 0],"float32"), )
paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 3, 0, 2, 4],"float32"), Tensor([1, 3, 4, 2, 4],"float32"), )
paddle.Tensor.tile(Tensor([1, 10, 0],"float32"), list[662,1,1,], )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 1, 64],"float16"), Tensor([1, 2048, 1, 64],"float16"), Tensor([1, 0, 1, 64],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), is_causal=True, )
paddle.nn.functional.max_pool2d(Tensor([0, 3, 32, 32],"float32"), kernel_size=2, stride=2, padding=1, ceil_mode=False, )
paddle.nn.functional.hinge_embedding_loss(Tensor([0, 10, 5],"float64"), Tensor([0, 10, 5],"float64"), reduction="none", )
paddle.nn.functional.max_pool3d(Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[0,0,0,], return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.einsum("nbts,nbsc->nbtc", Tensor([2, 52, 7, 0],"float32"), Tensor([2, 52, 14, 0],"float32"), )
paddle.Tensor.tile(Tensor([11585, 1, 0],"float32"), list[1,418,1,], )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 0, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 2, 2, 2, 3],"float64"), weight=Tensor([3, 1, 3, 0, 3],"float64"), bias=Tensor([3],"float64"), output_padding=1, stride=2, padding=list[1,1,1,], groups=3, data_format="NDHWC", dilation=1, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[1119,1,1,], )
paddle.Tensor.tile(Tensor([10285, 0],"float32"), list[1,21,], )
paddle.nn.functional.conv2d(Tensor([8, 256, 128, 0],"float32"), Tensor([256, 256, 3, 3],"float32"), bias=None, stride=1, padding=1, )
paddle.nn.functional.temporal_shift(x=Tensor([2, 4, 0, 3],"float64"), seg_num=2, shift_ratio=0.4, )
paddle.nn.functional.prelu(x=Tensor([0, 2, 3, 4],"float32"), weight=Tensor([1],"float32"), )
paddle.nn.functional.conv2d(Tensor([1, 1024, 0, 256],"float32"), Tensor([1024, 128, 3, 3],"float32"), padding=1, groups=8, )
paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([52, 4, 5, 1, 8],"float32"), Tensor([52, 4, 5, 0, 8],"float32"), )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[298,1,1,], )
paddle.strided_slice(x=Tensor([5, 8, 6, 4, 2, 0],"float64"), axes=list[1,2,5,], starts=list[-3,3,4,], ends=list[3,0,1,], strides=list[-1,-1,-2,], )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 0, 4],"float32"), weight=Tensor([1, 3, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=list[2,2,1,], padding=1, dilation=list[2,2,2,], )
paddle.vision.ops.generate_proposals(Tensor([1, 3, 0, 15],"float32"), Tensor([1, 12, 10, 15],"float32"), Tensor([1, 2],"float32"), Tensor([450, 4],"float32"), Tensor([450, 4],"float32"), pre_nms_top_n=2000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 2, 2, 2, 3],"float32"), weight=Tensor([3, 1, 3, 0, 3],"float32"), bias=Tensor([3],"float32"), output_padding=1, stride=2, padding=list[1,1,1,], groups=3, data_format="NDHWC", dilation=1, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[220,1,1,], )
paddle.nn.functional.avg_pool1d(x=Tensor([2, 0, 8],"float64"), kernel_size=2, stride=1, padding=0, ceil_mode=True, )
paddle.nn.functional.log_softmax(Tensor([2, 0, 1],"float32"), 0, )
paddle.slice(Tensor([1024, 0, 1024],"float16"), axes=list[0,], starts=list[896,], ends=list[1024,], )
paddle.vision.ops.generate_proposals(Tensor([1, 9, 12, 9],"float32"), Tensor([1, 36, 12, 9],"float32"), Tensor([1, 0],"float32"), Tensor([972, 4],"float32"), Tensor([972, 4],"float32"), pre_nms_top_n=4000, post_nms_top_n=4000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.logcumsumexp(Tensor([0, 10, 10],"float32"), )
paddle.nn.functional.pad(Tensor([1, 3, 32, 0, 238],"float32"), tuple(0,2,0,0,0,0,), data_format="NCDHW", )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[284,1,1,], )
paddle.einsum("a...b,b...c,c...d", Tensor([0, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), )
paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 10],"int32"), bias=Tensor([10],"float32"), dequant_scales=Tensor([0],"float32"), act_method="gelu", compute_dtype="fp32", )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 0, 2],"float32"), Tensor([3, 1, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=1, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[366,1,1,], )
paddle.Tensor.flip(Tensor([2, 0],"float32"), 0, )
paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 512],"int32"), bias=Tensor([512],"float16"), dequant_scales=Tensor([512],"float32"), shift=Tensor([512],"float16"), smooth=Tensor([0],"float16"), act_method="gelu", compute_dtype="fp16", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 160, 0, 32],"float16"), output_size=2, data_format="NCHW", name=None, )
paddle.nn.functional.mse_loss(Tensor([3, 3, 0, 10],"float32"), Tensor([3, 3, 0, 10],"float32"), "none", )
paddle.Tensor.__rpow__(Tensor([0],"float32"), 550134.0076769129, )
paddle.nn.functional.temporal_shift(Tensor([128, 0, 28, 28],"float32"), 16, 0.0625, )
paddle.nn.functional.conv1d_transpose(Tensor([4, 16, 6],"float32"), Tensor([6, 0, 3],"float32"), bias=Tensor([8],"float32"), output_size=18, output_padding=0, padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NLC", )
paddle.nn.functional.conv1d(Tensor([1, 256, 0],"float32"), Tensor([256, 256, 3],"float32"), bias=Tensor([256],"float32"), padding=3, stride=list[1,], dilation=list[3,], groups=1, data_format="NCL", )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 0],"float32"), weight=Tensor([3, 1, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=1, output_padding=0, groups=1, dilation=1, output_size=None, data_format="NCL", name=None, )
paddle.nn.functional.mse_loss(Tensor([0, 10, 10],"float32"), Tensor([0, 10, 10],"float32"), "none", )
paddle.nn.functional.pad(Tensor([3, 0, 5],"complex64"), pad=list[1,2,], mode="constant", value=100, data_format="NCL", name=None, )
paddle.nn.functional.conv2d(x=Tensor([2, 4, 4, 3],"float64"), weight=Tensor([2, 3, 0, 3],"float64"), bias=Tensor([2],"float64"), stride=1, padding=0, data_format="NHWC", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[377,1,1,], )
paddle.slice(Tensor([16, 0, 104],"float32"), list[0,1,2,], list[0,0,64,], list[16,1,80,], )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[1041,1,1,], )
paddle.nn.functional.max_pool1d(Tensor([2, 0, 8],"float64"), 2, 1, 1, False, False, None, )
paddle.slice(Tensor([8, 20, 0],"float16"), list[0,], list[4,], list[8,], )
paddle.nanmedian(Tensor([2, 3, 4, 0],"float32"), axis=list[0,3,], keepdim=False, )
paddle.nn.functional.conv1d_transpose(Tensor([0, 6, 16],"float32"), Tensor([6, 8, 3],"float32"), bias=Tensor([8],"float32"), output_size=None, output_padding=0, padding=0, stride=list[2,], dilation=list[2,], groups=1, data_format="NCL", )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[220,1,1,], )
paddle.nn.functional.conv1d(Tensor([1, 128, 112],"float32"), Tensor([128, 128, 0],"float32"), bias=Tensor([128],"float32"), padding=3, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv1d(Tensor([1, 256, 28],"float32"), Tensor([256, 256, 0],"float32"), bias=Tensor([256],"float32"), padding=5, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 0],"float16"), bias=Tensor([512],"float16"), act_method="gelu", compute_dtype="fp16", )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 3, 4, 0],"float64"), output_size=list[2,3,], data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([8, 1, 0],"float32"), tuple(1,5,1,), )
paddle.nn.functional.conv1d_transpose(Tensor([2, 3, 0],"float32"), Tensor([3, 1, 3],"float32"), bias=Tensor([3],"float32"), output_size=None, output_padding=0, padding=list[1,], stride=list[1,], dilation=list[1,], groups=3, data_format="NCL", )
paddle.nn.functional.pad(Tensor([1, 16, 31, 28, 0],"float32"), tuple(0,0,0,4,0,0,), data_format="NDHWC", )
paddle.nn.functional.log_softmax(x=Tensor([0, 3, 4],"float64"), axis=1, )
paddle.nn.functional.conv1d_transpose(Tensor([4, 6, 16],"float32"), Tensor([6, 0, 3],"float32"), bias=Tensor([8],"float32"), output_size=None, output_padding=0, padding="valid", stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.Tensor.tile(Tensor([2, 13, 7, 0],"float32"), list[1,4,1,1,], )
paddle.Tensor.nonzero(Tensor([52640, 0],"bool"), )
paddle.nn.functional.avg_pool1d(Tensor([0, 3, 8],"float64"), 1, 1, 0, True, False, None, )
paddle.Tensor.tile(Tensor([1, 5, 0],"float32"), list[306,1,1,], )
paddle.nn.functional.l1_loss(Tensor([0, 4],"float32"), Tensor([0, 4],"float32"), )
paddle.nn.functional.pad(Tensor([1, 3, 0, 160],"float64"), pad=list[40,40,0,0,], mode="constant", value=0.0, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[110,1,1,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[329,1,1,], )
paddle.nn.functional.avg_pool2d(Tensor([0, 3, 4, 4],"float64"), kernel_size=list[3,3,], stride=list[3,3,], padding=list[0,0,0,0,], ceil_mode=True, exclusive=False, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 2, 2, 2, 3],"float64"), weight=Tensor([3, 0, 3, 3, 3],"float64"), bias=Tensor([3],"float64"), stride=1, padding=list[1,0,1,], groups=3, data_format="NDHWC", dilation=1, )
paddle.nn.functional.softmax(Tensor([0, 4, 5, 1, 7],"float32"), )
paddle.nn.functional.conv1d_transpose(Tensor([2, 3, 2],"float32"), Tensor([3, 1, 3],"float32"), bias=Tensor([0],"float32"), output_size=None, output_padding=0, padding=1, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 2, 2, 6],"float64"), weight=Tensor([6, 1, 0, 3],"float64"), bias=Tensor([3],"float64"), stride=2, padding=list[1,0,], output_padding=1, dilation=1, groups=3, output_size=None, data_format="NHWC", )
paddle.nn.functional.conv2d_transpose(Tensor([0, 256, 16, 16],"float32"), Tensor([256, 1, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=256, output_size=None, data_format="NCHW", )
paddle.vision.ops.generate_proposals(Tensor([1, 3, 10, 15],"float32"), Tensor([1, 12, 10, 15],"float32"), Tensor([1, 0],"float32"), Tensor([450, 4],"float32"), Tensor([450, 4],"float32"), pre_nms_top_n=2000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.slice(Tensor([1024, 16, 0],"float16"), axes=list[0,], starts=list[256,], ends=list[384,], )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([1, 3, 2, 2, 0],"float32"), bias=Tensor([1],"float32"), stride=1, padding=0, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 1, 64],"float16"), Tensor([0, 2048, 1, 64],"float16"), Tensor([1, 2048, 1, 64],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), is_causal=True, )
paddle.allclose(Tensor([0, 3],"float32"), Tensor([0, 3],"float32"), atol=0.0001, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 0, 4],"float64"), weight=Tensor([1, 3, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=2, padding=1, )
paddle.quantile(Tensor([2, 0],"float32"), list[0.3,0.7,], 1, )
paddle.Tensor.subtract(Tensor([0],"float32"), Tensor([0],"float32"), )
paddle.nn.functional.scaled_dot_product_attention(Tensor([4, 134, 4, 128],"float16"), Tensor([4, 134, 4, 0],"float16"), Tensor([4, 134, 4, 128],"float16"), attn_mask=Tensor([4, 1, 134, 134],"float16"), dropout_p=0.0, training=True, is_causal=False, )
paddle.nn.functional.max_pool3d(x=Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[0,0,0,], )
paddle.nn.functional.conv3d_transpose(Tensor([4, 4, 8, 8, 8],"float32"), Tensor([4, 0, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,1,2,2,1,1,], stride=1, dilation=1, groups=2, data_format="NCDHW", )
paddle.nn.functional.fractional_max_pool3d(x=Tensor([2, 0, 7, 7, 7],"float32"), output_size=list[3,3,None,], random_u=0.6, )
paddle.nn.functional.pad(Tensor([3, 0, 3, 1600, 3],"float32"), pad=list[2,2,0,0,0,0,], data_format="NCDHW", )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 0, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[999,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 3, 2],"float64"), Tensor([2, 0, 1, 1],"float64"), groups=1, padding=list[1,1,], data_format="NHWC", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[81,1,1,], )
paddle.nn.functional.softmax(Tensor([2, 300, 0],"float32"), axis=0, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 0, 2],"float32"), Tensor([3, 1, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([1, 3, 32, 0, 258],"float32"), tuple(0,2,0,0,0,0,), data_format="NCDHW", )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 0],"float64"), weight=Tensor([3, 1, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=1, output_padding=0, groups=1, dilation=1, output_size=None, data_format="NCL", name=None, )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 2],"float64"), weight=Tensor([3, 1, 3, 0, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,1,], dilation=2, )
paddle.nn.functional.conv2d(Tensor([1, 1024, 0, 32],"float32"), Tensor([256, 1024, 0, 1],"float32"), None, list[1,1,], 0, list[1,1,], 1, )
paddle.Tensor.tile(Tensor([10105, 0],"float32"), list[1,11,], )
paddle.nonzero(x=Tensor([0, 3],"float64"), as_tuple=False, )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 7, 0],"float32"), output_size=5, data_format="NCDHW", name=None, )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float64"), weight=Tensor([1, 3, 0, 3],"float64"), bias=Tensor([1],"float64"), stride=list[2,1,], padding=tuple(1,2,), dilation=tuple(2,2,), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[298,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 2],"float32"), Tensor([3, 1, 3, 0],"float32"), bias=None, padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 2],"float64"), weight=Tensor([3, 0, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=0, dilation=1, )
paddle.nonzero(Tensor([0],"int32"), True, )
paddle.nn.functional.pad(x=Tensor([1, 0, 1, 2, 3],"float64"), pad=tuple(0,1,1,1,2,0,), mode="constant", value=0, data_format="NCDHW", )
paddle.nn.functional.conv1d(Tensor([4, 16, 6],"float32"), Tensor([8, 6, 0],"float32"), bias=Tensor([8],"float32"), padding=list[1,2,], stride=list[1,], dilation=list[1,], groups=1, data_format="NLC", )
paddle.nn.functional.log_softmax(x=Tensor([2, 3, 0],"float64"), axis=2, )
paddle.slice(Tensor([1024, 16, 0],"float16"), axes=list[0,], starts=list[640,], ends=list[768,], )
paddle.slice(Tensor([13, 0, 104],"float32"), list[0,1,2,], list[0,0,48,], list[13,1,64,], )
paddle.nn.functional.max_pool3d(Tensor([2, 0, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding=0, )
paddle.Tensor.tile(Tensor([4, 0, 512],"float32"), tuple(1,13,1,), )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 0, 8, 32, 32],"float64"), output_size=3, data_format="NCDHW", name=None, )
paddle.einsum("i,j->ij", Tensor([10],"float32"), Tensor([0],"float32"), )
paddle.nn.functional.avg_pool1d(Tensor([13, 0, 120],"float32"), 25, 1, 0, True, False, None, )
paddle.nn.functional.conv2d(Tensor([1, 256, 128, 0],"float32"), Tensor([256, 256, 3, 3],"float32"), padding=1, groups=1, )
paddle.linalg.slogdet(Tensor([0, 3, 5, 5],"complex64"), )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 2],"float32"), weight=Tensor([3, 0, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=list[1,], groups=1, dilation=2, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[636,1,1,], )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 0],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
paddle.nn.functional.conv1d_transpose(Tensor([0, 256, 28],"float32"), Tensor([256, 128, 8],"float32"), bias=Tensor([128],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.pad(Tensor([4, 6, 0],"float64"), pad=list[2,2,], mode="constant", value=0.0, data_format="NCL", name=None, )
paddle.Tensor.tile(Tensor([10105, 0],"float32"), list[1,3,], )
paddle.einsum("nbka,ahc->nbkhc", Tensor([1, 0, 4, 2],"float32"), Tensor([2, 2, 4],"float32"), )
paddle.nn.functional.pad(Tensor([1, 0, 3, 4, 5],"complex64"), pad=list[1,2,1,1,3,4,], mode="constant", value=100, )
paddle.nn.functional.softmax(Tensor([2, 3, 0],"float32"), axis=-3, dtype=None, name=None, )
paddle.nn.functional.conv1d(Tensor([4, 6, 16],"float32"), Tensor([8, 6, 0],"float32"), bias=Tensor([8],"float32"), padding=list[1,], stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float32"), Tensor([3, 12, 12, 0, 12],"float32"), bias=Tensor([12],"float32"), padding=0, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.pad(Tensor([0, 3, 204, 152],"float64"), pad=list[1,3,2,4,], mode="constant", value=0.0, data_format="NCHW", )
paddle.nn.functional.conv1d(Tensor([1, 128, 112],"float32"), Tensor([128, 128, 0],"float32"), bias=Tensor([128],"float32"), padding=3, stride=list[1,], dilation=list[3,], groups=1, data_format="NCL", )
paddle.nn.functional.conv1d(Tensor([16, 64, 25500],"float32"), Tensor([128, 64, 0],"float32"), bias=Tensor([128],"float32"), padding=8, stride=list[1,], dilation=list[8,], groups=1, data_format="NCL", )
paddle.nn.functional.sequence_mask(Tensor([2, 0],"int64"), maxlen=10, dtype=VarType(uint8), )
paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 0],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0009313154732808471, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.quantile(Tensor([0, 7, 6],"float64"), q=0.75, axis=list[0,2,], )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[134,1,1,], )
paddle.nn.functional.conv3d(Tensor([4, 8, 8, 8, 3],"float32"), Tensor([5, 3, 0, 3, 3],"float32"), Tensor([5],"float32"), padding=0, stride=1, dilation=1, groups=1, data_format="NDHWC", )
paddle.nn.functional.avg_pool2d(Tensor([0, 512, 13, 13],"float32"), kernel_size=5, stride=3, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([1, 3, 32, 224, 0],"float32"), tuple(0,2,0,0,0,0,), data_format="NCDHW", )
paddle.nn.functional.avg_pool2d(x=Tensor([2, 0, 4, 4],"float64"), kernel_size=list[3,3,], stride=list[3,3,], padding=list[0,0,], ceil_mode=True, exclusive=False, )
paddle.nanmedian(Tensor([0, 3, 4, 5],"float32"), axis=2, keepdim=False, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 512, 32, 32],"float32"), Tensor([512, 1, 0, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=512, output_size=None, data_format="NCHW", )
paddle.einsum("...jk->...kj", Tensor([0, 10, 3],"float64"), )
paddle.vision.ops.generate_proposals(Tensor([1, 9, 12, 9],"float32"), Tensor([0, 36, 12, 9],"float32"), Tensor([1, 2],"float32"), Tensor([972, 4],"float32"), Tensor([972, 4],"float32"), pre_nms_top_n=4000, post_nms_top_n=4000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.selu(Tensor([2, 0],"float64"), 1.0507009873554805, 1.6732632423543772, None, )
paddle.nn.functional.lp_pool1d(Tensor([0, 3, 32],"float32"), 7.0, 2, None, 1, True, "NCL", None, )
paddle.nn.functional.pad(Tensor([1, 16, 16, 14, 0],"float16"), tuple(0,0,0,5,0,0,), data_format="NDHWC", )
paddle.nn.functional.conv1d(x=Tensor([2, 3, 0],"float32"), weight=Tensor([6, 1, 3],"float32"), bias=Tensor([6],"float32"), stride=2, padding=0, groups=3, )
paddle.meshgrid(Tensor([100],"float32"), Tensor([0],"float32"), )
paddle.nn.functional.conv1d(x=Tensor([2, 3, 4],"float32"), weight=Tensor([1, 3, 3],"float32"), bias=Tensor([0],"float32"), stride=2, padding=1, )
paddle.Tensor.__getitem__(Tensor([0, 3, 3],"float32"), tuple(slice(1,2,None),slice(2,None,None),slice(None,None,-1),), )
paddle.nn.functional.softmax(Tensor([4, 1, 81, 94, 0],"float32"), axis=2, )
paddle.nn.functional.conv1d(Tensor([13, 0, 32],"float32"), Tensor([32, 1, 3],"float32"), bias=None, padding=1, stride=list[1,], dilation=list[1,], groups=32, data_format="NLC", )
paddle.nn.functional.pad(Tensor([1, 16, 14, 19, 0],"float32"), tuple(0,2,0,0,0,0,), data_format="NDHWC", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 16, 16, 4],"float32"), Tensor([4, 0, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=0, stride=1, dilation=1, groups=2, data_format="NHWC", )
paddle.nn.functional.pixel_shuffle(Tensor([4, 128, 0, 128],"float16"), 2, "NCHW", None, )
paddle.nn.functional.scaled_dot_product_attention(query=Tensor([2, 64, 12, 64],"float16"), key=Tensor([2, 64, 12, 64],"float16"), value=Tensor([2, 64, 0, 64],"float16"), is_causal=True, )
paddle.reverse(Tensor([4, 0, 32],"float64"), axis=1, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 0, 16],"float32"), Tensor([6, 8, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[1,2,], dilation=list[2,2,], groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.__sub__(Tensor([3, 6, 3, 1, 2, 0],"float64"), Tensor([3, 6, 3, 1, 2, 0],"float64"), )
paddle.nn.functional.pad(Tensor([30, 0, 16, 128],"float32"), list[0,1,0,1,], value=-math.inf, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[164,1,1,], )
paddle.einsum("td,dnh->tnh", Tensor([15, 32],"float32"), Tensor([32, 0, 4],"float32"), )
paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([0],"float32"), shift=None, smooth=None, quant_scale=0.001479289960116148, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 0],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,], dilation=2, )
paddle.nn.functional.kl_div(Tensor([0, 20, 50],"float32"), Tensor([0, 20, 50],"float32"), "batchmean", False, )
paddle.nn.functional.conv3d(Tensor([4, 6, 8, 8, 8],"float32"), Tensor([8, 3, 0, 3, 3],"float32"), Tensor([8],"float32"), padding="same", stride=1, dilation=1, groups=2, data_format="NCDHW", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 3, 16, 0],"float32"), Tensor([3, 2, 3, 3],"float32"), bias=Tensor([6],"float32"), padding="valid", output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=3, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([16660, 0],"float32"), list[1,36,], )
paddle.nn.functional.conv2d_transpose(Tensor([0, 1024, 16, 16],"float32"), Tensor([1024, 1, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1024, output_size=None, data_format="NCHW", )
paddle.nn.functional.max_pool2d(Tensor([2, 32, 32, 0],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, data_format="NHWC", )
paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 0, 1, 1, 7],"float32"), Tensor([13, 0, 1, 7, 8],"float32"), )
paddle.nn.functional.pad(Tensor([1, 0, 3],"float32"), list[1,0,], value=4, mode="constant", data_format="NCL", )
paddle.einsum("a...b,b...c,c...d", Tensor([4, 3, 0, 4],"float64"), Tensor([4, 3, 0, 4],"float64"), Tensor([4, 3, 0, 4],"float64"), )
paddle.incubate.nn.functional.fused_feedforward(Tensor([0, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, Tensor([4],"float32"), Tensor([4],"float32"), dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )
paddle.slice(Tensor([1024, 0, 1024],"float16"), axes=list[0,], starts=list[128,], ends=list[256,], )
paddle.nn.functional.temporal_shift(x=Tensor([2, 2, 0, 3],"float64"), seg_num=2, )
paddle.nn.functional.max_pool2d(Tensor([1, 0, 224, 224],"float32"), kernel_size=3, )
paddle.Tensor.lgamma(Tensor([5, 0, 8, 10],"float64"), )
paddle.nn.functional.softmax(Tensor([0, 10],"float32"), -1, name=None, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 0, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=list[list[0,0,],list[0,0,],list[2,3,],list[1,2,],list[2,1,],], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.lp_pool2d(Tensor([2, 0, 32, 32],"float32"), 2, kernel_size=5, stride=3, ceil_mode=True, )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([0, 4, 4, 3],"float64"), output_size=3, data_format="NHWC", )
paddle.einsum("blkd,bldq->blkq", Tensor([13, 5, 1, 1],"float32"), Tensor([13, 5, 1, 0],"float32"), )
paddle.nn.functional.pixel_shuffle(Tensor([4, 81, 0, 4],"float64"), 3, "NCHW", None, )
paddle.nn.functional.poisson_nll_loss(Tensor([10, 0],"float32"), Tensor([10, 0],"float32"), log_input=True, full=False, epsilon=1e-08, reduction="mean", name=None, )
paddle.nn.functional.conv2d(Tensor([1, 256, 0, 128],"float32"), Tensor([256, 256, 0, 3],"float32"), padding=1, groups=1, )
paddle.nn.functional.temporal_shift(x=Tensor([2, 0, 4, 3],"float64"), seg_num=2, )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 2],"float32"), weight=Tensor([3, 1, 3, 0, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=list[1,0,1,], dilation=2, )
paddle.slice(Tensor([0, 1, 4],"float32"), axes=list[2,], starts=list[3,], ends=list[4,], )
paddle.Tensor.tile(Tensor([2, 2, 38, 0, 1],"float16"), list[1,1,1,1,2,], )
paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([52, 4, 5, 0, 8],"float32"), Tensor([52, 4, 5, 0, 8],"float32"), )
paddle.nn.functional.conv2d_transpose(Tensor([0, 3, 7, 7],"float32"), Tensor([3, 6, 5, 5],"float32"), bias=Tensor([6],"float32"), padding=2, output_padding=list[1,1,], stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 2],"float64"), weight=Tensor([3, 1, 0],"float64"), bias=Tensor([1],"float64"), stride=2, padding=list[1,], output_padding=0, groups=1, dilation=1, output_size=None, data_format="NCL", name=None, )
paddle.allclose(Tensor([0, 8, 32],"float32"), Tensor([0, 8, 32],"float32"), atol=0.0001, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 8, 0, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=list[list[0,0,],list[0,0,],list[2,3,],list[1,2,],list[2,1,],], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float64"), Tensor([3, 0, 3, 3, 3],"float64"), bias=Tensor([3],"float64"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=3, output_size=None, data_format="NCDHW", )
paddle.Tensor.__sub__(Tensor([0, 6, 3, 4, 1, 5],"float64"), Tensor([0, 6, 3, 4, 1, 5],"float64"), )
paddle.nn.functional.conv2d_transpose(Tensor([1, 4096, 16, 16],"float32"), Tensor([4096, 512, 0, 3],"float32"), padding=0, stride=2, groups=8, )
paddle.nn.functional.scaled_dot_product_attention(query=Tensor([2, 64, 12, 64],"float16"), key=Tensor([2, 64, 12, 0],"float16"), value=Tensor([2, 64, 12, 64],"float16"), is_causal=True, )
paddle.nn.functional.avg_pool1d(x=Tensor([0, 3, 8],"float64"), kernel_size=2, stride=1, padding=1, exclusive=False, )
paddle.Tensor.tile(Tensor([1, 8, 0, 1, 16],"float32"), list[1,1,1,4,1,], )
paddle.nn.functional.max_pool3d(x=Tensor([2, 3, 0, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=tuple(1,0,0,), )
paddle.nn.functional.conv2d_transpose(Tensor([4, 0, 16, 4],"float32"), Tensor([4, 1, 3, 3],"float32"), Tensor([4],"float32"), output_size=None, padding="valid", stride=tuple(1,2,), dilation=1, groups=4, data_format="NHWC", )
paddle.nn.functional.pad(Tensor([3, 0, 16, 16],"float32"), pad=tuple(1,1,1,1,), mode="constant", value=0.0, data_format="NCHW", name=None, )
paddle.nn.functional.max_pool1d(x=Tensor([2, 3, 0],"float64"), kernel_size=2, stride=1, padding=1, )
paddle.nn.functional.pad(Tensor([0, 64, 7],"float32"), tuple(-3,0,), data_format="NCL", )
paddle.Tensor.tile(Tensor([0, 4],"float32"), list[90,1,1,], )
paddle.nn.functional.conv2d_transpose(x=Tensor([0, 3, 2, 2],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=1, dilation=1, )
paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([0, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 1, 50, 50],"float16"), scale=0.125, )
paddle.nn.functional.max_pool2d(Tensor([0, 3, 33, 33],"float32"), kernel_size=5, stride=5, padding=0, ceil_mode=True, return_mask=True, )
paddle.nn.functional.pad(Tensor([0, 3, 4, 4],"float32"), list[1,1,1,1,], mode="reflect", data_format="NCHW", )
paddle.vision.ops.generate_proposals(Tensor([2, 3, 4, 4],"float32"), Tensor([2, 12, 4, 4],"float32"), Tensor([2, 3],"float32"), Tensor([4, 4, 0, 4],"float32"), Tensor([4, 4, 3, 4],"float32"), pre_nms_top_n=10, post_nms_top_n=5, return_rois_num=True, )
paddle.nn.functional.max_unpool3d(Tensor([1, 3, 2, 2, 0],"float64"), Tensor([1, 3, 2, 2, 0],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCDHW", output_size=None, name=None, )
paddle.nn.functional.scaled_dot_product_attention(query=Tensor([2, 64, 12, 64],"float16"), key=Tensor([2, 0, 12, 64],"float16"), value=Tensor([2, 64, 12, 64],"float16"), is_causal=True, )
paddle.einsum("nbka,ahc->nbkhc", Tensor([1, 3, 0, 6],"float16"), Tensor([6, 2, 4],"float16"), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[333,1,1,], )
paddle.Tensor.isnan(Tensor([2, 0],"float32"), )
paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([2, 1, 64],"float16"), )
paddle.nn.functional.avg_pool2d(Tensor([2, 0, 5, 5],"float32"), kernel_size=3, stride=1, padding=1, ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.temporal_shift(Tensor([128, 1024, 0, 14],"float32"), 8, 0.125, )
paddle.nn.functional.log_softmax(Tensor([2, 0, 1],"float64"), 0, )
paddle.vision.ops.generate_proposals(Tensor([1, 15, 42, 63],"float32"), Tensor([1, 60, 42, 0],"float32"), Tensor([1, 2],"float32"), Tensor([39690, 4],"float32"), Tensor([39690, 4],"float32"), pre_nms_top_n=12000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.pixel_shuffle(x=Tensor([4, 81, 0, 4],"float64"), upscale_factor=3, data_format="NCHW", )
paddle.masked_fill(Tensor([0],"float32"), Tensor([0],"bool"), Tensor([0],"float32"), )
paddle.nn.functional.max_pool2d(Tensor([2, 0, 224, 224],"float32"), kernel_size=list[5,3,], stride=list[1,2,], padding=tuple(2,1,), )
paddle.Tensor.tile(Tensor([27540, 0],"float32"), list[1,420,], )
paddle.nn.functional.avg_pool2d(Tensor([64, 0, 64, 64],"float16"), kernel_size=1, stride=1, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([1, 128, 128, 128],"float32"), Tensor([128, 64, 3, 0],"float32"), bias=Tensor([64],"float32"), padding=1, output_padding=1, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[145,1,1,], )
paddle.nn.functional.temporal_shift(x=Tensor([2, 2, 4, 0],"float32"), seg_num=2, )
paddle.nn.functional.mse_loss(Tensor([0, 10, 10],"float32"), Tensor([0, 10, 10],"float32"), "sum", )
paddle.einsum("sec,ecm->sm", Tensor([2, 60, 0],"float32"), Tensor([60, 2, 0],"float32"), )
paddle.nn.functional.max_pool3d(Tensor([1, 3, 4, 0, 6],"float64"), kernel_size=2, stride=2, return_mask=True, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([1, 3, 3, 3, 0],"float32"), bias=Tensor([1],"float32"), stride=2, padding=1, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 512, 32, 32],"float32"), Tensor([512, 1, 4, 0],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=512, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d(Tensor([1, 2048, 16, 0],"float32"), Tensor([2048, 512, 3, 3],"float32"), padding=1, groups=4, )
paddle.nn.functional.pad(Tensor([1, 16, 0, 14, 384],"float32"), tuple(0,0,0,1,0,0,), data_format="NDHWC", )
paddle.nn.functional.max_pool1d(Tensor([2, 0, 8],"float64"), 3, 4, 0, False, False, None, )
paddle.nn.functional.conv2d(Tensor([4, 3, 16, 0],"float32"), Tensor([5, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[0,0,],list[1,1,],list[2,2,],], stride=1, dilation=1, groups=1, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[45,1,1,], )
paddle.nn.functional.max_pool1d(Tensor([2, 0, 32],"float32"), kernel_size=2, stride=2, padding="SAME", )
paddle.nn.functional.conv1d(Tensor([16, 64, 0],"float32"), Tensor([128, 64, 3],"float32"), bias=Tensor([128],"float32"), padding=16, stride=list[1,], dilation=list[16,], groups=1, data_format="NCL", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[293,1,1,], )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 0],"float64"), weight=Tensor([1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=list[2,1,], padding=tuple(1,2,), dilation=tuple(2,2,), )
paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, residual_alpha=1.4142135623730951, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([58, 64],"float16"), )
paddle.nn.functional.max_pool2d(Tensor([0, 3, 32, 32],"float64"), kernel_size=list[3,3,], stride=tuple(1,2,), padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nanmedian(Tensor([2, 0],"float64"), axis=1, keepdim=False, mode="min", )
paddle.nn.functional.softmax(Tensor([0, 8, 153, 89],"float32"), axis=-1, )
paddle.nn.functional.lp_pool2d(Tensor([2, 0, 32, 32],"float32"), norm_type=2.0, kernel_size=2, stride=2, padding=0, ceil_mode=False, data_format="NCHW", name=None, )
paddle.slice(Tensor([0, 3],"float16"), axes=list[1,], starts=list[2,], ends=list[3,], )
paddle.nn.functional.conv2d_transpose(Tensor([1, 1024, 2, 0],"float32"), Tensor([1024, 512, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[266,1,1,], )
paddle.einsum("nbqa,ahc->nbqhc", Tensor([1, 3, 0, 6],"float16"), Tensor([6, 2, 4],"float16"), )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 4, 0, 3],"float64"), output_size=3, data_format="NHWC", name=None, )
paddle.nn.functional.sequence_mask(Tensor([0, 3],"int64"), maxlen=10, dtype=VarType(int64), )
paddle.Tensor.tile(Tensor([1, 7, 0],"float32"), list[1763,1,1,], )
paddle.nn.functional.conv3d_transpose(Tensor([4, 4, 8, 8, 0],"float32"), Tensor([4, 3, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,2,1,], stride=1, dilation=1, groups=2, data_format="NCDHW", )
paddle.vision.ops.generate_proposals(Tensor([1, 9, 10, 8],"float32"), Tensor([1, 36, 10, 8],"float32"), Tensor([1, 2],"float32"), Tensor([0, 4],"float32"), Tensor([720, 4],"float32"), pre_nms_top_n=4000, post_nms_top_n=4000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[156,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([4, 16, 16, 6],"float32"), Tensor([6, 0, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NHWC", )
paddle.nn.functional.max_pool3d(Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,1,1,], return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.nn.functional.max_unpool3d(Tensor([1, 0, 2, 2, 3],"float64"), Tensor([1, 0, 2, 2, 3],"int32"), kernel_size=2, stride=2, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 0],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=0, dilation=1, output_size=None, data_format="NCHW", )
paddle.allclose(Tensor([0],"int64"), Tensor([0],"int64"), 50.0, 48.0, False, )
paddle.nn.functional.conv2d(Tensor([1, 128, 256, 0],"float32"), Tensor([3, 128, 1, 1],"float32"), padding=0, groups=1, )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[60,1,1,], )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 2],"float64"), weight=Tensor([3, 1, 3],"float64"), bias=Tensor([0],"float64"), stride=1, padding=1, output_padding=0, groups=1, dilation=1, output_size=None, data_format="NCL", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 16, 16, 3],"float32"), Tensor([3, 0, 3, 3],"float32"), Tensor([5],"float32"), output_size=None, padding=0, stride=1, dilation=1, groups=1, data_format="NHWC", )
paddle.Tensor.tile(Tensor([1, 0],"float32"), tuple(200,1,), )
paddle.einsum("...jk, ...kl->...jl", Tensor([0, 10, 3],"float64"), Tensor([0, 3, 10],"float64"), )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 2],"float32"), weight=Tensor([3, 2, 3, 0, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=0, dilation=1, )
paddle.logcumsumexp(Tensor([0, 12],"float32"), dtype="float32", axis=1, )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([0, 3, 8, 32, 32],"float64"), output_size=tuple(3,3,3,), data_format="NDHWC", )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 8, 8],"float64"), weight=Tensor([3, 1, 5, 0],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[list[0,0,],list[0,0,],list[1,2,],list[3,4,],], output_padding=0, dilation=1, groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([1, 16, 14, 0, 384],"float16"), tuple(0,2,0,0,0,0,), data_format="NDHWC", )
paddle.nn.functional.max_pool2d(Tensor([0, 512, 1, 40],"float32"), kernel_size=tuple(1,1,), stride=1, padding=0, )
paddle.Tensor.__rpow__(Tensor([2, 2, 0],"float32"), 3.0, )
paddle.nn.functional.pad(Tensor([4, 0, 188, 140],"float32"), pad=list[1,1,1,1,], mode="constant", value=0.0, data_format="NCHW", name=None, )
paddle.nanmedian(Tensor([2, 3, 4, 0],"float32"), axis=0, keepdim=False, )
paddle.nn.functional.conv2d_transpose(Tensor([20, 256, 19, 34],"float32"), Tensor([256, 1, 4, 0],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=256, output_size=None, data_format="NCHW", )
paddle.Tensor.__sub__(Tensor([3, 0, 3, 4, 1, 5],"float64"), Tensor([3, 0, 3, 4, 1, 5],"float64"), )
paddle.nn.functional.max_pool3d(x=Tensor([2, 8, 8, 8, 0],"float64"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], data_format="NDHWC", )
paddle.nonzero(Tensor([1, 0],"float32"), )
paddle.nanmedian(Tensor([0, 3, 4, 5],"float32"), axis=2, keepdim=False, mode="min", )
paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([0, 4, 1, 1, 7],"float32"), Tensor([0, 4, 1, 7, 8],"float32"), )
paddle.meshgrid(list[Tensor([140],"float32"),Tensor([188],"float32"),Tensor([0],"float32"),], )
paddle.Tensor.__pow__(Tensor([0, 1024],"float32"), Tensor([0, 1024],"float32"), )
paddle.nn.functional.pad(Tensor([4, 3, 6, 6, 0],"float32"), pad=list[2,2,2,2,2,2,], mode="constant", value=0.0, data_format="NCDHW", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([0, 3, 2, 2],"float64"), Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=1, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[1,200,1,], )
paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 3, 0, 2, 4],"float16"), Tensor([1, 3, 0, 2, 4],"float16"), )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([1, 3, 3, 3, 0],"float32"), bias=Tensor([1],"float32"), stride=list[2,2,1,], padding=tuple(1,2,2,), dilation=tuple(2,2,2,), )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 0, 8, 8],"float32"), Tensor([6, 8, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[2,2,2,], dilation=list[2,1,2,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 2],"float64"), Tensor([3, 1, 3, 0],"float64"), bias=Tensor([1],"float64"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.allclose(tuple(Tensor([2, 20, 0],"float32"),), tuple(Tensor([2, 20, 0],"float32"),), rtol=0.0001, atol=0.0001, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 1, 64],"float16"), Tensor([1, 2048, 1, 64],"float16"), Tensor([1, 0, 1, 64],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), is_causal=False, )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 2, 2, 2, 3],"float64"), weight=Tensor([3, 1, 0, 3, 3],"float64"), bias=Tensor([3],"float64"), output_padding=1, stride=2, padding=list[1,1,1,], groups=3, data_format="NDHWC", dilation=1, )
paddle.nn.functional.fractional_max_pool2d(Tensor([0, 3, 7, 7],"float32"), output_size=list[3,3,], kernel_size=2, random_u=0.6, return_mask=False, name=None, )
paddle.nn.functional.pad(Tensor([2, 0, 4, 4, 4],"float32"), list[1,1,1,1,1,1,], mode="reflect", data_format="NCDHW", )
paddle.nn.functional.softmax(Tensor([0, 3],"float32"), axis=-1, dtype="float32", )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 4, 4],"float64"), weight=Tensor([3, 1, 0, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding="vaLiD", output_padding=0, dilation=1, groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 8],"float64"), weight=Tensor([3, 2, 0],"float64"), bias=Tensor([2],"float64"), stride=1, padding=list[list[0,0,],list[0,0,],list[3,4,],], output_padding=0, groups=1, dilation=1, output_size=None, data_format="NCL", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[109,1,1,], )
paddle.nn.functional.conv3d(x=Tensor([2, 4, 4, 4, 3],"float64"), weight=Tensor([2, 3, 3, 3, 0],"float64"), bias=Tensor([2],"float64"), stride=1, padding=0, data_format="NDHWC", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 0, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=list[1,2,2,3,2,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.scaled_dot_product_attention(query=Tensor([2, 64, 12, 64],"float16"), key=Tensor([2, 64, 0, 64],"float16"), value=Tensor([2, 64, 12, 64],"float16"), is_causal=True, )
paddle.nanmedian(Tensor([2, 0, 4, 5],"float32"), axis=list[0,1,3,], keepdim=False, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([1, 3, 3, 0, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=0, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 16],"float32"), Tensor([6, 8, 0, 3],"float32"), bias=Tensor([8],"float32"), padding="valid", output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 4, 128],"float16"), Tensor([1, 2048, 4, 128],"float16"), Tensor([0, 2048, 4, 128],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), dropout_p=0.0, training=True, is_causal=False, )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 3, 4, 0],"float64"), output_size=list[1,4,], )
paddle.nn.functional.conv2d_transpose(Tensor([4, 64, 0, 64],"float32"), Tensor([64, 1, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=64, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv1d(x=Tensor([2, 3, 4],"float64"), weight=Tensor([1, 3, 0],"float64"), bias=Tensor([1],"float64"), stride=list[1,], padding=list[1,], dilation=tuple(2,), )
paddle.nn.functional.conv2d_transpose(Tensor([1, 128, 80, 80],"float32"), Tensor([128, 0, 2, 2],"float32"), bias=Tensor([128],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[1021,1,1,], )
paddle.nn.functional.max_pool1d(x=Tensor([0, 3, 8],"float64"), kernel_size=2, stride=1, padding=1, )
paddle.Tensor.__sub__(Tensor([0, 6, 3, 1, 2, 5],"float64"), Tensor([0, 6, 3, 1, 2, 5],"float64"), )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[1388,1,1,], )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[158,1,1,], )
paddle.Tensor.tile(Tensor([16121, 1, 0],"float32"), list[1,811,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 2],"float32"), Tensor([3, 1, 3, 0],"float32"), bias=None, padding=0, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 4, 128],"float16"), Tensor([1, 2048, 0, 128],"float16"), Tensor([1, 2048, 4, 128],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.lp_pool1d(Tensor([0, 3, 32],"float32"), norm_type=7, kernel_size=2, stride=2, padding=list[1,], ceil_mode=True, )
paddle.Tensor.tile(Tensor([19125, 0],"float32"), list[1,44,], )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([0, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )
paddle.Tensor.tile(Tensor([27540, 0],"float32"), list[1,405,], )
paddle.nn.functional.log_softmax(Tensor([2, 2, 0],"float64"), 0, )
paddle.nn.functional.conv2d(Tensor([4, 16, 0, 3],"float32"), Tensor([5, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[1,2,], stride=1, dilation=1, groups=1, data_format="NHWC", )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[171,1,1,], )
paddle.nn.functional.fractional_max_pool2d(x=Tensor([2, 0, 7, 7],"float32"), kernel_size=list[2,2,], output_size=list[3,3,], random_u=0.6, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 0, 16],"float32"), Tensor([6, 8, 3, 3],"float32"), bias=Tensor([8],"float32"), padding="same", output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.poisson_nll_loss(Tensor([10, 0],"float32"), Tensor([10, 0],"float32"), log_input=True, full=True, epsilon=1e-08, reduction="mean", )
paddle.nn.functional.softmax(x=Tensor([2, 0, 4],"float64"), axis=0, )
paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 0],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.001479289960116148, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([1, 3, 0, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=2, padding=0, )
paddle.nn.functional.pad(Tensor([1, 16, 0, 14, 384],"float16"), tuple(0,0,0,3,0,0,), data_format="NDHWC", )
paddle.nn.functional.scaled_dot_product_attention(Tensor([4, 134, 0, 128],"float16"), Tensor([4, 134, 4, 128],"float16"), Tensor([4, 134, 4, 128],"float16"), attn_mask=Tensor([4, 1, 134, 134],"float16"), dropout_p=0.0, training=True, is_causal=False, )
paddle.Tensor.tile(Tensor([1000, 1, 0],"float32"), list[1,5,1,], )
paddle.Tensor.tile(Tensor([4, 0, 512],"float32"), tuple(1,14,1,), )
paddle.Tensor.tile(Tensor([1, 33, 0],"float32"), list[8,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([4, 256, 16, 0],"float32"), Tensor([256, 1, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=256, output_size=None, data_format="NCHW", )
paddle.allclose(Tensor([4, 0],"float32"), Tensor([4, 0],"float32"), atol=1e-06, )
paddle.Tensor.tile(Tensor([1, 256, 0, 2],"float32"), list[4,1,1,1,], )
paddle.Tensor.tile(Tensor([4, 0, 64, 64],"float32"), list[1,36,1,1,], )
paddle.nn.functional.layer_norm(Tensor([0, 6, 6, 3],"float32"), list[6,6,3,], weight=None, bias=Tensor([108],"float32"), epsilon=1e-05, )
paddle.nn.functional.pad(Tensor([0, 160000, 1],"float64"), pad=list[256,256,], mode="reflect", data_format="NLC", )
paddle.einsum("...jk, ...kl->...jl", Tensor([3, 10, 3],"float64"), Tensor([3, 3, 0],"float64"), )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 0],"float64"), weight=Tensor([3, 1, 3],"float64"), bias=Tensor([3],"float64"), stride=1, padding=list[1,], output_padding=0, groups=3, dilation=1, output_size=None, data_format="NCL", name=None, )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float32"), weight=Tensor([1, 3, 0, 3],"float32"), bias=Tensor([1],"float32"), stride=list[2,1,], padding=1, dilation=2, )
paddle.vision.ops.generate_proposals(Tensor([1, 9, 10, 8],"float32"), Tensor([1, 36, 0, 8],"float32"), Tensor([1, 2],"float32"), Tensor([720, 4],"float32"), Tensor([720, 4],"float32"), pre_nms_top_n=4000, post_nms_top_n=4000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.Tensor.tile(Tensor([10709, 1, 0],"float32"), list[1,405,1,], )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 0, 4, 4],"float64"), output_size=list[1,4,], )
paddle.nn.functional.max_pool2d(x=Tensor([0, 3, 32, 32],"float64"), kernel_size=list[3,3,], stride=list[1,2,], )
paddle.nn.functional.pad(Tensor([1, 8141, 0],"float32"), list[1,0,], value=0, mode="constant", data_format="NCL", )
paddle.nn.functional.pad(Tensor([0, 3, 4],"float64"), list[1,1,], mode="circular", data_format="NCL", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[105,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 16],"float32"), Tensor([6, 8, 0, 3],"float32"), bias=Tensor([8],"float32"), padding=list[list[0,0,],list[0,0,],list[1,2,],list[2,1,],], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 0],"float64"), weight=Tensor([6, 1, 3, 3],"float64"), bias=Tensor([6],"float64"), stride=2, padding=0, groups=3, )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[385,1,1,], )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([1, 3, 3, 3, 3],"float32"), bias=Tensor([0],"float32"), stride=list[2,2,1,], padding=list[1,2,2,], dilation=tuple(2,2,2,), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[289,1,1,], )
paddle.nn.functional.pad(Tensor([1, 3, 140, 0],"float64"), pad=list[0,0,40,40,], mode="constant", value=0.0, data_format="NCHW", )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([0, 768, 16, 7, 10],"float16"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
paddle.nn.functional.max_pool1d(Tensor([0, 3, 8],"float64"), 2, 2, 0, False, False, None, )
paddle.cummin(Tensor([0, 100],"float32"), )
paddle.searchsorted(sorted_sequence=Tensor([7],"float64"), values=Tensor([2, 2, 0],"float64"), right=True, )
paddle.Tensor.frexp(Tensor([4, 5, 0],"float32"), )
paddle.einsum("bnij,jbnd->ibnd", Tensor([11, 4, 0, 7],"float32"), Tensor([7, 11, 4, 8],"float32"), )
paddle.nn.functional.max_pool3d(Tensor([0, 3, 32, 32, 32],"float32"), kernel_size=2, stride=None, padding=0, return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 3, 16, 16],"float32"), Tensor([3, 2, 3, 0],"float32"), bias=Tensor([6],"float32"), padding="valid", output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=3, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[500,1,1,], )
paddle.expand(x=Tensor([1, 1, 1],"int64"), shape=Tensor([0],"int32"), )
paddle.nn.functional.max_pool2d(Tensor([0, 3, 32, 32],"float32"), kernel_size=list[2,2,], stride=None, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([2, 0, 100, 100],"float32"), pad=list[1,2,3,4,], mode="reflect", value=0.0, data_format="NCHW", name="shape", )
paddle.nanmedian(Tensor([0, 3, 4, 5],"float32"), axis=list[0,1,3,], keepdim=False, mode="min", )
paddle.Tensor.tile(Tensor([1, 0],"float32"), tuple(18,1,), )
paddle.incubate.nn.functional.fused_feedforward(Tensor([0, 1, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=True, )
paddle.Tensor.tile(Tensor([3938, 1, 0],"float32"), list[1,121,1,], )
paddle.nn.functional.conv3d_transpose(Tensor([4, 8, 8, 8, 4],"float32"), Tensor([4, 0, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding="valid", stride=tuple(1,2,1,), dilation=tuple(2,1,1,), groups=2, data_format="NDHWC", )
paddle.slice(Tensor([2, 0, 104],"float32"), list[0,1,2,], list[0,0,88,], list[2,2,104,], )
paddle.meshgrid(Tensor([216],"float32"), Tensor([248],"float32"), Tensor([0],"float32"), Tensor([2],"float32"), )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([0, 128, 28, 28],"float32"), output_size=7, data_format="NCHW", name=None, )
paddle.einsum("blkd,bldq->blkq", Tensor([13, 5, 0, 1],"float32"), Tensor([13, 5, 0, 3],"float32"), )
paddle.nn.functional.pad(Tensor([0, 16, 14, 17, 384],"float16"), tuple(0,4,0,0,0,0,), data_format="NDHWC", )
paddle.nn.functional.pad(Tensor([0, 4, 5, 6],"complex64"), pad=list[1,2,2,1,], mode="replicate", value=0.0, data_format="NCHW", name=None, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 0, 2, 2],"float64"), Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float32"), weight=Tensor([1, 3, 2, 0],"float32"), bias=Tensor([1],"float32"), stride=1, padding=0, )
paddle.nn.functional.pad(Tensor([0, 64, 3],"float32"), tuple(1,0,), data_format="NCL", )
paddle.einsum("binh,tnh->bnit", Tensor([13, 0, 4, 4],"float32"), Tensor([4, 4, 4],"float32"), )
paddle.nn.functional.pad(Tensor([1, 3, 32, 224, 0],"float32"), tuple(0,1,0,0,0,0,), data_format="NCDHW", )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 32, 0],"float64"), output_size=list[2,3,3,], )
paddle.logaddexp(Tensor([0, 3, 2],"float16"), Tensor([0, 3, 2],"float32"), )
paddle.linalg.slogdet(Tensor([0, 5, 5],"complex128"), )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 0],"float64"), weight=Tensor([1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=2, padding=1, )
paddle.nn.functional.pad(Tensor([1183744, 1, 1, 0],"float32"), list[0,0,0,0,], )
paddle.nn.functional.max_pool2d(Tensor([2, 0, 32, 32],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=True, data_format="NCHW", name=None, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float64"), weight=Tensor([1, 3, 3, 0, 3],"float64"), bias=Tensor([1],"float64"), stride=2, padding=1, )
paddle.nn.functional.softmax(Tensor([2, 8, 0, 65],"float16"), -1, )
paddle.nn.functional.max_unpool1d(Tensor([0, 3, 8],"float64"), Tensor([0, 3, 8],"int32"), kernel_size=2, stride=None, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[128,1,1,], )
paddle.nn.functional.conv3d(Tensor([4, 3, 8, 0, 8],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[0,0,],list[1,1,],list[2,2,],list[2,2,],], stride=1, dilation=1, groups=1, data_format="NCDHW", )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float64"), weight=Tensor([6, 1, 0, 3, 3],"float64"), bias=Tensor([6],"float64"), stride=2, padding=0, groups=3, )
paddle.nn.functional.conv2d(Tensor([1, 1024, 256, 0],"float32"), Tensor([1024, 128, 3, 3],"float32"), padding=1, groups=8, )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[182,1,1,], )
paddle.nn.functional.conv2d(Tensor([1, 1024, 0, 128],"float32"), Tensor([1024, 256, 0, 3],"float32"), padding=1, groups=4, )
paddle.einsum("i, i", Tensor([0],"float64"), Tensor([0],"float64"), )
paddle.nn.functional.conv1d_transpose(Tensor([2, 3, 2],"float64"), Tensor([3, 2, 0],"float64"), bias=Tensor([2],"float64"), output_size=None, output_padding=0, padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.sequence_mask(Tensor([0, 3],"int64"), maxlen=10, dtype=VarType(int32), )
paddle.nn.functional.max_pool2d(Tensor([0, 256, 27, 27],"float32"), kernel_size=3, stride=1, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.temporal_shift(x=Tensor([4, 4, 0, 3],"float64"), seg_num=4, )
paddle.nn.functional.pad(Tensor([1, 0, 3],"float32"), list[1,0,], value=6, mode="constant", data_format="NCL", )
paddle.Tensor.tile(Tensor([121, 0],"float32"), list[4,1,1,], )
paddle.nn.functional.conv1d(Tensor([1, 1024, 0],"float32"), Tensor([1024, 1024, 3],"float32"), bias=Tensor([1024],"float32"), padding=1, stride=list[2,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.strided_slice(x=Tensor([3, 4, 5, 0],"float64"), axes=list[1,2,3,], starts=list[-3,0,2,], ends=list[3,2,4,], strides=list[1,1,1,], )
paddle.nn.functional.max_pool1d(x=Tensor([2, 3, 0],"float64"), kernel_size=3, stride=4, padding=0, )
paddle.nn.functional.softmax(Tensor([0, 4, 7, 7],"float32"), axis=3, )
paddle.Tensor.__getitem__(Tensor([3, 3, 0],"float32"), tuple(slice(1,2,None),slice(2,None,None),slice(None,None,-1),), )
paddle.nn.functional.local_response_norm(Tensor([0, 3, 40, 40],"float32"), 5, 0.0001, 0.75, 1.0, "NCHW", None, )
paddle.Tensor.tile(Tensor([8, 1, 1, 0],"float32"), list[1,5,4,1,], )
paddle.nn.functional.avg_pool1d(Tensor([2, 3, 0],"float64"), 2, 1, 1, False, False, None, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 128, 0, 70],"float32"), Tensor([128, 128, 2, 2],"float32"), bias=None, padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.fractional_max_pool3d(Tensor([0, 3, 7, 7, 7],"float32"), output_size=5, kernel_size=None, random_u=0.5, return_mask=False, name=None, )
paddle.nn.quant.weight_only_linear(Tensor([0, 512],"float16"), weight=Tensor([512, 512],"int8"), weight_scale=Tensor([512],"float16"), weight_dtype="int8", )
paddle.nn.functional.conv1d_transpose(Tensor([4, 3, 16],"float32"), Tensor([3, 2, 0],"float32"), bias=Tensor([6],"float32"), output_size=None, output_padding=0, padding="valid", stride=list[1,], dilation=list[1,], groups=3, data_format="NCL", )
paddle.nonzero(x=Tensor([3, 0],"float32"), as_tuple=False, )
paddle.Tensor.lgamma(Tensor([100, 0, 100],"float64"), )
paddle.nn.functional.conv1d(Tensor([2, 3, 0],"float64"), Tensor([1, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=0, stride=list[2,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.pad(Tensor([0, 3, 4, 4, 4],"float64"), list[1,1,1,1,1,1,], mode="replicate", data_format="NCDHW", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 256, 16, 16],"float32"), Tensor([256, 1, 0, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=256, output_size=None, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([0, 2, 3],"float64"), pad=list[1,2,], mode="reflect", value=0.0, data_format="NCL", name=None, )
paddle.nn.functional.conv2d(Tensor([1, 1024, 128, 0],"float32"), Tensor([1024, 256, 3, 3],"float32"), padding=1, groups=4, )
paddle.nn.quant.weight_quantize(Tensor([0, 256],"float16"), algo="weight_only_int4", group_size=-1, )
paddle.einsum("i,j->ij", Tensor([0],"float32"), Tensor([2],"float32"), )
paddle.Tensor.__getitem__(Tensor([3, 3, 0],"float32"), tuple(slice(None,None,-1),slice(None,None,-1),slice(None,None,-1),), )
paddle.nn.functional.max_pool3d(Tensor([2, 3, 8, 8, 0],"float64"), kernel_size=list[3,3,3,], stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[844,1,1,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[144,1,1,], )
paddle.nn.functional.sequence_mask(Tensor([2, 0],"int64"), maxlen=30, dtype=type(numpy.int32), )
paddle.nn.functional.max_unpool3d(Tensor([1, 0, 2, 2, 3],"float64"), Tensor([1, 0, 2, 2, 3],"int32"), kernel_size=2, stride=None, )
paddle.nn.functional.pad(Tensor([1, 0, 32, 224, 271],"float32"), tuple(0,1,0,0,0,0,), data_format="NCDHW", )
paddle.nn.functional.pad(Tensor([1, 0, 32, 242, 224],"float32"), tuple(0,0,0,2,0,0,), data_format="NCDHW", )
paddle.nn.functional.mse_loss(Tensor([5, 0],"float64"), label=Tensor([5, 0],"float64"), reduction="mean", name=None, )
paddle.nn.functional.avg_pool2d(x=Tensor([2, 0, 4, 4],"float64"), kernel_size=list[3,3,], stride=list[1,1,], padding=list[0,0,], )
paddle.nn.quant.weight_only_linear(Tensor([0, 32, 64],"float16"), Tensor([256, 64],"int8"), bias=Tensor([256],"float16"), weight_scale=Tensor([256],"float16"), weight_dtype="int8", group_size=-1, )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 2],"float32"), weight=Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([0],"float32"), stride=1, padding=list[1,0,1,], dilation=2, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([0, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([4, 134, 4, 128],"float16"), Tensor([0, 134, 4, 128],"float16"), Tensor([4, 134, 4, 128],"float16"), attn_mask=Tensor([4, 1, 134, 134],"float16"), dropout_p=0.0, training=True, is_causal=False, )
paddle.cartesian_prod(list[Tensor([2],"complex128"),Tensor([0],"complex128"),Tensor([3],"complex128"),], )
paddle.einsum("ijk,lk->ijl", Tensor([3, 0, 5],"float64"), Tensor([2, 5],"float64"), )
paddle.vision.ops.generate_proposals(Tensor([1, 0, 42, 63],"float32"), Tensor([1, 60, 42, 63],"float32"), Tensor([1, 2],"float32"), Tensor([39690, 4],"float32"), Tensor([39690, 4],"float32"), pre_nms_top_n=12000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 64, 0, 216],"float32"), Tensor([64, 128, 1, 1],"float32"), bias=None, padding=0, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.max_pool1d(x=Tensor([2, 0, 8],"float64"), kernel_size=2, stride=2, padding=1, )
paddle.Tensor.tile(Tensor([4, 1, 0],"float32"), tuple(1,12,1,), )
paddle.nn.functional.conv1d_transpose(Tensor([1, 128, 0],"float32"), Tensor([128, 64, 8],"float32"), bias=Tensor([64],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.incubate.nn.functional.fused_feedforward(x=Tensor([0, 2, 2],"float32"), linear1_weight=Tensor([2, 2],"float32"), linear2_weight=Tensor([2, 2],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 0, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[376,1,1,], )
paddle.nn.functional.pad(Tensor([30, 0, 32, 128],"float32"), list[2,3,2,3,], value=0, )
paddle.nn.functional.pad(Tensor([0, 3, 200, 150],"float64"), pad=list[10,10,10,10,], mode="constant", value=0.0, data_format="NCHW", )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float32"), weight=Tensor([1, 3, 0, 3],"float32"), bias=Tensor([1],"float32"), stride=list[2,1,], padding=list[1,2,], dilation=tuple(2,2,), )
paddle.einsum("a...a->a...", Tensor([0, 3, 2, 1, 4, 5],"float64"), )
paddle.nn.functional.conv1d(Tensor([1, 256, 28],"float32"), Tensor([256, 256, 0],"float32"), bias=Tensor([256],"float32"), padding=3, stride=list[1,], dilation=list[3,], groups=1, data_format="NCL", )
paddle.nn.functional.kl_div(Tensor([0, 20],"float64"), Tensor([0, 20],"float64"), "batchmean", False, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[137,1,1,], )
paddle.kthvalue(Tensor([2, 0, 10],"float64"), 2, 2, )
paddle.nn.functional.pad(Tensor([3, 0, 5],"complex128"), pad=list[1,1,], mode="constant", value=100, data_format="NCL", name=None, )
paddle.einsum("bhwc,wkc->bhwk", Tensor([48, 32, 32, 64],"float32"), Tensor([32, 0, 64],"float32"), )
paddle.nn.functional.conv2d(Tensor([2, 192, 4, 0],"float32"), Tensor([384, 192, 1, 0],"float32"), None, list[2,2,], 0, list[1,1,], 1, "NCHW", )
paddle.nn.functional.avg_pool3d(Tensor([0, 1, 40, 40, 7],"float32"), kernel_size=tuple(1,1,5,), stride=1, )
paddle.slice(Tensor([2, 3, 0],"float32"), axes=list[1,], starts=list[1,], ends=list[2,], )
paddle.nn.functional.lp_pool1d(Tensor([2, 32, 0],"float32"), norm_type=7, kernel_size=2, stride=2, padding=list[1,], ceil_mode=True, data_format="NLC", )
paddle.einsum("i,ij->", Tensor([2],"float64"), Tensor([2, 0],"float64"), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[195,1,1,], )
paddle.nn.functional.max_unpool3d(Tensor([1, 0, 4, 5, 6],"float64"), Tensor([1, 0, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
paddle.nn.functional.pad(Tensor([0, 3, 4, 4],"float64"), list[1,1,1,1,], mode="reflect", data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 0, 128, 2],"float32"), list[4,1,1,1,], )
paddle.nn.functional.max_pool1d(Tensor([0, 3, 8],"float64"), 1, 1, 0, False, False, None, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[1662,1,1,], )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[553,1,1,], )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[250,1,1,], )
paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 512],"int32"), bias=Tensor([512],"float16"), dequant_scales=Tensor([0],"float32"), shift=Tensor([256],"float16"), smooth=Tensor([256],"float16"), act_method="geglu", compute_dtype="fp16", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.Tensor.imag(Tensor([16, 0, 511],"complex64"), )
paddle.Tensor.tile(Tensor([16660, 0],"float32"), list[1,49,], )
paddle.Tensor.tile(Tensor([27540, 0],"float32"), list[1,143,], )
paddle.nn.functional.conv1d_transpose(Tensor([0, 3, 2],"float64"), Tensor([3, 1, 3],"float64"), bias=Tensor([1],"float64"), output_size=None, output_padding=0, padding=list[1,], stride=list[1,], dilation=list[2,], groups=1, data_format="NCL", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[268,1,1,], )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([0, 3, 4, 4],"float32"), output_size=list[3,3,], data_format="NCHW", name=None, )
paddle.einsum("bnij,bjnd->bind", Tensor([0, 4, 2, 2],"float32"), Tensor([0, 2, 4, 4],"float32"), )
paddle.nn.functional.conv2d(Tensor([16, 3, 268, 0],"float32"), weight=Tensor([3, 1, 13, 0],"float32"), groups=3, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float32"), weight=Tensor([3, 1, 3, 3],"float32"), bias=Tensor([0],"float32"), stride=1, padding=0, dilation=1, )
paddle.nn.functional.max_pool3d(Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[1,2,3,], padding=list[0,0,0,], return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[173,1,1,], )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 1, 64],"float16"), Tensor([1, 2048, 1, 64],"float16"), Tensor([1, 2048, 1, 0],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), is_causal=True, )
paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1],"int32"), Tensor([1, 0],"int32"), mask=Tensor([1, 1, 50, 50],"float16"), scale=0.125, )
paddle.nn.functional.pixel_shuffle(Tensor([2, 0, 4, 9],"float64"), 3, "NHWC", None, )
paddle.vision.ops.prior_box(Tensor([4, 96, 0, 80],"float32"), Tensor([4, 3, 0, 640],"float32"), list[16.0,24.0,], list[], list[1.0,], list[0.1,0.1,0.2,0.2,], False, False, list[8.0,8.0,], 0.5, False, None, )
paddle.nn.functional.l1_loss(Tensor([10, 0],"float32"), Tensor([10, 0],"float32"), )
paddle.nn.functional.avg_pool2d(x=Tensor([2, 0, 4, 4],"float64"), kernel_size=list[3,3,], stride=list[1,1,], padding=list[1,1,], exclusive=False, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 0],"float32"), Tensor([6, 8, 3, 3],"float32"), bias=Tensor([8],"float32"), padding="same", output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv1d(x=Tensor([2, 3, 4],"float32"), weight=Tensor([1, 3, 0],"float32"), bias=Tensor([1],"float32"), stride=2, padding=1, )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[345,1,1,], )
paddle.nn.functional.temporal_shift(x=Tensor([6, 4, 2, 0],"float32"), seg_num=2, shift_ratio=0.2, )
paddle.isin(Tensor([0, 8],"bfloat16"), Tensor([0, 3],"bfloat16"), False, True, )
paddle.nn.functional.pad(Tensor([1, 0, 3, 2],"float32"), pad=list[0,1,0,0,], mode="constant", value=-1000000.0, data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[270,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 0],"float32"), Tensor([3, 1, 3, 3],"float32"), bias=None, padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.log_softmax(x=Tensor([2, 3, 0],"float64"), axis=-1, )
paddle.Tensor.tile(Tensor([2, 0, 4],"float32"), list[1,200,1,], )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 0, 7, 7],"float32"), output_size=list[3,3,3,], )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 0],"float32"), Tensor([3, 1, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=1, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.avg_pool3d(Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
paddle.allclose(Tensor([0],"float16"), Tensor([0],"float16"), atol=0.001, )
paddle.nn.functional.conv2d(Tensor([1, 1, 32, 32],"float32"), Tensor([6, 1, 0, 3],"float32"), bias=Tensor([6],"float32"), padding=1, stride=list[1,1,], dilation=list[1,1,], groups=1, data_format="NCHW", )
paddle.vision.ops.generate_proposals(Tensor([1, 15, 40, 60],"float32"), Tensor([1, 60, 0, 60],"float32"), Tensor([1, 2],"float32"), Tensor([36000, 4],"float32"), Tensor([36000, 4],"float32"), pre_nms_top_n=12000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.einsum("ibnd,jbnd->bnij", Tensor([0, 10, 4, 8],"float32"), Tensor([7, 10, 4, 8],"float32"), )
paddle.Tensor.subtract(Tensor([2, 0, 4],"float32"), Tensor([2, 0, 4],"float32"), )
paddle.nn.functional.avg_pool1d(Tensor([2, 0, 8],"float64"), 2, 1, 1, False, False, None, )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 0],"float32"), weight=Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=list[1,0,1,], dilation=1, )
paddle.nn.functional.pad(Tensor([0, 14701, 3],"float32"), list[1,0,], value=4, mode="constant", data_format="NCL", )
paddle.vision.ops.generate_proposals(Tensor([2, 3, 4, 4],"float32"), Tensor([2, 12, 4, 4],"float32"), Tensor([2, 3],"float32"), Tensor([4, 0, 3, 4],"float32"), Tensor([4, 4, 3, 4],"float32"), pre_nms_top_n=10, post_nms_top_n=5, return_rois_num=True, )
paddle.Tensor.tile(Tensor([14, 0, 768],"float16"), tuple(1,8,1,), )
paddle.nn.functional.temporal_shift(Tensor([128, 1024, 14, 0],"float16"), 8, 0.125, )
paddle.Tensor.tile(Tensor([6942, 1, 0],"float32"), list[1,136,1,], )
paddle.nn.functional.max_pool2d(Tensor([0, 3, 32, 32],"float64"), kernel_size=list[3,3,], stride=None, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.allclose(x=Tensor([2, 2, 0, 3],"float64"), y=Tensor([2, 2, 0, 3],"float64"), rtol=-3.0, atol=-2.0, )
paddle.slice(Tensor([0, 1, 104],"float32"), list[0,1,2,], list[0,0,72,], list[3,1,88,], )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[680,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([4, 3, 7, 7],"float32"), Tensor([3, 6, 0, 5],"float32"), bias=Tensor([6],"float32"), padding=2, output_padding=list[1,1,], stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 0, 2],"float64"), Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.temporal_shift(x=Tensor([6, 0, 2, 2],"float32"), seg_num=2, shift_ratio=0.2, )
paddle.nn.functional.softmax(Tensor([13, 0, 5, 1, 7],"float32"), )
paddle.slice(Tensor([0, 1, 104],"float32"), list[0,1,2,], list[0,0,16,], list[16,1,32,], )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 1, 2, 40],"float16"), Tensor([1, 1, 2, 40],"float16"), Tensor([1, 1, 0, 40],"float16"), attn_mask=Tensor([1, 2, 1, 1],"float16"), )
paddle.slice(Tensor([13, 9, 0],"float32"), axes=list[1,], starts=list[2,], ends=list[9,], )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 100, 1, 64],"float16"), Tensor([0, 100, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 100],"float16"), is_causal=False, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 0, 4, 3],"float64"), output_size=tuple(3,3,), data_format="NHWC", name=None, )
paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float16"), residual=Tensor([0, 256],"float16"), residual_alpha=0.69204696, quant_scale=0.15, quant_round_type=1, quant_max_bound=127, quant_min_bound=-127, )
paddle.nn.functional.max_unpool2d(Tensor([1, 0, 2, 2],"float32"), Tensor([1, 0, 2, 2],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCHW", output_size=None, name=None, )
paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([0, 4, 5, 1, 8],"float32"), Tensor([0, 4, 5, 7, 8],"float32"), )
paddle.nn.functional.softmax(Tensor([4, 1, 81, 0, 311],"float32"), axis=2, )
paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 0],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.001479289960116148, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.nn.functional.conv2d(Tensor([1, 3, 224, 224],"float32"), Tensor([3, 3, 0, 3],"float32"), Tensor([3],"float32"), list[3,2,], 0, list[1,1,], 1, "NCHW", )
paddle.Tensor.__setitem__(Tensor([6, 0, 4, 3],"bool"), list[list[2,3,4,],list[1,2,5,],], 100, )
paddle.nn.functional.avg_pool2d(Tensor([2, 0, 32, 32],"float32"), kernel_size=list[2,2,], stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )
paddle.nn.functional.avg_pool3d(Tensor([0, 1, 7, 3, 1600],"float32"), kernel_size=tuple(5,1,1,), stride=1, )
paddle.nn.functional.pad(x=Tensor([3, 3, 0],"float32"), pad=list[1,1,], mode="constant", value=0.0, data_format="NCL", )
paddle.Tensor.__setitem__(Tensor([6, 5, 0, 3],"complex128"), list[list[2,3,4,],list[1,2,5,],], 100, )
paddle.nn.functional.max_pool2d(Tensor([2, 0, 27, 27],"float32"), kernel_size=3, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([52, 4, 5, 1, 0],"float32"), Tensor([52, 4, 5, 7, 0],"float32"), )
paddle.nn.functional.l1_loss(Tensor([20, 0, 2],"float16"), Tensor([20, 0, 2],"float32"), reduction="sum", )
paddle.nn.functional.temporal_shift(x=Tensor([2, 2, 4, 0],"float64"), seg_num=2, )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 2, 2, 2, 3],"float32"), weight=Tensor([3, 0, 3, 3, 3],"float32"), bias=Tensor([3],"float32"), output_padding=1, stride=2, padding=list[1,1,1,], groups=3, data_format="NDHWC", dilation=1, )
paddle.nn.functional.max_pool1d(Tensor([0, 3, 8],"float64"), 2, 2, 1, False, False, None, )
paddle.nn.functional.margin_ranking_loss(Tensor([10, 0],"float64"), Tensor([10, 0],"float64"), Tensor([10, 0],"float64"), 0.2, "sum", None, )
paddle.nn.functional.pad(Tensor([1, 0, 15, 14, 384],"float32"), tuple(0,0,0,6,0,0,), data_format="NDHWC", )
paddle.nn.functional.avg_pool3d(Tensor([0, 8, 8, 8, 3],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NDHWC", name=None, )
paddle.einsum("sec,ecm->sm", Tensor([2, 60, 2],"float32"), Tensor([60, 2, 0],"float32"), )
paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([0],"float32"), shift=None, smooth=None, quant_scale=0.0016999575309455395, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.nn.functional.conv3d(Tensor([4, 3, 8, 8, 0],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[0,0,],list[1,1,],list[2,2,],list[2,2,],], stride=1, dilation=1, groups=1, data_format="NCDHW", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float64"), Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([0],"float64"), padding=1, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[395,1,1,], )
paddle.nn.functional.max_pool3d(Tensor([0, 2, 6, 33, 33],"float32"), kernel_size=5, stride=5, padding=0, ceil_mode=True, return_mask=True, )
paddle.Tensor.tile(Tensor([100, 1, 0],"float32"), list[1,2,1,], )
paddle.Tensor.tile(Tensor([1, 5, 0],"float32"), list[2654,1,1,], )
paddle.strided_slice(x=Tensor([0, 4, 5, 6],"float32"), axes=list[1,2,3,], starts=list[-3,0,2,], ends=list[3,2,4,], strides=list[1,1,1,], )
paddle.einsum("bhwc,wkc->bhwk", Tensor([432, 0, 14, 64],"float32"), Tensor([14, 14, 64],"float32"), )
paddle.Tensor.tile(Tensor([8, 0, 512],"float32"), tuple(1,13,1,), )
paddle.nn.functional.conv2d(Tensor([1, 256, 128, 0],"float32"), Tensor([256, 256, 3, 0],"float32"), padding=1, groups=1, )
paddle.einsum("nbqa,ahc->nbqhc", Tensor([1, 3, 5, 6],"float32"), Tensor([6, 0, 4],"float32"), )
paddle.nn.functional.pad(Tensor([1, 3, 32, 0, 224],"float32"), tuple(0,0,0,1,0,0,), data_format="NCDHW", )
paddle.nn.functional.kl_div(Tensor([0, 20],"float64"), Tensor([0, 20],"float64"), "none", False, )
paddle.linalg.slogdet(Tensor([0, 3, 5, 5],"float32"), )
paddle.einsum("..., f -> ... f", Tensor([0],"float32"), Tensor([16],"float32"), )
paddle.masked_fill(Tensor([0, 40],"float32"), Tensor([40],"bool"), Tensor([1],"float32"), )
paddle.nn.functional.group_norm(Tensor([1, 1024, 0, 26],"float32"), num_groups=32, epsilon=1e-05, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), )
paddle.Tensor.tile(Tensor([1, 0, 256],"float32"), list[2,1,1,], )
paddle.nn.functional.avg_pool2d(Tensor([64, 0, 17, 17],"float32"), kernel_size=3, stride=1, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.vision.ops.distribute_fpn_proposals(Tensor([410, 0],"float32"), 2, 5, 4, 224, rois_num=Tensor([4],"int64"), )
paddle.nn.functional.conv2d(Tensor([4, 6, 16, 0],"float32"), Tensor([8, 3, 3, 3],"float32"), Tensor([8],"float32"), padding="same", stride=1, dilation=1, groups=2, data_format="NCHW", )
paddle.nn.functional.conv2d(Tensor([4, 16, 16, 3],"float32"), Tensor([5, 3, 0, 3],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[1,1,],list[2,2,],list[0,0,],], stride=1, dilation=1, groups=1, data_format="NHWC", )
paddle.Tensor.__rpow__(Tensor([0, 12],"float64"), 2, )
paddle.nn.functional.max_pool1d(x=Tensor([0, 1, 2],"float64"), kernel_size=2, stride=2, padding=0, return_mask=True, )
paddle.incubate.softmax_mask_fuse(x=Tensor([0, 8, 8, 1020],"float16"), mask=Tensor([0, 1, 8, 1020],"float16"), )
paddle.nn.functional.conv2d(Tensor([8, 128, 256, 256],"float32"), Tensor([128, 128, 3, 0],"float32"), bias=None, stride=1, padding=1, )
paddle.nn.functional.max_pool3d(Tensor([0, 3, 4, 4, 6],"float64"), kernel_size=2, stride=2, return_mask=True, )
paddle.nn.functional.pad(Tensor([1, 2, 0],"float32"), pad=list[1,2,], mode="constant", value=0.0, data_format="NCL", name=None, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 0, 1, 64],"float16"), Tensor([1, 0, 1, 64],"float16"), Tensor([1, 0, 1, 64],"float16"), attn_mask=Tensor([1, 0, 2048, 2048],"float16"), is_causal=False, )
paddle.einsum("bhlqk,bhkd->bhlqd", Tensor([13, 4, 3, 1, 1],"float32"), Tensor([13, 4, 0, 8],"float32"), )
paddle.nn.functional.channel_shuffle(Tensor([2, 0, 4, 4],"float64"), 3, "NCHW", None, )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 0],"float64"), weight=Tensor([1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=list[2,1,], padding=list[1,2,], dilation=tuple(2,2,), )
paddle.allclose(Tensor([1, 0],"float32"), Tensor([1, 0],"float32"), atol=1e-06, rtol=1e-06, )
paddle.nn.functional.mse_loss(Tensor([10, 0],"float32"), Tensor([10, 0],"float32"), "sum", )
paddle.nn.functional.softmax(Tensor([1008, 0, 7],"float32"), axis=-1, )
paddle.nn.functional.l1_loss(Tensor([1, 0, 256, 256],"float32"), Tensor([1, 0, 256, 256],"float32"), "mean", name=None, )
paddle.nn.functional.conv1d_transpose(Tensor([2, 2, 3],"float64"), Tensor([3, 0, 3],"float64"), bias=Tensor([3],"float64"), output_size=None, output_padding=1, padding=list[1,], stride=list[2,], dilation=list[1,], groups=3, data_format="NLC", )
paddle.nn.functional.temporal_shift(Tensor([128, 1024, 0, 14],"float16"), 8, 0.125, )
paddle.nn.functional.conv1d_transpose(Tensor([2, 3, 2],"float64"), Tensor([3, 0, 3],"float64"), bias=Tensor([1],"float64"), output_size=None, output_padding=0, padding=list[1,], stride=list[1,], dilation=list[2,], groups=1, data_format="NCL", )
paddle.allclose(Tensor([2, 3, 8, 0],"float32"), Tensor([2, 3, 8, 0],"float32"), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[929,1,1,], )
paddle.quantile(Tensor([5, 0, 4],"float64"), q=list[0.3,0.44,], axis=-2, )
paddle.nn.functional.pixel_shuffle(Tensor([2, 9, 4, 0],"float64"), 3, "NCHW", )
paddle.nn.functional.conv2d(Tensor([1, 1024, 10, 26],"float32"), Tensor([256, 1024, 1, 0],"float32"), None, list[1,1,], 0, list[1,1,], 1, )
paddle.nn.functional.pad(Tensor([1, 3, 184, 0],"float64"), pad=list[0,0,40,40,], mode="constant", value=0.0, data_format="NCHW", )
paddle.nn.functional.max_pool2d(Tensor([0, 192, 20, 20],"float32"), kernel_size=9, stride=1, padding=4, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.Tensor.__pow__(Tensor([0, 3, 2],"float16"), Tensor([0, 3, 2],"float32"), )
paddle.nn.functional.conv1d(Tensor([2, 3, 4],"float32"), Tensor([1, 3, 3],"float32"), bias=Tensor([0],"float32"), padding=0, stride=list[2,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv2d_transpose(Tensor([2, 128, 124, 108],"float32"), Tensor([128, 0, 2, 2],"float32"), bias=None, padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 512],"int32"), bias=Tensor([512],"float32"), dequant_scales=Tensor([512],"float32"), shift=Tensor([512],"float32"), smooth=Tensor([0],"float32"), act_method="gelu", compute_dtype="fp32", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[310,1,1,], )
paddle.nn.functional.max_pool1d(Tensor([0, 1, 2],"float64"), 2, 2, 0, True, False, None, )
paddle.nn.functional.max_pool2d(Tensor([0, 384, 32, 32],"float32"), kernel_size=9, stride=1, padding=4, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 0, 4, 4],"float64"), weight=Tensor([1, 3, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=2, padding=0, )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[216,1,1,], )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 8, 96],"float16"), Tensor([2, 0, 8, 96],"float16"), Tensor([2, 100, 8, 96],"float16"), attn_mask=None, is_causal=False, )
paddle.Tensor.tile(Tensor([1, 0],"float32"), list[1,28,], )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 3, 7, 0],"float32"), output_size=5, data_format="NCHW", name=None, )
paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), Tensor([256],"float32"), Tensor([0],"float32"), 1e-05, begin_norm_axis=1, quant_scale=0.15, quant_round_type=1, quant_max_bound=127, quant_min_bound=-127, )
paddle.subtract(Tensor([0],"float64"), Tensor([0],"float64"), name="Normal_log_prob", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[612,1,1,], )
paddle.reverse(Tensor([1, 1, 0],"float32"), axis=1, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[514,1,1,], )
paddle.nn.functional.sequence_mask(Tensor([2, 0],"int64"), maxlen=10, dtype=VarType(float64), )
paddle.flip(Tensor([0, 3],"float32"), 1, )
paddle.nn.functional.pad(Tensor([12288, 1, 1, 0],"float32"), list[0,1,0,1,], )
paddle.Tensor.tile(Tensor([1, 7, 0],"float32"), list[704,1,1,], )
paddle.nn.functional.temporal_shift(Tensor([240, 0, 14, 14],"float32"), 8, 0.125, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([0, 17980, 3],"float32"), list[1,0,], value=3, mode="constant", data_format="NCL", )
paddle.nn.functional.avg_pool1d(Tensor([2, 0, 8],"float32"), 2, 2, 0, True, False, None, )
paddle.nonzero(Tensor([3, 0],"float32"), True, )
paddle.nn.functional.max_pool1d(Tensor([0, 3, 8],"float64"), 2, 1, 0, False, True, None, )
paddle.where(Tensor([2, 0],"float32"), )
paddle.nn.functional.pad(Tensor([1, 0, 204, 152],"float64"), pad=list[1,3,2,4,], mode="constant", value=0.0, data_format="NCHW", )
paddle.Tensor.flip(Tensor([2, 0],"float32"), 1, )
paddle.nn.functional.pad(Tensor([1, 1, 2, 0],"float64"), pad=list[1,0,1,2,], mode="constant", value=0.0, data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([0, 1, 10, 21],"float32"), pad=list[0,1,0,0,], mode="constant", value=-10000.0, data_format="NCHW", name=None, )
paddle.nn.functional.conv2d(Tensor([3, 16, 2, 2],"float32"), Tensor([16, 16, 3, 0],"float32"), None, list[2,2,], 1, list[1,1,], 1, "NCHW", )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 0, 4],"float32"), weight=Tensor([1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=2, padding=0, )
paddle.nn.functional.max_pool2d(Tensor([1, 0, 6, 6],"float32"), kernel_size=2, stride=2, padding=0, return_mask=True, )
paddle.nn.functional.pad(Tensor([3, 1, 3, 0, 1600],"float32"), pad=list[0,0,0,0,2,2,], data_format="NCDHW", )
paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 3, 5, 2, 4],"float16"), Tensor([1, 3, 0, 2, 4],"float16"), )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 100, 1, 64],"float16"), Tensor([2, 100, 0, 64],"float16"), attn_mask=Tensor([2, 1, 1, 100],"float16"), is_causal=False, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 0, 4, 4],"float32"), weight=Tensor([1, 3, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=2, padding=0, )
paddle.nn.functional.conv2d_transpose(Tensor([0, 4, 16, 16],"float32"), Tensor([4, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,2,], stride=1, dilation=1, groups=2, data_format="NCHW", )
paddle.einsum("i,j", Tensor([3],"float64"), Tensor([0],"float64"), )
paddle.nn.functional.fractional_max_pool3d(x=Tensor([2, 0, 7, 7, 7],"float32"), output_size=list[3,3,3,], random_u=0.3, )
paddle.nn.functional.max_pool2d(Tensor([0, 3, 6, 6],"float32"), kernel_size=2, stride=2, padding=0, return_mask=True, )
paddle.nn.functional.pad(Tensor([1, 0, 14, 15, 384],"float32"), tuple(0,1,0,0,0,0,), data_format="NDHWC", )
paddle.nn.functional.pad(Tensor([30, 64, 0, 112],"float32"), list[0,1,0,1,], value=-math.inf, )
paddle.nn.functional.softmax(Tensor([2, 0, 4],"float32"), axis=0, dtype="float64", )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([0, 160, 16, 32],"float32"), output_size=4, data_format="NCHW", name=None, )
paddle.nn.functional.log_softmax(Tensor([2, 3, 0, 5],"float32"), 1, )
paddle.nn.functional.max_pool3d(x=Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([4, 16, 16, 4],"float32"), Tensor([4, 0, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding="valid", stride=tuple(1,2,), dilation=tuple(2,1,), groups=2, data_format="NHWC", )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([0, 512, 7, 7],"float32"), output_size=tuple(7,7,), data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([1, 2, 3, 0, 5],"complex128"), pad=list[1,2,1,1,3,4,], mode="constant", value=100, data_format="NCDHW", )
paddle.nn.functional.max_pool1d(Tensor([0, 3, 32],"float32"), kernel_size=2, stride=2, padding="SAME", )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[137,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([4, 256, 16, 16],"float32"), Tensor([256, 1, 4, 0],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=256, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d(Tensor([8, 256, 0, 128],"float32"), Tensor([256, 256, 3, 3],"float32"), bias=None, stride=1, padding=1, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 0],"float32"), Tensor([6, 8, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[1,2,], dilation=list[2,2,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([3, 64, 16, 0],"float32"), pad=tuple(1,1,1,1,), mode="constant", value=0.0, data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([1, 7974, 0],"float32"), list[1,0,], value=1, mode="constant", data_format="NCL", )
paddle.nn.functional.avg_pool2d(Tensor([2, 0, 32, 32],"float32"), kernel_size=2, stride=None, padding="SAME", )
paddle.mm(input=Tensor([7],"float64"), mat2=Tensor([0],"float64"), )
paddle.nn.functional.conv3d(Tensor([4, 6, 8, 0, 8],"float32"), Tensor([12, 1, 3, 0, 3],"float32"), None, padding="valid", stride=1, dilation=1, groups=6, data_format="NCDHW", )
paddle.nn.functional.mse_loss(Tensor([0],"float32"), Tensor([0],"float32"), )
paddle.nn.functional.conv1d(Tensor([2, 3, 0],"float32"), Tensor([1, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[2,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.Tensor.tile(Tensor([1929, 0, 1],"float32"), list[1,90,1,], )
paddle.nn.functional.conv1d(Tensor([13, 256, 2048],"float32"), Tensor([20, 256, 0],"float32"), bias=None, padding=2, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.max_pool3d(x=Tensor([0, 320, 4, 56, 56],"float32"), kernel_size=list[1,1,1,], stride=list[1,1,1,], padding=list[0,0,0,], data_format="NCDHW", )
paddle.nn.functional.avg_pool2d(Tensor([0, 3, 4, 4],"float64"), kernel_size=list[3,3,], stride=tuple(1,1,), padding=tuple(0,0,), ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.softmax(Tensor([2, 3, 0],"float32"), axis=-1, dtype="float64", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 4, 16, 16],"float32"), Tensor([4, 3, 3, 0],"float32"), Tensor([6],"float32"), output_size=None, padding=list[list[0,0,],list[0,0,],list[1,1,],list[2,2,],], stride=1, dilation=1, groups=2, data_format="NCHW", )
paddle.nn.functional.local_response_norm(x=Tensor([0, 40, 40, 3],"float32"), size=5, data_format="NHWC", )
paddle.nn.functional.max_pool3d(Tensor([2, 3, 8, 0, 8],"float64"), kernel_size=list[3,3,3,], stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[679,1,1,], )
paddle.nn.functional.pad(Tensor([0, 8141, 3],"float32"), list[1,0,], value=0, mode="constant", data_format="NCL", )
paddle.nn.functional.pad(Tensor([1, 3, 32, 224, 0],"float32"), tuple(0,3,0,0,0,0,), data_format="NCDHW", )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[333,1,1,], )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([0, 3, 8, 32, 32],"float64"), output_size=list[1,1,1,], )
paddle.strided_slice(x=Tensor([3, 0, 5, 6],"float64"), axes=list[1,2,3,], starts=list[-3,0,2,], ends=list[3,2,4,], strides=list[1,1,1,], )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[162,1,1,], )
paddle.vision.ops.generate_proposals(Tensor([1, 15, 40, 60],"float32"), Tensor([1, 0, 40, 60],"float32"), Tensor([1, 2],"float32"), Tensor([36000, 4],"float32"), Tensor([36000, 4],"float32"), pre_nms_top_n=12000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.isin(Tensor([0, 8],"bfloat16"), Tensor([2, 3],"bfloat16"), False, True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 0, 1, 64],"float16"), Tensor([2, 0, 1, 64],"float16"), Tensor([2, 0, 1, 64],"float16"), attn_mask=Tensor([2, 0, 1, 100],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.Tensor.isnan(Tensor([0, 3],"float64"), )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 100, 1, 64],"float16"), Tensor([0, 100, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 100],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([0, 3, 5, 7, 7],"float32"), output_size=list[3,3,3,], data_format="NDHWC", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[3066,1,1,], )
paddle.nn.functional.prelu(x=Tensor([0, 3, 3],"float64"), weight=Tensor([3],"float64"), )
paddle.einsum("...jk->...kj", Tensor([3, 10, 0],"float64"), )
paddle.nn.functional.conv3d(x=Tensor([2, 4, 4, 4, 3],"float32"), weight=Tensor([2, 3, 3, 0, 3],"float32"), bias=Tensor([2],"float32"), stride=1, padding=0, data_format="NDHWC", )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 2, 2, 6],"float32"), weight=Tensor([6, 0, 3, 3],"float32"), bias=Tensor([3],"float32"), output_size=list[4,6,], stride=2, padding=list[1,0,], groups=3, data_format="NHWC", dilation=1, )
paddle.nn.functional.pad(Tensor([1, 2, 0, 4, 5],"complex64"), pad=list[1,2,1,1,3,4,], mode="constant", value=100, data_format="NCDHW", )
paddle.Tensor.tile(Tensor([1360, 0],"float32"), list[1,39,], )
paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 0],"int32"), bias=Tensor([512],"float16"), dequant_scales=Tensor([512],"float32"), shift=Tensor([512],"float16"), smooth=Tensor([512],"float16"), act_method="gelu", compute_dtype="fp16", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.nn.functional.max_pool2d(Tensor([0, 192, 27, 27],"float32"), kernel_size=3, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([1, 6, 0],"float32"), list[558,1,1,], )
paddle.nn.functional.conv2d(Tensor([1024, 1, 0, 131],"float32"), Tensor([1, 1, 0, 4],"float32"), )
paddle.reverse(Tensor([12, 4, 0],"float64"), axis=0, )
paddle.Tensor.tile(Tensor([1, 0, 256, 2],"float32"), list[4,1,1,1,], )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float64"), weight=Tensor([3, 1, 3, 0],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,], dilation=2, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv3d(Tensor([4, 8, 0, 8, 3],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[1,2,3,1,2,3,], stride=2, dilation=1, groups=1, data_format="NDHWC", )
paddle.nn.functional.flashmask_attention(Tensor([1, 2048, 8, 96],"float16"), Tensor([1, 2048, 8, 96],"float16"), Tensor([1, 2048, 8, 0],"float16"), startend_row_indices=Tensor([1, 1, 2048, 1],"int32"), causal=True, )
paddle.einsum("blq,bhlk->bhlqk", Tensor([1, 3, 5],"float64"), Tensor([1, 0, 3, 4],"float64"), )
paddle.nn.functional.pad(Tensor([1, 3, 32, 0, 297],"float32"), tuple(0,3,0,0,0,0,), data_format="NCDHW", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[473,1,1,], )
paddle.nn.functional.max_pool2d(Tensor([0, 64, 17, 17],"float32"), kernel_size=tuple(3,3,), stride=tuple(2,2,), padding=tuple(0,0,), return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([0, 25500, 1],"float32"), pad=list[512,512,], mode="reflect", data_format="NLC", )
paddle.nn.functional.max_pool3d(Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=tuple(1,0,0,), return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 512, 128, 0],"float32"), output_size=tuple(3,3,), data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[165,1,1,], )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 0, 4],"float64"), weight=Tensor([1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=list[2,1,], padding=1, dilation=list[2,2,], )
paddle.nn.functional.softmax(Tensor([0, 8, 1, 129],"float16"), -1, )
paddle.nn.functional.l1_loss(Tensor([2, 3, 32, 0],"float32"), Tensor([2, 3, 32, 0],"float32"), "mean", name=None, )
paddle.nn.functional.softmax(Tensor([2, 3, 0],"float32"), axis=1, dtype="float64", )
paddle.nn.functional.lp_pool1d(Tensor([2, 0, 32],"float16"), norm_type=5, kernel_size=5, stride=3, padding=list[0,], )
paddle.einsum("...->...", Tensor([0, 5, 5],"float64"), )
paddle.nn.functional.pad(x=Tensor([1, 1, 1, 2, 0],"float64"), pad=tuple(0,1,1,1,2,0,), mode="constant", value=0, data_format="NCDHW", )
paddle.nn.functional.softmax(Tensor([2, 0, 4, 5],"float32"), axis=0, )
paddle.nn.functional.conv2d_transpose(Tensor([1, 2048, 16, 0],"float32"), Tensor([2048, 512, 3, 3],"float32"), padding=0, stride=2, groups=4, )
paddle.nn.functional.pad(Tensor([1, 16, 0, 15, 384],"float32"), tuple(0,1,0,0,0,0,), data_format="NDHWC", )
paddle.einsum("xy,yz->xz", Tensor([4, 4],"complex64"), Tensor([4, 0],"complex64"), )
paddle.nn.functional.max_pool3d(Tensor([2, 3, 32, 32, 0],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float32"), weight=Tensor([3, 1, 0, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=0, dilation=1, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 16],"float32"), Tensor([6, 8, 0, 3],"float32"), bias=Tensor([8],"float32"), padding="same", output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[117,1,1,], )
paddle.nn.functional.max_pool1d(x=Tensor([0, 3, 8],"float32"), kernel_size=2, stride=2, padding=0, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 0, 96],"float16"), Tensor([2, 100, 0, 96],"float16"), Tensor([2, 100, 0, 96],"float16"), attn_mask=None, is_causal=False, )
paddle.searchsorted(sorted_sequence=Tensor([0],"float32"), values=Tensor([0],"float32"), )
paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 0, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 0, 2, 2],"float64"), weight=Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=1, dilation=1, )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 2],"float64"), weight=Tensor([3, 1, 3],"float64"), bias=Tensor([0],"float64"), stride=1, padding=list[1,], output_padding=0, groups=1, dilation=2, output_size=None, data_format="NCL", name=None, )
paddle.nn.functional.pad(Tensor([3, 4, 5, 0, 7],"complex64"), pad=list[1,1,1,1,1,1,], mode="constant", value=100, data_format="NCDHW", name=None, )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([0, 3, 8, 32, 32],"float64"), output_size=list[2,3,3,], data_format="NCDHW", name=None, )
paddle.nn.functional.conv3d_transpose(Tensor([0, 6, 8, 8, 8],"float32"), Tensor([6, 8, 1, 1, 1],"float32"), bias=Tensor([8],"float32"), padding=tuple(2,3,1,), output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.Tensor.tile(Tensor([1, 0, 768],"float32"), list[23,1,1,], )
paddle.matmul(Tensor([2],"float16"), Tensor([0],"float16"), False, False, )
paddle.einsum("i , j -> i j", Tensor([0],"float32"), Tensor([0],"float32"), )
paddle.nn.functional.conv2d_transpose(Tensor([1, 128, 128, 128],"float32"), Tensor([128, 0, 3, 3],"float32"), bias=Tensor([64],"float32"), padding=1, output_padding=1, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.avg_pool2d(Tensor([2, 3, 4, 0],"float64"), kernel_size=list[3,3,], stride=list[3,3,], padding=list[0,0,0,0,], ceil_mode=True, exclusive=False, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([20, 256, 0, 34],"float32"), Tensor([256, 1, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=256, output_size=None, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([3, 0, 5, 6, 7],"complex128"), pad=list[1,2,2,1,1,0,], mode="circular", value=0.0, data_format="NCDHW", name=None, )
paddle.nn.functional.conv3d(x=Tensor([2, 4, 4, 4, 3],"float32"), weight=Tensor([2, 3, 0, 3, 3],"float32"), bias=Tensor([2],"float32"), stride=1, padding=0, data_format="NDHWC", )
paddle.nn.functional.scaled_dot_product_attention(query=Tensor([2, 64, 0, 64],"float16"), key=Tensor([2, 64, 0, 64],"float16"), value=Tensor([2, 64, 0, 64],"float16"), is_causal=True, )
paddle.nn.functional.fractional_max_pool2d(x=Tensor([2, 0, 7, 7],"float32"), output_size=list[None,3,], random_u=0.6, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 0, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )
paddle.nonzero(x=Tensor([3, 0, 7],"float16"), )
paddle.nanmedian(Tensor([2, 3, 0, 5],"float32"), axis=list[0,2,1,3,], keepdim=False, )
paddle.nn.functional.max_pool1d(Tensor([0, 3, 8],"float64"), 3, 4, 0, False, False, None, )
paddle.nn.functional.max_pool3d(Tensor([1, 0, 4, 4, 6],"float32"), kernel_size=2, stride=2, return_mask=True, )
paddle.nn.functional.interpolate(x=Tensor([0, 3, 5, 7, 7],"float32"), mode="area", size=list[2,3,5,], )
paddle.nn.functional.pad(Tensor([1183744, 0, 1, 1],"float32"), list[0,0,0,0,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[165,1,1,], )
paddle.nn.functional.conv2d(Tensor([64, 1, 28, 28],"float32"), Tensor([6, 1, 3, 0],"float32"), bias=None, padding=1, stride=list[1,1,], dilation=list[1,1,], groups=1, data_format="NCHW", )
paddle.nn.functional.softmax(Tensor([0, 16],"float32"), axis=-1, )
paddle.where(Tensor([2, 0],"bool"), )
paddle.allclose(x=Tensor([0],"float64"), y=Tensor([0],"float64"), rtol=1e-06, atol=0.001, equal_nan=False, )
paddle.nonzero(x=Tensor([2, 10, 0],"float32"), )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[179,1,1,], )
paddle.nn.functional.conv2d(Tensor([4, 6, 0, 16],"float32"), Tensor([8, 3, 3, 3],"float32"), Tensor([8],"float32"), padding="same", stride=1, dilation=1, groups=2, data_format="NCHW", )
paddle.nn.functional.conv3d_transpose(Tensor([4, 8, 8, 8, 4],"float32"), Tensor([4, 1, 3, 3, 0],"float32"), Tensor([4],"float32"), output_size=None, padding="valid", stride=tuple(1,2,1,), dilation=1, groups=4, data_format="NDHWC", )
paddle.nn.functional.conv1d(Tensor([4, 6, 16],"float32"), Tensor([8, 6, 0],"float32"), bias=Tensor([8],"float32"), padding="valid", stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 1024, 32, 128],"bfloat16"), Tensor([1, 1024, 32, 0],"bfloat16"), Tensor([1, 1024, 32, 128],"bfloat16"), attn_mask=None, is_causal=True, )
paddle.nn.functional.conv1d(Tensor([13, 20, 0],"float32"), Tensor([256, 20, 0],"float32"), bias=None, padding=2, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.fractional_max_pool3d(x=Tensor([2, 3, 7, 0, 7],"float32"), output_size=list[3,None,3,], random_u=0.6, )
paddle.nn.functional.conv2d(Tensor([1, 1024, 0, 26],"float32"), Tensor([256, 1024, 0, 1],"float32"), None, list[1,1,], 0, list[1,1,], 1, )
paddle.subtract(Tensor([1, 3, 256, 0],"float32"), Tensor([1, 3, 256, 0],"float32"), )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float32"), weight=Tensor([3, 1, 3, 0],"float32"), bias=Tensor([1],"float32"), stride=1, padding=list[1,0,], dilation=1, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[117,1,1,], )
paddle.nn.functional.max_pool2d(x=Tensor([0, 3, 32, 32],"float64"), kernel_size=list[2,2,], )
paddle.einsum("ijk->kji", Tensor([0, 10, 3],"float64"), )
paddle.nn.functional.conv1d(Tensor([13, 7, 32],"float32"), Tensor([16, 32, 0],"float32"), bias=None, padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NLC", )
paddle.nanquantile(Tensor([0, 7, 6],"float64"), q=0.75, axis=list[0,2,], )
paddle.einsum("...,...->...", Tensor([0, 5, 5],"float64"), Tensor([0, 5, 5],"float64"), )
paddle.nn.functional.max_pool2d(Tensor([2, 3, 33, 0],"float32"), kernel_size=5, stride=5, padding=0, ceil_mode=True, return_mask=True, )
paddle.nn.functional.gelu(Tensor([0, 3],"float16"), approximate=True, )
paddle.einsum("m,d->md", Tensor([196],"float32"), Tensor([0],"float32"), )
paddle.slice(Tensor([1024, 0, 1024],"float16"), axes=list[0,], starts=list[640,], ends=list[768,], )
paddle.Tensor.tile(Tensor([1, 4, 0],"float32"), list[222,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([20, 64, 38, 0],"float32"), Tensor([64, 1, 8, 8],"float32"), bias=None, padding=2, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=64, output_size=None, data_format="NCHW", )
paddle.nn.functional.avg_pool2d(Tensor([0, 256, 10, 10],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.kl_div(Tensor([0, 20],"float64"), Tensor([0, 20],"float64"), "mean", False, )
paddle.nn.functional.conv2d_transpose(Tensor([1, 2048, 128, 128],"float32"), Tensor([2048, 128, 3, 0],"float32"), padding=0, stride=2, groups=8, )
paddle.nn.functional.max_pool2d(Tensor([64, 0, 1, 40],"float16"), kernel_size=tuple(1,1,), stride=1, padding=0, )
paddle.nn.functional.max_unpool3d(Tensor([1, 3, 2, 0, 3],"float64"), Tensor([1, 3, 2, 0, 3],"int32"), kernel_size=2, stride=None, )
paddle.einsum("nbqa,ahc->nbqhc", Tensor([1, 3, 5, 6],"float16"), Tensor([6, 2, 0],"float16"), )
paddle.nn.functional.conv3d(Tensor([4, 8, 8, 8, 3],"float32"), Tensor([5, 3, 0, 3, 3],"float32"), Tensor([5],"float32"), padding=list[1,2,1,], stride=1, dilation=1, groups=1, data_format="NDHWC", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 3, 7, 0],"float32"), Tensor([3, 6, 5, 5],"float32"), bias=Tensor([6],"float32"), padding=2, output_padding=list[1,1,], stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.kthvalue(Tensor([2, 0, 250],"float64"), 244, -1, )
paddle.kthvalue(Tensor([0, 200, 40],"float32"), k=2, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 1024, 32, 128],"bfloat16"), Tensor([0, 1024, 32, 128],"bfloat16"), Tensor([1, 1024, 32, 128],"bfloat16"), attn_mask=None, is_causal=True, )
paddle.nn.functional.channel_shuffle(Tensor([2, 9, 0, 4],"float64"), 3, "NCHW", None, )
paddle.nn.functional.l1_loss(Tensor([0, 499, 2],"float32"), Tensor([0, 499, 2],"float32"), "sum", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 3, 0, 16],"float32"), Tensor([3, 2, 3, 3],"float32"), bias=Tensor([6],"float32"), padding="valid", output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=3, output_size=None, data_format="NCHW", )
paddle.vision.ops.generate_proposals(Tensor([1, 15, 40, 60],"float32"), Tensor([1, 60, 40, 0],"float32"), Tensor([1, 2],"float32"), Tensor([36000, 4],"float32"), Tensor([36000, 4],"float32"), pre_nms_top_n=12000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.pad(Tensor([3, 1, 3, 0, 3],"float32"), pad=list[2,2,0,0,0,0,], data_format="NCDHW", )
paddle.nn.functional.max_pool3d(x=Tensor([2, 3, 8, 0, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=1, )
paddle.nn.functional.softmax(Tensor([2, 3, 0],"float64"), -1, name=None, )
paddle.nn.functional.channel_shuffle(Tensor([2, 0, 4, 9],"float64"), 3, "NHWC", )
paddle.Tensor.fill_diagonal_(Tensor([0, 3],"float64"), 4, 0, True, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 0, 4, 4],"float64"), output_size=list[3,3,], data_format="NCHW", name=None, )
paddle.cummin(Tensor([100, 0],"float32"), axis=0, )
paddle.nanmedian(Tensor([0, 3, 4, 5],"float32"), axis=tuple(1,2,3,), keepdim=False, mode="min", )
paddle.nn.functional.avg_pool2d(Tensor([32, 1024, 0, 14],"float32"), kernel_size=2, stride=2, padding="SAME", ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([1, 3, 0, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=list[2,2,1,], padding=1, dilation=2, )
paddle.allclose(x=Tensor([0],"float64"), y=Tensor([0],"float64"), rtol=1e-06, atol=0.001, equal_nan=True, )
paddle.nn.functional.conv2d_transpose(Tensor([0, 6, 16, 16],"float32"), Tensor([6, 8, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[2,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv1d(Tensor([1, 128, 0],"float32"), Tensor([128, 128, 3],"float32"), bias=Tensor([128],"float32"), padding=5, stride=list[1,], dilation=list[5,], groups=1, data_format="NCL", )
paddle.einsum("i...->...", Tensor([5, 10, 0, 3],"float64"), )
paddle.einsum("xy,yz->xz", Tensor([4, 0],"float32"), Tensor([4, 0],"float32"), )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float64"), weight=Tensor([3, 0, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=0, dilation=1, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 0, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )
paddle.nn.functional.conv2d_transpose(Tensor([20, 64, 76, 136],"float32"), Tensor([64, 0, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=64, output_size=None, data_format="NCHW", )
paddle.nn.functional.avg_pool3d(x=Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], )
paddle.Tensor.__sub__(Tensor([0, 1, 10285],"float32"), Tensor([0, 1, 10285],"float32"), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[4506,1,1,], )
paddle.nn.functional.pad(Tensor([2, 3, 32, 0],"float32"), list[2,3,2,3,], value=0, )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 2],"float32"), weight=Tensor([3, 0, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=1, dilation=1, )
paddle.nn.functional.conv1d(Tensor([2, 3, 4],"float64"), Tensor([1, 3, 0],"float64"), bias=Tensor([1],"float64"), padding=tuple(1,), stride=list[1,], dilation=list[2,], groups=1, data_format="NCL", )
paddle.Tensor.tile(Tensor([4, 0, 512],"float32"), tuple(1,6,1,), )
paddle.nn.functional.conv1d_transpose(Tensor([4, 6, 0],"float32"), Tensor([6, 4, 3],"float32"), bias=Tensor([8],"float32"), output_size=None, output_padding=0, padding="valid", stride=list[1,], dilation=list[1,], groups=2, data_format="NCL", )
paddle.Tensor.digamma(Tensor([5, 7, 8, 0],"float64"), )
paddle.isin(Tensor([0, 8],"bfloat16"), Tensor([0, 3],"bfloat16"), False, False, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 1024, 0, 18],"float32"), output_size=1, data_format="NCHW", name=None, )
paddle.nn.functional.pixel_shuffle(Tensor([2, 0, 4, 4],"float64"), 3, "NCHW", )
paddle.vision.ops.generate_proposals(Tensor([1, 3, 10, 14],"float32"), Tensor([1, 12, 10, 0],"float32"), Tensor([1, 2],"float32"), Tensor([420, 4],"float32"), Tensor([420, 4],"float32"), pre_nms_top_n=2000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.slice(Tensor([0, 1, 104],"float32"), list[0,1,2,], list[0,0,56,], list[3,1,72,], )
paddle.Tensor.isnan(Tensor([0],"float32"), )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 5, 0, 7],"float32"), output_size=5, )
paddle.nn.functional.conv2d_transpose(Tensor([0, 128, 124, 108],"float32"), Tensor([128, 128, 2, 2],"float32"), bias=None, padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([10285, 0],"float32"), list[1,99,], )
paddle.Tensor.tile(Tensor([1001, 1, 0],"float32"), list[1,4,1,], )
paddle.nn.functional.softmax(Tensor([0, 4, 10, 10],"float32"), axis=-1, dtype=Dtype(float32), )
paddle.nn.functional.pixel_shuffle(x=Tensor([2, 0, 4, 9],"float64"), upscale_factor=3, data_format="NHWC", )
paddle.slice(Tensor([16, 0, 104],"float32"), list[0,1,2,], list[0,0,8,], list[16,1,24,], )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 0, 8, 16],"bfloat16"), Tensor([1, 1024, 2, 16],"bfloat16"), Tensor([1, 1024, 2, 16],"bfloat16"), attn_mask=None, is_causal=True, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[121,1,1,], )
paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 0],"float16"), Tensor([256],"float16"), act_method="gelu", )
paddle.nn.functional.conv2d(Tensor([1, 1024, 256, 256],"float32"), Tensor([1024, 128, 0, 3],"float32"), padding=1, groups=8, )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 0, 32],"float64"), output_size=list[1,1,1,], )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 0, 7, 7],"float32"), output_size=5, data_format="NCDHW", name=None, )
paddle.Tensor.tile(Tensor([12240, 0],"float32"), list[1,97,], )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 16],"float32"), Tensor([6, 0, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=list[list[0,0,],list[0,0,],list[1,2,],list[2,1,],], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([1, 0, 3, 4, 5],"complex128"), pad=list[1,2,1,1,3,4,], mode="constant", value=100, data_format="NDHWC", )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 7, 0],"float32"), output_size=list[3,3,3,], data_format="NCDHW", name=None, )
paddle.nn.functional.l1_loss(Tensor([2, 0],"float64"), Tensor([2, 0],"float64"), "mean", name=None, )
paddle.nn.functional.conv1d_transpose(Tensor([2, 3, 2],"float64"), Tensor([3, 1, 3],"float64"), bias=Tensor([0],"float64"), output_size=None, output_padding=0, padding=list[1,], stride=list[1,], dilation=list[2,], groups=1, data_format="NCL", )
paddle.nn.functional.max_pool2d(Tensor([13, 1, 0, 1],"float32"), tuple(2,1,), stride=tuple(2,1,), ceil_mode=True, )
paddle.nn.functional.max_pool2d(Tensor([2, 3, 0, 32],"float32"), kernel_size=2, stride=None, padding="SAME", return_mask=True, )
paddle.Tensor.tile(Tensor([1, 5, 0],"float32"), list[420,1,1,], )
paddle.nn.functional.fractional_max_pool2d(x=Tensor([0, 3, 7, 7],"float16"), output_size=list[3,3,], random_u=0.3, )
paddle.index_fill(Tensor([20, 40],"float32"), Tensor([0],"int64"), 1, -1, )
paddle.einsum("ij,jk", Tensor([0, 10],"float64"), Tensor([10, 6],"float64"), )
paddle.Tensor.__pow__(Tensor([4, 0, 2],"float16"), Tensor([4, 0, 2],"float32"), )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([0, 3, 8, 32, 32],"float64"), output_size=tuple(3,3,3,), data_format="NDHWC", name=None, )
paddle.slice(Tensor([16, 0, 104],"float32"), list[0,1,2,], list[0,0,72,], list[16,2,88,], )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 0],"float32"), weight=Tensor([3, 1, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=list[1,0,], dilation=2, )
paddle.vision.ops.generate_proposals(Tensor([1, 9, 12, 9],"float32"), Tensor([1, 36, 12, 9],"float32"), Tensor([1, 2],"float32"), Tensor([0, 4],"float32"), Tensor([972, 4],"float32"), pre_nms_top_n=4000, post_nms_top_n=4000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.pad(Tensor([4, 0, 6],"float64"), pad=list[2,2,], mode="constant", value=0.0, data_format="NCL", name=None, )
paddle.nn.functional.conv2d_transpose(x=Tensor([0, 3, 2, 2],"float32"), weight=Tensor([3, 1, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=0, dilation=1, )
paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 0, 2],"float32"), linear1_weight=Tensor([2, 2],"float32"), linear2_weight=Tensor([2, 2],"float32"), dropout1_rate=0, dropout2_rate=0, )
paddle.Tensor.tile(Tensor([10285, 0],"float32"), list[1,25,], )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[310,1,1,], )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 0, 16, 32],"float16"), output_size=4, data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([0, 1, 256, 256],"float32"), list[2,2,2,2,], )
paddle.nn.functional.mse_loss(Tensor([0, 3, 10, 10],"float32"), Tensor([0, 3, 10, 10],"float32"), "sum", )
paddle.nn.functional.max_pool2d(Tensor([12, 0, 40, 40],"float16"), 3, stride=1, padding=1, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([0, 27860, 3],"float32"), list[1,0,], value=6, mode="constant", data_format="NCL", )
paddle.incubate.nn.functional.fused_feedforward(Tensor([0, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="relu", pre_layer_norm=False, )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 2],"float64"), weight=Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([0],"float64"), stride=1, padding=1, dilation=1, )
paddle.nn.functional.conv1d(Tensor([1, 128, 112],"float32"), Tensor([128, 128, 0],"float32"), bias=Tensor([128],"float32"), padding=9, stride=list[1,], dilation=list[3,], groups=1, data_format="NCL", )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float32"), weight=Tensor([3, 0, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=list[1,0,], dilation=1, )
paddle.nn.functional.l1_loss(Tensor([0, 10, 5],"float32"), Tensor([0, 10, 5],"float32"), reduction="sum", )
paddle.meshgrid(list[Tensor([140],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),], )
paddle.nn.functional.conv1d(Tensor([4, 6, 16],"float32"), Tensor([8, 6, 0],"float32"), bias=Tensor([8],"float32"), padding="same", stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.max_pool3d(Tensor([2, 3, 0, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,2,1,], return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.nn.functional.fractional_max_pool3d(x=Tensor([2, 0, 7, 7, 7],"float32"), output_size=5, random_u=0.5, )
paddle.nanmedian(Tensor([0, 5],"float64"), axis=1, mode="min", )
paddle.slice(Tensor([1024, 2, 0],"float16"), axes=list[0,], starts=list[384,], ends=list[512,], )
paddle.nn.functional.pad(Tensor([0, 4, 5],"complex64"), pad=list[1,1,], mode="constant", value=100, data_format="NCL", name=None, )
paddle.nn.functional.pad(Tensor([0, 1, 40, 40],"float32"), pad=list[2,2,0,0,], )
paddle.einsum("ij,jk,kl", Tensor([3, 4],"float64"), Tensor([4, 5],"float64"), Tensor([5, 0],"float64"), )
paddle.allclose(x=Tensor([0],"float64"), y=Tensor([0],"float64"), rtol=1e-05, atol=0.001, equal_nan=False, )
paddle.einsum("nbqa,ahc->nbqhc", Tensor([1, 0, 5, 6],"float32"), Tensor([6, 2, 4],"float32"), )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float64"), weight=Tensor([3, 0, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=1, output_padding=0, dilation=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d(Tensor([1, 1024, 10, 0],"float32"), Tensor([256, 1024, 1, 0],"float32"), None, list[1,1,], 0, list[1,1,], 1, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 8, 8, 3],"float64"), weight=Tensor([3, 0, 5, 5],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[list[0,0,],list[1,2,],list[3,4,],list[0,0,],], output_padding=0, dilation=1, groups=1, output_size=None, data_format="NHWC", )
paddle.nn.functional.pad(Tensor([0, 2, 3, 4, 5],"complex128"), pad=list[1,2,1,1,3,4,], mode="constant", value=100, )
paddle.nn.functional.conv2d_transpose(Tensor([1, 256, 64, 64],"float32"), Tensor([256, 128, 0, 3],"float32"), bias=Tensor([128],"float32"), padding=1, output_padding=1, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([0, 1, 2, 3, 2],"float64"), pad=list[1,0,1,0,0,1,], mode="replicate", value=0.0, data_format="NCDHW", name=None, )
paddle.nn.functional.pad(Tensor([1, 16, 14, 15, 0],"float32"), tuple(0,1,0,0,0,0,), data_format="NDHWC", )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 2],"float64"), weight=Tensor([3, 2, 0, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=0, dilation=1, )
paddle.nn.functional.conv3d_transpose(Tensor([4, 4, 8, 8, 8],"float32"), Tensor([4, 0, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[list[0,0,],list[0,0,],list[1,1,],list[1,1,],list[2,2,],], stride=1, dilation=1, groups=2, data_format="NCDHW", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[1763,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([0, 3, 2, 2],"float32"), Tensor([3, 1, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 256, 0, 16],"float32"), Tensor([256, 1, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=256, output_size=None, data_format="NCHW", )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 0, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 0, 7],"float32"), output_size=list[3,3,3,], data_format="NDHWC", name=None, )
paddle.nanmedian(Tensor([2, 3, 0, 5],"float32"), axis=list[0,2,1,3,], keepdim=False, mode="min", )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 128, 32, 0],"float32"), output_size=tuple(3,3,), data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([0, 1, 40, 40],"float32"), pad=list[0,0,2,2,], )
paddle.nn.functional.pad(x=Tensor([3, 2, 1, 0],"float64"), pad=list[1,1,2,3,], mode="constant", value=2.0, data_format="NCHW", )
paddle.nn.functional.max_unpool3d(Tensor([1, 3, 2, 2, 0],"float64"), Tensor([1, 3, 2, 2, 0],"int32"), kernel_size=2, stride=None, )
paddle.nn.functional.conv2d_transpose(Tensor([1, 512, 0, 16],"float32"), Tensor([512, 512, 3, 3],"float32"), padding=0, stride=2, groups=1, )
paddle.nn.functional.pad(Tensor([3, 0, 5, 6, 7],"complex64"), pad=list[1,2,2,1,1,0,], mode="reflect", value=0.0, data_format="NCDHW", name=None, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[451,1,1,], )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[192,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 16],"float32"), Tensor([6, 0, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[2,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv3d(Tensor([4, 8, 8, 8, 3],"float32"), Tensor([5, 3, 3, 0, 3],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[1,1,],list[2,2,],list[1,1,],list[0,0,],], stride=1, dilation=1, groups=1, data_format="NDHWC", )
paddle.nn.functional.pad(Tensor([0, 16, 14, 18, 384],"float32"), tuple(0,3,0,0,0,0,), data_format="NDHWC", )
paddle.roll(Tensor([2, 4, 0],"float64"), Tensor([3],"int64"), list[0,1,2,], name=None, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 0, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
paddle.einsum("...qk,...kd->...qd", Tensor([52, 4, 3, 1, 2],"float32"), Tensor([52, 4, 3, 2, 0],"float32"), )
paddle.nn.functional.max_pool2d(Tensor([1, 0, 17, 257],"float32"), tuple(3,3,), tuple(2,2,), tuple(0,0,), False, )
paddle.nn.functional.pad(Tensor([2, 0, 8, 8],"float32"), list[0,1,0,1,], value=0, )
paddle.nn.functional.conv2d(Tensor([1, 256, 128, 128],"float32"), Tensor([256, 256, 3, 0],"float32"), padding=1, groups=1, )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[1,100,1,], )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 0, 4],"float32"), weight=Tensor([6, 1, 3, 3],"float32"), bias=Tensor([6],"float32"), stride=2, padding=0, groups=3, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([1, 3, 3, 0, 3],"float32"), bias=Tensor([1],"float32"), stride=2, padding=0, )
paddle.nn.functional.pad(Tensor([1536, 1, 1, 0],"float32"), list[0,1,0,1,], )
paddle.searchsorted(sorted_sequence=Tensor([0, 5],"float32"), values=Tensor([0, 3],"float32"), )
paddle.meshgrid(Tensor([216],"float32"), Tensor([248],"float32"), Tensor([1],"float32"), Tensor([0],"float32"), )
paddle.nn.functional.pad(Tensor([0, 1, 2, 2, 3],"float64"), pad=list[1,1,1,0,1,0,], mode="reflect", value=0.0, data_format="NCDHW", name=None, )
paddle.nn.functional.pad(Tensor([1, 16, 14, 0, 384],"float32"), tuple(0,4,0,0,0,0,), data_format="NDHWC", )
paddle.nn.functional.softmax(Tensor([2, 0, 4],"float32"), axis=0, dtype="float32", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[616,1,1,], )
paddle.nn.functional.scaled_dot_product_attention(query=Tensor([2, 64, 0, 64],"float16"), key=Tensor([2, 64, 12, 64],"float16"), value=Tensor([2, 64, 12, 64],"float16"), is_causal=True, )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 7, 0],"float32"), output_size=list[3,3,3,], data_format="NDHWC", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 4, 3, 3],"float64"), Tensor([4, 2, 0, 1],"float64"), groups=1, )
paddle.Tensor.tile(Tensor([8, 1, 0],"float32"), tuple(1,7,1,), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[114,1,1,], )
paddle.nn.functional.conv2d(Tensor([1, 1024, 256, 0],"float32"), Tensor([24, 128, 1, 1],"float32"), padding=0, groups=8, )
paddle.nn.functional.mse_loss(Tensor([0, 96, 2],"float32"), Tensor([0, 96, 2],"float32"), reduction="none", )
paddle.Tensor.__rpow__(Tensor([0],"float32"), 10000.0, )
paddle.nn.functional.conv2d(Tensor([1, 1, 101, 0],"float32"), Tensor([64, 1, 7, 0],"float32"), None, list[2,2,], 0, list[1,1,], 1, )
paddle.nn.functional.conv1d(Tensor([16, 80, 0],"float32"), Tensor([128, 80, 0],"float32"), bias=None, padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv2d_transpose(x=Tensor([0, 3, 2, 2],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,], dilation=1, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float32"), Tensor([3, 2, 3, 3, 0],"float32"), bias=Tensor([2],"float32"), padding=0, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.max_pool1d(Tensor([0, 32, 7],"float32"), 7, )
paddle.Tensor.tile(Tensor([2, 1, 0, 1],"float32"), tuple(4,1,4,4,), )
paddle.nn.functional.conv1d(Tensor([16, 80, 0],"float32"), Tensor([128, 80, 1],"float32"), bias=None, padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 0, 2, 2],"float64"), Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[2,2,2,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.kl_div(Tensor([5, 0],"float64"), Tensor([5, 0],"float64"), "mean", False, )
paddle.einsum("nbts,nbsc->nbtc", Tensor([2, 52, 7, 14],"float32"), Tensor([2, 52, 14, 0],"float32"), )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 8, 16],"float16"), Tensor([1, 2048, 2, 16],"float16"), Tensor([0, 2048, 2, 16],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), is_causal=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 1024, 32, 128],"bfloat16"), Tensor([1, 1024, 32, 128],"bfloat16"), Tensor([1, 1024, 0, 128],"bfloat16"), attn_mask=None, is_causal=True, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[66,1,1,], )
paddle.nn.functional.max_pool3d(x=Tensor([8, 8, 32, 112, 0],"float32"), kernel_size=list[1,3,3,], stride=list[1,2,2,], padding=list[0,1,1,], data_format="NCDHW", )
paddle.nn.functional.pad(Tensor([1, 16, 15, 0, 384],"float16"), tuple(0,0,0,1,0,0,), data_format="NDHWC", )
paddle.einsum("bind,snd->bnis", Tensor([13, 0, 4, 4],"float32"), Tensor([2, 4, 4],"float32"), )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float64"), weight=Tensor([1, 3, 0, 3],"float64"), bias=Tensor([1],"float64"), stride=list[2,1,], padding=1, dilation=2, )
paddle.nn.functional.avg_pool3d(x=Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,0,0,], exclusive=True, )
paddle.nn.functional.margin_ranking_loss(Tensor([10, 0],"float64"), Tensor([10, 0],"float64"), Tensor([10, 0],"float64"), 0.0, "none", )
paddle.nn.functional.softmax(Tensor([2, 0, 4],"float64"), axis=-3, dtype=None, name=None, )
paddle.Tensor.__sub__(Tensor([12, 3, 10, 10, 0],"float32"), Tensor([12, 3, 10, 10, 0],"float32"), )
paddle.nn.functional.fractional_max_pool3d(x=Tensor([2, 0, 7, 7, 7],"float32"), kernel_size=2, output_size=list[3,3,3,], random_u=0.6, )
paddle.nn.functional.max_pool3d(Tensor([2, 3, 8, 0, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=1, return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.nn.functional.multi_margin_loss(Tensor([5, 0],"float64"), Tensor([5],"int64"), p=1, margin=1.0, weight=None, reduction="mean", name=None, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 0, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding="valid", output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=2, output_size=None, data_format="NCDHW", )
paddle.nn.functional.max_pool2d(Tensor([2, 3, 32, 0],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, )
paddle.Tensor.tile(Tensor([1, 4, 0],"float32"), list[2297,1,1,], )
paddle.nn.functional.softmax(Tensor([0, 3, 4],"float32"), -1, name=None, )
paddle.nn.functional.pad(Tensor([421120, 25, 0],"float32"), list[0,1,], "constant", 1.0, data_format="NCL", )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 128, 32, 0],"float32"), output_size=tuple(1,1,), data_format="NCHW", name=None, )
paddle.nn.quant.weight_quantize(Tensor([0, 256],"float16"), algo="weight_only_int8", arch=86, group_size=-1, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 0, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
paddle.nn.functional.layer_norm(Tensor([1, 0, 2],"float32"), 2, epsilon=1e-05, weight=Tensor([2],"float32"), bias=Tensor([2],"float32"), )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 0, 8, 32, 32],"float64"), output_size=list[2,3,3,], data_format="NCDHW", name=None, )
paddle.Tensor.tile(Tensor([1, 33, 0],"float32"), list[64,1,1,], )
paddle.allclose(Tensor([0, 64, 16],"float32"), Tensor([0, 64, 16],"float32"), atol=1e-05, )
paddle.nn.functional.pad(Tensor([1, 3, 184, 0],"float64"), pad=list[52,52,0,0,], mode="constant", value=0.0, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 4, 16, 16],"float32"), Tensor([4, 3, 3, 0],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,1,2,2,], stride=1, dilation=1, groups=2, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 4, 0],"float32"), list[380,1,1,], )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 100, 1, 0],"float16"), Tensor([2, 100, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 100],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.avg_pool3d(Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,2,1,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([512, 256, 2, 0],"float32"), output_size=list[1,25,], data_format="NCHW", name=None, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 0],"float64"), Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.einsum("i,i->i", Tensor([0],"float64"), Tensor([0],"float64"), )
paddle.nanmedian(Tensor([2, 0, 4, 5],"float32"), axis=tuple(1,2,3,), keepdim=False, )
paddle.nn.functional.pad(Tensor([1, 3, 32, 0, 231],"float32"), tuple(0,1,0,0,0,0,), data_format="NCDHW", )
paddle.nn.functional.conv2d(Tensor([1, 2048, 0, 128],"float32"), Tensor([2048, 256, 3, 3],"float32"), padding=1, groups=8, )
paddle.nanmedian(Tensor([2, 0, 4, 5],"float32"), axis=2, keepdim=False, mode="min", )
paddle.nn.quant.weight_quantize(Tensor([64, 0],"float16"), algo="weight_only_int4", arch=86, group_size=-1, )
paddle.nn.functional.prelu(x=Tensor([3, 3, 0],"float64"), weight=Tensor([3],"float64"), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[875,1,1,], )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[87,1,1,], )
paddle.vision.ops.generate_proposals(Tensor([1, 9, 12, 9],"float32"), Tensor([1, 36, 12, 0],"float32"), Tensor([1, 2],"float32"), Tensor([972, 4],"float32"), Tensor([972, 4],"float32"), pre_nms_top_n=4000, post_nms_top_n=4000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.l1_loss(Tensor([1, 3, 256, 0],"float32"), Tensor([1, 3, 256, 0],"float32"), "mean", name=None, )
paddle.nn.functional.conv1d(Tensor([4, 6, 16],"float32"), Tensor([512, 6, 0],"float32"), bias=Tensor([512],"float32"), padding="valid", stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.Tensor.tile(Tensor([2, 2, 1, 64, 0],"float16"), list[1,1,1,1,2,], )
paddle.slice(Tensor([4, 0, 6],"float32"), axes=list[0,1,2,], starts=list[-3,0,2,], ends=list[3,2,4,], )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[50,1,1,], )
paddle.nn.functional.channel_shuffle(Tensor([2, 0, 4, 9],"float64"), 3, "NHWC", None, )
paddle.incubate.nn.functional.fused_rms_norm(Tensor([0, 1, 64],"float16"), Tensor([64],"float16"), None, 1e-06, begin_norm_axis=1, )
paddle.subtract(Tensor([0, 96, 2],"float32"), Tensor([0, 96, 2],"float32"), )
paddle.nn.functional.conv2d(Tensor([3, 16, 2, 0],"float32"), Tensor([16, 16, 3, 3],"float32"), None, list[2,2,], 1, list[1,1,], 1, "NCHW", )
paddle.nn.functional.temporal_shift(x=Tensor([2, 4, 3, 0],"float64"), seg_num=2, shift_ratio=0.4, data_format="NHWC", )
paddle.slice(Tensor([0, 2, 100, 100],"float32"), axes=list[1,], starts=list[1,], ends=list[2,], )
paddle.nn.functional.temporal_shift(Tensor([128, 0, 14, 14],"float32"), 8, 0.125, )
paddle.nn.functional.pad(Tensor([1, 1, 0, 21],"float32"), pad=list[0,1,0,0,], mode="constant", value=-10000.0, data_format="NCHW", name=None, )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 32, 0],"float32"), output_size=list[3,3,3,], )
paddle.Tensor.tile(Tensor([4, 0, 512],"float32"), tuple(1,5,1,), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[75,1,1,], )
paddle.einsum("ij,kj->ik", Tensor([0, 5],"float64"), Tensor([0, 5],"float64"), )
paddle.nn.functional.conv2d_transpose(Tensor([0, 16, 16, 4],"float32"), Tensor([4, 1, 3, 3],"float32"), Tensor([4],"float32"), output_size=None, padding="valid", stride=tuple(1,2,), dilation=1, groups=4, data_format="NHWC", )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[505,1,1,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[97,1,1,], )
paddle.nn.functional.mse_loss(Tensor([0, 10],"float32"), Tensor([0, 10],"float32"), "mean", )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float64"), weight=Tensor([1, 3, 3, 3, 3],"float64"), bias=Tensor([0],"float64"), stride=2, padding=1, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[110,1,1,], )
paddle.cummax(Tensor([0, 100],"float32"), axis=-1, )
paddle.nn.functional.avg_pool3d(Tensor([2, 0, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
paddle.allclose(x=Tensor([0, 2, 2, 3],"float64"), y=Tensor([0, 2, 2, 3],"float64"), )
paddle.nn.functional.avg_pool1d(Tensor([0, 3, 8],"float64"), 3, 4, 0, True, False, None, )
paddle.nn.functional.max_pool2d(Tensor([1, 0, 320, 432],"float32"), kernel_size=3, stride=2, padding=1, )
paddle.einsum("nbts,nbsc->nbtc", Tensor([2, 52, 0, 14],"float32"), Tensor([2, 52, 14, 8],"float32"), )
paddle.nn.functional.l1_loss(Tensor([10, 0, 5],"float32"), Tensor([10, 0, 5],"float32"), reduction="none", )
paddle.Tensor.tile(Tensor([10285, 0],"float32"), list[1,48,], )
paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([0, 1, 50, 50],"float16"), scale=0.125, )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[265,1,1,], )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 0, 32],"float64"), output_size=tuple(3,3,3,), data_format="NCDHW", )
paddle.nn.functional.scaled_dot_product_attention(query=Tensor([2, 64, 12, 64],"float16"), key=Tensor([0, 64, 12, 64],"float16"), value=Tensor([2, 64, 12, 64],"float16"), is_causal=True, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 96, 2, 2],"float32"), Tensor([96, 0, 4, 4],"float32"), bias=Tensor([96],"float32"), padding=0, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.pixel_shuffle(x=Tensor([4, 0, 4, 4],"float64"), upscale_factor=3, data_format="NCHW", )
paddle.slice(Tensor([13, 0, 104],"float32"), list[0,1,2,], list[0,0,64,], list[13,1,80,], )
paddle.nn.functional.pixel_shuffle(Tensor([2, 4, 0, 9],"float64"), 3, "NHWC", None, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 4, 16, 16],"float32"), Tensor([4, 3, 0, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,1,2,2,], stride=1, dilation=1, groups=2, data_format="NCHW", )
paddle.nn.functional.softmax(Tensor([0, 1, 16, 49, 49],"float16"), -1, name=None, )
paddle.nn.functional.max_unpool2d(Tensor([0, 1, 2, 2],"float32"), Tensor([0, 1, 2, 2],"int64"), kernel_size=2, stride=None, output_size=list[1,1,4,5,], )
paddle.Tensor.lgamma(Tensor([100, 100, 0],"float64"), )
paddle.searchsorted(sorted_sequence=Tensor([2, 5],"float32"), values=Tensor([2, 0],"float32"), )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 0, 8, 32, 32],"float64"), output_size=tuple(3,3,3,), )
paddle.Tensor.__rpow__(Tensor([0],"float32"), 63496.04207872797, )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[328,1,1,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[2019,1,1,], )
paddle.nn.functional.pad(Tensor([3, 1, 40, 0],"float32"), pad=list[0,0,2,2,], )
paddle.nn.functional.fractional_max_pool3d(Tensor([2, 0, 7, 7, 7],"float32"), output_size=5, kernel_size=None, random_u=0.5, return_mask=False, name=None, )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[1892,1,1,], )
paddle.kthvalue(x=Tensor([3, 0, 4],"float64"), k=3, axis=0, )
paddle.nn.functional.pad(Tensor([1, 0, 32, 239, 224],"float32"), tuple(0,0,0,1,0,0,), data_format="NCDHW", )
paddle.nn.functional.margin_ranking_loss(Tensor([10, 0],"float64"), Tensor([10, 0],"float64"), Tensor([10, 0],"float64"), 0.0, "none", None, )
paddle.nn.functional.softmax(x=Tensor([2, 0, 4],"float32"), )
paddle.vision.ops.generate_proposals(Tensor([0, 3, 4, 4],"float32"), Tensor([2, 12, 4, 4],"float32"), Tensor([2, 3],"float32"), Tensor([4, 4, 3, 4],"float32"), Tensor([4, 4, 3, 4],"float32"), pre_nms_top_n=10, post_nms_top_n=5, return_rois_num=True, )
paddle.nn.functional.max_pool3d(x=Tensor([2, 0, 8, 8, 8],"float32"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], )
paddle.Tensor.tile(Tensor([4, 0, 512],"float32"), tuple(1,7,1,), )
paddle.Tensor.trunc(Tensor([0, 8],"float32"), )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 8, 1, 1, 0],"float32"), bias=Tensor([8],"float32"), padding=tuple(2,3,1,), output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.max_pool2d(Tensor([1, 0, 368, 368],"float32"), kernel_size=2, stride=1, padding="SAME", return_mask=False, ceil_mode=True, data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([3, 0, 5, 6],"complex64"), pad=list[1,2,2,1,], mode="constant", value=100, data_format="NCHW", name=None, )
paddle.allclose(Tensor([2, 0],"float32"), Tensor([2, 0],"float32"), atol=0.0001, )
paddle.nn.functional.pad(Tensor([0, 3, 32, 239, 224],"float32"), tuple(0,0,0,1,0,0,), data_format="NCDHW", )
paddle.nn.functional.layer_norm(Tensor([0, 6, 6, 3],"float32"), list[6,6,3,], weight=Tensor([108],"float32"), bias=None, epsilon=1e-05, )
paddle.nn.functional.mse_loss(Tensor([2, 0, 10],"float32"), Tensor([2, 0, 10],"float32"), "none", )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([0, 3, 4, 4],"float64"), output_size=list[1,4,], )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 0, 4, 4],"float32"), weight=Tensor([1, 3, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=list[2,2,1,], padding=1, dilation=tuple(2,2,2,), )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float64"), weight=Tensor([6, 1, 3, 3, 0],"float64"), bias=Tensor([6],"float64"), stride=2, padding=0, groups=3, )
paddle.nn.functional.lp_pool2d(Tensor([0, 3, 32, 32],"float32"), norm_type=2.0, kernel_size=2, stride=2, padding=0, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.max_unpool3d(Tensor([1, 3, 2, 2, 0],"float64"), Tensor([1, 3, 2, 2, 0],"int32"), kernel_size=2, stride=2, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([0, 3, 7, 7],"float32"), list[2,5,], )
paddle.nn.functional.conv3d_transpose(Tensor([2, 2, 2, 2, 3],"float32"), Tensor([3, 1, 3, 3, 0],"float32"), bias=Tensor([3],"float32"), padding=list[1,1,1,], output_padding=1, stride=list[2,2,2,], dilation=list[1,1,1,], groups=3, output_size=None, data_format="NDHWC", )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 3, 0, 4],"float64"), output_size=list[3,3,], data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[205,1,1,], )
paddle.allclose(Tensor([14, 0],"float32"), Tensor([14, 0],"float32"), atol=0.001, )
paddle.searchsorted(sorted_sequence=Tensor([2, 0],"float32"), values=Tensor([2, 0],"float32"), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[712,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([1, 256, 14, 14],"float32"), Tensor([256, 0, 2, 2],"float32"), bias=Tensor([256],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 2, 2, 6],"float32"), weight=Tensor([6, 1, 0, 3],"float32"), bias=Tensor([3],"float32"), output_padding=1, stride=2, padding=list[1,0,], groups=3, data_format="NHWC", dilation=1, )
paddle.nn.functional.pad(Tensor([0, 4, 5, 6],"complex64"), pad=list[1,2,2,1,], mode="circular", value=0.0, data_format="NCHW", name=None, )
paddle.nanmedian(Tensor([2, 3, 4, 0],"float32"), axis=list[0,1,3,], keepdim=False, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[2297,1,1,], )
paddle.nn.functional.pad(Tensor([1, 7485, 0],"float32"), list[1,0,], value=2, mode="constant", data_format="NCL", )
paddle.Tensor.tile(Tensor([10105, 0],"float32"), list[1,9,], )
paddle.nn.functional.max_pool3d(Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,2,1,], return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 0, 2, 2],"float64"), weight=Tensor([3, 2, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=0, dilation=1, )
paddle.nn.functional.max_pool3d(x=Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,2,1,], )
paddle.nn.functional.pad(Tensor([1, 16, 14, 0, 384],"float16"), tuple(0,3,0,0,0,0,), data_format="NDHWC", )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 2],"float64"), Tensor([3, 1, 3, 3],"float64"), bias=Tensor([0],"float64"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.max_pool3d(Tensor([1, 3, 0, 4, 6],"float64"), kernel_size=2, stride=2, padding=0, return_mask=True, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 0],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )
paddle.nn.functional.conv2d_transpose(Tensor([0, 256, 16, 16],"float32"), Tensor([256, 1, 16, 16],"float32"), bias=None, padding=4, output_padding=0, stride=list[8,8,], dilation=list[1,1,], groups=256, output_size=None, data_format="NCHW", )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 128, 28, 0],"float32"), output_size=7, data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([1, 0, 3, 4, 5],"complex64"), pad=list[1,2,1,1,3,4,], mode="constant", value=100, data_format="NDHWC", )
paddle.nanmedian(Tensor([2, 0, 4, 5],"float32"), axis=2, keepdim=False, )
paddle.slice(Tensor([2, 0, 4, 5, 6],"float32"), axes=list[0,1,2,], starts=list[1,0,2,], ends=list[3,3,4,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[184,1,1,], )
paddle.einsum("nbqa,ahc->nbqhc", Tensor([1, 3, 5, 6],"float16"), Tensor([6, 0, 4],"float16"), )
paddle.nn.functional.conv2d(Tensor([1, 6, 16, 16],"float32"), Tensor([16, 6, 5, 0],"float32"), bias=Tensor([16],"float32"), padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, data_format="NCHW", )
paddle.einsum("blqd,bmdk->blqk", Tensor([0, 5, 1, 1],"float32"), Tensor([0, 1, 1, 1],"float32"), )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 2, 2, 6],"float64"), weight=Tensor([6, 1, 3, 0],"float64"), bias=Tensor([3],"float64"), stride=1, padding=list[1,0,], groups=3, data_format="NHWC", dilation=1, )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 2],"float32"), weight=Tensor([3, 0, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=list[1,0,1,], dilation=1, )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 5, 0, 7],"float32"), output_size=list[3,3,3,], data_format="NDHWC", )
paddle.Tensor.tile(Tensor([8, 1, 0, 2],"float32"), list[1,4,4,1,], )
paddle.nn.functional.l1_loss(Tensor([4, 0, 10],"float32"), Tensor([4, 0, 10],"float32"), reduction="none", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 8, 8, 8, 6],"float32"), Tensor([6, 8, 3, 3, 0],"float32"), bias=Tensor([8],"float32"), padding=list[list[0,0,],list[1,1,],list[2,2,],list[3,3,],list[0,0,],], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NDHWC", )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([0, 3, 4, 4],"float64"), output_size=list[1,4,], data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([1, 8, 0],"float32"), list[500,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([2, 2, 3, 0],"float64"), Tensor([2, 2, 1, 1],"float64"), groups=1, padding="VALID", )
paddle.nanmedian(Tensor([2, 3, 4, 0],"float32"), axis=list[0,3,], keepdim=False, mode="min", )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 2],"float64"), weight=Tensor([3, 1, 3],"float64"), bias=Tensor([0],"float64"), stride=2, padding=list[1,], groups=1, dilation=1, )
paddle.nn.functional.margin_ranking_loss(Tensor([2, 0],"float64"), other=Tensor([2, 0],"float64"), label=Tensor([2, 0],"float64"), margin=0.0, reduction="mean", name=None, )
paddle.Tensor.tile(Tensor([1, 5, 0],"float32"), list[288,1,1,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[841,1,1,], )
paddle.nn.functional.conv2d(Tensor([4, 16, 16, 3],"float32"), Tensor([5, 3, 3, 0],"float32"), Tensor([5],"float32"), padding=list[1,2,], stride=1, dilation=1, groups=1, data_format="NHWC", )
paddle.nn.functional.conv2d(Tensor([4, 16, 16, 3],"float32"), Tensor([5, 3, 3, 0],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[1,1,],list[2,2,],list[0,0,],], stride=1, dilation=1, groups=1, data_format="NHWC", )
paddle.einsum("...qk,...kd->...qd", Tensor([13, 4, 3, 0, 2],"float32"), Tensor([13, 4, 3, 2, 8],"float32"), )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 0],"float32"), Tensor([6, 4, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding="valid", output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=2, output_size=None, data_format="NCDHW", )
paddle.nanmedian(Tensor([2, 3, 4, 0],"float32"), axis=tuple(1,2,3,), keepdim=False, mode="min", )
paddle.nn.functional.conv2d_transpose(x=Tensor([0, 6, 2, 2],"float64"), weight=Tensor([6, 1, 3, 3],"float64"), bias=Tensor([3],"float64"), stride=1, padding=list[1,0,], groups=3, dilation=1, )
paddle.slice(Tensor([0, 8, 24],"float32"), axes=list[1,], starts=list[5,], ends=list[6,], )
paddle.meshgrid(list[Tensor([100],"float32"),Tensor([0],"float32"),], )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float32"), Tensor([3, 1, 3, 0, 3],"float32"), bias=Tensor([1],"float32"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.pad(Tensor([1, 3, 32, 239, 0],"float32"), tuple(0,0,0,1,0,0,), data_format="NCDHW", )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[172,1,1,], )
paddle.slice(Tensor([8, 2, 0, 100],"float16"), axes=list[1,], starts=list[1,], ends=list[2,], )
paddle.nn.functional.pad(Tensor([1, 16, 0, 14, 384],"float16"), tuple(0,0,0,6,0,0,), data_format="NDHWC", )
paddle.einsum("ij,jk", Tensor([0, 11],"float64"), Tensor([11, 6],"float64"), )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 3, 0, 7],"float32"), output_size=list[2,5,], data_format="NCHW", name=None, )
paddle.nn.functional.lp_pool1d(Tensor([2, 0, 32],"float32"), math.inf, 2, None, 1, False, "NCL", None, )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 0, 5, 7, 7],"float32"), list[2,3,5,], )
paddle.nn.functional.conv2d(Tensor([1, 1, 32, 32],"float32"), Tensor([6, 1, 3, 0],"float32"), bias=Tensor([6],"float32"), padding=1, stride=list[1,1,], dilation=list[1,1,], groups=1, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 4, 0],"float32"), list[531,1,1,], )
paddle.nn.functional.max_pool2d(x=Tensor([2, 0, 8, 8],"float32"), kernel_size=2, stride=2, padding=0, )
paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([0],"float32"), shift=None, smooth=None, quant_scale=0.0014022786635905504, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.nn.functional.margin_ranking_loss(Tensor([0, 10],"float64"), Tensor([0, 10],"float64"), Tensor([0, 10],"float64"), 0.0, "mean", None, )
paddle.Tensor.tile(Tensor([4, 0, 512],"float32"), tuple(1,9,1,), )
paddle.nanmedian(Tensor([0, 3, 4, 5],"float32"), axis=list[0,-1,], keepdim=False, mode="min", )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 0, 2],"float64"), weight=Tensor([3, 2, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=0, dilation=1, )
paddle.nanmedian(Tensor([2, 0, 4, 5],"float32"), axis=-1, keepdim=False, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([0],"float64"), stride=1, padding=1, output_padding=0, dilation=1, output_size=None, data_format="NCHW", )
paddle.reverse(Tensor([12, 4, 0],"float64"), axis=list[0,], )
paddle.nn.functional.conv2d_transpose(Tensor([0, 3, 2, 2],"float64"), Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[2,2,], groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 7, 0],"float32"), list[1328,1,1,], )
paddle.nn.functional.conv3d_transpose(Tensor([0, 3, 8, 8, 8],"float32"), Tensor([3, 2, 3, 3, 3],"float32"), bias=Tensor([6],"float32"), padding="valid", output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=3, output_size=None, data_format="NCDHW", )
paddle.nn.functional.avg_pool3d(x=Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,1,1,], exclusive=True, )
paddle.allclose(Tensor([13, 0],"float32"), Tensor([13, 0],"float32"), atol=0.001, )
paddle.Tensor.__sub__(Tensor([3, 6, 3, 4, 1, 0],"float64"), Tensor([3, 6, 3, 4, 1, 0],"float64"), )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[516,1,1,], )
paddle.vision.ops.prior_box(Tensor([4, 96, 40, 0],"float32"), Tensor([4, 3, 640, 0],"float32"), list[32.0,48.0,64.0,80.0,96.0,128.0,], list[], list[1.0,], list[0.1,0.1,0.2,0.2,], False, False, list[16.0,16.0,], 0.5, False, None, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 3, 0, 7],"float32"), output_size=5, data_format="NCHW", name=None, )
paddle.nn.functional.maxout(Tensor([2, 6, 5, 0],"float64"), 2, -1, )
paddle.vision.ops.generate_proposals(Tensor([1, 3, 10, 0],"float32"), Tensor([1, 12, 10, 14],"float32"), Tensor([1, 2],"float32"), Tensor([420, 4],"float32"), Tensor([420, 4],"float32"), pre_nms_top_n=2000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 0, 2],"float32"), weight=Tensor([3, 2, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=0, dilation=1, )
paddle.nn.functional.conv3d(Tensor([4, 8, 8, 0, 3],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[1,2,3,1,2,3,], stride=2, dilation=1, groups=1, data_format="NDHWC", )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 0, 4],"float32"), weight=Tensor([1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=2, padding=1, )
paddle.Tensor.tile(Tensor([4897, 0, 1],"float32"), list[1,143,1,], )
paddle.strided_slice(x=Tensor([5, 8, 6, 0, 2, 6],"float64"), axes=list[1,2,5,], starts=list[-3,3,4,], ends=list[3,0,1,], strides=list[-1,-1,-2,], )
paddle.nn.functional.layer_norm(Tensor([0, 2, 2],"float32"), 2, epsilon=1e-05, weight=None, bias=None, )
paddle.nn.functional.conv1d(Tensor([8, 256, 100],"float16"), Tensor([256, 64, 0],"float16"), bias=Tensor([256],"float16"), padding=1, stride=list[1,], dilation=list[1,], groups=4, data_format="NCL", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 64, 64, 64],"float32"), Tensor([64, 1, 0, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=64, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv1d_transpose(Tensor([4, 6, 16],"float32"), Tensor([6, 8, 0],"float32"), bias=Tensor([8],"float32"), output_size=None, output_padding=0, padding=list[2,], stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.pad(x=Tensor([0, 1, 1, 2, 3],"float64"), pad=tuple(0,1,1,1,2,0,), mode="constant", value=0, data_format="NCDHW", )
paddle.nn.functional.conv3d(x=Tensor([2, 4, 4, 4, 3],"float64"), weight=Tensor([2, 3, 3, 0, 3],"float64"), bias=Tensor([2],"float64"), stride=1, padding=0, data_format="NDHWC", )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float32"), weight=Tensor([1, 3, 0, 3],"float32"), bias=Tensor([1],"float32"), stride=2, padding=0, )
paddle.nn.quant.weight_only_linear(Tensor([123, 768],"float16"), weight=Tensor([2304, 768],"int8"), bias=None, weight_scale=Tensor([0],"float16"), weight_dtype="int8", )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[123,1,1,], )
paddle.nn.functional.avg_pool1d(Tensor([2, 0, 32],"float32"), 2, None, 0, True, False, None, )
paddle.nonzero(x=Tensor([0, 2, 2, 2],"float64"), as_tuple=False, )
paddle.Tensor.tile(Tensor([2, 0, 1, 64, 1],"float16"), list[1,1,1,1,2,], )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 6, 2, 2],"float64"), weight=Tensor([6, 0, 3, 3],"float64"), bias=Tensor([3],"float64"), stride=1, padding=list[1,0,], groups=3, dilation=1, )
paddle.nn.functional.l1_loss(Tensor([2, 0],"float64"), label=Tensor([2, 0],"float64"), reduction="mean", name=None, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([0, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )
paddle.nn.functional.pad(Tensor([0, 3, 100, 100],"float32"), pad=list[1,2,3,4,], mode="reflect", value=0.0, data_format="NCHW", name="shape", )
paddle.nn.functional.avg_pool3d(Tensor([2, 3, 0, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
paddle.nn.functional.conv1d(Tensor([2, 3, 4],"float32"), Tensor([1, 3, 0],"float32"), bias=Tensor([1],"float32"), padding=tuple(1,), stride=list[1,], dilation=list[2,], groups=1, data_format="NCL", )
paddle.Tensor.tile(Tensor([1, 0],"float32"), list[1,100,], )
paddle.nn.functional.max_pool3d(Tensor([0, 64, 16, 112, 112],"float32"), kernel_size=tuple(3,3,3,), stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.Tensor.tile(Tensor([1, 5, 0],"float32"), list[187,1,1,], )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 0, 4, 4],"float64"), weight=Tensor([1, 3, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=list[2,2,1,], padding=1, dilation=list[2,2,2,], )
paddle.nn.functional.conv3d_transpose(Tensor([0, 6, 8, 8, 8],"float32"), Tensor([6, 8, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[1,2,1,], dilation=list[2,2,2,], groups=1, output_size=list[12,19,12,], data_format="NCDHW", )
paddle.roll(Tensor([4, 5, 4, 0],"complex128"), Tensor([4],"int64"), list[0,1,2,3,], name=None, )
paddle.nn.functional.pad(Tensor([1, 16, 0, 19, 384],"float16"), tuple(0,2,0,0,0,0,), data_format="NDHWC", )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 0, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )
paddle.frexp(Tensor([4, 5, 0],"float64"), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[141,1,1,], )
paddle.Tensor.tile(Tensor([1000, 0, 80],"float32"), list[1,3,1,], )
paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 0],"int32"), bias=Tensor([512],"float16"), dequant_scales=Tensor([512],"float32"), shift=Tensor([256],"float16"), smooth=Tensor([256],"float16"), act_method="geglu", compute_dtype="fp16", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 256, 62, 54],"float32"), Tensor([256, 128, 4, 0],"float32"), bias=None, padding=0, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 0, 2],"float64"), weight=Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([3],"float64"), stride=1, padding=list[1,0,1,], groups=3, dilation=1, )
paddle.Tensor.digamma(Tensor([5, 7, 0, 10],"float64"), )
paddle.Tensor.tile(Tensor([1, 300, 0],"float32"), list[2,1,1,], )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 3, 7, 0],"float32"), output_size=5, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 0, 4, 4],"float64"), weight=Tensor([1, 3, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=list[2,2,1,], padding=list[1,2,2,], dilation=tuple(2,2,2,), )
paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 0],"float32"), bias=Tensor([512],"float32"), act_method="gelu", compute_dtype="default", )
paddle.nn.functional.hinge_embedding_loss(Tensor([3, 0],"float64"), Tensor([3, 0],"float64"), reduction="mean", margin=4.0, name=None, )
paddle.nn.functional.max_pool3d(x=Tensor([8, 8, 32, 0, 112],"float32"), kernel_size=list[1,3,3,], stride=list[1,2,2,], padding=list[0,1,1,], data_format="NCDHW", )
paddle.Tensor.tile(Tensor([4562, 0, 1],"float32"), list[1,135,1,], )
paddle.nn.functional.pad(Tensor([3, 4, 0],"complex64"), pad=list[1,2,], mode="constant", value=100, data_format="NCL", name=None, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 8, 8],"float64"), weight=Tensor([3, 1, 5, 0],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,2,3,4,], output_padding=0, dilation=1, groups=1, output_size=None, data_format="NCHW", )
paddle.slice(Tensor([0, 1, 104],"float32"), list[0,1,2,], list[0,0,40,], list[3,1,56,], )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([13, 0, 56, 56],"float32"), output_size=list[7,7,], data_format="NCHW", name=None, )
paddle.nn.functional.avg_pool2d(Tensor([0, 3, 4, 4],"float64"), kernel_size=list[3,3,], stride=list[3,3,], padding=list[0,0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([1, 1, 280, 0],"float32"), list[1,3,1,1,], )
paddle.matmul(x=Tensor([10],"float64"), y=Tensor([0],"float64"), )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 0],"float64"), weight=Tensor([3, 1, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,], output_padding=0, groups=1, dilation=2, output_size=None, data_format="NCL", name=None, )
paddle.nn.functional.temporal_shift(Tensor([128, 256, 0, 28],"float32"), 16, 0.0625, )
paddle.Tensor.__setitem__(Tensor([6, 5, 4, 0],"bool"), list[list[2,3,4,],list[1,2,5,],], 100, )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 0, 5, 7, 7],"float32"), output_size=5, )
paddle.nn.quant.weight_quantize(Tensor([0, 288],"float16"), algo="weight_only_int8", arch=80, group_size=-1, )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float32"), weight=Tensor([1, 3, 3, 0],"float32"), bias=Tensor([1],"float32"), stride=2, padding=0, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([1, 3, 3, 0, 3],"float32"), bias=Tensor([1],"float32"), stride=list[2,2,1,], padding=1, dilation=2, )
paddle.nn.functional.conv1d(Tensor([4, 6, 16],"float32"), Tensor([8, 6, 0],"float32"), bias=Tensor([8],"float32"), padding=2, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.pad(Tensor([3, 0, 5, 6],"complex128"), pad=list[1,2,2,1,], mode="constant", value=100, data_format="NCHW", name=None, )
paddle.vision.ops.generate_proposals(Tensor([2, 3, 4, 4],"float32"), Tensor([0, 12, 4, 4],"float32"), Tensor([2, 3],"float32"), Tensor([4, 4, 3, 4],"float32"), Tensor([4, 4, 3, 4],"float32"), pre_nms_top_n=10, post_nms_top_n=5, return_rois_num=True, )
paddle.nn.functional.poisson_nll_loss(Tensor([0, 3, 2],"bfloat16"), Tensor([0, 3, 2],"float16"), )
paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), rtol=1e-05, atol=1e-08, equal_nan=True, name="test_4", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 0],"float64"), Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([3],"float64"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=3, output_size=None, data_format="NCDHW", )
paddle.Tensor.imag(Tensor([16, 1025, 0],"complex64"), )
paddle.nn.functional.conv2d_transpose(Tensor([4, 16, 16, 4],"float32"), Tensor([4, 0, 3, 3],"float32"), Tensor([4],"float32"), output_size=None, padding="valid", stride=tuple(1,2,), dilation=1, groups=4, data_format="NHWC", )
paddle.vision.ops.generate_proposals(Tensor([0, 3, 10, 15],"float32"), Tensor([1, 12, 10, 15],"float32"), Tensor([1, 2],"float32"), Tensor([450, 4],"float32"), Tensor([450, 4],"float32"), pre_nms_top_n=2000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.avg_pool1d(Tensor([2, 0, 32],"float32"), kernel_size=2, stride=2, padding="SAME", )
paddle.nn.functional.pad(Tensor([1, 16, 0, 18, 384],"float32"), tuple(0,3,0,0,0,0,), data_format="NDHWC", )
paddle.einsum("a...d,...cb->...abcd", Tensor([10, 3, 2, 3, 4],"float64"), Tensor([0, 10],"float64"), )
paddle.nn.functional.softmax(Tensor([16, 16, 0],"float64"), 0, name=None, )
paddle.nn.functional.max_pool2d(Tensor([0, 244, 244, 2],"float32"), kernel_size=list[5,3,], stride=list[1,2,], padding=tuple(2,1,), )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 5],"float64"), weight=Tensor([3, 2, 0],"float64"), bias=Tensor([2],"float64"), stride=1, padding="SaME", output_padding=0, groups=1, dilation=1, output_size=None, data_format="NCL", )
paddle.slice(Tensor([1024, 0, 1024],"float16"), axes=list[0,], starts=list[256,], ends=list[384,], )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 7, 0],"float32"), output_size=list[None,3,None,], data_format="NCDHW", name=None, )
paddle.einsum("i,i->", Tensor([0],"float64"), Tensor([0],"float64"), )
paddle.nn.functional.conv1d_transpose(x=Tensor([0, 3, 2],"float32"), weight=Tensor([3, 1, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=list[1,], groups=3, dilation=1, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 0, 2],"float32"), Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([3],"float32"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=3, output_size=None, data_format="NCDHW", )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 2],"float64"), weight=Tensor([3, 0, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,], groups=3, dilation=1, )
paddle.nn.functional.avg_pool2d(Tensor([64, 0, 17, 17],"float16"), kernel_size=3, stride=1, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.vision.ops.prior_box(input=Tensor([2, 10, 32, 0],"float32"), image=Tensor([2, 10, 40, 40],"float32"), min_sizes=list[2.0,4.0,], clip=True, flip=True, )
paddle.nn.functional.conv1d(Tensor([1, 128, 0],"float32"), Tensor([128, 128, 3],"float32"), bias=Tensor([128],"float32"), padding=3, stride=list[1,], dilation=list[3,], groups=1, data_format="NCL", )
paddle.nn.functional.avg_pool2d(Tensor([256, 0, 8, 50],"float32"), kernel_size=tuple(2,1,), stride=tuple(2,1,), padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([1, 3, 0, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=2, padding=1, )
paddle.nn.functional.pad(Tensor([1, 16, 0, 56, 96],"float32"), tuple(0,0,0,2,0,0,), data_format="NDHWC", )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 2],"float32"), weight=Tensor([3, 1, 3],"float32"), bias=Tensor([0],"float32"), stride=1, padding=list[1,], groups=1, dilation=2, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 8, 8, 8, 6],"float32"), Tensor([6, 8, 3, 0, 3],"float32"), bias=Tensor([8],"float32"), padding=list[list[0,0,],list[1,1,],list[2,2,],list[3,3,],list[0,0,],], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NDHWC", )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 2],"float64"), weight=Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([0],"float64"), stride=1, padding=list[1,0,1,], dilation=1, )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float64"), weight=Tensor([1, 3, 3, 0],"float64"), bias=Tensor([1],"float64"), stride=2, padding=1, )
paddle.Tensor.imag(Tensor([2, 0, 2, 3],"complex128"), )
paddle.nn.functional.conv2d(Tensor([1, 3, 224, 0],"float32"), Tensor([3, 3, 3, 3],"float32"), Tensor([3],"float32"), list[4,3,], 0, list[1,1,], 1, "NCHW", )
paddle.nn.functional.conv1d_transpose(x=Tensor([0, 3, 2],"float64"), weight=Tensor([3, 2, 3],"float64"), bias=Tensor([2],"float64"), stride=1, padding=0, output_padding=0, groups=1, dilation=1, output_size=None, data_format="NCL", name=None, )
paddle.nn.functional.kl_div(Tensor([0, 20, 50],"float32"), Tensor([0, 20, 50],"float32"), "batchmean", True, )
paddle.slice(Tensor([1024, 0, 1024],"float16"), axes=list[0,], starts=list[768,], ends=list[896,], )
paddle.nn.functional.pad(Tensor([3, 4, 0, 6],"complex64"), pad=list[1,1,1,1,], mode="constant", value=100, data_format="NCHW", name=None, )
paddle.nn.functional.conv1d(Tensor([1, 128, 112],"float32"), Tensor([128, 128, 0],"float32"), bias=Tensor([128],"float32"), padding=5, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[516,1,1,], )
paddle.Tensor.tile(Tensor([8, 0, 512],"float32"), tuple(1,9,1,), )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 0, 8],"float32"), Tensor([6, 8, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[2,2,2,], dilation=list[2,1,2,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.pad(Tensor([1, 2, 3, 4, 0],"complex128"), pad=list[1,2,1,1,3,4,], mode="constant", value=100, )
paddle.Tensor.tile(Tensor([10285, 0],"float32"), list[1,238,], )
paddle.nn.functional.pad(Tensor([1, 2, 3, 0],"float64"), pad=list[2,1,2,1,], mode="replicate", value=0.0, data_format="NHWC", name=None, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[413,1,1,], )
paddle.Tensor.__getitem__(Tensor([3, 0, 3],"float32"), tuple(slice(1,-1,None),slice(0,2,None),slice(None,None,-1),), )
paddle.nn.functional.group_norm(Tensor([2, 3, 0, 2, 4],"float64"), num_groups=2, weight=Tensor([4],"float64"), bias=Tensor([4],"float64"), data_format="NDHWC", )
paddle.nn.functional.max_pool2d(Tensor([2, 3, 32, 0],"float32"), kernel_size=2, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([0, 1, 2, 3, 2],"float64"), pad=list[1,0,1,0,0,1,], mode="replicate", value=0.0, data_format="NDHWC", name=None, )
paddle.nn.functional.max_pool2d(Tensor([0, 3, 224, 224],"float32"), kernel_size=list[5,3,], stride=list[1,2,], padding=tuple(2,1,), )
paddle.allclose(Tensor([0, 32],"float32"), Tensor([0, 32],"float32"), )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 8, 8],"float64"), weight=Tensor([3, 0, 5, 5],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,2,3,4,], output_padding=0, dilation=1, groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d(Tensor([1024, 1, 131, 131],"float32"), Tensor([1, 1, 0, 4],"float32"), )
paddle.nn.functional.mse_loss(Tensor([3, 3, 10, 0],"float32"), Tensor([3, 3, 10, 0],"float32"), "sum", )
paddle.nn.functional.conv3d_transpose(Tensor([4, 4, 8, 8, 8],"float32"), Tensor([4, 3, 0, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,1,2,2,1,1,], stride=1, dilation=1, groups=2, data_format="NCDHW", )
paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 0],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 1, 50, 50],"float16"), scale=0.125, )
paddle.slice(Tensor([1024, 16, 0],"float16"), axes=list[0,], starts=list[384,], ends=list[512,], )
paddle.nn.functional.conv2d(Tensor([4, 16, 16, 3],"float32"), Tensor([5, 3, 0, 3],"float32"), Tensor([5],"float32"), padding=list[1,2,], stride=1, dilation=1, groups=1, data_format="NHWC", )
paddle.nn.functional.conv1d_transpose(Tensor([0, 6, 16],"float32"), Tensor([6, 8, 3],"float32"), bias=Tensor([8],"float32"), output_size=None, output_padding=0, padding=list[2,], stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.Tensor.tile(Tensor([1, 6, 0],"float32"), list[331,1,1,], )
paddle.nn.functional.max_pool2d(Tensor([0, 384, 32, 32],"float32"), kernel_size=13, stride=1, padding=6, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.kthvalue(Tensor([2, 0, 10],"float64"), 2, -1, )
paddle.nonzero(Tensor([10, 0, 28, 28],"float32"), )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float64"), weight=Tensor([1, 3, 0, 3],"float64"), bias=Tensor([1],"float64"), stride=list[2,1,], padding=1, dilation=tuple(2,2,), )
paddle.incubate.nn.functional.fused_feedforward(Tensor([0, 98, 508],"float64"), Tensor([508, 130],"float64"), Tensor([130, 508],"float64"), Tensor([130],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )
paddle.nanmedian(Tensor([0, 3],"float64"), axis=1, keepdim=False, mode="min", )
paddle.einsum("ijbs,ibns->bnij", Tensor([0, 7, 14, 2],"float32"), Tensor([0, 14, 4, 2],"float32"), )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 0, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=False, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float64"), weight=Tensor([3, 1, 3, 0],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,], dilation=1, )
paddle.strided_slice(x=Tensor([0, 6],"float32"), axes=list[0,1,], starts=list[3,4,], ends=list[5,2,], strides=list[4,-2,], )
paddle.nn.functional.conv1d(Tensor([1, 256, 28],"float32"), Tensor([256, 256, 0],"float32"), bias=Tensor([256],"float32"), padding=15, stride=list[1,], dilation=list[5,], groups=1, data_format="NCL", )
paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 8, 1, 0],"float32"), Tensor([1, 8, 113, 0],"float32"), )
paddle.nn.functional.avg_pool3d(x=Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[0,0,0,], exclusive=False, )
paddle.nn.functional.avg_pool2d(Tensor([0, 1024, 17, 17],"float16"), kernel_size=3, stride=1, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.einsum("i...->...", Tensor([5, 10, 3, 0],"float64"), )
paddle.einsum("a...b,b...c,c...a", Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 0, 4],"float64"), )
paddle.einsum("mbnd,mlb->lbnd", Tensor([1, 14, 4, 8],"float32"), Tensor([1, 0, 14],"float32"), )
paddle.allclose(Tensor([1124, 0],"float32"), Tensor([1124, 0],"float32"), )
paddle.nn.quant.weight_quantize(Tensor([128, 0],"float16"), algo="weight_only_int8", group_size=-1, )
paddle.nn.functional.sequence_mask(Tensor([2, 0],"int64"), maxlen=Tensor([1],"int32"), dtype=VarType(int32), )
paddle.nn.functional.scaled_dot_product_attention(Tensor([4, 134, 4, 128],"float16"), Tensor([4, 134, 4, 128],"float16"), Tensor([0, 134, 4, 128],"float16"), attn_mask=Tensor([4, 1, 134, 134],"float16"), dropout_p=0.0, training=True, is_causal=False, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[288,1,1,], )
paddle.einsum("nbka,ahc->nbkhc", Tensor([1, 3, 0, 2],"float32"), Tensor([2, 2, 4],"float32"), )
paddle.nn.functional.conv2d_transpose(Tensor([4, 512, 0, 32],"float32"), Tensor([512, 1, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=512, output_size=None, data_format="NCHW", )
paddle.nn.functional.max_pool2d(Tensor([1, 0, 4, 4],"float32"), kernel_size=2, stride=2, padding=0, return_mask=True, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([0, 3, 5, 7, 7],"float32"), output_size=5, data_format="NCDHW", name=None, )
paddle.nn.functional.pad(Tensor([1, 0, 14, 14, 384],"float16"), tuple(0,0,0,0,0,0,), data_format="NDHWC", )
paddle.nn.functional.conv2d(Tensor([4, 3, 16, 16],"float32"), Tensor([5, 3, 3, 0],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[0,0,],list[1,1,],list[2,2,],], stride=1, dilation=1, groups=1, data_format="NCHW", )
paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 0],"float32"), bias=None, act_method="gelu", )
paddle.nn.functional.conv2d_transpose(x=Tensor([0, 3, 2, 2],"float32"), weight=Tensor([3, 1, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=list[1,0,], dilation=1, )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[752,1,1,], )
paddle.nn.functional.conv1d(x=Tensor([2, 3, 4],"float64"), weight=Tensor([1, 3, 3],"float64"), bias=Tensor([0],"float64"), stride=2, padding=0, )
paddle.slice(Tensor([1024, 16, 0],"float16"), axes=list[0,], starts=list[896,], ends=list[1024,], )
paddle.nn.functional.fractional_max_pool3d(x=Tensor([2, 0, 7, 7, 7],"float32"), kernel_size=list[2,2,2,], output_size=list[3,3,3,], random_u=0.6, )
paddle.einsum("...ij,...jk->...ik", Tensor([1, 5],"float64"), Tensor([5, 0],"float64"), )
paddle.nn.functional.pad(Tensor([3, 4, 0],"complex64"), pad=list[1,1,], mode="constant", value=100, data_format="NCL", name=None, )
paddle.nn.functional.pad(Tensor([1183744, 1, 0, 1],"float32"), list[0,0,0,0,], )
paddle.nn.functional.pad(Tensor([12, 0, 16, 16],"float32"), list[2,1,2,1,], )
paddle.nn.functional.pad(Tensor([3, 0, 40, 40],"float32"), pad=list[0,0,2,2,], )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 0, 7, 7],"float32"), output_size=list[3,3,3,], data_format="NCDHW", name=None, )
paddle.nn.functional.conv3d(Tensor([4, 8, 0, 8, 3],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[1,1,],list[2,2,],list[1,1,],list[0,0,],], stride=1, dilation=1, groups=1, data_format="NDHWC", )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 0, 32],"float64"), output_size=3, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 0, 4],"float32"), weight=Tensor([1, 3, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=list[2,2,1,], padding=list[1,2,2,], dilation=tuple(2,2,2,), )
paddle.nn.functional.layer_norm(Tensor([7, 0, 1024],"float32"), 1024, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), epsilon=1e-05, )
paddle.allclose(Tensor([2, 0, 8, 8],"float32"), Tensor([2, 0, 8, 8],"float32"), )
paddle.Tensor.tile(Tensor([1, 4, 0],"float32"), list[307,1,1,], )
paddle.nn.functional.selu(Tensor([3, 5, 0, 10],"float64"), 1.5, 2.0, None, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 16],"float32"), Tensor([6, 8, 3, 0],"float32"), bias=Tensor([8],"float32"), padding=list[list[0,0,],list[0,0,],list[1,2,],list[2,1,],], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.softmax(Tensor([0, 1, 100],"float32"), axis=-1, dtype=Dtype(float32), )
paddle.nn.functional.max_pool2d(x=Tensor([2, 0, 32, 32],"float64"), kernel_size=list[3,3,], stride=tuple(1,2,), )
paddle.nn.functional.log_softmax(x=Tensor([2, 3, 0],"float64"), axis=1, )
paddle.nn.functional.conv2d_transpose(Tensor([1, 128, 128, 128],"float32"), Tensor([128, 64, 0, 3],"float32"), bias=Tensor([64],"float32"), padding=1, output_padding=1, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.einsum("bhlqk,bhkd->bhlqd", Tensor([52, 4, 3, 0, 1],"float32"), Tensor([52, 4, 1, 8],"float32"), )
paddle.nn.functional.max_pool1d(Tensor([2, 3, 0],"float64"), 2, 1, 1, False, False, None, )
paddle.nn.functional.max_pool2d(Tensor([13, 0, 4, 1],"float32"), tuple(2,1,), stride=tuple(2,1,), ceil_mode=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 0, 64],"float16"), Tensor([2, 100, 1, 64],"float16"), Tensor([2, 100, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 100],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.fractional_max_pool3d(Tensor([2, 0, 7, 7, 7],"float32"), output_size=list[2,3,5,], kernel_size=None, random_u=0.7, return_mask=False, name=None, )
paddle.nn.functional.max_pool2d(Tensor([3, 0, 18, 18],"float32"), kernel_size=tuple(3,3,), stride=tuple(2,2,), padding=tuple(0,0,), return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[225,1,1,], )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 2, 2, 2, 3],"float32"), weight=Tensor([3, 1, 0, 3, 3],"float32"), bias=Tensor([3],"float32"), output_padding=1, stride=2, padding=list[1,1,1,], groups=3, data_format="NDHWC", dilation=1, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float64"), weight=Tensor([3, 0, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=1, dilation=1, )
paddle.allclose(Tensor([14, 0, 16],"float32"), Tensor([14, 0, 16],"float32"), atol=1e-05, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 1024, 16, 0],"float32"), Tensor([1024, 1, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1024, output_size=None, data_format="NCHW", )
paddle.nn.quant.weight_only_linear(Tensor([131, 768],"float16"), weight=Tensor([2304, 768],"int8"), bias=None, weight_scale=Tensor([0],"float16"), weight_dtype="int8", )
paddle.logcumsumexp(Tensor([3, 0],"float32"), dtype="float32", )
paddle.nn.functional.max_unpool3d(Tensor([1, 3, 0, 2, 3],"float64"), Tensor([1, 3, 0, 2, 3],"int32"), kernel_size=2, stride=2, )
paddle.nn.functional.conv3d(Tensor([4, 8, 8, 0, 3],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[1,2,1,], stride=1, dilation=1, groups=1, data_format="NDHWC", )
paddle.einsum("ijk, ikl->ijl", Tensor([3, 0, 3],"float64"), Tensor([3, 3, 10],"float64"), )
paddle.nn.functional.pad(Tensor([0, 3, 224, 224],"float32"), list[0,1,0,0,], )
paddle.Tensor.digamma(Tensor([0, 5],"float64"), )
paddle.nn.functional.pad(Tensor([1, 0, 14, 12],"float32"), pad=list[1,0,1,2,], mode="constant", value=0.0, data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[387,1,1,], )
paddle.Tensor.tile(Tensor([1, 0],"float32"), tuple(22,1,), )
paddle.nn.quant.weight_only_linear(Tensor([1, 32, 64],"float16"), Tensor([0, 64],"int8"), bias=Tensor([256],"float16"), weight_scale=Tensor([256],"float16"), weight_dtype="int8", group_size=-1, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[414,1,1,], )
paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([0],"float32"), shift=None, smooth=None, quant_scale=0.0009313154732808471, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float32"), weight=Tensor([1, 3, 3, 0],"float32"), bias=Tensor([1],"float32"), stride=1, padding=0, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 0, 2],"float64"), Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([3],"float64"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=3, output_size=None, data_format="NCDHW", )
paddle.nn.functional.max_pool1d(Tensor([2, 3, 0],"float64"), 3, 4, 0, False, False, None, )
paddle.nn.functional.softmax(Tensor([16, 11, 64, 0],"float32"), axis=1, )
paddle.nn.functional.mse_loss(Tensor([2, 0, 10],"float32"), Tensor([2, 0, 10],"float32"), "mean", )
paddle.slice(Tensor([16, 0, 104],"float32"), list[0,1,2,], list[0,0,80,], list[16,1,96,], )
paddle.nanmedian(Tensor([4, 0],"float64"), axis=1, mode="min", )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 0, 7, 7],"float32"), output_size=list[3,3,3,], data_format="NDHWC", )
paddle.nn.functional.conv2d(Tensor([1, 1024, 128, 0],"float32"), Tensor([1024, 256, 3, 0],"float32"), padding=1, groups=4, )
paddle.slice(Tensor([1024, 0, 1024],"float16"), axes=list[0,], starts=list[512,], ends=list[768,], )
paddle.nn.functional.pad(Tensor([4, 3, 6, 0, 6],"float32"), pad=list[2,2,2,2,2,2,], mode="constant", value=0.0, data_format="NCDHW", name=None, )
paddle.einsum("nbka,ahc->nbkhc", Tensor([0, 3, 4, 2],"float32"), Tensor([2, 2, 4],"float32"), )
paddle.Tensor.tile(Tensor([1, 0, 64, 64, 2],"float32"), tuple(16,10,1,1,1,), )
paddle.nn.functional.pad(Tensor([0, 3, 140, 160],"float64"), pad=list[40,40,0,0,], mode="constant", value=0.0, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([20, 256, 19, 34],"float32"), Tensor([256, 0, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=256, output_size=None, data_format="NCHW", )
paddle.nn.functional.softmax(x=Tensor([0, 3, 4],"float64"), axis=1, )
paddle.nn.functional.pad(Tensor([1, 0, 1, 1],"float16"), pad=list[0,0,0,1,], mode="replicate", value=0.0, data_format="NCHW", name=None, )
paddle.nn.functional.conv2d(Tensor([1, 2048, 16, 16],"float32"), Tensor([12, 512, 0, 1],"float32"), padding=0, groups=4, )
paddle.nn.functional.avg_pool2d(Tensor([0, 1280, 5, 5],"float32"), kernel_size=3, stride=1, padding=1, ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.softmax(Tensor([2, 0, 2048],"float32"), axis=0, )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[156,1,1,], )
paddle.roll(Tensor([0],"float64"), Tensor([1],"int64"), list[0,], name=None, )
paddle.index_fill(Tensor([0],"int64"), Tensor([0],"int64"), 0, 2, )
paddle.nn.functional.avg_pool2d(Tensor([0, 512, 3, 80],"float32"), kernel_size=list[2,2,], stride=list[2,2,], padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 8, 16],"float16"), Tensor([1, 2048, 2, 16],"float16"), Tensor([1, 2048, 0, 16],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), is_causal=True, )
paddle.nn.functional.max_pool2d(Tensor([0, 384, 32, 32],"float32"), kernel_size=5, stride=1, padding=2, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.max_pool2d(Tensor([13, 0, 7, 7],"float32"), kernel_size=1, stride=2, padding=0, )
paddle.nn.functional.pad(Tensor([4, 0, 6, 6, 6],"float64"), pad=list[2,2,2,2,2,2,], mode="constant", value=0.0, data_format="NCDHW", name=None, )
paddle.vision.ops.prior_box(Tensor([4, 96, 0, 40],"float32"), Tensor([4, 3, 0, 640],"float32"), list[32.0,48.0,64.0,80.0,96.0,128.0,], list[], list[1.0,], list[0.1,0.1,0.2,0.2,], False, False, list[16.0,16.0,], 0.5, False, None, )
paddle.nonzero(Tensor([10, 0],"float32"), as_tuple=False, )
paddle.flip(Tensor([2, 0],"float32"), 1, )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[176,1,1,], )
paddle.nn.functional.conv1d(Tensor([13, 20, 0],"float32"), Tensor([256, 20, 5],"float32"), bias=None, padding=2, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.log_softmax(Tensor([0, 3, 4, 5],"float32"), -1, )
paddle.incubate.nn.functional.fused_feedforward(Tensor([0, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 1, 64],"float16"), Tensor([1, 2048, 1, 64],"float16"), Tensor([0, 2048, 1, 64],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), is_causal=True, )
paddle.Tensor.tile(Tensor([10285, 0],"float32"), list[1,29,], )
paddle.einsum("i,i", Tensor([0],"float64"), Tensor([0],"float64"), )
paddle.nn.functional.conv2d_transpose(Tensor([1, 1024, 16, 16],"float32"), Tensor([1024, 256, 4, 0],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.maxout(x=Tensor([9, 2, 0, 6],"float64"), groups=2, axis=3, )
paddle.nn.functional.conv2d(Tensor([16, 3, 268, 268],"float32"), weight=Tensor([3, 1, 0, 13],"float32"), groups=3, )
paddle.nn.functional.ctc_loss(Tensor([40, 128, 6625],"float32"), Tensor([128, 25],"int32"), Tensor([0],"int64"), Tensor([128],"int64"), 0, "none", norm_by_times=False, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[86,1,1,], )
paddle.nn.functional.channel_shuffle(Tensor([2, 4, 0, 9],"float64"), 3, "NHWC", None, )
paddle.nn.functional.mse_loss(Tensor([16, 0, 1],"float32"), Tensor([16, 0, 1],"float32"), )
paddle.meshgrid(list[Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),], )
paddle.nn.functional.softmax(Tensor([104, 0, 18, 18],"float16"), )
paddle.einsum("i,j->ij", Tensor([0],"float32"), Tensor([128],"float32"), )
paddle.nn.functional.avg_pool2d(Tensor([2, 0, 32, 32],"float64"), kernel_size=list[2,2,], stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 0],"float64"), weight=Tensor([3, 2, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=0, dilation=1, )
paddle.slice(Tensor([0, 1, 104],"float32"), list[0,1,2,], list[0,0,48,], list[13,1,64,], )
paddle.nn.functional.conv2d_transpose(Tensor([4, 4, 0, 16],"float32"), Tensor([4, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,2,], stride=1, dilation=1, groups=2, data_format="NCHW", )
paddle.einsum("bmtd,mdhr->bmhtr", Tensor([13, 2, 0, 16],"float32"), Tensor([2, 16, 0, 1],"float32"), )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 0, 8, 32, 32],"float64"), output_size=list[1,1,1,], data_format="NCDHW", name=None, )
paddle.allclose(Tensor([16, 0],"float32"), Tensor([16, 0],"float32"), )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 2],"float32"), weight=Tensor([3, 2, 0, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=0, dilation=1, )
paddle.nn.functional.pad(Tensor([1, 3, 204, 0],"float64"), pad=list[1,3,2,4,], mode="constant", value=0.0, data_format="NCHW", )
paddle.nn.functional.conv3d_transpose(Tensor([4, 3, 8, 8, 8],"float32"), Tensor([3, 5, 0, 3, 3],"float32"), Tensor([5],"float32"), output_size=None, padding=0, stride=1, dilation=1, groups=1, data_format="NCDHW", )
paddle.subtract(Tensor([1, 1, 30, 0],"float32"), Tensor([1, 1, 30, 0],"float32"), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[691,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([4, 3, 7, 7],"float32"), Tensor([3, 6, 5, 0],"float32"), bias=Tensor([6],"float32"), padding=2, output_padding=list[1,1,], stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.avg_pool3d(Tensor([3, 0, 3, 1600, 7],"float32"), kernel_size=tuple(1,1,5,), stride=1, )
paddle.nn.functional.conv2d(Tensor([1, 128, 12, 0],"float32"), Tensor([128, 128, 3, 0],"float32"), None, list[1,1,], 1, list[1,1,], 1, )
paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), Tensor([256],"float32"), Tensor([0],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float16"), residual=Tensor([16, 256],"float16"), residual_alpha=0.69204696, quant_scale=0.15, quant_round_type=1, quant_max_bound=127, quant_min_bound=-127, )
paddle.nn.functional.conv1d(Tensor([13, 1, 1024],"float32"), Tensor([32, 1, 0],"float32"), bias=None, padding=0, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.Tensor.__setitem__(Tensor([0, 5, 4, 3],"bool"), list[list[2,3,4,],list[1,2,5,],], 100, )
paddle.nn.functional.conv1d(Tensor([16, 64, 0],"float32"), Tensor([128, 64, 3],"float32"), bias=Tensor([128],"float32"), padding=4, stride=list[1,], dilation=list[4,], groups=1, data_format="NCL", )
paddle.nn.functional.pad(Tensor([1, 0, 224, 224],"float32"), list[0,1,0,0,], )
paddle.nn.functional.max_unpool2d(Tensor([1, 0, 2, 2],"int64"), Tensor([1, 0, 2, 2],"int32"), kernel_size=2, stride=None, output_size=list[1,1,4,5,], )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 0, 2, 2],"float64"), Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=1, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.pad(Tensor([16, 25500, 0],"float32"), pad=list[256,256,], mode="reflect", data_format="NLC", )
paddle.nn.functional.pad(Tensor([1, 16, 0, 14, 384],"float32"), tuple(0,0,0,0,0,0,), data_format="NDHWC", )
paddle.Tensor.__rpow__(Tensor([4, 5, 0],"float32"), 2, )
paddle.where(Tensor([0, 13091],"float32"), )
paddle.cummin(Tensor([100, 0],"float32"), axis=-2, dtype="int32", )
paddle.slice(Tensor([21, 0, 4],"float32"), axes=list[2,], starts=list[1,], ends=list[2,], )
paddle.nn.functional.conv2d(Tensor([64, 1, 28, 28],"float32"), Tensor([6, 1, 0, 3],"float32"), bias=None, padding=1, stride=list[1,1,], dilation=list[1,1,], groups=1, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 16],"float32"), Tensor([6, 0, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[1,2,], dilation=list[2,2,], groups=1, output_size=list[20,36,], data_format="NCHW", )
paddle.Tensor.tile(Tensor([8, 1, 0],"float32"), tuple(1,4,1,), )
paddle.Tensor.isnan(Tensor([3, 3, 0],"float32"), )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[70,1,1,], )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 8, 3, 3, 0],"float32"), bias=Tensor([8],"float32"), padding=list[list[0,0,],list[0,0,],list[2,3,],list[1,2,],list[2,1,],], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 3, 4, 0],"float64"), output_size=list[1,4,], data_format="NCHW", name=None, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 8, 96],"float16"), Tensor([2, 100, 8, 96],"float16"), Tensor([2, 100, 0, 96],"float16"), attn_mask=None, is_causal=False, )
paddle.nn.functional.log_softmax(Tensor([2, 3, 0],"float64"), 2, )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 0, 32],"float64"), output_size=tuple(3,3,3,), )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 1, 64],"float16"), Tensor([1, 2048, 1, 64],"float16"), Tensor([1, 2048, 0, 64],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), is_causal=False, )
paddle.nn.functional.pad(Tensor([0, 25, 3],"float32"), list[0,1,], "constant", 1.0, data_format="NCL", )
paddle.nn.functional.conv2d(Tensor([1, 2048, 0, 128],"float32"), Tensor([24, 256, 0, 1],"float32"), padding=0, groups=8, )
paddle.nn.functional.log_softmax(Tensor([2, 3, 0],"float64"), 1, )
paddle.nn.functional.pad(Tensor([0, 3, 4],"float32"), list[1,1,], mode="replicate", data_format="NCL", )
paddle.nn.functional.max_pool3d(x=Tensor([2, 3, 8, 8, 0],"float64"), kernel_size=list[3,3,3,], stride=2, padding=1, )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([0, 3, 8, 32, 32],"float64"), output_size=list[2,2,2,], )
paddle.nn.functional.poisson_nll_loss(Tensor([0, 2],"float16"), Tensor([0, 2],"float16"), log_input=True, full=False, epsilon=1e-08, reduction="mean", )
paddle.nn.functional.max_pool3d(Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,0,0,], return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[4101,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([1, 256, 128, 0],"float32"), Tensor([256, 128, 3, 3],"float32"), padding=0, stride=2, groups=1, )
paddle.nn.functional.avg_pool3d(Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=list[1,0,0,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
paddle.einsum("bind,bjnd->bnij", Tensor([13, 2, 0, 4],"float32"), Tensor([13, 4, 0, 4],"float32"), )
paddle.nn.functional.conv1d_transpose(Tensor([4, 16, 6],"float32"), Tensor([6, 8, 0],"float32"), bias=Tensor([8],"float32"), output_size=None, output_padding=2, padding=0, stride=list[3,], dilation=list[1,], groups=1, data_format="NLC", )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 0],"float32"), weight=Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=1, dilation=1, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([0, 3, 7, 7],"float32"), output_size=list[3,3,], data_format="NHWC", name=None, )
paddle.nn.functional.pad(Tensor([0, 25500, 1],"float32"), pad=list[1024,1024,], mode="reflect", data_format="NLC", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[2591,1,1,], )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 0],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )
paddle.nn.functional.avg_pool2d(x=Tensor([2, 0, 4, 4],"float64"), kernel_size=list[3,3,], stride=list[3,3,], padding=list[0,0,0,0,], ceil_mode=False, exclusive=False, )
paddle.nn.functional.pixel_shuffle(Tensor([4, 128, 0, 128],"float32"), 2, "NCHW", None, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float64"), weight=Tensor([1, 3, 0, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=list[2,2,1,], padding=1, dilation=list[2,2,2,], )
paddle.nn.functional.margin_ranking_loss(Tensor([10, 0],"float64"), Tensor([10, 0],"float64"), Tensor([10, 0],"float64"), 0.0, "sum", None, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([0, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=True, )
paddle.nn.functional.conv1d_transpose(Tensor([0, 3, 2],"float32"), Tensor([3, 1, 3],"float32"), bias=Tensor([3],"float32"), output_size=None, output_padding=0, padding=list[1,], stride=list[1,], dilation=list[1,], groups=3, data_format="NCL", )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 0, 7, 7],"float32"), output_size=5, data_format="NCHW", name=None, )
paddle.allclose(Tensor([0, 3],"float32"), Tensor([0, 3],"float32"), atol=0.01, )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 5, 7, 0],"float32"), output_size=5, )
paddle.nn.functional.pad(Tensor([0, 1, 3, 1600, 3],"float32"), pad=list[2,2,0,0,0,0,], data_format="NCDHW", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[760,1,1,], )
paddle.nn.functional.pad(Tensor([0, 16, 15, 14, 384],"float32"), tuple(0,0,0,6,0,0,), data_format="NDHWC", )
paddle.nn.functional.scaled_dot_product_attention(query=Tensor([2, 64, 12, 64],"float16"), key=Tensor([2, 64, 12, 64],"float16"), value=Tensor([2, 0, 12, 64],"float16"), is_causal=True, )
paddle.nn.functional.pad(Tensor([1, 16, 61, 0, 96],"float32"), tuple(0,0,0,2,0,0,), data_format="NDHWC", )
paddle.Tensor.tile(Tensor([19125, 0],"float32"), list[1,54,], )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([0, 3, 7, 7],"float32"), output_size=list[2,5,], data_format="NCHW", name=None, )
paddle.nn.functional.conv1d(Tensor([1, 256, 0],"float32"), Tensor([256, 256, 3],"float32"), bias=Tensor([256],"float32"), padding=1, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv2d(Tensor([1, 128, 0, 32],"float32"), Tensor([128, 128, 0, 3],"float32"), None, list[1,1,], 1, list[1,1,], 1, )
paddle.nn.functional.max_pool3d(Tensor([2, 3, 32, 32, 0],"float32"), kernel_size=2, stride=None, padding=0, return_mask=False, ceil_mode=True, data_format="NCDHW", name=None, )
paddle.nn.functional.layer_norm(Tensor([0, 186, 1024],"float32"), 1024, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), epsilon=1e-05, )
paddle.nn.functional.conv1d(x=Tensor([2, 4, 3],"float32"), weight=Tensor([2, 3, 0],"float32"), bias=Tensor([2],"float32"), stride=1, padding=0, data_format="NLC", )
paddle.einsum("bind,bjnd->bnij", Tensor([13, 0, 4, 4],"float32"), Tensor([13, 4, 4, 4],"float32"), )
paddle.nn.functional.conv3d_transpose(Tensor([4, 3, 8, 8, 0],"float32"), Tensor([3, 5, 3, 3, 3],"float32"), Tensor([5],"float32"), output_size=None, padding=0, stride=1, dilation=1, groups=1, data_format="NCDHW", )
paddle.nn.functional.softmax(Tensor([14, 0, 7, 7],"float32"), axis=3, )
paddle.nn.functional.conv2d(Tensor([8, 128, 255, 0],"float32"), Tensor([256, 128, 1, 0],"float32"), bias=None, stride=2, padding=0, )
paddle.Tensor.fill_diagonal_(Tensor([3, 0],"float32"), 4, 1, False, )
paddle.nn.functional.conv1d(Tensor([4, 16, 6],"float32"), Tensor([8, 6, 0],"float32"), bias=Tensor([8],"float32"), padding=2, stride=list[1,], dilation=list[1,], groups=1, data_format="NLC", )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 0, 8, 32, 32],"float64"), output_size=list[2,2,2,], )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 3, 0, 4],"float64"), output_size=list[1,4,], data_format="NCHW", name=None, )
paddle.nn.functional.fractional_max_pool2d(x=Tensor([2, 0, 7, 7],"float64"), output_size=list[3,3,], random_u=None, )
paddle.nn.functional.kl_div(Tensor([5, 0],"float32"), Tensor([5, 0],"float64"), "mean", False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 1, 64],"float16"), Tensor([1, 2048, 1, 0],"float16"), Tensor([1, 2048, 1, 64],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), is_causal=True, )
paddle.slice(Tensor([1024, 0, 1024],"float16"), axes=list[0,], starts=list[512,], ends=list[640,], )
paddle.nn.functional.conv2d_transpose(Tensor([4, 16, 16, 4],"float32"), Tensor([4, 3, 0, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=0, stride=1, dilation=1, groups=2, data_format="NHWC", )
paddle.vision.ops.distribute_fpn_proposals(Tensor([0, 4],"float32"), 2, 5, 4, 224, rois_num=Tensor([8],"int64"), )
paddle.nn.functional.kl_div(Tensor([40, 0, 50],"float32"), Tensor([40, 0, 50],"float32"), "batchmean", True, )
paddle.Tensor.tile(Tensor([8, 1, 1, 0],"float32"), list[1,3,4,1,], )
paddle.slice(Tensor([3, 0],"float64"), axes=list[0,], starts=list[2,], ends=list[3,], )
paddle.nn.functional.conv1d_transpose(Tensor([0, 3, 16],"float32"), Tensor([3, 2, 3],"float32"), bias=Tensor([6],"float32"), output_size=None, output_padding=0, padding="valid", stride=list[1,], dilation=list[1,], groups=3, data_format="NCL", )
paddle.Tensor.tile(Tensor([1, 6, 0],"float32"), list[434,1,1,], )
paddle.nn.functional.conv1d(Tensor([1, 128, 112],"float32"), Tensor([128, 128, 0],"float32"), bias=Tensor([128],"float32"), padding=25, stride=list[1,], dilation=list[5,], groups=1, data_format="NCL", )
paddle.nn.functional.conv1d(x=Tensor([2, 3, 4],"float64"), weight=Tensor([1, 3, 3],"float64"), bias=Tensor([0],"float64"), stride=list[1,], padding=1, dilation=2, )
paddle.nn.quant.weight_quantize(Tensor([0, 256],"float16"), algo="weight_only_int8", group_size=-1, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 0, 4],"float64"), weight=Tensor([1, 3, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=list[2,2,1,], padding=list[1,2,2,], dilation=tuple(2,2,2,), )
paddle.Tensor.tile(Tensor([10105, 0],"float32"), list[1,12,], )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([0],"float64"), stride=1, padding=1, dilation=1, )
paddle.nn.functional.conv2d_transpose(Tensor([0, 3, 2, 2],"float32"), Tensor([3, 1, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(x=Tensor([0, 3, 2, 2],"float32"), weight=Tensor([3, 1, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=1, dilation=1, )
paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([0, 3, 5, 2, 4],"float32"), Tensor([0, 3, 4, 2, 4],"float32"), )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float64"), Tensor([3, 1, 3, 0, 3],"float64"), bias=Tensor([3],"float64"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=3, output_size=None, data_format="NCDHW", )
paddle.kthvalue(Tensor([30, 200, 0],"float32"), k=1, axis=1, )
paddle.Tensor.frexp(Tensor([4, 5, 0],"float64"), )
paddle.nn.functional.conv2d(Tensor([2, 192, 4, 4],"float32"), Tensor([48, 192, 1, 0],"float32"), None, list[1,1,], 0, list[1,1,], 1, "NCHW", )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[1312,1,1,], )
paddle.nn.functional.avg_pool2d(Tensor([0, 128, 20, 20],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 6, 0, 2],"float64"), Tensor([6, 1, 3, 3],"float64"), bias=Tensor([3],"float64"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=3, output_size=None, data_format="NCHW", )
paddle.nn.functional.lp_pool1d(Tensor([0, 32, 3],"float32"), norm_type=7, kernel_size=2, stride=2, padding=list[1,], ceil_mode=True, data_format="NLC", )
paddle.nn.functional.conv2d_transpose(x=Tensor([0, 3, 2, 2],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=1, dilation=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.log_softmax(x=Tensor([0, 3, 4],"float64"), axis=2, dtype=type(numpy.float32), )
paddle.nn.functional.conv1d(Tensor([1, 1280, 0],"float32"), Tensor([1280, 1280, 3],"float32"), bias=Tensor([1280],"float32"), padding=1, stride=list[2,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.roll(Tensor([0],"float32"), Tensor([1],"int64"), list[0,], name=None, )
paddle.nn.functional.pad(Tensor([2, 0, 16, 16],"float32"), list[0,1,0,1,], value=0, )
paddle.Tensor.tile(Tensor([1, 4, 0],"float32"), list[473,1,1,], )
paddle.logaddexp(Tensor([0, 2, 3, 4],"float64"), Tensor([0, 2, 3, 4],"float64"), )
paddle.einsum("blq,bhlk->bhlqk", Tensor([13, 5, 1],"float32"), Tensor([13, 0, 5, 2],"float32"), )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float32"), weight=Tensor([1, 3, 3, 3],"float32"), bias=Tensor([0],"float32"), stride=list[2,1,], padding=list[1,2,], dilation=tuple(2,2,), )
paddle.nn.functional.conv3d_transpose(Tensor([0, 3, 2, 2, 2],"float64"), Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=1, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.Tensor.isnan(Tensor([3, 0, 3, 4, 2, 5],"float64"), )
paddle.Tensor.tile(Tensor([2, 0, 1, 1],"float32"), tuple(4,1,4,4,), )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 0, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[479,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 16],"float32"), Tensor([6, 8, 3, 0],"float32"), bias=Tensor([8],"float32"), padding=list[1,2,2,1,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 1024, 8, 16],"bfloat16"), Tensor([1, 1024, 2, 16],"bfloat16"), Tensor([0, 1024, 2, 16],"bfloat16"), attn_mask=None, is_causal=True, )
paddle.nn.functional.conv3d_transpose(Tensor([4, 4, 8, 8, 8],"float32"), Tensor([4, 0, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,2,1,], stride=1, dilation=1, groups=2, data_format="NCDHW", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 16],"float32"), Tensor([6, 0, 1, 1],"float32"), bias=Tensor([8],"float32"), padding=tuple(2,3,), output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.avg_pool3d(Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
paddle.index_fill(Tensor([8],"int64"), Tensor([0],"int64"), 0, 2, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([0, 256, 2, 25],"float16"), output_size=list[1,25,], data_format="NCHW", name=None, )
paddle.nn.functional.max_pool3d(Tensor([0, 3, 4, 4, 4],"float64"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], return_mask=False, ceil_mode=True, data_format="NCDHW", name=None, )
paddle.nn.functional.conv1d_transpose(x=Tensor([0, 3, 2],"float64"), weight=Tensor([3, 1, 3],"float64"), bias=Tensor([3],"float64"), stride=1, padding=list[1,], output_padding=0, groups=3, dilation=1, output_size=None, data_format="NCL", name=None, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 160, 0, 11],"float32"), tuple(1,1,), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[265,1,1,], )
paddle.vision.ops.generate_proposals(Tensor([0, 9, 12, 9],"float32"), Tensor([1, 36, 12, 9],"float32"), Tensor([1, 2],"float32"), Tensor([972, 4],"float32"), Tensor([972, 4],"float32"), pre_nms_top_n=4000, post_nms_top_n=4000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.square_error_cost(Tensor([8, 100, 0],"float16"), Tensor([8, 100, 0],"float32"), )
paddle.cummax(Tensor([0, 100],"float32"), )
paddle.nn.functional.avg_pool2d(Tensor([0, 3, 16, 16],"float32"), kernel_size=2, stride=2, )
paddle.nn.functional.avg_pool3d(x=Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=list[1,0,0,], exclusive=True, )
paddle.einsum("..., ...", Tensor([3, 10],"float64"), Tensor([0, 3, 10],"float64"), )
paddle.incubate.nn.functional.fused_feedforward(x=Tensor([0, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln2_scale=Tensor([2],"float32"), ln2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[1021,1,1,], )
paddle.nn.functional.pad(Tensor([12, 0, 32, 32],"float32"), tuple(0,0,0,0,), data_format="NHWC", )
paddle.nn.functional.conv1d_transpose(Tensor([0, 6, 16],"float32"), Tensor([6, 8, 3],"float32"), bias=Tensor([8],"float32"), output_size=None, output_padding=0, padding="valid", stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.einsum("...jk->...kj", Tensor([3, 0, 3],"float64"), )
paddle.index_fill(Tensor([128],"int64"), Tensor([0],"int64"), 0, 6, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 2, 2, 2, 3],"float32"), Tensor([3, 0, 3, 3, 3],"float32"), bias=Tensor([3],"float32"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=3, output_size=None, data_format="NDHWC", )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 7, 0],"float32"), output_size=list[2,3,5,], data_format="NCDHW", name=None, )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 0],"float32"), weight=Tensor([3, 1, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=list[1,], groups=3, dilation=1, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([0, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=False, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 0, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 4, 0],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding="vaLiD", output_padding=0, dilation=1, groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.avg_pool3d(Tensor([2, 3, 0, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,1,1,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([1, 256, 64, 64],"float32"), Tensor([256, 0, 3, 3],"float32"), bias=Tensor([128],"float32"), padding=1, output_padding=1, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.__rpow__(Tensor([4, 5, 0],"float64"), 2, )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[66,1,1,], )
paddle.einsum("nbka,ahc->nbkhc", Tensor([1, 3, 4, 2],"float32"), Tensor([2, 0, 4],"float32"), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[187,1,1,], )
paddle.nn.functional.max_pool3d(Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 0],"float32"), Tensor([3, 1, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.einsum("blqd,bmdk->blqk", Tensor([52, 5, 0, 1],"float32"), Tensor([52, 1, 0, 1],"float32"), )
paddle.nn.functional.pixel_unshuffle(Tensor([2, 12, 12, 0],"float64"), 3, "NHWC", None, )
paddle.nn.functional.pad(Tensor([3, 1, 0, 40],"float32"), pad=list[2,2,0,0,], )
paddle.nn.functional.softmax(Tensor([16, 16, 0],"float64"), 1, name=None, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 8, 8],"float64"), weight=Tensor([3, 1, 5, 5],"float64"), bias=Tensor([0],"float64"), stride=1, padding="sAmE", output_padding=0, dilation=1, groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.lp_pool2d(Tensor([0, 3, 32, 32],"float32"), norm_type=2.0, kernel_size=5, stride=3, padding=0, ceil_mode=True, data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[223,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([1, 1024, 16, 16],"float32"), Tensor([1024, 0, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 0, 4, 4],"float64"), output_size=list[3,3,], )
paddle.nn.functional.pad(Tensor([1536, 0, 1, 1],"float32"), list[0,1,0,1,], )
paddle.nn.functional.pad(Tensor([0, 2, 3],"float64"), pad=list[2,2,], mode="replicate", value=0.0, data_format="NCL", name=None, )
paddle.nanmedian(Tensor([2, 0, 4, 5],"float32"), axis=list[0,3,], keepdim=False, mode="min", )
paddle.nn.functional.pad(Tensor([1, 0, 256, 256],"float32"), pad=list[3,3,3,3,], mode="reflect", value=0.0, data_format="NCHW", name=None, )
paddle.einsum("sec,ecm->sm", Tensor([0, 60, 2],"float32"), Tensor([60, 2, 64],"float32"), )
paddle.nn.functional.cosine_embedding_loss(Tensor([10],"float32"), Tensor([10],"float32"), Tensor([0],"int64"), margin=0.5, reduction="mean", name=None, )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 2],"float32"), weight=Tensor([3, 0, 3],"float32"), bias=Tensor([1],"float32"), stride=2, padding=list[1,], groups=1, dilation=1, )
paddle.Tensor.tile(Tensor([1, 6, 0],"float32"), list[543,1,1,], )
paddle.slice(Tensor([16, 0, 104],"float32"), list[0,1,2,], list[0,0,56,], list[16,2,72,], )
paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([0],"float32"), shift=None, smooth=None, quant_scale=0.0009928022045642138, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.Tensor.tile(Tensor([10105, 0],"float32"), list[1,8,], )
paddle.logaddexp(Tensor([1, 2, 3, 0],"float64"), Tensor([1, 2, 3, 0],"float64"), )
paddle.nn.functional.max_pool3d(x=Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[0,0,0,], )
paddle.nanmedian(Tensor([2, 3, 4, 0],"float32"), axis=list[0,2,1,3,], keepdim=False, )
paddle.nn.functional.conv2d_transpose(Tensor([0, 4096, 16, 16],"float32"), Tensor([4096, 512, 3, 3],"float32"), padding=0, stride=2, groups=8, )
paddle.nn.functional.avg_pool2d(Tensor([0, 128, 64, 64],"float16"), kernel_size=1, stride=1, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([1, 3, 0, 224, 231],"float32"), tuple(0,1,0,0,0,0,), data_format="NCDHW", )
paddle.nn.functional.pad(Tensor([0, 1, 100, 111],"float32"), pad=list[0,1,0,0,], mode="constant", value=-10000.0, data_format="NCHW", name=None, )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 2],"float32"), weight=Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([0],"float32"), stride=1, padding=1, dilation=1, )
paddle.nn.functional.kl_div(Tensor([0, 20],"float32"), Tensor([0, 20],"float64"), "mean", False, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[543,1,1,], )
paddle.einsum("...ijk, ...xijk -> ...xjk", Tensor([6, 8, 8],"float32"), Tensor([0, 6, 8, 8],"float32"), )
paddle.nn.functional.conv1d(Tensor([8, 256, 100],"float32"), Tensor([256, 64, 0],"float32"), bias=Tensor([256],"float32"), padding=1, stride=list[1,], dilation=list[1,], groups=4, data_format="NCL", )
paddle.nn.functional.pad(Tensor([0, 32, 32, 32],"float32"), tuple(0,0,0,0,), data_format="NHWC", )
paddle.nn.functional.max_pool2d(x=Tensor([2, 0, 32, 32],"float64"), kernel_size=list[2,2,], )
paddle.nn.functional.max_pool2d(Tensor([2, 3, 32, 0],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=True, data_format="NCHW", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 0],"float32"), Tensor([6, 4, 3, 3],"float32"), bias=Tensor([8],"float32"), padding="valid", output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=2, output_size=None, data_format="NCHW", )
paddle.nn.functional.max_pool2d(Tensor([1, 2, 0, 32],"float32"), kernel_size=2, stride=2, padding=0, return_mask=True, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([0, 64, 16, 128],"float32"), list[0,1,0,1,], value=-math.inf, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 1, 64],"float16"), Tensor([1, 2048, 0, 64],"float16"), Tensor([1, 2048, 1, 64],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), is_causal=False, )
paddle.flip(Tensor([32, 0, 112, 112],"float32"), axis=-1, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[999,1,1,], )
paddle.Tensor.tile(Tensor([8, 1, 0],"float32"), tuple(1,12,1,), )
paddle.nn.functional.avg_pool3d(x=Tensor([0, 3, 8, 8, 8],"float32"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], )
paddle.nn.functional.conv2d_transpose(Tensor([2, 2, 0, 3],"float64"), Tensor([2, 2, 1, 1],"float64"), groups=1, padding="SAME", )
paddle.nn.functional.max_unpool2d(Tensor([1, 1, 0, 2],"float32"), Tensor([1, 1, 0, 2],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCHW", output_size=None, name=None, )
paddle.nn.functional.conv3d_transpose(Tensor([4, 4, 8, 8, 8],"float32"), Tensor([4, 3, 3, 3, 0],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,1,2,2,1,1,], stride=1, dilation=1, groups=2, data_format="NCDHW", )
paddle.nn.functional.conv1d_transpose(Tensor([2, 2, 3],"float32"), Tensor([3, 1, 0],"float32"), bias=Tensor([3],"float32"), output_size=None, output_padding=1, padding=list[1,], stride=list[2,], dilation=list[1,], groups=3, data_format="NLC", )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 3, 0, 7],"float32"), output_size=list[None,3,], )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 0, 8, 96],"float16"), Tensor([2, 100, 8, 96],"float16"), Tensor([2, 100, 8, 96],"float16"), attn_mask=None, is_causal=False, )
paddle.nn.functional.conv1d(Tensor([8, 256, 0],"float32"), Tensor([256, 64, 3],"float32"), bias=Tensor([256],"float32"), padding=1, stride=list[1,], dilation=list[1,], groups=4, data_format="NCL", )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([13, 256, 0, 56],"float32"), output_size=list[7,7,], data_format="NCHW", name=None, )
paddle.nn.functional.conv2d(Tensor([1, 128, 256, 256],"float32"), Tensor([3, 128, 0, 1],"float32"), padding=0, groups=1, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 2],"float32"), Tensor([3, 0, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[2,2,], groups=1, output_size=None, data_format="NCHW", )
paddle.vision.ops.generate_proposals(Tensor([1, 4, 16, 16],"float32"), Tensor([1, 16, 16, 16],"float32"), Tensor([1, 2],"float32"), Tensor([16, 16, 4, 0],"float32"), Tensor([16, 16, 4, 4],"float32"), pre_nms_top_n=12000, post_nms_top_n=5000, nms_thresh=0.7, min_size=3.0, eta=1.0, pixel_offset=True, return_rois_num=True, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[947,1,1,], )
paddle.nn.functional.max_pool2d(Tensor([1, 0, 20, 20],"float32"), kernel_size=9, stride=1, padding=4, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.avg_pool3d(x=Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=tuple(1,0,0,), exclusive=True, )
paddle.reverse(Tensor([0, 1, 2],"float32"), axis=list[0,], )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 1, 2, 40],"float16"), Tensor([1, 1, 0, 40],"float16"), Tensor([1, 1, 2, 40],"float16"), attn_mask=Tensor([1, 2, 1, 1],"float16"), )
paddle.kthvalue(x=Tensor([3, 0, 4],"float64"), k=4, axis=2, keepdim=False, )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float64"), weight=Tensor([1, 3, 3, 3],"float64"), bias=Tensor([0],"float64"), stride=list[2,1,], padding=1, dilation=list[2,2,], )
paddle.nn.functional.conv2d_transpose(Tensor([0, 3, 2, 2],"float32"), Tensor([3, 10, 27, 27],"float32"), bias=Tensor([10],"float32"), padding=0, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.einsum("bnij,bjnd->bind", Tensor([13, 4, 0, 2],"float32"), Tensor([13, 2, 4, 4],"float32"), )
paddle.Tensor.__sub__(Tensor([12, 0, 10, 10, 1],"float32"), Tensor([12, 0, 10, 10, 1],"float32"), )
paddle.allclose(x=Tensor([0],"float32"), y=Tensor([0],"float32"), rtol=0.01, atol=0.01, equal_nan=False, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([1, 3, 3, 3, 3],"float32"), bias=Tensor([0],"float32"), stride=list[2,2,1,], padding=1, dilation=2, )
paddle.Tensor.flip(Tensor([16, 3, 0, 224],"float32"), 0, )
paddle.nn.functional.maxout(Tensor([10, 9, 3, 0],"float64"), 3, 1, None, )
paddle.nn.functional.pad(Tensor([4, 3, 0, 6, 6],"float32"), pad=list[2,2,2,2,2,2,], mode="constant", value=0.0, data_format="NCDHW", name=None, )
paddle.Tensor.tile(Tensor([1, 0, 256],"float32"), list[4,1,1,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[680,1,1,], )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([1, 3, 3, 0, 3],"float32"), bias=Tensor([1],"float32"), stride=list[2,2,1,], padding=list[1,2,2,], dilation=tuple(2,2,2,), )
paddle.nn.functional.mse_loss(Tensor([2, 0, 10],"float32"), Tensor([2, 0, 10],"float32"), "sum", )
paddle.nn.functional.conv3d_transpose(Tensor([4, 4, 8, 8, 8],"float32"), Tensor([4, 3, 3, 0, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[list[0,0,],list[0,0,],list[1,1,],list[1,1,],list[2,2,],], stride=1, dilation=1, groups=2, data_format="NCDHW", )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([0, 512, 128, 128],"float32"), output_size=tuple(2,2,), data_format="NCHW", name=None, )
paddle.nn.functional.max_pool2d(Tensor([1, 256, 20, 0],"float32"), 1, stride=2, )
paddle.einsum("nbka,ahc->nbkhc", Tensor([1, 3, 5, 6],"float16"), Tensor([6, 0, 4],"float16"), )
paddle.nn.functional.max_pool2d(Tensor([0, 1, 4, 1],"float32"), tuple(2,1,), stride=tuple(2,1,), ceil_mode=True, )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 0],"float32"), weight=Tensor([6, 1, 3, 3],"float32"), bias=Tensor([6],"float32"), stride=2, padding=0, groups=3, )
paddle.nn.functional.lp_pool2d(Tensor([0, 3, 32, 32],"float32"), math.inf, kernel_size=list[2,4,], stride=2, ceil_mode=False, )
paddle.einsum("k...,...jk->...k", Tensor([10, 3, 2, 3, 4],"float64"), Tensor([0, 10],"float64"), )
paddle.slice(Tensor([0, 1, 4],"float32"), axes=list[2,], starts=list[1,], ends=list[2,], )
paddle.Tensor.tile(Tensor([2, 4, 0],"float32"), list[1,1,2,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[2026,1,1,], )
paddle.vision.ops.prior_box(input=Tensor([2, 10, 32, 0],"float32"), image=Tensor([2, 10, 40, 0],"float32"), min_sizes=list[2.0,4.0,], clip=True, flip=True, )
paddle.index_fill(Tensor([0],"int64"), Tensor([0],"int64"), 0, 7, )
paddle.nn.functional.fractional_max_pool2d(Tensor([2, 0, 7, 7],"float32"), output_size=5, kernel_size=None, random_u=0.5, return_mask=False, name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([0, 3, 2, 2],"float32"), Tensor([3, 1, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[2,2,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.max_pool2d(Tensor([1, 64, 0, 399],"float32"), kernel_size=3, stride=2, padding=1, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 0, 7, 7],"float32"), list[2,5,], )
paddle.Tensor.__sub__(Tensor([3, 6, 0, 1, 2, 5],"float64"), Tensor([3, 6, 0, 1, 2, 5],"float64"), )
paddle.nn.functional.conv2d(Tensor([1, 2048, 16, 16],"float32"), Tensor([2048, 512, 0, 3],"float32"), padding=1, groups=4, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 2, 2, 6],"float32"), Tensor([6, 0, 3, 3],"float32"), bias=Tensor([3],"float32"), padding=list[1,0,], output_padding=1, stride=list[2,2,], dilation=list[1,1,], groups=3, output_size=None, data_format="NHWC", )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[760,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([2, 2, 2, 6],"float64"), Tensor([6, 0, 3, 3],"float64"), bias=Tensor([3],"float64"), padding=list[1,0,], output_padding=1, stride=list[2,2,], dilation=list[1,1,], groups=3, output_size=None, data_format="NHWC", )
paddle.Tensor.isnan(Tensor([0, 6, 3, 4, 2, 5],"float64"), )
paddle.Tensor.tile(Tensor([5535, 1, 0],"float32"), list[1,394,1,], )
paddle.reverse(Tensor([1, 0, 4],"float32"), axis=1, )
paddle.nn.functional.layer_norm(Tensor([0, 6, 6, 3],"float64"), list[6,6,3,], weight=None, bias=Tensor([108],"float64"), epsilon=1e-05, )
paddle.Tensor.fill_diagonal_(Tensor([0, 3],"float32"), 1, 0, False, )
paddle.nn.functional.softmax(Tensor([0, 4, 7, 7],"float32"), axis=-1, dtype="float32", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[284,1,1,], )
paddle.vision.ops.generate_proposals(Tensor([0, 15, 40, 60],"float32"), Tensor([1, 60, 40, 60],"float32"), Tensor([1, 2],"float32"), Tensor([36000, 4],"float32"), Tensor([36000, 4],"float32"), pre_nms_top_n=12000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.Tensor.__sub__(Tensor([1, 1, 0],"float32"), Tensor([1, 1, 0],"float32"), )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[122,1,1,], )
paddle.vision.ops.generate_proposals(Tensor([1, 15, 40, 0],"float32"), Tensor([1, 60, 40, 60],"float32"), Tensor([1, 2],"float32"), Tensor([36000, 4],"float32"), Tensor([36000, 4],"float32"), pre_nms_top_n=12000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float64"), weight=Tensor([1, 3, 3, 3],"float64"), bias=Tensor([0],"float64"), stride=2, padding=1, )
paddle.roll(Tensor([0, 4, 2],"float64"), Tensor([3],"int64"), list[0,1,2,], name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 3, 16, 16],"float32"), Tensor([3, 0, 3, 3],"float32"), bias=Tensor([6],"float32"), padding="valid", output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=3, output_size=None, data_format="NCHW", )
paddle.nn.functional.maxout(Tensor([0, 4, 3, 3],"float64"), 2, 1, None, )
paddle.nn.functional.temporal_shift(x=Tensor([2, 0, 3, 3],"float64"), seg_num=2, shift_ratio=0.4, )
paddle.nn.quant.weight_quantize(Tensor([0, 288],"float16"), algo="weight_only_int8", arch=86, group_size=-1, )
paddle.nn.functional.pad(Tensor([1, 0, 280, 350],"float32"), pad=list[2,2,2,2,], mode="constant", value=0.0, data_format="NCHW", )
paddle.nn.functional.selu(Tensor([0, 2],"float32"), 1.0507009873554805, 1.6732632423543772, None, )
paddle.nn.functional.prelu(Tensor([1, 2, 3, 0],"float32"), Tensor([1],"float32"), data_format="NCHW", )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 2],"float32"), weight=Tensor([3, 1, 3],"float32"), bias=Tensor([0],"float32"), stride=1, padding=1, dilation=1, )
paddle.cummax(Tensor([100, 0],"float32"), )
paddle.Tensor.fill_diagonal_(Tensor([7, 0],"float64"), 1, offset=0, wrap=False, )
paddle.Tensor.frexp(Tensor([0, 5, 2],"float64"), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[216,1,1,], )
paddle.Tensor.tile(Tensor([24565, 0],"float32"), list[1,180,], )
paddle.nn.functional.conv2d_transpose(Tensor([20, 256, 19, 0],"float32"), Tensor([256, 1, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=256, output_size=None, data_format="NCHW", )
paddle.nn.functional.l1_loss(Tensor([10, 10, 0],"float32"), Tensor([10, 10, 0],"float32"), reduction="none", )
paddle.nn.functional.max_pool2d(Tensor([1, 24, 40, 0],"float32"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.log_softmax(Tensor([2, 3, 0, 5],"float32"), 1, None, )
paddle.nn.functional.conv3d_transpose(Tensor([4, 8, 8, 8, 4],"float32"), Tensor([4, 0, 3, 3, 3],"float32"), None, output_size=None, padding="same", stride=1, dilation=1, groups=2, data_format="NDHWC", )
paddle.nn.functional.pad(Tensor([1, 16, 0, 14, 384],"float16"), tuple(0,0,0,5,0,0,), data_format="NDHWC", )
paddle.nn.functional.conv2d(Tensor([1, 128, 256, 0],"float32"), Tensor([3, 128, 1, 0],"float32"), padding=0, groups=1, )
paddle.Tensor.tile(Tensor([4897, 1, 0],"float32"), list[1,143,1,], )
paddle.slice(Tensor([8, 2, 100, 0],"float32"), axes=list[1,], starts=list[1,], ends=list[2,], )
paddle.nn.functional.avg_pool2d(Tensor([64, 0, 64, 64],"float32"), kernel_size=1, stride=1, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([3, 0, 5, 6, 7],"complex128"), pad=list[1,2,2,1,1,0,], mode="reflect", value=0.0, data_format="NCDHW", name=None, )
paddle.nn.functional.pad(Tensor([16, 25500, 0],"float32"), pad=list[1024,1024,], mode="reflect", data_format="NLC", )
paddle.nn.functional.conv2d(Tensor([2, 192, 0, 4],"float32"), Tensor([48, 192, 1, 1],"float32"), None, list[1,1,], 0, list[1,1,], 1, "NCHW", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[307,1,1,], )
paddle.nn.functional.softmax(Tensor([0, 16, 18, 18],"float32"), )
paddle.nn.functional.pad(Tensor([0, 4, 5],"complex128"), pad=list[1,1,], mode="constant", value=100, data_format="NCL", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 2, 3, 3],"float64"), Tensor([2, 2, 0, 1],"float64"), groups=1, padding="SAME", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 0, 2, 2],"float32"), Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[2,2,2,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.softmax(Tensor([13, 4, 0, 1, 7],"float32"), )
paddle.slice(Tensor([2, 8, 0],"float32"), axes=list[1,], starts=list[2,], ends=list[3,], )
paddle.Tensor.__rpow__(Tensor([0],"float32"), 20, )
paddle.einsum("...qk,...kd->...qd", Tensor([13, 4, 0, 1, 2],"float32"), Tensor([13, 4, 0, 2, 8],"float32"), )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([0, 4, 4, 3],"float64"), output_size=list[3,3,], data_format="NHWC", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 16],"float32"), Tensor([6, 8, 0, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[1,2,], dilation=list[2,2,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([2, 48, 8, 0],"float32"), list[0,1,0,1,], value=0, )
paddle.index_fill(Tensor([0],"int64"), Tensor([30],"int64"), 0, 7, )
paddle.Tensor.quantile(Tensor([3, 6, 3, 0, 2, 5],"float64"), q=list[0.25,0.5,0.75,], axis=3, keepdim=False, )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[394,1,1,], )
paddle.Tensor.fill_diagonal_(Tensor([3, 0],"bool"), 0, offset=0, wrap=True, )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 2],"float64"), weight=Tensor([3, 0, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=1, dilation=1, )
paddle.vision.ops.prior_box(input=Tensor([2, 10, 0, 32],"float32"), image=Tensor([2, 10, 0, 40],"float32"), min_sizes=list[2.0,4.0,], clip=True, flip=True, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[475,1,1,], )
paddle.nn.functional.pad(Tensor([1, 0, 32, 224, 238],"float32"), tuple(0,2,0,0,0,0,), data_format="NCDHW", )
paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 0, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=list[1,0,0,], exclusive=True, )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([0, 3, 5, 7, 7],"float32"), output_size=list[2,3,5,], )
paddle.nn.functional.conv2d_transpose(Tensor([1, 1024, 2, 2],"float32"), Tensor([1024, 0, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([30, 1, 32, 0],"float32"), list[2,3,2,3,], value=0, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 256, 0, 35],"float32"), Tensor([256, 128, 4, 4],"float32"), bias=None, padding=0, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float64"), weight=Tensor([1, 3, 3, 3],"float64"), bias=Tensor([0],"float64"), stride=list[2,1,], padding=1, dilation=2, )
paddle.nn.functional.conv2d_transpose(Tensor([1, 2048, 16, 16],"float32"), Tensor([2048, 0, 3, 3],"float32"), padding=0, stride=2, groups=4, )
paddle.nn.functional.max_pool1d(x=Tensor([0, 3, 8],"float64"), kernel_size=2, stride=2, padding=1, )
paddle.nn.functional.conv2d_transpose(Tensor([0, 512, 16, 16],"float32"), Tensor([512, 512, 3, 3],"float32"), padding=0, stride=2, groups=1, )
paddle.nn.functional.max_unpool2d(Tensor([0, 1, 2, 2],"float32"), Tensor([0, 1, 2, 2],"int32"), kernel_size=2, stride=2, output_size=tuple(5,5,), )
paddle.slice(Tensor([4, 5, 0],"float32"), axes=list[0,1,2,], starts=list[-3,0,2,], ends=list[3,2,4,], )
paddle.einsum("bhwc,wkc->bhwk", Tensor([48, 0, 32, 64],"float32"), Tensor([32, 32, 64],"float32"), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[424,1,1,], )
paddle.nn.functional.mse_loss(Tensor([0, 3, 10, 10],"float32"), Tensor([0, 3, 10, 10],"float32"), "none", )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 4, 4, 0],"float64"), output_size=3, data_format="NHWC", name=None, )
paddle.slice(Tensor([0, 9, 16],"float32"), axes=list[1,], starts=list[1,], ends=list[8,], )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 3, 0, 7],"float32"), output_size=5, )
paddle.nn.functional.conv1d(Tensor([2, 3, 4],"float32"), Tensor([1, 3, 3],"float32"), bias=Tensor([0],"float32"), padding=1, stride=list[1,], dilation=list[2,], groups=1, data_format="NCL", )
paddle.nn.functional.pad(Tensor([1, 3, 0, 224, 297],"float32"), tuple(0,3,0,0,0,0,), data_format="NCDHW", )
paddle.slice(Tensor([0, 1, 104],"float32"), list[0,1,2,], list[0,0,80,], list[13,1,96,], )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[115,1,1,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[266,1,1,], )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 0, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )
paddle.Tensor.tile(Tensor([1, 5, 0],"float32"), list[301,1,1,], )
paddle.nn.functional.lp_pool1d(Tensor([2, 3, 0],"float32"), norm_type=7, kernel_size=2, stride=2, padding=list[1,], ceil_mode=True, )
paddle.logcumsumexp(Tensor([0, 12],"float32"), dtype="float32", axis=None, )
paddle.nn.functional.conv2d(Tensor([1, 1, 0, 165],"float32"), Tensor([64, 1, 0, 7],"float32"), None, list[2,2,], 0, list[1,1,], 1, )
paddle.nanquantile(Tensor([2, 0],"float32"), list[0.3,0.7,], 1, )
paddle.nn.functional.conv3d_transpose(Tensor([4, 3, 8, 0, 8],"float32"), Tensor([3, 5, 3, 3, 3],"float32"), Tensor([5],"float32"), output_size=None, padding=0, stride=1, dilation=1, groups=1, data_format="NCDHW", )
paddle.nn.functional.hinge_embedding_loss(Tensor([10, 0, 5],"float64"), Tensor([10, 0, 5],"float64"), reduction="mean", margin=1.0, name=None, )
paddle.nn.functional.fractional_max_pool3d(Tensor([0, 3, 7, 7, 7],"float32"), output_size=list[3,3,3,], kernel_size=list[2,2,2,], random_u=0.6, return_mask=False, name=None, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 0, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 0],"float32"), Tensor([3, 1, 3, 3],"float32"), bias=None, padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[2,2,], groups=1, output_size=None, data_format="NCHW", )
paddle.einsum("binh,tnh->bnit", Tensor([13, 0, 4, 4],"float32"), Tensor([8, 4, 4],"float32"), )
paddle.nn.functional.lp_pool2d(Tensor([0, 32, 32, 3],"float32"), norm_type=2.0, kernel_size=2, stride=list[2,2,], padding=0, ceil_mode=False, data_format="NHWC", name=None, )
paddle.nn.functional.kl_div(Tensor([5, 0],"float64"), Tensor([5, 0],"float64"), "batchmean", False, )
paddle.nn.functional.sequence_mask(Tensor([0],"float64"), maxlen=20, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 0],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 8, 8, 8, 6],"float32"), Tensor([6, 8, 3, 3, 0],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NDHWC", )
paddle.slice(Tensor([0, 8, 24],"float32"), axes=list[1,], starts=list[3,], ends=list[4,], )
paddle.nn.functional.max_pool1d(Tensor([0, 3, 8],"float64"), 2, 1, list[1,], False, False, None, )
paddle.einsum("bind,snd->bnis", Tensor([13, 2, 4, 4],"float32"), Tensor([0, 4, 4],"float32"), )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 512, 128, 0],"float32"), output_size=tuple(2,2,), data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([1, 0, 3],"float64"), pad=list[1,2,], mode="replicate", value=0.0, data_format="NCL", name=None, )
paddle.incubate.nn.functional.fused_feedforward(Tensor([0, 128, 768],"float32"), Tensor([768, 3072],"float32"), Tensor([3072, 768],"float32"), Tensor([3072],"float32"), Tensor([768],"float32"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )
paddle.nn.functional.conv3d_transpose(Tensor([0, 3, 2, 2, 2],"float32"), Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 2, 2, 2, 3],"float64"), Tensor([3, 1, 3, 3, 0],"float64"), bias=Tensor([3],"float64"), padding=list[1,1,1,], output_padding=1, stride=list[2,2,2,], dilation=list[1,1,1,], groups=3, output_size=None, data_format="NDHWC", )
paddle.nn.functional.sequence_mask(Tensor([0],"int32"), maxlen=4, dtype="float32", )
paddle.nn.functional.conv1d(x=Tensor([2, 3, 4],"float32"), weight=Tensor([1, 3, 0],"float32"), bias=Tensor([1],"float32"), stride=list[1,], padding=list[1,], dilation=tuple(2,), )
paddle.einsum("a...a->a...", Tensor([5, 3, 2, 0, 4, 5],"float64"), )
paddle.nn.functional.pad(Tensor([1, 0, 16, 14, 384],"float32"), tuple(0,0,0,5,0,0,), data_format="NDHWC", )
paddle.nn.functional.pad(Tensor([1, 0, 3, 4],"float64"), pad=list[2,1,2,1,], mode="replicate", value=0.0, data_format="NCHW", name=None, )
paddle.slice(Tensor([1024, 2, 0],"float16"), axes=list[0,], starts=list[640,], ends=list[768,], )
paddle.nn.functional.pad(Tensor([3, 0, 5, 6, 7],"complex128"), pad=list[1,1,1,1,1,1,], mode="constant", value=100, data_format="NCDHW", name=None, )
paddle.nn.functional.avg_pool3d(Tensor([2, 0, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding=list[0,0,0,0,0,0,], divisor_override=8, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 0, 2],"float32"), Tensor([3, 1, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[2,2,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.max_pool1d(Tensor([0, 3, 6],"float32"), kernel_size=5, stride=5, padding=0, ceil_mode=True, return_mask=True, )
paddle.Tensor.tile(Tensor([10285, 0],"float32"), list[1,188,], )
paddle.nn.functional.avg_pool3d(Tensor([2, 3, 0, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=list[1,0,0,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
paddle.nn.functional.pad(Tensor([0, 256, 64, 64],"float32"), pad=list[1,1,1,1,], mode="reflect", value=0.0, data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([8, 0, 512],"float32"), tuple(1,5,1,), )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[62,1,1,], )
paddle.Tensor.tile(Tensor([16, 0, 1],"float32"), repeat_times=list[1,12,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([2, 256, 62, 54],"float32"), Tensor([256, 0, 4, 4],"float32"), bias=None, padding=0, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nonzero(Tensor([0, 100],"float32"), )
paddle.vision.ops.generate_proposals(Tensor([1, 15, 40, 60],"float32"), Tensor([1, 60, 40, 60],"float32"), Tensor([1, 0],"float32"), Tensor([36000, 4],"float32"), Tensor([36000, 4],"float32"), pre_nms_top_n=12000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.margin_ranking_loss(Tensor([10, 0],"float64"), Tensor([10, 0],"float64"), Tensor([10, 0],"float64"), 0.2, "none", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[617,1,1,], )
paddle.nn.functional.max_pool2d(Tensor([0, 32, 32, 3],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, data_format="NHWC", )
paddle.nn.functional.max_pool2d(x=Tensor([0, 3, 32, 32],"float64"), kernel_size=list[3,3,], stride=tuple(1,2,), )
paddle.allclose(Tensor([2, 3, 0, 8],"float32"), Tensor([2, 3, 0, 8],"float32"), )
paddle.nn.functional.fractional_max_pool2d(x=Tensor([2, 0, 7, 7],"float32"), output_size=list[2,5,], random_u=0.7, )
paddle.Tensor.tile(Tensor([10105, 0],"float32"), list[1,6,], )
paddle.nn.functional.conv2d_transpose(Tensor([1, 4096, 16, 16],"float32"), Tensor([4096, 512, 3, 0],"float32"), padding=0, stride=2, groups=8, )
paddle.nn.functional.max_pool1d(Tensor([0, 3, 32],"float32"), kernel_size=2, stride=2, padding=0, return_mask=True, )
paddle.nn.functional.conv1d(Tensor([4, 6, 16],"float32"), Tensor([8, 6, 0],"float32"), bias=Tensor([8],"float32"), padding=0, stride=list[1,], dilation=list[2,], groups=1, data_format="NCL", )
paddle.nn.quant.weight_only_linear(Tensor([100, 320],"float16"), weight=Tensor([0, 320],"int8"), weight_scale=Tensor([512],"float16"), weight_dtype="int8", )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 0],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
paddle.Tensor.tile(Tensor([1, 4, 0],"float32"), list[298,1,1,], )
paddle.nn.functional.conv1d_transpose(x=Tensor([0, 3, 2],"float32"), weight=Tensor([3, 2, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=0, dilation=1, )
paddle.nn.functional.interpolate(x=Tensor([2, 3, 0, 7, 7],"float32"), mode="area", size=list[2,3,5,], )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 0, 4, 80],"float32"), list[1,40,], )
paddle.nn.functional.conv1d(x=Tensor([2, 3, 4],"float32"), weight=Tensor([1, 3, 2],"float32"), bias=Tensor([0],"float32"), stride=1, padding=0, )
paddle.meshgrid(Tensor([0],"float32"), Tensor([0],"float32"), )
paddle.vision.ops.generate_proposals(Tensor([1, 9, 12, 9],"float32"), Tensor([1, 36, 0, 9],"float32"), Tensor([1, 2],"float32"), Tensor([972, 4],"float32"), Tensor([972, 4],"float32"), pre_nms_top_n=4000, post_nms_top_n=4000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 0, 2],"float32"), Tensor([3, 1, 3, 3],"float32"), bias=None, padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([0, 16, 18, 14, 384],"float16"), tuple(0,0,0,3,0,0,), data_format="NDHWC", )
paddle.slice(Tensor([3, 4, 5, 0],"float64"), axes=list[-2,], starts=list[2,], ends=list[3,], )
paddle.Tensor.tile(Tensor([1, 5, 0],"float32"), list[224,1,1,], )
paddle.nn.functional.softmax(Tensor([2, 0, 4],"float64"), 0, name=None, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([512, 0, 2, 25],"float32"), output_size=list[1,25,], data_format="NCHW", name=None, )
paddle.nn.functional.sequence_mask(Tensor([0, 3],"int64"), maxlen=10, dtype=VarType(float64), )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 2, 3],"float64"), weight=Tensor([3, 0, 3],"float64"), bias=Tensor([3],"float64"), stride=1, padding=list[1,], output_padding=0, groups=3, dilation=1, output_size=None, data_format="NLC", name=None, )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 2, 2, 2, 3],"float64"), weight=Tensor([3, 0, 3, 3, 3],"float64"), bias=Tensor([3],"float64"), output_padding=1, stride=2, padding=list[1,1,1,], groups=3, data_format="NDHWC", dilation=1, )
paddle.roll(Tensor([4, 0, 4, 4],"complex128"), Tensor([4],"int64"), list[0,1,2,3,], name=None, )
paddle.nn.functional.temporal_shift(x=Tensor([0, 2, 4, 3],"float32"), seg_num=2, )
paddle.nn.functional.pad(Tensor([0, 2, 3],"float64"), pad=list[1,2,], mode="constant", value=0.0, data_format="NCL", name=None, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[1799,1,1,], )
paddle.nanmedian(Tensor([2, 3, 0, 5],"float32"), axis=list[0,3,], keepdim=False, mode="min", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 0, 2],"float32"), Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.einsum("ij,j->i", Tensor([0, 5],"float64"), Tensor([5],"float64"), )
paddle.nn.functional.group_norm(Tensor([0, 3, 2, 4],"float64"), num_groups=2, weight=Tensor([4],"float64"), bias=Tensor([4],"float64"), data_format="NHWC", )
paddle.nn.functional.pad(Tensor([1, 0, 3],"float64"), pad=list[1,2,], mode="reflect", value=0.0, data_format="NCL", name=None, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 0, 4, 4],"float32"), weight=Tensor([6, 1, 3, 3, 3],"float32"), bias=Tensor([6],"float32"), stride=2, padding=0, groups=3, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 0, 2],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=1, dilation=1, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[752,1,1,], )
paddle.nanmedian(Tensor([2, 0, 4, 5],"float32"), axis=-1, keepdim=False, mode="min", )
paddle.einsum("...qk,...kd->...qd", Tensor([13, 4, 3, 1, 2],"float32"), Tensor([13, 4, 3, 2, 0],"float32"), )
paddle.slice(Tensor([13, 0, 104],"float32"), list[0,1,2,], list[0,0,56,], list[13,1,72,], )
paddle.nn.functional.conv2d_transpose(Tensor([20, 64, 76, 136],"float32"), Tensor([64, 1, 0, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=64, output_size=None, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([1, 0, 32, 245, 224],"float32"), tuple(0,0,0,3,0,0,), data_format="NCDHW", )
paddle.slice(Tensor([0, 2, 104],"float32"), list[0,1,2,], list[0,0,32,], list[16,2,48,], )
paddle.nn.functional.sequence_mask(Tensor([2, 0],"int64"), maxlen=Tensor([1],"int32"), dtype=VarType(float32), )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 0, 8, 32, 32],"float64"), output_size=tuple(3,3,3,), data_format="NDHWC", )
paddle.nanmedian(Tensor([2, 3, 4, 0],"float32"), axis=tuple(1,2,), keepdim=False, )
paddle.nn.functional.max_pool2d(Tensor([2, 0, 244, 2],"float32"), kernel_size=list[5,3,], stride=list[1,2,], padding=tuple(2,1,), )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 3, 2],"float64"), Tensor([2, 2, 1, 0],"float64"), groups=1, padding=list[1,1,], data_format="NHWC", )
paddle.nn.functional.softmax(Tensor([13, 0, 7, 7],"float32"), axis=-1, dtype="float32", )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 0, 32, 32],"float64"), output_size=list[2,2,2,], )
paddle.quantile(Tensor([4, 7, 0],"float64"), q=0.75, axis=list[0,2,], )
paddle.nn.functional.pad(Tensor([0, 2, 3, 1],"float64"), pad=list[1,1,1,0,], mode="reflect", value=0.0, data_format="NHWC", name=None, )
paddle.slice(Tensor([16, 0, 104],"float32"), list[0,1,2,], list[0,0,80,], list[16,2,96,], )
paddle.nn.functional.conv1d_transpose(Tensor([1, 256, 28],"float32"), Tensor([256, 0, 8],"float32"), bias=Tensor([128],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.fractional_max_pool3d(x=Tensor([0, 3, 7, 7, 7],"float32"), output_size=list[3,None,3,], random_u=0.6, )
paddle.nn.functional.pad(Tensor([0, 4, 5],"complex128"), pad=list[1,2,], mode="circular", value=0.0, data_format="NCL", name=None, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 8, 0],"float64"), weight=Tensor([3, 1, 5, 5],"float64"), bias=Tensor([1],"float64"), stride=1, padding="sAmE", output_padding=0, dilation=1, groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 0, 2],"float64"), weight=Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,1,], dilation=2, )
paddle.allclose(Tensor([0, 20, 32],"float32"), Tensor([0, 20, 32],"float32"), )
paddle.nn.functional.pad(Tensor([0, 6, 6],"float64"), pad=list[2,2,], mode="constant", value=0.0, data_format="NCL", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([0, 64, 38, 68],"float32"), Tensor([64, 1, 8, 8],"float32"), bias=None, padding=2, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=64, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([4, 0, 512],"float32"), tuple(1,3,1,), )
paddle.nn.functional.pad(Tensor([3, 4, 0, 6],"complex128"), pad=list[1,1,1,1,], mode="constant", value=100, data_format="NCHW", name=None, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 2, 2, 2, 3],"float64"), Tensor([3, 1, 0, 3, 3],"float64"), bias=Tensor([3],"float64"), padding=list[1,1,1,], output_padding=1, stride=list[2,2,2,], dilation=list[1,1,1,], groups=3, output_size=None, data_format="NDHWC", )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[184,1,1,], )
paddle.nn.functional.pad(Tensor([1, 0, 15, 14, 384],"float32"), tuple(0,0,0,1,0,0,), data_format="NDHWC", )
paddle.nn.functional.conv1d_transpose(x=Tensor([0, 3, 2],"float32"), weight=Tensor([3, 1, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=1, output_padding=0, groups=1, dilation=1, output_size=None, data_format="NCL", name=None, )
paddle.nn.functional.avg_pool3d(x=Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,2,1,], exclusive=True, )
paddle.einsum("bhwc,hkc->bhwk", Tensor([0, 32, 32, 64],"float32"), Tensor([32, 32, 64],"float32"), )
paddle.einsum("ibnd,jbnd->bnij", Tensor([0, 10, 4, 8],"float32"), Tensor([14, 10, 4, 8],"float32"), )
paddle.nn.functional.pad(Tensor([1, 0, 100, 111],"float32"), pad=list[0,1,0,0,], mode="constant", value=-10000.0, data_format="NCHW", name=None, )
paddle.nn.functional.fractional_max_pool2d(x=Tensor([0, 3, 7, 7],"float32"), output_size=list[None,3,], random_u=0.6, )
paddle.nn.functional.conv2d(Tensor([1, 1024, 12, 32],"float32"), Tensor([256, 1024, 1, 0],"float32"), None, list[1,1,], 0, list[1,1,], 1, )
paddle.nn.functional.pad(Tensor([1, 2, 3, 0, 5],"complex64"), pad=list[1,2,1,1,3,4,], mode="constant", value=100, )
paddle.nn.functional.conv2d_transpose(Tensor([0, 1024, 16, 16],"float32"), Tensor([1024, 256, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[75,1,1,], )
paddle.einsum("i,i->", Tensor([0],"float32"), Tensor([0],"float32"), )
paddle.nn.functional.channel_shuffle(Tensor([2, 4, 4, 0],"float64"), 3, "NHWC", None, )
paddle.nn.functional.conv2d_transpose(Tensor([0, 256, 62, 54],"float32"), Tensor([256, 128, 4, 4],"float32"), bias=None, padding=0, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 6, 0],"float32"), list[233,1,1,], )
paddle.searchsorted(sorted_sequence=Tensor([5],"float32"), values=Tensor([2, 0],"float32"), )
paddle.allclose(Tensor([1, 20, 0],"float32"), Tensor([1, 20, 0],"float32"), )
paddle.nn.functional.conv2d_transpose(Tensor([1, 2048, 128, 0],"float32"), Tensor([2048, 128, 3, 3],"float32"), padding=0, stride=2, groups=8, )
paddle.nn.functional.conv3d(x=Tensor([2, 4, 4, 4, 3],"float32"), weight=Tensor([2, 3, 3, 3, 0],"float32"), bias=Tensor([2],"float32"), stride=1, padding=0, data_format="NDHWC", )
paddle.vision.ops.generate_proposals(Tensor([1, 3, 10, 15],"float32"), Tensor([1, 0, 10, 15],"float32"), Tensor([1, 2],"float32"), Tensor([450, 4],"float32"), Tensor([450, 4],"float32"), pre_nms_top_n=2000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([0, 3, 4, 4],"float64"), output_size=list[3,3,], data_format="NCHW", name=None, )
paddle.einsum("ij,jk", Tensor([4, 0],"float64"), Tensor([11, 0],"float64"), )
paddle.nn.functional.avg_pool2d(Tensor([2, 0, 4, 4],"float64"), kernel_size=list[3,3,], stride=list[1,1,], padding=list[0,0,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.poisson_nll_loss(Tensor([0, 2],"float32"), Tensor([0, 2],"float32"), log_input=True, full=True, epsilon=1e-08, reduction="mean", )
paddle.nn.functional.pad(Tensor([1, 16, 15, 14, 0],"float16"), tuple(0,0,0,1,0,0,), data_format="NDHWC", )
paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 8, 1, 0],"float32"), Tensor([1, 8, 109, 0],"float32"), )
paddle.nn.functional.lp_pool1d(Tensor([2, 0, 3],"float32"), norm_type=7, kernel_size=2, stride=2, padding=list[1,], ceil_mode=True, data_format="NLC", )
paddle.nn.functional.prelu(Tensor([1, 2, 0, 4],"float32"), Tensor([1],"float32"), )
paddle.nanmedian(Tensor([0, 3, 4, 5],"float32"), axis=list[0,3,], keepdim=False, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([1, 3, 3, 3, 3],"float32"), bias=Tensor([0],"float32"), stride=list[2,2,1,], padding=1, dilation=list[2,2,2,], )
paddle.Tensor.__sub__(Tensor([0, 1002],"float32"), Tensor([0, 1002],"float32"), )
paddle.nn.functional.poisson_nll_loss(Tensor([4, 0, 2],"bfloat16"), Tensor([4, 0, 2],"float16"), )
paddle.nn.functional.conv3d_transpose(x=Tensor([0, 3, 2, 2, 2],"float64"), weight=Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=1, dilation=1, )
paddle.einsum("ij,jk", Tensor([4, 11],"float64"), Tensor([11, 0],"float64"), )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 0, 5, 7, 7],"float32"), output_size=list[2,3,5,], data_format="NCDHW", name=None, )
paddle.nn.functional.mse_loss(Tensor([0, 96, 2],"float32"), Tensor([0, 96, 2],"float32"), )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 1024, 0, 128],"bfloat16"), Tensor([1, 1024, 32, 128],"bfloat16"), Tensor([1, 1024, 32, 128],"bfloat16"), attn_mask=None, is_causal=True, )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[83,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([0, 256, 14, 14],"float32"), Tensor([256, 256, 2, 2],"float32"), bias=Tensor([256],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[280,1,1,], )
paddle.nn.functional.max_unpool1d(Tensor([1, 3, 0],"float64"), Tensor([1, 3, 0],"int32"), kernel_size=2, stride=None, )
paddle.Tensor.tile(Tensor([8, 0, 512],"float32"), tuple(1,12,1,), )
paddle.nn.functional.max_pool1d(Tensor([2, 0, 8],"float64"), 2, 2, 1, False, False, None, )
paddle.nn.functional.poisson_nll_loss(Tensor([10, 0],"float32"), Tensor([10, 0],"float32"), log_input=False, full=False, epsilon=1e-08, reduction="mean", name=None, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 2, 2, 6],"float32"), weight=Tensor([6, 1, 3, 0],"float32"), bias=Tensor([3],"float32"), output_padding=1, stride=2, padding=list[1,0,], groups=3, data_format="NHWC", dilation=1, )
paddle.reverse(Tensor([1, 0, 2],"float32"), axis=list[0,], )
paddle.quantile(Tensor([5, 0, 4],"float64"), q=list[0.1,0.2,0.3,], axis=list[1,2,], keepdim=True, )
paddle.Tensor.fill_diagonal_(Tensor([2, 2, 0],"float64"), 1, 0, False, )
paddle.Tensor.flip(Tensor([16, 3, 224, 0],"float32"), 0, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 100, 1, 64],"float16"), Tensor([2, 0, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 100],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[1,200,1,], )
paddle.slice(Tensor([0, 1, 104],"float32"), list[0,1,2,], list[0,0,24,], list[16,1,40,], )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 8, 3, 3, 0],"float32"), bias=Tensor([8],"float32"), padding=list[1,2,2,3,2,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.searchsorted(sorted_sequence=Tensor([7],"float64"), values=Tensor([0, 2, 2],"float64"), right=True, )
paddle.nn.functional.conv2d(Tensor([1, 128, 0, 256],"float32"), Tensor([3, 128, 1, 1],"float32"), padding=0, groups=1, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 4, 3, 3],"float64"), Tensor([4, 0, 1, 1],"float64"), groups=1, )
paddle.searchsorted(sorted_sequence=Tensor([7],"float64"), values=Tensor([2, 0, 2],"float64"), right=True, )
paddle.Tensor.tile(Tensor([1, 5, 0],"float32"), list[302,1,1,], )
paddle.nn.functional.avg_pool2d(x=Tensor([2, 3, 0, 4],"float64"), kernel_size=list[3,3,], stride=list[3,3,], padding=list[0,0,], ceil_mode=True, exclusive=False, )
paddle.nn.functional.avg_pool3d(Tensor([2, 3, 0, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=tuple(1,0,0,), ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
paddle.nn.functional.max_pool3d(Tensor([0, 3, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding=list[0,0,0,0,0,0,], )
paddle.nn.functional.conv3d_transpose(Tensor([0, 6, 8, 8, 8],"float32"), Tensor([6, 8, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding="valid", output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 512, 2, 0],"float32"), output_size=tuple(7,7,), data_format="NCHW", name=None, )
paddle.nn.functional.avg_pool2d(x=Tensor([2, 0, 4, 4],"float64"), kernel_size=list[3,3,], stride=tuple(1,1,), padding=list[0,0,], )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 0, 7, 7],"float32"), list[2,3,5,], )
paddle.Tensor.digamma(Tensor([0, 7, 8],"float64"), )
paddle.nn.functional.conv2d_transpose(Tensor([0, 256, 32, 32],"float32"), Tensor([256, 1, 8, 8],"float32"), bias=None, padding=2, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=256, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[225,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([0, 6, 16, 16],"float32"), Tensor([6, 8, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=list[1,2,2,1,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 3, 7, 0],"float32"), output_size=list[2,5,], data_format="NCHW", name=None, )
paddle.nn.functional.l1_loss(Tensor([100, 0],"float32"), Tensor([100, 0],"float32"), reduction="sum", )
paddle.Tensor.tile(Tensor([100, 0, 80],"float32"), list[1,2,1,], )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 7, 0],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
paddle.nn.functional.pad(Tensor([4, 0, 6],"float32"), pad=list[2,2,], mode="constant", value=0.0, data_format="NCL", name=None, )
paddle.nn.functional.l1_loss(Tensor([16, 107, 0],"float32"), Tensor([16, 107, 0],"float32"), )
paddle.nn.functional.lp_pool1d(Tensor([2, 0, 32],"float32"), norm_type=7, kernel_size=2, stride=2, padding=list[1,], ceil_mode=True, )
paddle.einsum("k...,jk", Tensor([2, 4, 5, 3],"float64"), Tensor([0, 2],"float64"), )
paddle.isin(Tensor([0, 64],"bfloat16"), Tensor([0, 256],"bfloat16"), False, False, )
paddle.nn.functional.lp_pool2d(Tensor([2, 0, 32, 32],"float16"), norm_type=2.0, kernel_size=3, stride=2, padding=0, ceil_mode=False, data_format="NCHW", name=None, )
paddle.slice(Tensor([3, 0, 5, 6],"float64"), axes=list[-2,], starts=list[2,], ends=list[3,], )
paddle.nn.functional.conv2d_transpose(Tensor([1, 128, 0, 128],"float32"), Tensor([128, 64, 3, 3],"float32"), bias=Tensor([64],"float32"), padding=1, output_padding=1, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.avg_pool2d(Tensor([32, 1024, 14, 0],"float32"), kernel_size=2, stride=2, padding="SAME", ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([1, 3, 0, 200],"float64"), pad=list[2,2,2,2,], mode="constant", value=0.0, data_format="NCHW", )
paddle.nn.functional.avg_pool1d(x=Tensor([0, 3, 8],"float32"), kernel_size=2, stride=2, padding=0, )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[405,1,1,], )
paddle.nn.functional.conv1d_transpose(Tensor([0, 3, 2],"float64"), Tensor([3, 1, 3],"float64"), bias=Tensor([1],"float64"), output_size=None, output_padding=0, padding=1, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv2d_transpose(Tensor([0, 4, 16, 16],"float32"), Tensor([4, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,1,2,2,], stride=1, dilation=1, groups=2, data_format="NCHW", )
paddle.nn.functional.max_unpool1d(Tensor([1, 0, 8],"float64"), Tensor([1, 0, 8],"int32"), kernel_size=2, stride=None, )
paddle.nn.functional.max_pool3d(Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 0, 8, 32, 32],"float64"), output_size=list[1,1,1,], )
paddle.nn.functional.pad(Tensor([1, 79949, 0],"float32"), pad=list[200,200,], mode="reflect", data_format="NLC", )
paddle.nn.functional.pad(Tensor([1, 16, 0, 28, 192],"float32"), tuple(0,0,0,4,0,0,), data_format="NDHWC", )
paddle.nn.functional.conv2d(Tensor([1, 1024, 128, 0],"float32"), Tensor([12, 256, 1, 1],"float32"), padding=0, groups=4, )
paddle.nn.functional.square_error_cost(Tensor([0, 100, 100],"float16"), Tensor([0, 100, 100],"float32"), )
paddle.Tensor.tile(Tensor([1, 2, 2, 1, 0],"float32"), list[1,1,1,2,1,], )
paddle.nn.functional.margin_ranking_loss(Tensor([10, 0],"float64"), Tensor([10, 0],"float64"), Tensor([10, 0],"float64"), 0.0, "sum", )
paddle.nn.functional.pixel_shuffle(x=Tensor([2, 4, 4, 0],"float64"), upscale_factor=3, data_format="NHWC", )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 2],"float32"), Tensor([3, 1, 0, 3],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nonzero(x=Tensor([3, 0, 2, 2],"float64"), as_tuple=False, )
paddle.nn.functional.pixel_shuffle(Tensor([2, 4, 4, 0],"float64"), 3, "NHWC", None, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 0, 4, 4],"float64"), weight=Tensor([1, 3, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=2, padding=1, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 128, 124, 0],"float32"), Tensor([128, 128, 2, 2],"float32"), bias=None, padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 0, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=False, )
paddle.nn.functional.layer_norm(Tensor([0, 6, 6, 3],"float64"), list[6,6,3,], weight=Tensor([108],"float64"), bias=Tensor([108],"float64"), epsilon=1e-05, )
paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 0, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=1, exclusive=True, )
paddle.nn.functional.margin_ranking_loss(Tensor([0, 10],"float64"), Tensor([0, 10],"float64"), Tensor([0, 10],"float64"), 0.0, "sum", )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[929,1,1,], )
paddle.nn.functional.max_pool2d(Tensor([1, 0, 32, 32],"float32"), kernel_size=13, stride=1, padding=6, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[87,1,1,], )
paddle.nn.functional.square_error_cost(Tensor([0],"float32"), Tensor([0],"float32"), )
paddle.nn.functional.kl_div(Tensor([40, 20, 0],"float32"), Tensor([40, 20, 0],"float32"), "batchmean", False, )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 5],"float64"), weight=Tensor([3, 0, 3],"float64"), bias=Tensor([2],"float64"), stride=1, padding="SaME", output_padding=0, groups=1, dilation=1, output_size=None, data_format="NCL", )
paddle.nn.functional.max_pool3d(Tensor([1, 6, 9, 6, 0],"float32"), list[5,5,5,], stride=list[1,1,1,], padding=list[0,0,0,], data_format="NDHWC", )
paddle.nn.functional.fractional_max_pool3d(Tensor([0, 3, 7, 7, 7],"float32"), output_size=list[3,3,3,], kernel_size=None, random_u=0.3, return_mask=False, name=None, )
paddle.nn.functional.fractional_max_pool3d(x=Tensor([0, 3, 7, 7, 7],"float32"), kernel_size=list[2,2,2,], output_size=list[3,3,3,], random_u=0.6, )
paddle.nn.functional.conv2d(Tensor([1, 128, 12, 32],"float32"), Tensor([128, 128, 3, 0],"float32"), None, list[1,1,], 1, list[1,1,], 1, )
paddle.nn.functional.hinge_embedding_loss(Tensor([10, 10, 0],"float64"), Tensor([10, 10, 0],"float64"), reduction="sum", margin=1.0, name=None, )
paddle.nn.functional.conv3d_transpose(Tensor([4, 4, 8, 0, 8],"float32"), Tensor([4, 3, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[list[0,0,],list[0,0,],list[1,1,],list[1,1,],list[2,2,],], stride=1, dilation=1, groups=2, data_format="NCDHW", )
paddle.searchsorted(sorted_sequence=Tensor([5],"float32"), values=Tensor([4, 0],"float32"), )
paddle.nn.functional.pad(Tensor([12, 16, 0, 64],"float32"), tuple(0,0,0,0,), data_format="NHWC", )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 0, 2, 2],"float32"), output_size=tuple(7,7,), data_format="NCHW", name=None, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 0],"float32"), weight=Tensor([1, 3, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=2, padding=1, )
paddle.einsum("ibm,hm->ibh", Tensor([7, 10, 32],"float32"), Tensor([0, 32],"float32"), )
paddle.nn.functional.prelu(Tensor([1, 2, 0, 4],"float32"), Tensor([2],"float32"), data_format="NCHW", )
paddle.nn.functional.conv2d(Tensor([2, 192, 0, 4],"float32"), Tensor([384, 192, 1, 1],"float32"), None, list[2,2,], 0, list[1,1,], 1, "NCHW", )
paddle.nn.functional.pad(x=Tensor([1, 1, 1, 2, 0],"float64"), pad=tuple(2,1,3,0,2,0,), mode="replicate", data_format="NDHWC", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[328,1,1,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[553,1,1,], )
paddle.Tensor.tile(Tensor([1, 8, 2, 1, 0],"float32"), list[1,1,1,4,1,], )
paddle.nn.functional.avg_pool3d(Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
paddle.nn.functional.conv1d_transpose(Tensor([2, 3, 2],"float64"), Tensor([3, 1, 3],"float64"), bias=Tensor([0],"float64"), output_size=None, output_padding=0, padding=list[1,], stride=list[2,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.max_pool3d(x=Tensor([8, 0, 32, 56, 56],"float32"), kernel_size=list[1,1,1,], stride=list[1,1,1,], padding=list[0,0,0,], data_format="NCDHW", )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 4, 128],"float16"), Tensor([1, 2048, 4, 128],"float16"), Tensor([1, 2048, 4, 0],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), dropout_p=0.0, training=True, is_causal=False, )
paddle.Tensor.lgamma(Tensor([5, 7, 0, 10],"float64"), )
paddle.matmul(Tensor([3, 3],"float64"), Tensor([50000, 2, 3, 0],"float64"), )
paddle.nn.functional.pad(Tensor([2, 3, 0, 32],"float32"), list[2,3,2,3,], value=0, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([0, 512, 128, 128],"float32"), output_size=tuple(3,3,), data_format="NCHW", name=None, )
paddle.trace(x=Tensor([2, 3, 0],"float64"), offset=0, axis1=-3, axis2=-2, )
paddle.nn.functional.temporal_shift(Tensor([32, 256, 28, 0],"float32"), 16, 0.0625, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float32"), Tensor([3, 1, 3, 0, 3],"float32"), bias=Tensor([1],"float32"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[2,2,2,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 2, 2, 2, 3],"float32"), weight=Tensor([3, 1, 3, 3, 0],"float32"), bias=Tensor([3],"float32"), output_padding=1, stride=2, padding=list[1,1,1,], groups=3, data_format="NDHWC", dilation=1, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 1024, 0, 14],"float32"), output_size=1, data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[130,1,1,], )
paddle.Tensor.isnan(Tensor([2, 0, 4],"float64"), )
paddle.kthvalue(Tensor([0, 200, 40],"float32"), k=1, axis=1, keepdim=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 1024, 8, 0],"bfloat16"), Tensor([1, 1024, 2, 0],"bfloat16"), Tensor([1, 1024, 2, 0],"bfloat16"), attn_mask=None, is_causal=True, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 0, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )
paddle.einsum("bhwc,hkc->bhwk", Tensor([48, 32, 32, 64],"float32"), Tensor([32, 0, 64],"float32"), )
paddle.nn.functional.log_softmax(Tensor([2, 3, 0],"float64"), -1, )
paddle.nn.functional.pad(Tensor([0, 64, 16, 16],"float32"), pad=tuple(1,1,1,1,), mode="constant", value=0.0, data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([1, 4, 0],"float32"), list[629,1,1,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[83,1,1,], )
paddle.Tensor.imag(Tensor([0, 4],"complex128"), )
paddle.nn.functional.avg_pool2d(Tensor([0, 3, 32, 32],"float32"), kernel_size=list[2,2,], stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.avg_pool3d(Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCDHW", name=None, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([1, 3, 2, 0, 2],"float32"), bias=Tensor([1],"float32"), stride=1, padding=0, )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 0, 32, 32],"float64"), output_size=tuple(3,3,3,), data_format="NCDHW", name=None, )
paddle.Tensor.tile(Tensor([1, 128, 0, 2],"float32"), list[4,1,1,1,], )
paddle.nn.functional.max_pool2d(Tensor([0, 3, 32, 32],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=True, data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([3, 4, 5, 6, 0],"complex128"), pad=list[1,2,2,1,1,0,], mode="constant", value=100, data_format="NCDHW", name=None, )
paddle.einsum("blqd,bmdk->blqk", Tensor([13, 0, 1, 1],"float32"), Tensor([13, 0, 1, 1],"float32"), )
paddle.index_fill(Tensor([0],"int64"), Tensor([38],"int64"), 0, 6, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[300,1,1,], )
paddle.vision.ops.generate_proposals(Tensor([1, 4, 16, 16],"float32"), Tensor([0, 16, 16, 16],"float32"), Tensor([1, 2],"float32"), Tensor([16, 16, 4, 4],"float32"), Tensor([16, 16, 4, 4],"float32"), pre_nms_top_n=12000, post_nms_top_n=5000, nms_thresh=0.7, min_size=3.0, eta=1.0, pixel_offset=True, return_rois_num=True, )
paddle.einsum("b h i d, b h j d -> b h i j", Tensor([1, 8, 1, 0],"float32"), Tensor([1, 8, 109, 0],"float32"), )
paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 0, 1, 113],"float32"), Tensor([1, 0, 113, 64],"float32"), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[198,1,1,], )
paddle.nn.functional.pad(x=Tensor([1, 1, 1, 0, 3],"float64"), pad=tuple(0,1,1,1,2,0,), mode="constant", value=0, data_format="NCDHW", )
paddle.nn.functional.conv2d_transpose(Tensor([2, 64, 248, 216],"float32"), Tensor([64, 0, 1, 1],"float32"), bias=None, padding=0, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.strided_slice(x=Tensor([3, 4, 5, 0],"float32"), axes=list[1,2,3,], starts=list[-3,0,2,], ends=list[3,2,4,], strides=list[1,1,1,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[306,1,1,], )
paddle.Tensor.tile(Tensor([1, 512, 4, 0],"float32"), tuple(4,1,1,1,), )
paddle.nn.functional.avg_pool3d(x=Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float32"), weight=Tensor([1, 3, 0, 3],"float32"), bias=Tensor([1],"float32"), stride=2, padding=1, )
paddle.slice(Tensor([4, 0, 20],"float16"), list[0,], list[2,], list[4,], )
paddle.nn.functional.max_unpool2d(Tensor([1, 0, 3, 3],"float32"), Tensor([1, 0, 3, 3],"int32"), kernel_size=2, padding=0, output_size=list[1,1,7,7,], )
paddle.nn.functional.conv2d(Tensor([1, 128, 0, 32],"float32"), Tensor([128, 128, 3, 3],"float32"), None, list[1,1,], 1, list[1,1,], 1, )
paddle.nn.functional.pad(Tensor([0, 3, 4, 4, 4],"float32"), list[1,1,1,1,1,1,], mode="circular", data_format="NCDHW", )
paddle.nn.functional.max_pool1d(Tensor([2, 3, 0],"float64"), 2, 2, 1, False, False, None, )
paddle.nn.functional.fractional_max_pool2d(x=Tensor([0, 3, 7, 7],"float32"), kernel_size=list[2,2,], output_size=list[3,3,], random_u=0.6, )
paddle.nn.functional.pad(Tensor([1, 0, 210, 156],"float64"), pad=list[1,1,1,1,], mode="replicate", value=0.0, data_format="NCHW", )
paddle.nn.functional.max_pool2d(Tensor([2, 0, 32, 32],"float64"), kernel_size=list[2,2,], stride=None, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.incubate.nn.functional.fused_bias_act(Tensor([101, 0],"float16"), Tensor([256],"float16"), act_method="gelu", )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 2, 2, 6],"float64"), weight=Tensor([6, 0, 3, 3],"float64"), bias=Tensor([3],"float64"), stride=2, padding=list[1,0,], output_padding=1, dilation=1, groups=3, output_size=None, data_format="NHWC", )
paddle.nn.functional.conv2d(Tensor([1, 2048, 0, 128],"float32"), Tensor([24, 256, 1, 1],"float32"), padding=0, groups=8, )
paddle.Tensor.__sub__(Tensor([0, 1001],"float32"), Tensor([0, 1001],"float32"), )
paddle.nn.functional.conv2d_transpose(Tensor([0, 64, 76, 136],"float32"), Tensor([64, 1, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=64, output_size=None, data_format="NCHW", )
paddle.nn.functional.poisson_nll_loss(Tensor([10, 0],"float32"), Tensor([10, 0],"float32"), log_input=True, full=False, epsilon=1e-08, reduction="sum", )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float32"), weight=Tensor([6, 1, 3, 0],"float32"), bias=Tensor([6],"float32"), stride=2, padding=0, groups=3, )
paddle.nanmedian(Tensor([2, 3, 0, 5],"float32"), axis=tuple(1,2,), keepdim=False, )
paddle.reverse(Tensor([2, 1, 0],"float32"), axis=0, )
paddle.nn.functional.kl_div(Tensor([5, 0],"float32"), label=Tensor([5, 0],"float32"), reduction="mean", name=None, )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float32"), weight=Tensor([1, 3, 0, 2],"float32"), bias=Tensor([1],"float32"), stride=1, padding=0, )
paddle.nn.functional.poisson_nll_loss(Tensor([0, 2],"float32"), Tensor([0, 2],"float32"), log_input=True, full=False, epsilon=1e-08, reduction="sum", )
paddle.nn.functional.conv2d_transpose(Tensor([20, 128, 38, 68],"float32"), Tensor([128, 1, 4, 0],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=128, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 4, 0],"float32"), list[487,1,1,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[350,1,1,], )
paddle.nn.functional.pixel_shuffle(Tensor([2, 9, 4, 0],"float32"), upscale_factor=3, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[131,1,1,], )
paddle.einsum("nbtc,nbsc->nbts", Tensor([2, 0, 7, 8],"float32"), Tensor([2, 0, 14, 8],"float32"), )
paddle.nn.functional.pad(Tensor([4, 128, 94, 0],"float32"), pad=list[1,1,1,1,], mode="constant", value=0.0, data_format="NCHW", name=None, )
paddle.nn.quant.weight_only_linear(Tensor([0, 768],"float16"), weight=Tensor([2304, 768],"int8"), bias=None, weight_scale=Tensor([2304],"float16"), weight_dtype="int8", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float32"), Tensor([3, 0, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.pad(Tensor([0, 3, 4, 4, 4],"float64"), list[1,1,1,1,1,1,], mode="reflect", data_format="NCDHW", )
paddle.nn.functional.softmax(Tensor([16, 0, 128],"float64"), 0, name=None, )
paddle.nn.functional.conv3d(Tensor([4, 8, 8, 8, 3],"float32"), Tensor([5, 3, 3, 0, 3],"float32"), Tensor([5],"float32"), padding=list[1,2,1,], stride=1, dilation=1, groups=1, data_format="NDHWC", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float32"), Tensor([3, 1, 3, 0, 3],"float32"), bias=Tensor([3],"float32"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=3, output_size=None, data_format="NCDHW", )
paddle.nn.functional.pad(Tensor([1, 3, 0, 152],"float64"), pad=list[1,3,2,4,], mode="constant", value=0.0, data_format="NCHW", )
paddle.nn.functional.avg_pool3d(Tensor([3, 0, 40, 40, 7],"float32"), kernel_size=tuple(1,1,5,), stride=1, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float32"), Tensor([3, 0, 3, 3, 3],"float32"), bias=Tensor([3],"float32"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=3, output_size=None, data_format="NCDHW", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 64, 64, 64],"float32"), Tensor([64, 1, 4, 0],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=64, output_size=None, data_format="NCHW", )
paddle.Tensor.frexp(Tensor([10, 0],"float32"), )
paddle.cartesian_prod(list[Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),], )
paddle.nn.functional.conv1d(Tensor([8, 256, 0],"float16"), Tensor([256, 64, 3],"float16"), bias=Tensor([256],"float16"), padding=1, stride=list[1,], dilation=list[1,], groups=4, data_format="NCL", )
paddle.Tensor.lgamma(Tensor([0, 7, 8, 10],"float64"), )
paddle.nn.functional.conv1d_transpose(Tensor([2, 3, 0],"float64"), Tensor([3, 1, 3],"float64"), bias=Tensor([3],"float64"), output_size=None, output_padding=0, padding=list[1,], stride=list[1,], dilation=list[1,], groups=3, data_format="NCL", )
paddle.nn.functional.pad(Tensor([1, 0, 15, 14, 384],"float16"), tuple(0,0,0,6,0,0,), data_format="NDHWC", )
paddle.einsum("bhlqk,bhkd->bhlqd", Tensor([13, 4, 0, 1, 1],"float32"), Tensor([13, 4, 1, 8],"float32"), )
paddle.einsum("abcd,dfg->abcfg", Tensor([2, 4, 5, 3],"float64"), Tensor([3, 0, 5],"float64"), )
paddle.Tensor.flip(Tensor([4, 2, 64, 0],"float32"), 1, )
paddle.nn.functional.conv3d(Tensor([4, 8, 0, 8, 3],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[1,1,2,2,3,3,], stride=1, dilation=2, groups=1, data_format="NDHWC", )
paddle.nn.functional.conv1d_transpose(Tensor([4, 3, 0],"float32"), Tensor([3, 2, 3],"float32"), bias=Tensor([6],"float32"), output_size=None, output_padding=0, padding="valid", stride=list[1,], dilation=list[1,], groups=3, data_format="NCL", )
paddle.nn.functional.conv3d_transpose(Tensor([0, 3, 2, 2, 2],"float64"), Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([3],"float64"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=3, output_size=None, data_format="NCDHW", )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[293,1,1,], )
paddle.einsum("b h i d, b h j d -> b h i j", Tensor([1, 8, 0, 64],"float32"), Tensor([1, 8, 1, 64],"float32"), )
paddle.nn.functional.avg_pool2d(Tensor([0, 3, 32, 32],"float64"), kernel_size=list[2,2,], stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.mse_loss(Tensor([0, 1],"float32"), Tensor([0, 1],"float32"), "none", )
paddle.einsum("a...a->...", Tensor([0, 3, 2, 1, 4, 5],"float64"), )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 32, 0],"float64"), output_size=tuple(3,3,3,), data_format="NDHWC", )
paddle.nn.functional.ctc_loss(Tensor([40, 128, 6625],"float32"), Tensor([128, 25],"int32"), Tensor([128],"int64"), Tensor([0],"int64"), 0, "none", norm_by_times=False, )
paddle.nn.functional.softmax(Tensor([0, 1, 81, 94, 311],"float32"), axis=2, )
paddle.nn.functional.softmax(Tensor([2, 6, 0],"float64"), axis=-3, dtype=None, name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([20, 128, 38, 68],"float32"), Tensor([128, 1, 0, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=128, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[97,1,1,], )
paddle.nn.functional.maxout(x=Tensor([9, 2, 2, 0],"float64"), groups=2, axis=3, )
paddle.nn.functional.pad(Tensor([0, 3, 184, 308],"float64"), pad=list[0,0,40,40,], mode="constant", value=0.0, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([1, 2, 3, 0, 5],"complex128"), pad=list[1,2,1,1,3,4,], mode="constant", value=100, data_format="NDHWC", )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 0, 2],"float64"), Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=1, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d(Tensor([3, 8, 0, 4],"float32"), Tensor([8, 8, 3, 3],"float32"), None, list[2,2,], 1, list[1,1,], 1, "NCHW", )
paddle.Tensor.tile(Tensor([8, 0, 1, 2],"float32"), list[1,5,4,1,], )
paddle.slice(Tensor([0, 2, 104],"float32"), list[0,1,2,], list[0,0,80,], list[2,2,96,], )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 1024, 32, 128],"bfloat16"), Tensor([1, 1024, 32, 128],"bfloat16"), Tensor([1, 1024, 32, 0],"bfloat16"), attn_mask=None, is_causal=True, )
paddle.Tensor.tile(Tensor([1, 0, 7],"float32"), list[4,1,1,], )
paddle.nn.functional.max_pool3d(x=Tensor([8, 64, 4, 0, 112],"float32"), kernel_size=list[1,3,3,], stride=list[1,2,2,], padding=list[0,1,1,], data_format="NCDHW", )
paddle.slice(Tensor([52, 9, 0],"float32"), axes=list[1,], starts=list[1,], ends=list[8,], )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 8, 96],"float16"), Tensor([0, 101, 8, 96],"float16"), Tensor([2, 101, 8, 96],"float16"), attn_mask=None, is_causal=False, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 8, 8, 8, 6],"float32"), Tensor([6, 8, 3, 0, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NDHWC", )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float32"), weight=Tensor([1, 3, 3, 3],"float32"), bias=Tensor([0],"float32"), stride=list[2,1,], padding=1, dilation=list[2,2,], )
paddle.Tensor.tile(Tensor([121, 0],"float32"), list[90,1,1,], )
paddle.einsum("xy,yz->xz", Tensor([0, 4],"float32"), Tensor([4, 4],"float32"), )
paddle.nn.functional.max_pool3d(x=Tensor([2, 3, 0, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,0,0,], )
paddle.nn.functional.max_pool2d(Tensor([1, 1, 4, 0],"float32"), kernel_size=2, stride=2, padding=0, return_mask=True, ceil_mode=False, data_format="NCHW", name=None, )
paddle.reverse(Tensor([0, 4, 8],"float64"), axis=0, )
paddle.nn.functional.pad(Tensor([1, 16, 15, 14, 0],"float32"), tuple(0,0,0,1,0,0,), data_format="NDHWC", )
paddle.nn.functional.avg_pool1d(x=Tensor([0, 3, 8],"float64"), kernel_size=1, stride=1, padding=0, )
paddle.Tensor.tile(Tensor([8, 0, 1, 2],"float32"), list[1,3,4,1,], )
paddle.nn.quant.weight_quantize(Tensor([0, 256],"float16"), algo="weight_only_int4", arch=86, group_size=-1, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 0, 4, 128],"float16"), Tensor([1, 0, 4, 128],"float16"), Tensor([1, 0, 4, 128],"float16"), attn_mask=Tensor([1, 0, 2048, 2048],"float16"), dropout_p=0.0, training=True, is_causal=False, )
paddle.nn.functional.conv2d(Tensor([2, 192, 4, 4],"float32"), Tensor([48, 192, 0, 1],"float32"), None, list[1,1,], 0, list[1,1,], 1, "NCHW", )
paddle.nn.functional.pad(Tensor([0, 4, 5, 6, 7],"complex128"), pad=list[1,1,1,1,1,1,], mode="constant", value=100, data_format="NCDHW", name=None, )
paddle.nn.functional.softmax(Tensor([16, 11, 0, 64],"float32"), axis=1, )
paddle.nanmedian(Tensor([2, 3, 0, 5],"float32"), axis=-2, keepdim=False, mode="min", )
paddle.nn.functional.lp_pool1d(Tensor([0, 3, 32],"float64"), 5.0, 5, 3, 0, False, "NCL", None, )
paddle.einsum("ibnd,snd->ibns", Tensor([0, 14, 4, 8],"float32"), Tensor([2, 4, 8],"float32"), )
paddle.nn.functional.avg_pool3d(x=Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=list[1,0,0,], exclusive=True, )
paddle.nn.functional.hinge_embedding_loss(Tensor([4, 3, 0, 4],"float64"), Tensor([4, 3, 0, 4],"float64"), reduction="mean", margin=-4.0, name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([20, 128, 38, 68],"float32"), Tensor([128, 0, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=128, output_size=None, data_format="NCHW", )
paddle.nn.functional.temporal_shift(x=Tensor([0, 4, 3, 3],"float64"), seg_num=2, shift_ratio=0.4, )
paddle.nn.functional.conv1d(Tensor([16, 64, 25500],"float32"), Tensor([128, 64, 0],"float32"), bias=Tensor([128],"float32"), padding=4, stride=list[1,], dilation=list[4,], groups=1, data_format="NCL", )
paddle.nn.functional.conv2d_transpose(Tensor([1, 1024, 0, 16],"float32"), Tensor([1024, 256, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([1, 3, 224, 0],"float32"), list[0,1,0,0,], )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 0, 96],"float16"), Tensor([2, 100, 8, 96],"float16"), Tensor([2, 100, 8, 96],"float16"), attn_mask=None, is_causal=False, )
paddle.nn.functional.conv1d(Tensor([1, 256, 0],"float32"), Tensor([256, 256, 11],"float32"), bias=Tensor([256],"float32"), padding=5, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.einsum("bind,snd->bnis", Tensor([0, 4, 4, 4],"float32"), Tensor([2, 4, 4],"float32"), )
paddle.einsum("blq,bhlk->bhlqk", Tensor([13, 5, 0],"float32"), Tensor([13, 4, 5, 2],"float32"), )
paddle.nn.functional.zeropad2d(Tensor([4, 0, 224, 224],"float32"), list[2,2,2,2,], )
paddle.Tensor.tile(Tensor([27540, 0],"float32"), list[1,121,], )
paddle.cumsum(x=Tensor([1, 2, 1, 0],"float64"), axis=Tensor([1],"float64"), )
paddle.nn.functional.sequence_mask(Tensor([0, 2, 3, 3, 3],"float64"), maxlen=5, dtype=type(numpy.int32), )
paddle.nn.functional.group_norm(Tensor([2, 4, 3, 0],"float64"), num_groups=2, weight=Tensor([4],"float64"), bias=Tensor([4],"float64"), data_format="NCHW", )
paddle.nn.functional.max_pool1d(x=Tensor([0, 3, 8],"float64"), kernel_size=2, stride=1, padding=list[1,], )
paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 8, 8, 0],"float64"), kernel_size=list[3,3,3,], stride=2, padding=1, exclusive=True, )
paddle.nn.functional.selu(Tensor([0, 3, 3],"float64"), 1.0507009873554805, 1.6732632423543772, None, )
paddle.einsum("abcd,dfg->abcfg", Tensor([2, 4, 0, 3],"float64"), Tensor([3, 4, 5],"float64"), )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 0, 7, 7],"float32"), output_size=list[None,3,None,], data_format="NCDHW", name=None, )
paddle.nn.quant.weight_only_linear(Tensor([1, 32, 128],"float16"), Tensor([288, 128],"int8"), bias=Tensor([288],"float16"), weight_scale=Tensor([0],"float16"), weight_dtype="int8", group_size=-1, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 0, 4, 3],"float64"), output_size=list[3,3,], data_format="NHWC", name=None, )
paddle.nn.functional.selu(Tensor([3, 0, 5, 10],"float64"), 1.5, 2.0, None, )
paddle.nn.functional.fractional_max_pool2d(x=Tensor([0, 3, 7, 7],"float32"), kernel_size=2, output_size=list[3,3,], random_u=0.6, )
paddle.slice(Tensor([2, 0, 104],"float32"), list[0,1,2,], list[0,0,40,], list[2,2,56,], )
paddle.nn.functional.conv2d(Tensor([1, 1024, 128, 128],"float32"), Tensor([12, 256, 0, 1],"float32"), padding=0, groups=4, )
paddle.Tensor.tile(Tensor([10105, 0],"float32"), list[1,2,], )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([0, 3, 8, 32, 32],"float64"), output_size=tuple(3,3,3,), )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 0, 4],"float64"), weight=Tensor([1, 3, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=list[2,2,1,], padding=1, dilation=2, )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 0],"float64"), weight=Tensor([3, 1, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,], groups=1, dilation=2, )
paddle.nn.quant.weight_only_linear(Tensor([0, 320],"float16"), weight=Tensor([512, 320],"int8"), weight_scale=Tensor([512],"float16"), weight_dtype="int8", )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 4, 4, 0],"float64"), output_size=tuple(3,3,), data_format="NHWC", name=None, )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[2026,1,1,], )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 0, 4, 4],"float32"), weight=Tensor([1, 3, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=list[2,2,1,], padding=list[1,2,2,], dilation=tuple(2,2,2,), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[1043,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([4, 3, 0, 16],"float32"), Tensor([3, 5, 3, 3],"float32"), None, output_size=None, padding=0, stride=1, dilation=1, groups=1, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([1, 16, 0, 19, 384],"float32"), tuple(0,2,0,0,0,0,), data_format="NDHWC", )
paddle.nn.functional.pad(Tensor([16, 3, 0, 256],"float32"), list[14,14,14,14,], )
paddle.meshgrid(list[Tensor([0],"float32"),Tensor([0],"float32"),], )
paddle.nn.functional.pad(Tensor([0, 2, 3, 4, 5],"complex128"), pad=list[1,2,1,1,3,4,], mode="constant", value=100, data_format="NDHWC", )
paddle.nn.functional.conv2d_transpose(x=Tensor([0, 3, 2, 2],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,], dilation=2, )
paddle.slice(Tensor([2, 8, 0],"float32"), axes=list[1,], starts=list[4,], ends=list[5,], )
paddle.nn.functional.log_softmax(Tensor([2, 0, 4, 5],"float32"), -1, )
paddle.slice(Tensor([0, 2, 104],"float32"), list[0,1,2,], list[0,0,24,], list[2,2,40,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[122,1,1,], )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 0, 4, 4],"float64"), output_size=list[2,3,], data_format="NCHW", name=None, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([0, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 256, 16, 16],"float32"), Tensor([256, 0, 16, 16],"float32"), bias=None, padding=4, output_padding=0, stride=list[8,8,], dilation=list[1,1,], groups=256, output_size=None, data_format="NCHW", )
paddle.einsum("bmtd,mdhr->bmhtr", Tensor([13, 2, 0, 16],"float32"), Tensor([2, 16, 4, 1],"float32"), )
paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), rtol=1e-05, atol=1e-08, equal_nan=False, name="test_3", )
paddle.slice(Tensor([0, 2, 104],"float32"), list[0,1,2,], list[0,0,48,], list[2,2,64,], )
paddle.gammaln(Tensor([10, 20, 0],"float32"), )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 2],"float32"), weight=Tensor([3, 1, 3, 0, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=list[1,0,1,], dilation=1, )
paddle.nn.functional.pad(Tensor([1, 16, 0, 14, 384],"float32"), tuple(0,0,0,5,0,0,), data_format="NDHWC", )
paddle.nn.functional.softmax(Tensor([2, 3, 0],"float32"), axis=0, dtype="float32", )
paddle.nn.functional.avg_pool2d(Tensor([56, 0, 32, 32],"float32"), kernel_size=2, stride=2, )
paddle.Tensor.tile(Tensor([10105, 0],"float32"), list[1,18,], )
paddle.nn.functional.conv2d_transpose(Tensor([1, 256, 0, 128],"float32"), Tensor([256, 128, 3, 3],"float32"), padding=0, stride=2, groups=1, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[434,1,1,], )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 2],"float64"), weight=Tensor([3, 2, 0],"float64"), bias=Tensor([2],"float64"), stride=1, padding=0, output_padding=0, groups=1, dilation=1, output_size=None, data_format="NCL", name=None, )
paddle.nn.functional.conv1d_transpose(Tensor([4, 6, 16],"float32"), Tensor([6, 8, 0],"float32"), bias=Tensor([8],"float32"), output_size=None, output_padding=0, padding=0, stride=list[2,], dilation=list[2,], groups=1, data_format="NCL", )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 0, 8],"float64"), weight=Tensor([3, 1, 5, 5],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[list[0,0,],list[0,0,],list[1,2,],list[3,4,],], output_padding=0, dilation=1, groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 0, 28, 28],"float16"), output_size=7, data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([0, 79949, 1],"float32"), pad=list[200,200,], mode="reflect", data_format="NLC", )
paddle.Tensor.tile(Tensor([5112, 1, 0],"float32"), list[1,188,1,], )
paddle.nn.functional.softmax(Tensor([0, 10],"float32"), )
paddle.nn.functional.conv3d(Tensor([4, 3, 8, 8, 8],"float32"), Tensor([5, 3, 3, 3, 0],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[0,0,],list[1,1,],list[2,2,],list[2,2,],], stride=1, dilation=1, groups=1, data_format="NCDHW", )
paddle.Tensor.tile(Tensor([1, 5, 0],"float32"), list[450,1,1,], )
paddle.nn.functional.pad(Tensor([1, 0, 3],"float32"), pad=list[1,2,], mode="reflect", value=0.0, data_format="NCL", name=None, )
paddle.nn.functional.softmax(x=Tensor([0, 4, 12, 12],"float32"), axis=-1, )
paddle.Tensor.tile(Tensor([4, 1, 0],"float32"), tuple(1,13,1,), )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([0, 160, 16, 32],"float32"), output_size=2, data_format="NCHW", name=None, )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 5, 0, 7],"float32"), output_size=list[3,3,3,], )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 0],"float32"), Tensor([6, 8, 3, 3],"float32"), bias=Tensor([8],"float32"), padding="valid", output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.kthvalue(x=Tensor([0, 2, 4],"float64"), k=4, axis=2, keepdim=True, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 0],"float32"), weight=Tensor([1, 3, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=list[2,2,1,], padding=tuple(1,2,2,), dilation=tuple(2,2,2,), )
paddle.nn.functional.temporal_shift(x=Tensor([0, 4, 3, 3],"float64"), seg_num=4, )
paddle.nn.functional.conv2d_transpose(Tensor([1, 1024, 16, 0],"float32"), Tensor([1024, 256, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.max_pool3d(Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([1, 3, 0, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=0, )
paddle.nn.functional.pad(Tensor([1, 17980, 0],"float32"), list[1,0,], value=3, mode="constant", data_format="NCL", )
paddle.nn.functional.conv1d(Tensor([1, 256, 0],"float32"), Tensor([256, 256, 11],"float32"), bias=Tensor([256],"float32"), padding=25, stride=list[1,], dilation=list[5,], groups=1, data_format="NCL", )
paddle.nn.functional.pad(x=Tensor([0, 1, 1, 2, 3],"float64"), pad=tuple(0,1,1,1,2,0,), mode="circular", value=0, data_format="NDHWC", )
paddle.slice(Tensor([0, 8, 24],"float32"), axes=list[1,], starts=list[2,], ends=list[3,], )
paddle.nn.functional.max_pool2d(Tensor([2, 8, 0, 64],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=True, data_format="NCHW", name=None, )
paddle.matmul(Tensor([23, 23],"float16"), Tensor([512, 23, 0],"float16"), )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 8, 0],"float16"), Tensor([2, 100, 8, 0],"float16"), Tensor([2, 100, 8, 0],"float16"), attn_mask=None, is_causal=False, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[436,1,1,], )
paddle.logaddexp(Tensor([0, 200, 300],"float32"), Tensor([0, 200, 300],"float32"), )
paddle.Tensor.tile(Tensor([1, 7, 0],"float32"), list[377,1,1,], )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([512, 256, 0, 25],"float32"), output_size=list[1,25,], data_format="NCHW", name=None, )
paddle.allclose(Tensor([0, 13, 32],"float32"), Tensor([0, 13, 32],"float32"), rtol=0.0001, atol=0.0001, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 2, 2, 6],"float64"), Tensor([6, 1, 0, 3],"float64"), bias=Tensor([3],"float64"), padding=list[1,0,], output_padding=1, stride=list[2,2,], dilation=list[1,1,], groups=3, output_size=None, data_format="NHWC", )
paddle.nn.functional.hinge_embedding_loss(Tensor([10, 0, 5],"float64"), Tensor([10, 0, 5],"float64"), reduction="sum", )
paddle.nn.functional.pad(Tensor([1, 3, 0, 224, 271],"float32"), tuple(0,1,0,0,0,0,), data_format="NCDHW", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[1470,1,1,], )
paddle.vision.ops.prior_box(Tensor([4, 48, 0, 80],"float32"), Tensor([4, 3, 0, 640],"float32"), list[16.0,24.0,], list[], list[1.0,], list[0.1,0.1,0.2,0.2,], False, False, list[8.0,8.0,], 0.5, False, None, )
paddle.nn.functional.pad(Tensor([0, 4, 5, 6, 7],"complex128"), pad=list[1,2,2,1,1,0,], mode="reflect", value=0.0, data_format="NCDHW", name=None, )
paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([0],"float32"), shift=None, smooth=None, quant_scale=0.0010981468949466944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.nn.functional.pad(Tensor([1, 2, 3, 4, 0],"complex64"), pad=list[1,2,1,1,3,4,], mode="constant", value=100, data_format="NDHWC", )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 8, 8],"float64"), weight=Tensor([3, 0, 5, 5],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[list[0,0,],list[0,0,],list[1,2,],list[3,4,],], output_padding=0, dilation=1, groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([19125, 0],"float32"), list[1,69,], )
paddle.nn.functional.pad(Tensor([4, 3, 6, 6, 0],"float64"), pad=list[2,2,2,2,2,2,], mode="constant", value=0.0, data_format="NCDHW", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([1, 256, 64, 64],"float32"), Tensor([256, 128, 3, 0],"float32"), bias=Tensor([128],"float32"), padding=1, output_padding=1, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([1, 0, 2, 2, 2],"float32"), pad=list[2,2,2,2,2,2,], mode="replicate", value=0.0, data_format="NCDHW", name=None, )
paddle.logaddexp(Tensor([1, 0, 3, 4],"float64"), Tensor([1, 0, 3, 4],"float64"), )
paddle.einsum("abcd,dfg->abcfg", Tensor([2, 0, 5, 3],"float64"), Tensor([3, 4, 5],"float64"), )
paddle.logaddexp(Tensor([4, 3, 0],"float16"), Tensor([4, 3, 0],"float32"), )
paddle.nonzero(x=Tensor([3, 2, 0, 2],"float64"), as_tuple=False, )
paddle.nn.functional.fractional_max_pool2d(Tensor([0, 3, 7, 7],"float32"), output_size=list[2,5,], kernel_size=None, random_u=0.7, return_mask=False, name=None, )
paddle.nn.functional.pad(Tensor([1, 16, 14, 0, 384],"float16"), tuple(0,1,0,0,0,0,), data_format="NDHWC", )
paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 0],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0009654839523136616, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[993,1,1,], )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[274,1,1,], )
paddle.nn.functional.interpolate(x=Tensor([0, 3, 7, 7],"float32"), mode="area", size=list[2,5,], )
paddle.nn.functional.max_pool3d(x=Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,0,0,], )
paddle.trace(x=Tensor([0, 3, 2],"float64"), offset=-1, axis1=2, axis2=-2, )
paddle.nn.functional.softmax(Tensor([24, 256, 0],"float32"), axis=1, )
paddle.nn.functional.conv2d(Tensor([1, 2048, 16, 16],"float32"), Tensor([2048, 512, 3, 0],"float32"), padding=1, groups=4, )
paddle.nn.functional.log_softmax(Tensor([0, 3, 2, 4],"float32"), 1, )
paddle.nn.functional.max_pool1d(x=Tensor([2, 0, 8],"float64"), kernel_size=2, stride=1, padding=0, ceil_mode=True, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 0, 8, 8],"float32"), Tensor([3, 2, 3, 3, 3],"float32"), bias=Tensor([6],"float32"), padding="valid", output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=3, output_size=None, data_format="NCDHW", )
paddle.nn.functional.pad(Tensor([0, 16, 14, 15, 384],"float16"), tuple(0,1,0,0,0,0,), data_format="NDHWC", )
paddle.einsum("a...a->...", Tensor([5, 0, 2, 1, 4, 5],"float64"), )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 0, 16],"float32"), Tensor([6, 8, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[2,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.softmax(Tensor([0, 3, 4],"float32"), axis=-1, dtype="float32", )
paddle.nn.functional.pad(Tensor([1024, 1, 0, 1],"float32"), list[2,2,2,2,], )
paddle.nn.functional.conv2d(Tensor([16, 3, 260, 260],"float32"), weight=Tensor([3, 1, 0, 5],"float32"), groups=3, )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float64"), weight=Tensor([1, 3, 3, 3],"float64"), bias=Tensor([0],"float64"), stride=list[2,1,], padding=1, dilation=tuple(2,2,), )
paddle.nn.functional.pad(Tensor([0, 1, 32, 128],"float32"), list[2,3,2,3,], value=0, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 0, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=True, )
paddle.nn.functional.l1_loss(Tensor([10, 10, 0],"float32"), Tensor([10, 10, 0],"float32"), "none", name=None, )
paddle.nn.functional.pad(Tensor([1024, 1, 256, 0],"float32"), list[2,2,2,2,], )
paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 0, 5, 2, 4],"float32"), Tensor([1, 0, 4, 2, 4],"float32"), )
paddle.nn.functional.pad(Tensor([3, 64, 0, 16],"float32"), pad=tuple(1,1,1,1,), mode="constant", value=0.0, data_format="NCHW", name=None, )
paddle.Tensor.__rpow__(Tensor([0],"float32"), 10000, )
paddle.nn.functional.gelu(Tensor([0, 16, 64, 64],"float16"), approximate=True, )
paddle.nonzero(Tensor([3, 2, 2, 0],"float64"), True, )
paddle.einsum("a...b,b...c,c...d", Tensor([4, 0, 1, 4],"float64"), Tensor([4, 0, 1, 4],"float64"), Tensor([4, 0, 1, 4],"float64"), )
paddle.nn.functional.conv2d_transpose(Tensor([2, 2, 3, 0],"float64"), Tensor([2, 2, 1, 0],"float64"), groups=1, padding=list[1,0,0,1,], )
paddle.nn.functional.l1_loss(Tensor([2, 3, 0, 32],"float32"), Tensor([2, 3, 0, 32],"float32"), "mean", name=None, )
paddle.nn.quant.weight_only_linear(Tensor([100, 320],"float16"), weight=Tensor([512, 320],"int8"), weight_scale=Tensor([0],"float16"), weight_dtype="int8", )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 4, 0],"float16"), Tensor([1, 2048, 4, 0],"float16"), Tensor([1, 2048, 4, 0],"float16"), attn_mask=Tensor([1, 1, 2048, 0],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.slice(Tensor([1024, 2, 0],"float16"), axes=list[0,], starts=list[768,], ends=list[1024,], )
paddle.nn.functional.avg_pool2d(Tensor([32, 256, 56, 0],"float32"), kernel_size=2, stride=2, padding="SAME", ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([1000, 1, 0],"float32"), list[1,3,1,], )
paddle.einsum("ijk, ikl->ijl", Tensor([0, 10, 3],"float64"), Tensor([0, 3, 10],"float64"), )
paddle.Tensor.fill_diagonal_(Tensor([0, 3],"float64"), 1, offset=2, wrap=True, )
paddle.nn.functional.max_pool3d(Tensor([2, 3, 0, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([13, 4, 5, 0, 8],"float32"), Tensor([13, 4, 5, 0, 8],"float32"), )
paddle.nn.functional.pad(Tensor([1, 3, 32, 245, 0],"float32"), tuple(0,0,0,3,0,0,), data_format="NCDHW", )
paddle.nn.functional.max_pool3d(Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[167,1,1,], )
paddle.nn.functional.max_pool3d(Tensor([0, 5, 6, 8, 3],"float32"), list[3,3,3,], stride=list[1,1,1,], padding=1, data_format="NDHWC", )
paddle.nn.functional.max_pool2d(Tensor([1, 64, 0, 432],"float32"), kernel_size=3, stride=2, padding=1, )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 32, 0],"float64"), output_size=3, data_format="NCDHW", name=None, )
paddle.slice(Tensor([2, 0, 104],"float32"), list[0,1,2,], list[0,0,80,], list[2,2,96,], )
paddle.nn.functional.softmax(x=Tensor([2, 3, 0],"float64"), axis=1, )
paddle.Tensor.tile(Tensor([10285, 0],"float32"), list[1,23,], )
paddle.vision.ops.prior_box(Tensor([4, 48, 80, 0],"float32"), Tensor([4, 3, 640, 640],"float32"), list[16.0,24.0,], list[], list[1.0,], list[0.1,0.1,0.2,0.2,], False, False, list[8.0,8.0,], 0.5, False, None, )
paddle.slice(Tensor([0, 4, 5, 6],"float64"), axes=list[-2,], starts=list[2,], ends=list[3,], )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 4, 4, 0],"float64"), output_size=tuple(3,3,), data_format="NHWC", )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float64"), weight=Tensor([1, 3, 3, 0, 3],"float64"), bias=Tensor([1],"float64"), stride=2, padding=0, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float64"), weight=Tensor([1, 3, 0, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=list[2,2,1,], padding=1, dilation=2, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 0, 2, 2],"float32"), Tensor([3, 12, 12, 12, 12],"float32"), bias=Tensor([12],"float32"), padding=0, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([52, 4, 5, 0, 8],"float32"), Tensor([52, 4, 5, 7, 8],"float32"), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[390,1,1,], )
paddle.Tensor.__getitem__(Tensor([0, 5, 4, 3],"complex128"), list[list[2,3,4,],list[1,2,5,],], )
paddle.nn.functional.conv2d_transpose(Tensor([2, 4, 3, 3],"float64"), Tensor([4, 2, 1, 0],"float64"), groups=1, )
paddle.slice(Tensor([16, 0, 104],"float32"), list[0,1,2,], list[0,0,32,], list[16,2,48,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[166,1,1,], )
paddle.nn.functional.conv1d(Tensor([1, 256, 28],"float32"), Tensor([256, 256, 0],"float32"), bias=Tensor([256],"float32"), padding=15, stride=list[1,], dilation=list[3,], groups=1, data_format="NCL", )
paddle.nanmedian(Tensor([2, 3, 0, 5],"float32"), axis=tuple(1,2,3,), keepdim=False, )
paddle.nn.functional.max_unpool3d(Tensor([0, 1, 4, 5, 6],"float64"), Tensor([0, 1, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
paddle.Tensor.tile(Tensor([19125, 0],"float32"), list[1,143,], )
paddle.nn.functional.max_pool3d(x=Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,2,1,], )
paddle.Tensor.tile(Tensor([10285, 0],"float32"), list[1,66,], )
paddle.nn.functional.lp_pool1d(Tensor([0, 3, 32],"float32"), norm_type=math.inf, kernel_size=2, stride=2, padding=list[1,], ceil_mode=True, )
paddle.nn.functional.max_pool1d(Tensor([0, 3, 32],"float32"), 2, None, 0, False, False, None, )
paddle.nn.functional.max_pool3d(x=Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=2, padding=1, )
paddle.nn.functional.max_unpool1d(Tensor([1, 0, 8],"int64"), Tensor([1, 0, 8],"int32"), kernel_size=2, stride=2, output_size=tuple(1,3,16,), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[366,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([0, 128, 38, 68],"float32"), Tensor([128, 1, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=128, output_size=None, data_format="NCHW", )
paddle.nn.functional.max_unpool2d(Tensor([1, 0, 2, 2],"float32"), Tensor([1, 0, 2, 2],"int64"), kernel_size=2, stride=None, output_size=list[1,1,4,5,], )
paddle.Tensor.tile(Tensor([5535, 0, 1],"float32"), list[1,394,1,], )
paddle.nn.functional.pad(Tensor([0, 64, 16, 112],"float32"), list[0,1,0,1,], value=-math.inf, )
paddle.nn.functional.hinge_embedding_loss(Tensor([0, 3, 3, 4],"float64"), Tensor([0, 3, 3, 4],"float64"), reduction="none", margin=-4.0, name=None, )
paddle.cartesian_prod(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),], )
paddle.nn.functional.layer_norm(Tensor([0, 165, 1024],"float32"), 1024, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), epsilon=1e-05, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 256, 32, 0],"float32"), Tensor([256, 1, 8, 8],"float32"), bias=None, padding=2, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=256, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 6, 0],"float32"), list[596,1,1,], )
paddle.nn.functional.max_pool3d(Tensor([2, 8, 8, 8, 0],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], return_mask=False, ceil_mode=False, data_format="NDHWC", name=None, )
paddle.nn.functional.margin_ranking_loss(Tensor([10, 0],"float64"), Tensor([10, 0],"float64"), Tensor([10, 0],"float64"), 0.0, "mean", )
paddle.roll(Tensor([4, 5, 0, 4],"complex128"), Tensor([2],"int64"), tuple(1,3,), name=None, )
paddle.nn.functional.pad(Tensor([0, 3, 4, 4],"float32"), list[1,1,1,1,], mode="circular", data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 2],"float32"), Tensor([3, 1, 3, 3],"float32"), bias=Tensor([0],"float32"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nanmedian(Tensor([2, 0, 4, 5],"float32"), axis=0, keepdim=False, mode="min", )
paddle.nn.functional.max_unpool2d(Tensor([1, 0, 3, 3],"float32"), Tensor([1, 0, 3, 3],"int32"), kernel_size=2, padding=0, output_size=list[7,7,], )
paddle.nn.functional.pad(Tensor([1, 16, 18, 0, 384],"float32"), tuple(0,0,0,3,0,0,), data_format="NDHWC", )
paddle.slice(Tensor([0, 3, 4, 5, 6],"float32"), axes=list[0,1,2,], starts=list[1,0,2,], ends=list[3,3,4,], )
paddle.nn.functional.l1_loss(Tensor([0, 500, 2],"float16"), Tensor([0, 500, 2],"float32"), reduction="sum", )
paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 512],"int32"), bias=Tensor([512],"float16"), dequant_scales=Tensor([512],"float32"), shift=Tensor([0],"float16"), smooth=Tensor([256],"float16"), act_method="geglu", compute_dtype="fp16", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 8, 3, 0, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[2,2,2,], dilation=list[2,1,2,], groups=1, output_size=None, data_format="NCDHW", )
paddle.subtract(Tensor([0, 3],"complex64"), Tensor([0, 3],"float32"), name="Normal_log_prob", )
paddle.isin(Tensor([8, 0],"bfloat16"), Tensor([4, 256],"bfloat16"), False, True, )
paddle.nn.functional.conv2d(Tensor([3, 16, 0, 2],"float32"), Tensor([16, 16, 0, 3],"float32"), None, list[2,2,], 1, list[1,1,], 1, "NCHW", )
paddle.Tensor.tile(Tensor([1, 0, 2],"float32"), list[1,1,2,], )
paddle.nn.functional.conv1d(Tensor([16, 64, 0],"float32"), Tensor([128, 64, 3],"float32"), bias=Tensor([128],"float32"), padding=512, stride=list[1,], dilation=list[512,], groups=1, data_format="NCL", )
paddle.einsum("i , j -> i j", Tensor([1],"float32"), Tensor([0],"float32"), )
paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 0],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0009928022045642138, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 2, 2, 6],"float32"), Tensor([6, 1, 3, 0],"float32"), bias=None, padding=list[1,0,], output_padding=1, stride=list[2,2,], dilation=list[1,1,], groups=3, output_size=None, data_format="NHWC", )
paddle.slice(Tensor([0, 2, 104],"float32"), list[0,1,2,], list[0,0,32,], list[2,2,48,], )
paddle.nn.functional.conv2d(Tensor([2, 24, 8, 8],"float32"), Tensor([24, 24, 0, 3],"float32"), None, list[1,1,], 1, list[1,1,], 1, "NCHW", )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 0, 8, 32, 32],"float64"), output_size=tuple(3,3,3,), data_format="NCDHW", name=None, )
paddle.vision.ops.generate_proposals(Tensor([1, 4, 16, 0],"float32"), Tensor([1, 16, 16, 16],"float32"), Tensor([1, 2],"float32"), Tensor([16, 16, 4, 4],"float32"), Tensor([16, 16, 4, 4],"float32"), pre_nms_top_n=12000, post_nms_top_n=5000, nms_thresh=0.7, min_size=3.0, eta=1.0, pixel_offset=True, return_rois_num=True, )
paddle.nn.functional.fractional_max_pool2d(x=Tensor([0, 3, 7, 7],"float32"), output_size=list[3,None,], random_u=0.6, )
paddle.nn.functional.conv2d(Tensor([64, 1, 28, 0],"float32"), Tensor([6, 1, 3, 0],"float32"), bias=None, padding=1, stride=list[1,1,], dilation=list[1,1,], groups=1, data_format="NCHW", )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=False, )
paddle.nn.functional.conv2d(Tensor([1024, 1, 258, 258],"float32"), Tensor([1, 1, 0, 4],"float32"), )
paddle.nn.functional.conv1d(Tensor([1, 256, 28],"float32"), Tensor([256, 256, 0],"float32"), bias=Tensor([256],"float32"), padding=5, stride=list[1,], dilation=list[5,], groups=1, data_format="NCL", )
paddle.nn.functional.group_norm(Tensor([2, 4, 0, 2],"float64"), num_groups=2, weight=Tensor([4],"float64"), bias=Tensor([4],"float64"), data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[280,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([0, 2, 3, 3],"float64"), Tensor([2, 2, 1, 1],"float64"), groups=1, padding="SAME", )
paddle.cartesian_prod(list[Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),], )
paddle.nn.functional.max_pool1d(Tensor([1, 3, 0],"float64"), kernel_size=2, stride=2, return_mask=True, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[1466,1,1,], )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 0, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
paddle.nn.functional.conv3d_transpose(Tensor([4, 3, 0, 8, 8],"float32"), Tensor([3, 5, 3, 3, 3],"float32"), Tensor([5],"float32"), output_size=None, padding=0, stride=1, dilation=1, groups=1, data_format="NCDHW", )
paddle.vision.ops.generate_proposals(Tensor([2, 3, 4, 0],"float32"), Tensor([2, 12, 4, 4],"float32"), Tensor([2, 3],"float32"), Tensor([4, 4, 3, 4],"float32"), Tensor([4, 4, 3, 4],"float32"), pre_nms_top_n=10, post_nms_top_n=5, return_rois_num=True, )
paddle.Tensor.imag(Tensor([2, 20, 2, 0],"complex128"), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[231,1,1,], )
paddle.nn.functional.log_softmax(x=Tensor([0, 3, 4],"float64"), axis=2, dtype="float32", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 16, 16, 6],"float32"), Tensor([6, 8, 0, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NHWC", )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 0, 2, 2],"float32"), weight=Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([3],"float32"), stride=1, padding=list[1,0,1,], groups=3, dilation=1, )
paddle.nonzero(Tensor([1, 0, 28, 28],"float32"), )
paddle.cummax(Tensor([100, 0],"int32"), axis=0, )
paddle.take(Tensor([3, 0],"float32"), Tensor([2, 3],"int64"), mode="raise", )
paddle.nn.functional.lp_pool1d(Tensor([0, 3, 32],"float16"), 5.0, 5, 3, 0, False, "NCL", None, )
paddle.slice(Tensor([2, 0, 104],"float32"), list[0,1,2,], list[0,0,32,], list[2,2,48,], )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([1, 3, 2, 2, 2],"float32"), bias=Tensor([0],"float32"), stride=1, padding=0, )
paddle.nn.functional.mse_loss(Tensor([16, 96, 0],"float32"), Tensor([16, 96, 0],"float32"), )
paddle.searchsorted(Tensor([0],"float16"), Tensor([0],"float16"), )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([1, 3, 3, 3, 3],"float32"), bias=Tensor([0],"float32"), stride=2, padding=1, )
paddle.nn.functional.conv1d(Tensor([1, 128, 0],"float32"), Tensor([128, 128, 7],"float32"), bias=Tensor([128],"float32"), padding=9, stride=list[1,], dilation=list[3,], groups=1, data_format="NCL", )
paddle.nn.functional.max_pool2d(Tensor([0, 128, 40, 40],"float32"), 3, stride=1, padding=1, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 8, 0],"float32"), list[414,1,1,], )
paddle.vision.ops.prior_box(Tensor([4, 48, 40, 0],"float32"), Tensor([4, 3, 640, 640],"float32"), list[32.0,48.0,64.0,80.0,96.0,128.0,], list[], list[1.0,], list[0.1,0.1,0.2,0.2,], False, False, list[16.0,16.0,], 0.5, False, None, )
paddle.meshgrid(Tensor([2],"float64"), Tensor([4],"float64"), Tensor([0],"float64"), )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 0, 7, 7],"float32"), output_size=list[None,3,None,], )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 0, 4],"float32"), weight=Tensor([1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=list[2,1,], padding=list[1,2,], dilation=tuple(2,2,), )
paddle.einsum("ibnd,jbnd->bnij", Tensor([7, 0, 4, 8],"float32"), Tensor([7, 0, 4, 8],"float32"), )
paddle.nn.functional.conv1d(Tensor([16, 64, 0],"float32"), Tensor([128, 64, 3],"float32"), bias=Tensor([128],"float32"), padding=128, stride=list[1,], dilation=list[128,], groups=1, data_format="NCL", )
paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 3, 5, 2, 4],"float16"), Tensor([0, 3, 5, 2, 4],"float16"), )
paddle.nn.quant.weight_only_linear(Tensor([1, 0, 64],"float16"), Tensor([128, 64],"int8"), bias=Tensor([256],"float16"), weight_scale=Tensor([256],"float16"), weight_dtype="int4", group_size=-1, )
paddle.nn.functional.pad(Tensor([1, 16, 14, 14, 0],"float32"), tuple(0,0,0,0,0,0,), data_format="NDHWC", )
paddle.Tensor.tile(Tensor([1, 5, 0],"float32"), list[2591,1,1,], )
paddle.nn.functional.maxout(Tensor([0, 4, 3, 3],"float32"), 2, 1, None, )
paddle.roll(Tensor([0, 5, 4, 4],"complex128"), Tensor([1],"int64"), 3, name=None, )
paddle.nn.functional.softmax(Tensor([0, 17, 1600, 4],"float32"), axis=1, )
paddle.strided_slice(x=Tensor([0, 8, 6, 4, 2, 6],"float64"), axes=list[1,2,5,], starts=list[-3,3,4,], ends=list[3,0,1,], strides=list[-1,-1,-2,], )
paddle.nn.functional.avg_pool1d(x=Tensor([2, 0, 8],"float64"), kernel_size=3, stride=4, padding=0, )
paddle.nn.functional.conv1d_transpose(x=Tensor([0, 3, 2],"float32"), weight=Tensor([3, 1, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=1, dilation=1, )
paddle.einsum("lbnd,mlb->mbnd", Tensor([8, 14, 0, 8],"float32"), Tensor([1, 8, 14],"float32"), )
paddle.nn.functional.conv1d_transpose(Tensor([4, 6, 16],"float32"), Tensor([6, 0, 3],"float32"), bias=Tensor([8],"float32"), output_size=None, output_padding=0, padding="valid", stride=list[1,], dilation=list[1,], groups=2, data_format="NCL", )
paddle.vision.ops.generate_proposals(Tensor([1, 3, 10, 14],"float32"), Tensor([1, 12, 10, 14],"float32"), Tensor([1, 2],"float32"), Tensor([420, 0],"float32"), Tensor([420, 4],"float32"), pre_nms_top_n=2000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.pad(Tensor([1, 16, 14, 0, 384],"float32"), tuple(0,5,0,0,0,0,), data_format="NDHWC", )
paddle.searchsorted(Tensor([5],"float64"), Tensor([0, 3],"float64"), right=True, )
paddle.nn.functional.pad(Tensor([1, 0, 3],"float32"), list[1,0,], value=3, mode="constant", data_format="NCL", )
paddle.nn.functional.max_pool2d(x=Tensor([2, 0, 32, 32],"float64"), kernel_size=list[3,3,], stride=list[1,1,], )
paddle.einsum("binh,tnh->bnit", Tensor([0, 2, 4, 4],"float32"), Tensor([8, 4, 4],"float32"), )
paddle.nn.functional.conv2d(Tensor([1, 1024, 0, 128],"float32"), Tensor([1024, 256, 3, 3],"float32"), padding=1, groups=4, )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 0, 7, 10],"float16"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
paddle.nn.functional.pad(Tensor([0, 64, 256, 256],"float32"), pad=list[3,3,3,3,], mode="reflect", value=0.0, data_format="NCHW", name=None, )
paddle.Tensor.flip(Tensor([0, 2, 64, 64],"float32"), 1, )
paddle.nn.functional.conv3d(Tensor([4, 3, 8, 8, 8],"float32"), Tensor([5, 3, 0, 3, 3],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[0,0,],list[1,1,],list[2,2,],list[2,2,],], stride=1, dilation=1, groups=1, data_format="NCDHW", )
paddle.nn.functional.kl_div(Tensor([5, 0],"float64"), Tensor([5, 0],"float32"), )
paddle.nn.functional.conv2d_transpose(Tensor([20, 64, 38, 68],"float32"), Tensor([64, 0, 8, 8],"float32"), bias=None, padding=2, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=64, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 0, 768],"float32"), list[8,1,1,], )
paddle.nn.functional.pad(Tensor([1, 1, 0, 2],"float32"), pad=list[0,1,0,0,], mode="constant", value=-1000000.0, data_format="NCHW", name=None, )
paddle.nn.functional.conv3d_transpose(Tensor([4, 8, 8, 8, 3],"float32"), Tensor([3, 5, 3, 0, 3],"float32"), Tensor([5],"float32"), output_size=None, padding=0, stride=1, dilation=1, groups=1, data_format="NDHWC", )
paddle.nn.functional.max_pool2d(Tensor([256, 112, 112, 0],"float16"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NHWC", name=None, )
paddle.reverse(Tensor([0, 13, 3],"int64"), list[0,], )
paddle.index_fill(Tensor([0],"int64"), Tensor([5],"int64"), 0, 2, )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[143,1,1,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[450,1,1,], )
paddle.Tensor.tile(Tensor([144, 0],"float32"), list[48,1,1,], )
paddle.nn.functional.l1_loss(Tensor([1, 0],"float32"), Tensor([1, 0],"float32"), )
paddle.nn.functional.avg_pool1d(x=Tensor([2, 3, 0],"float64"), kernel_size=2, stride=1, padding=1, exclusive=False, )
paddle.nn.functional.fractional_max_pool2d(x=Tensor([0, 3, 7, 7],"float64"), output_size=list[3,3,], random_u=None, )
paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), atol=1e-06, rtol=1e-06, )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 2],"float32"), weight=Tensor([3, 1, 3],"float32"), bias=Tensor([0],"float32"), stride=1, padding=1, output_padding=0, groups=1, dilation=1, output_size=None, data_format="NCL", name=None, )
paddle.nn.functional.conv3d_transpose(Tensor([0, 3, 2, 2, 2],"float32"), Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=1, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nonzero(Tensor([0, 192],"float32"), )
paddle.einsum("ij,ij->j", Tensor([0, 2],"float64"), Tensor([0, 2],"float64"), )
paddle.nn.functional.lp_pool1d(Tensor([0, 3, 32],"float32"), math.inf, 2, None, 1, False, "NCL", None, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[539,1,1,], )
paddle.nn.functional.conv1d(Tensor([13, 64, 0],"float32"), Tensor([64, 1, 4],"float32"), bias=Tensor([64],"float32"), padding=3, stride=list[1,], dilation=list[1,], groups=64, data_format="NCL", )
paddle.nn.functional.pad(Tensor([0, 4, 5, 6, 7],"complex64"), pad=list[1,2,2,1,1,0,], mode="replicate", value=0.0, data_format="NCDHW", name=None, )
paddle.einsum("ijkl, lmn->kmn", Tensor([2, 4, 0, 3],"float64"), Tensor([3, 2, 4],"float64"), )
paddle.nn.functional.pad(Tensor([1, 0, 64, 64],"float32"), pad=list[1,1,1,1,], mode="reflect", value=0.0, data_format="NCHW", name=None, )
paddle.nn.functional.conv3d_transpose(Tensor([4, 8, 8, 8, 4],"float32"), Tensor([4, 3, 3, 0, 3],"float32"), None, output_size=None, padding="same", stride=1, dilation=1, groups=2, data_format="NDHWC", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 8, 8, 8],"float32"), Tensor([3, 2, 3, 3, 0],"float32"), bias=Tensor([6],"float32"), padding="valid", output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=3, output_size=None, data_format="NCDHW", )
paddle.nn.functional.fractional_max_pool3d(Tensor([2, 0, 7, 7, 7],"float32"), output_size=list[3,3,3,], kernel_size=None, random_u=0.3, return_mask=False, name=None, )
paddle.einsum("blkd,bldq->blkq", Tensor([52, 5, 1, 0],"float32"), Tensor([52, 5, 1, 0],"float32"), )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1024, 2048, 7, 0],"float32"), output_size=1, )
paddle.nn.functional.lp_pool1d(Tensor([2, 3, 0],"float32"), 4.0, 3, 2, 1, False, "NCL", None, )
paddle.nn.functional.pad(Tensor([2, 0, 4, 4],"float32"), list[1,1,1,1,], mode="circular", data_format="NCHW", )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 32, 0],"float64"), output_size=list[1,1,1,], )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([0, 3, 8, 32, 32],"float64"), output_size=tuple(3,3,3,), data_format="NCDHW", name=None, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([0, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[50,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([2, 256, 62, 54],"float32"), Tensor([256, 128, 0, 4],"float32"), bias=None, padding=0, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.max_pool3d(Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=tuple(1,0,0,), return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.nn.functional.pad(Tensor([2, 64, 0, 16],"float32"), list[0,1,0,1,], value=0, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 8, 96],"float16"), Tensor([2, 100, 0, 96],"float16"), Tensor([2, 100, 8, 96],"float16"), attn_mask=None, is_causal=False, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([0, 3, 4, 4],"float64"), output_size=list[2,3,], data_format="NCHW", name=None, )
paddle.einsum("bhwc,wkc->bhwk", Tensor([0, 32, 32, 64],"float32"), Tensor([32, 32, 64],"float32"), )
paddle.meshgrid(list[Tensor([10],"float64"),Tensor([10],"float64"),Tensor([10],"float64"),Tensor([10],"float64"),Tensor([0],"float64"),], )
paddle.vision.ops.generate_proposals(Tensor([1, 4, 16, 16],"float32"), Tensor([1, 0, 16, 16],"float32"), Tensor([1, 2],"float32"), Tensor([16, 16, 4, 4],"float32"), Tensor([16, 16, 4, 4],"float32"), pre_nms_top_n=12000, post_nms_top_n=5000, nms_thresh=0.7, min_size=3.0, eta=1.0, pixel_offset=True, return_rois_num=True, )
paddle.nanmedian(Tensor([2, 3, 0, 5],"float32"), axis=list[0,1,3,], keepdim=False, )
paddle.nn.functional.conv2d(Tensor([2, 192, 4, 0],"float32"), Tensor([96, 192, 1, 1],"float32"), None, list[1,1,], 0, list[1,1,], 1, "NCHW", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 0],"float32"), Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=1, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 0, 4],"float64"), weight=Tensor([6, 1, 3, 3],"float64"), bias=Tensor([6],"float64"), stride=2, padding=0, groups=3, )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 0, 5, 7, 7],"float32"), output_size=list[None,3,None,], )
paddle.nn.functional.conv2d_transpose(Tensor([0, 2048, 16, 16],"float32"), Tensor([2048, 512, 3, 3],"float32"), padding=0, stride=2, groups=4, )
paddle.nn.functional.max_pool2d(Tensor([1, 64, 320, 0],"float32"), kernel_size=3, stride=2, padding=1, )
paddle.nn.functional.flashmask_attention(Tensor([1, 2048, 8, 96],"float16"), Tensor([1, 2048, 0, 96],"float16"), Tensor([1, 2048, 8, 96],"float16"), startend_row_indices=Tensor([1, 1, 2048, 1],"int32"), causal=True, )
paddle.nn.functional.max_unpool2d(Tensor([0, 3, 3, 3],"float32"), Tensor([0, 3, 3, 3],"int32"), kernel_size=2, padding=0, output_size=list[1,1,7,7,], )
paddle.Tensor.tile(Tensor([10285, 0],"float32"), list[1,35,], )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 0],"float32"), Tensor([3, 2, 3, 3, 3],"float32"), bias=Tensor([2],"float32"), padding=0, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.max_pool2d(Tensor([0, 3, 32, 32],"float64"), kernel_size=list[3,3,], stride=list[1,2,], padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 0, 4],"float32"), weight=Tensor([1, 3, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=list[2,2,1,], padding=tuple(1,2,2,), dilation=tuple(2,2,2,), )
paddle.slice(Tensor([13, 9, 0],"float32"), axes=list[1,], starts=list[1,], ends=list[8,], )
paddle.nn.functional.channel_shuffle(Tensor([2, 4, 4, 0],"float64"), 3, "NHWC", )
paddle.nn.functional.mse_loss(Tensor([0, 96, 1],"float32"), Tensor([0, 96, 1],"float32"), )
paddle.matmul(Tensor([10],"float32"), Tensor([0],"float32"), )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([0, 3, 7, 7],"float32"), output_size=5, )
paddle.nn.functional.pad(Tensor([0, 3, 4],"float32"), list[1,1,], mode="reflect", data_format="NCL", )
paddle.nn.functional.conv3d(x=Tensor([2, 4, 4, 4, 3],"float64"), weight=Tensor([2, 3, 0, 3, 3],"float64"), bias=Tensor([2],"float64"), stride=1, padding=0, data_format="NDHWC", )
paddle.nn.functional.layer_norm(Tensor([0, 20],"float16"), list[20,], Tensor([20],"float32"), Tensor([20],"float32"), )
paddle.nn.functional.softmax(Tensor([128, 1, 16, 0, 49],"float32"), -1, name=None, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 0, 2],"float32"), Tensor([3, 2, 3, 3, 3],"float32"), bias=Tensor([2],"float32"), padding=0, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.frexp(Tensor([10, 0],"float32"), )
paddle.slice(Tensor([3, 0, 104],"float32"), list[0,1,2,], list[0,0,80,], list[3,1,96,], )
paddle.nn.functional.conv3d_transpose(Tensor([4, 8, 8, 8, 4],"float32"), Tensor([4, 3, 0, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[list[0,0,],list[1,2,],list[1,2,],list[2,1,],list[0,0,],], stride=1, dilation=1, groups=2, data_format="NDHWC", )
paddle.nn.functional.pad(Tensor([4, 64, 188, 0],"float32"), pad=list[1,1,1,1,], mode="constant", value=0.0, data_format="NCHW", name=None, )
paddle.nn.functional.avg_pool1d(Tensor([0, 3, 8],"float64"), 2, 1, 0, True, True, None, )
paddle.nn.functional.pad(Tensor([1, 2, 3, 4, 0],"complex128"), pad=list[1,2,1,1,3,4,], mode="constant", value=100, data_format="NDHWC", )
paddle.nn.functional.pad(Tensor([3, 0, 5, 6, 7],"complex128"), pad=list[1,2,2,1,1,0,], mode="constant", value=100, data_format="NCDHW", name=None, )
paddle.strided_slice(x=Tensor([3, 0, 5, 6],"float32"), axes=list[1,2,3,], starts=list[-3,0,2,], ends=list[3,2,4,], strides=list[1,1,1,], )
paddle.einsum("...ij,...i->j...", Tensor([10, 11],"float64"), Tensor([3, 4, 0, 10],"float64"), )
paddle.nanmedian(Tensor([0, 3, 4, 5],"float32"), axis=list[0,3,], keepdim=False, mode="min", )
paddle.nn.functional.pad(Tensor([1, 2, 0],"float64"), pad=list[1,1,], mode="reflect", value=0.0, data_format="NLC", name=None, )
paddle.nn.functional.maxout(Tensor([0, 6, 5, 4],"float64"), 2, 1, )
paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 0],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0016999575309455395, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.nn.functional.square_error_cost(Tensor([8, 0, 100],"float16"), Tensor([8, 0, 100],"float32"), )
paddle.nn.functional.conv1d(x=Tensor([2, 3, 4],"float32"), weight=Tensor([6, 1, 0],"float32"), bias=Tensor([6],"float32"), stride=2, padding=0, groups=3, )
paddle.nanmedian(Tensor([2, 3, 0, 5],"float32"), axis=list[0,1,3,], keepdim=False, mode="min", )
paddle.nn.functional.pad(Tensor([1, 1, 3, 0],"float32"), pad=list[0,1,0,0,], mode="constant", value=-1000000.0, data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([1, 16, 0, 15, 384],"float16"), tuple(0,1,0,0,0,0,), data_format="NDHWC", )
paddle.Tensor.lgamma(Tensor([0],"float32"), )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 8, 96],"float16"), Tensor([2, 101, 0, 96],"float16"), Tensor([2, 101, 8, 96],"float16"), attn_mask=None, is_causal=False, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[210,1,1,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[302,1,1,], )
paddle.nn.functional.selu(x=Tensor([3, 3, 0],"float64"), alpha=0, scale=1.0507009873554805, )
paddle.slice(Tensor([11, 0, 4],"float32"), axes=list[2,], starts=list[3,], ends=list[4,], )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 8, 96],"float16"), Tensor([2, 0, 8, 96],"float16"), Tensor([2, 101, 8, 96],"float16"), attn_mask=None, is_causal=False, )
paddle.nn.functional.max_pool2d(x=Tensor([2, 0, 32, 32],"float64"), kernel_size=list[3,3,], stride=list[1,2,], )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 0, 32, 32],"float64"), output_size=tuple(3,3,3,), data_format="NDHWC", name=None, )
paddle.slice(Tensor([0, 1, 104],"float32"), list[0,1,2,], list[0,0,16,], list[3,1,32,], )
paddle.einsum("a...a->...", Tensor([5, 3, 2, 0, 4, 5],"float64"), )
paddle.kthvalue(x=Tensor([3, 0, 4],"float32"), k=3, axis=0, )
paddle.flip(Tensor([20, 3, 0, 112],"float32"), axis=-1, )
paddle.einsum("b h i j, b h j d -> b h i d", Tensor([0, 8, 1, 113],"float32"), Tensor([0, 8, 113, 64],"float32"), )
paddle.Tensor.imag(Tensor([16, 257, 0],"complex64"), )
paddle.einsum("...qk,...kd->...qd", Tensor([52, 4, 3, 0, 2],"float32"), Tensor([52, 4, 3, 2, 8],"float32"), )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 0],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
paddle.nn.functional.max_pool2d(Tensor([1, 3, 0, 6],"float32"), kernel_size=2, stride=2, padding=0, return_mask=True, )
paddle.nn.functional.conv1d_transpose(Tensor([0, 3, 2],"float64"), Tensor([3, 2, 3],"float64"), bias=Tensor([2],"float64"), output_size=None, output_padding=0, padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.pad(Tensor([1, 16, 14, 19, 0],"float16"), tuple(0,2,0,0,0,0,), data_format="NDHWC", )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([1, 3, 3, 3, 3],"float32"), bias=Tensor([0],"float32"), stride=2, padding=0, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 2, 2, 6],"float32"), Tensor([6, 0, 3, 3],"float32"), bias=None, padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=3, output_size=None, data_format="NHWC", )
paddle.nn.functional.max_pool3d(Tensor([2, 3, 32, 0, 32],"float32"), kernel_size=2, stride=None, padding="SAME", return_mask=True, )
paddle.nn.functional.max_unpool3d(Tensor([0, 3, 2, 2, 3],"int64"), Tensor([0, 3, 2, 2, 3],"int32"), kernel_size=2, stride=2, output_size=list[1,3,4,4,6,], )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 0, 32],"float32"), output_size=list[3,3,3,], )
paddle.Tensor.tile(Tensor([1, 4, 0],"float32"), list[518,1,1,], )
paddle.nn.functional.avg_pool2d(Tensor([256, 128, 0, 50],"float32"), kernel_size=tuple(2,1,), stride=tuple(2,1,), padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.subtract(Tensor([0, 1],"float32"), Tensor([0, 1],"float32"), )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[564,1,1,], )
paddle.nn.functional.max_pool3d(Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=1, return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 1, 0, 40],"float16"), Tensor([1, 1, 0, 40],"float16"), Tensor([1, 1, 0, 40],"float16"), attn_mask=Tensor([1, 2, 0, 1],"float16"), )
paddle.nn.functional.interpolate(x=Tensor([2, 3, 5, 0, 7],"float32"), mode="area", size=list[2,3,5,], )
paddle.nn.functional.conv3d(Tensor([4, 6, 8, 8, 8],"float32"), Tensor([12, 1, 3, 3, 0],"float32"), None, padding="valid", stride=1, dilation=1, groups=6, data_format="NCDHW", )
paddle.slice(Tensor([2, 0],"float16"), axes=list[0,], starts=list[1,], ends=list[2,], )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 2, 3],"float32"), weight=Tensor([3, 0, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=list[1,], groups=3, dilation=1, data_format="NLC", )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 0, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )
paddle.Tensor.tile(Tensor([6942, 0, 1],"float32"), list[1,136,1,], )
paddle.slice(Tensor([1024, 2, 0],"float16"), axes=list[0,], starts=list[128,], ends=list[256,], )
paddle.nn.functional.conv3d(Tensor([4, 8, 8, 8, 3],"float32"), Tensor([5, 3, 3, 0, 3],"float32"), Tensor([5],"float32"), padding=list[1,2,3,1,2,3,], stride=2, dilation=1, groups=1, data_format="NDHWC", )
paddle.nn.functional.softmax(Tensor([16, 16, 0],"float64"), 1, )
paddle.Tensor.tile(Tensor([3938, 0, 1],"float32"), list[1,121,1,], )
paddle.nn.functional.conv1d(x=Tensor([2, 3, 4],"float64"), weight=Tensor([1, 3, 0],"float64"), bias=Tensor([1],"float64"), stride=list[1,], padding=1, dilation=2, )
paddle.Tensor.tile(Tensor([1360, 0],"float32"), list[1,38,], )
paddle.nn.functional.pad(Tensor([1, 25757, 0],"float32"), list[1,0,], value=5, mode="constant", data_format="NCL", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 256, 16, 0],"float32"), Tensor([256, 1, 16, 16],"float32"), bias=None, padding=4, output_padding=0, stride=list[8,8,], dilation=list[1,1,], groups=256, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 0],"float32"), Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([3],"float32"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=3, output_size=None, data_format="NCDHW", )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 2],"float64"), Tensor([3, 0, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=1, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.prelu(Tensor([1, 2, 3, 0],"float32"), Tensor([1],"float32"), )
paddle.allclose(Tensor([13, 8, 0],"float32"), Tensor([13, 8, 0],"float32"), atol=0.0001, )
paddle.nn.functional.avg_pool3d(Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=tuple(1,0,0,), ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
paddle.Tensor.fill_diagonal_(Tensor([0, 3],"float32"), 1, offset=0, wrap=False, )
paddle.logcumsumexp(Tensor([10, 0, 10],"float32"), )
paddle.slice(Tensor([16, 0, 104],"float32"), list[0,1,2,], list[0,0,24,], list[16,1,40,], )
paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 0, 2],"float32"), linear1_weight=Tensor([2, 2],"float32"), linear2_weight=Tensor([2, 2],"float32"), activation="gelu", dropout1_rate=0, dropout2_rate=0, )
paddle.einsum("a...a->a...", Tensor([5, 3, 0, 1, 4, 5],"float64"), )
paddle.roll(Tensor([4, 5, 0, 4],"complex128"), Tensor([1],"int64"), 3, name=None, )
paddle.nn.functional.selu(Tensor([3, 3, 0],"float64"), 1.0507009873554805, 1.6732632423543772, None, )
paddle.einsum("i...->...", Tensor([2, 3, 0],"float64"), )
paddle.nn.functional.lp_pool1d(Tensor([2, 0, 32],"float32"), norm_type=4, kernel_size=3, stride=2, padding=list[1,], )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 0],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=1, dilation=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv1d(Tensor([1, 256, 0],"float32"), Tensor([256, 256, 7],"float32"), bias=Tensor([256],"float32"), padding=9, stride=list[1,], dilation=list[3,], groups=1, data_format="NCL", )
paddle.nn.functional.conv2d(Tensor([2, 192, 4, 4],"float32"), Tensor([96, 192, 0, 1],"float32"), None, list[1,1,], 0, list[1,1,], 1, "NCHW", )
paddle.einsum("ij,jk", Tensor([4, 0],"float64"), Tensor([10, 0],"float64"), )
paddle.nn.functional.conv2d(Tensor([3, 8, 4, 4],"float32"), Tensor([8, 8, 0, 3],"float32"), None, list[2,2,], 1, list[1,1,], 1, "NCHW", )
paddle.nn.functional.softmax(x=Tensor([16, 4, 0, 12],"float32"), axis=-1, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[517,1,1,], )
paddle.Tensor.tile(Tensor([1, 1, 0, 1, 8],"float32"), list[1,1,1,2,1,], )
paddle.Tensor.tile(Tensor([96, 3, 1, 0, 4],"float32"), list[1,1,2,1,1,], )
paddle.index_fill(Tensor([0],"int64"), Tensor([28],"int64"), 0, 5, )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[173,1,1,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[222,1,1,], )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 0],"float64"), weight=Tensor([1, 3, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=list[2,2,1,], padding=list[1,2,2,], dilation=tuple(2,2,2,), )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 0],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=True, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 0],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )
paddle.nn.functional.conv2d_transpose(Tensor([0, 6, 16, 16],"float32"), Tensor([6, 8, 3, 3],"float32"), bias=Tensor([8],"float32"), padding="same", output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.lp_pool2d(Tensor([2, 0, 32, 32],"float64"), 2, kernel_size=5, stride=3, ceil_mode=False, )
paddle.nanmedian(Tensor([2, 3, 0, 5],"float32"), axis=tuple(1,2,3,), keepdim=False, mode="min", )
paddle.nn.functional.conv3d_transpose(Tensor([4, 4, 8, 8, 8],"float32"), Tensor([4, 3, 0, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,2,1,], stride=1, dilation=1, groups=2, data_format="NCDHW", )
paddle.Tensor.tile(Tensor([16660, 0],"float32"), list[1,135,], )
paddle.nn.functional.max_pool2d(Tensor([0, 8, 16, 64],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=True, data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([0, 4, 5, 6],"complex128"), pad=list[1,2,2,1,], mode="replicate", value=0.0, data_format="NCHW", name=None, )
paddle.nn.functional.conv2d(Tensor([8, 3, 256, 256],"float32"), Tensor([128, 3, 0, 1],"float32"), bias=None, stride=1, padding=0, )
paddle.slice(Tensor([0, 2, 104],"float32"), list[0,1,2,], list[0,0,88,], list[2,2,104,], )
paddle.slice(Tensor([1024, 16, 0],"float16"), axes=list[0,], starts=list[768,], ends=list[896,], )
paddle.nn.functional.conv2d_transpose(Tensor([1, 256, 128, 128],"float32"), Tensor([256, 128, 0, 3],"float32"), padding=0, stride=2, groups=1, )
paddle.nn.functional.conv2d(Tensor([1, 128, 256, 256],"float32"), Tensor([128, 128, 3, 0],"float32"), padding=1, groups=1, )
paddle.nn.functional.max_pool3d(Tensor([2, 3, 8, 0, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,2,1,], return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 8, 0, 109],"float32"), Tensor([1, 8, 109, 64],"float32"), )
paddle.nn.functional.pad(Tensor([1, 0, 184, 308],"float64"), pad=list[0,0,40,40,], mode="constant", value=0.0, data_format="NCHW", )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([0, 3, 7, 7],"float32"), output_size=list[None,3,], )
paddle.nn.functional.log_softmax(Tensor([2, 3, 0, 5],"float32"), -1, )
paddle.einsum("b h i d, b h j d -> b h i j", Tensor([1, 8, 1, 0],"float32"), Tensor([1, 8, 1, 0],"float32"), )
paddle.allclose(Tensor([13, 0, 32],"float32"), Tensor([13, 0, 32],"float32"), rtol=0.0001, atol=0.0001, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 0],"float32"), Tensor([3, 12, 12, 12, 12],"float32"), bias=Tensor([12],"float32"), padding=0, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.max_pool1d(Tensor([2, 0, 8],"float64"), 2, 1, list[1,1,], False, False, None, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float32"), Tensor([3, 0, 3, 3, 3],"float32"), bias=Tensor([2],"float32"), padding=0, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.mse_loss(Tensor([3, 3, 0, 10],"float32"), Tensor([3, 3, 0, 10],"float32"), "sum", )
paddle.nn.functional.pad(Tensor([1, 0, 14, 19, 384],"float32"), tuple(0,2,0,0,0,0,), data_format="NDHWC", )
paddle.nn.functional.mse_loss(Tensor([10, 0],"float32"), Tensor([10, 0],"float32"), "none", )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 0, 64],"float16"), Tensor([1, 2048, 0, 64],"float16"), Tensor([1, 2048, 0, 64],"float16"), attn_mask=Tensor([1, 1, 0, 2048],"float16"), is_causal=True, )
paddle.nn.functional.pad(Tensor([1, 2, 0],"float64"), pad=list[1,2,], mode="constant", value=0.0, data_format="NCL", name=None, )
paddle.nn.quant.weight_quantize(Tensor([64, 0],"float16"), algo="weight_only_int4", arch=80, group_size=-1, )
paddle.nn.functional.temporal_shift(Tensor([32, 256, 0, 28],"float32"), 16, 0.0625, )
paddle.einsum("...i->...", Tensor([0, 3, 11],"float64"), )
paddle.einsum("sec,sm->ecm", Tensor([2, 0, 2],"float32"), Tensor([2, 64],"float32"), )
paddle.nn.functional.conv1d(Tensor([13, 32, 0],"float32"), Tensor([64, 8, 1],"float32"), bias=Tensor([64],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=4, data_format="NCL", )
paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 8, 1, 113],"float32"), Tensor([0, 8, 113, 64],"float32"), )
paddle.nn.functional.softmax(Tensor([0, 3],"float32"), axis=1, dtype="float32", )
paddle.nn.functional.temporal_shift(x=Tensor([6, 4, 0, 2],"float32"), seg_num=2, shift_ratio=0.2, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 2],"float32"), Tensor([3, 0, 3, 3],"float32"), bias=None, padding=1, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 0, 2],"float32"), weight=Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=list[1,0,1,], dilation=2, )
paddle.nn.functional.flashmask_attention(Tensor([1, 2048, 8, 96],"float16"), Tensor([1, 2048, 8, 96],"float16"), Tensor([0, 2048, 8, 96],"float16"), startend_row_indices=Tensor([1, 1, 2048, 1],"int32"), causal=True, )
paddle.nn.functional.pad(Tensor([1, 1, 0, 1],"float32"), pad=list[0,1,0,0,], mode="constant", value=-1000000.0, data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([1, 16, 14, 0, 384],"float32"), tuple(0,6,0,0,0,0,), data_format="NDHWC", )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 0, 4],"float32"), weight=Tensor([1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=list[2,1,], padding=1, dilation=tuple(2,2,), )
paddle.nn.functional.avg_pool1d(x=Tensor([2, 3, 0],"float64"), kernel_size=3, stride=4, padding=0, )
paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 0, 2],"float32"), linear1_weight=Tensor([2, 2],"float32"), linear2_weight=Tensor([2, 2],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, )
paddle.nn.functional.group_norm(Tensor([1, 1024, 10, 0],"float32"), num_groups=32, epsilon=1e-05, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), )
paddle.nn.functional.maxout(Tensor([100, 4, 0, 3],"float64"), 2, 1, None, )
paddle.nn.functional.max_pool1d(Tensor([2, 0, 32],"float32"), kernel_size=2, stride=2, padding=0, return_mask=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 0, 8, 16],"float16"), Tensor([1, 0, 2, 16],"float16"), Tensor([1, 0, 2, 16],"float16"), attn_mask=Tensor([1, 0, 2048, 2048],"float16"), is_causal=True, )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 0, 7],"float32"), output_size=list[3,3,3,], data_format="NCDHW", name=None, )
paddle.Tensor.tile(Tensor([8, 0, 1, 2],"float32"), list[1,4,4,1,], )
paddle.nn.functional.max_pool3d(x=Tensor([2, 3, 0, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,2,1,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[208,1,1,], )
paddle.Tensor.tile(Tensor([1, 0, 256],"float16"), list[2,1,1,], )
paddle.slice(Tensor([3, 0, 104],"float32"), list[0,1,2,], list[0,0,32,], list[3,1,48,], )
paddle.nn.functional.conv1d_transpose(Tensor([4, 16, 6],"float32"), Tensor([6, 0, 3],"float32"), bias=Tensor([8],"float32"), output_size=None, output_padding=2, padding=0, stride=list[3,], dilation=list[1,], groups=1, data_format="NLC", )
paddle.nn.functional.prelu(Tensor([0, 2, 3, 4],"float32"), Tensor([1],"float32"), )
paddle.nn.functional.hinge_embedding_loss(Tensor([4, 3, 0, 4],"float64"), Tensor([4, 3, 0, 4],"float64"), reduction="none", margin=-4.0, name=None, )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[141,1,1,], )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 8, 96],"float16"), Tensor([2, 101, 8, 96],"float16"), Tensor([2, 0, 8, 96],"float16"), attn_mask=None, is_causal=False, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 0],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 0, 4, 4],"float32"), output_size=list[3,3,], )
paddle.nn.functional.group_norm(Tensor([1, 1024, 0, 32],"float32"), num_groups=32, epsilon=1e-05, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), )
paddle.nn.quant.weight_only_linear(Tensor([1, 0, 64],"float16"), Tensor([256, 64],"int8"), bias=Tensor([256],"float16"), weight_scale=Tensor([256],"float16"), weight_dtype="int8", group_size=-1, )
paddle.quantile(x=Tensor([3, 6, 3, 4, 0, 5],"float64"), q=tuple(0.11,0.5,0.73,0.9,), axis=4, keepdim=False, )
paddle.nn.functional.conv2d(Tensor([3, 3, 0, 32],"float32"), Tensor([64, 3, 0, 7],"float32"), None, list[2,2,], 3, list[1,1,], 1, "NCHW", )
paddle.vision.ops.generate_proposals(Tensor([1, 15, 42, 63],"float32"), Tensor([1, 60, 42, 63],"float32"), Tensor([1, 2],"float32"), Tensor([39690, 0],"float32"), Tensor([39690, 4],"float32"), pre_nms_top_n=12000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 0, 16, 32],"float32"), output_size=4, data_format="NCHW", name=None, )
paddle.nn.functional.local_response_norm(x=Tensor([0, 3, 40, 40, 3],"float32"), size=5, data_format="NDHWC", )
paddle.nn.functional.pad(Tensor([1, 16, 0, 16, 384],"float16"), tuple(0,5,0,0,0,0,), data_format="NDHWC", )
paddle.searchsorted(sorted_sequence=Tensor([0, 5],"float64"), values=Tensor([0, 3],"float64"), )
paddle.Tensor.tile(Tensor([121, 0],"float32"), list[48,1,1,], )
paddle.nn.functional.sequence_mask(Tensor([0, 3],"int64"), maxlen=Tensor([1],"int32"), dtype=VarType(uint8), )
paddle.nn.functional.pad(Tensor([1, 3, 0, 224],"float32"), list[0,1,0,0,], )
paddle.Tensor.trunc(Tensor([2, 0, 8],"float32"), )
paddle.roll(Tensor([0, 5, 4],"float64"), Tensor([3],"int64"), list[0,1,2,], name=None, )
paddle.Tensor.tile(Tensor([8, 0, 512],"float32"), tuple(1,2,1,), )
paddle.nn.functional.mse_loss(Tensor([0, 3, 10, 10],"float32"), Tensor([0, 3, 10, 10],"float32"), "mean", )
paddle.nn.functional.hinge_embedding_loss(Tensor([0, 3],"float64"), Tensor([0, 3],"float64"), reduction="mean", margin=-4.0, name=None, )
paddle.vision.ops.generate_proposals(Tensor([0, 9, 10, 8],"float32"), Tensor([1, 36, 10, 8],"float32"), Tensor([1, 2],"float32"), Tensor([720, 4],"float32"), Tensor([720, 4],"float32"), pre_nms_top_n=4000, post_nms_top_n=4000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[301,1,1,], )
paddle.Tensor.tile(Tensor([1, 33, 0],"float32"), list[7,1,1,], )
paddle.Tensor.tile(Tensor([1, 4, 0],"float32"), list[287,1,1,], )
paddle.nn.functional.avg_pool3d(Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=2, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
paddle.nn.functional.max_pool2d(x=Tensor([0, 3, 32, 32],"float64"), kernel_size=list[3,3,], stride=list[1,1,], ceil_mode=True, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[1078,1,1,], )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([0, 128, 32, 32],"float32"), output_size=tuple(2,2,), data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([4, 0, 512],"float32"), tuple(1,10,1,), )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 16],"float32"), Tensor([6, 0, 3, 3],"float32"), bias=Tensor([8],"float32"), padding="valid", output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=2, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 4, 0],"float32"), list[590,1,1,], )
paddle.Tensor.tile(Tensor([1, 6, 0],"float32"), list[472,1,1,], )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 0, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 512, 7, 0],"float32"), output_size=tuple(7,7,), data_format="NCHW", name=None, )
paddle.einsum("iox,ojx->ijx", Tensor([2, 3, 0],"complex64"), Tensor([3, 2, 0],"complex64"), )
paddle.nn.functional.layer_norm(Tensor([1, 0, 2],"float32"), 2, epsilon=1e-05, weight=None, bias=None, )
paddle.nn.functional.conv1d(Tensor([1, 128, 0],"float32"), Tensor([128, 128, 11],"float32"), bias=Tensor([128],"float32"), padding=5, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 0, 2, 2],"float32"), weight=Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=1, dilation=1, )
paddle.einsum("sec,ecm->sm", Tensor([10, 60, 10],"float32"), Tensor([60, 10, 0],"float32"), )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 0, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding="valid", output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.max_pool2d(Tensor([0, 192, 20, 20],"float32"), kernel_size=13, stride=1, padding=6, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 0, 7],"float32"), output_size=5, data_format="NCDHW", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([0, 6, 2, 2],"float32"), Tensor([6, 1, 3, 3],"float32"), bias=None, padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=3, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 100, 0],"float32"), list[4,1,1,], )
paddle.nn.functional.pad(Tensor([3, 4, 0, 6],"complex64"), pad=list[1,2,2,1,], mode="constant", value=100, data_format="NCHW", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 2, 2, 6],"float32"), Tensor([6, 1, 3, 0],"float32"), bias=None, padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=3, output_size=None, data_format="NHWC", )
paddle.einsum("ji,j", Tensor([1, 4],"float64"), Tensor([0],"float64"), )
paddle.nn.functional.pad(Tensor([1, 16, 14, 0, 384],"float16"), tuple(0,0,0,0,0,0,), data_format="NDHWC", )
paddle.einsum("blqd,bmdk->blqk", Tensor([13, 5, 1, 1],"float32"), Tensor([13, 1, 0, 1],"float32"), )
paddle.einsum("i,j", Tensor([0],"float64"), Tensor([11],"float64"), )
paddle.einsum("ij,kl->ijkl", Tensor([4, 0],"float64"), Tensor([3, 7],"float64"), )
paddle.Tensor.tile(Tensor([96, 0, 1, 4, 4],"float32"), list[1,1,2,1,1,], )
paddle.nn.functional.max_pool2d(Tensor([0, 1, 4, 5],"float32"), kernel_size=2, stride=2, return_mask=True, )
paddle.isin(Tensor([0, 64],"bfloat16"), Tensor([4, 256],"bfloat16"), False, True, )
paddle.nn.functional.maxout(Tensor([10, 9, 0, 3],"float64"), 3, 1, None, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 2, 2, 6],"float64"), weight=Tensor([6, 0, 3, 3],"float64"), bias=Tensor([3],"float64"), output_size=list[4,6,], stride=2, padding=list[1,0,], groups=3, data_format="NHWC", dilation=1, )
paddle.Tensor.tile(Tensor([4, 2, 0, 64],"float32"), list[1,36,1,1,], )
paddle.einsum("sec,sm->ecm", Tensor([10, 60, 0],"float32"), Tensor([10, 64],"float32"), )
paddle.meshgrid(Tensor([216],"float32"), Tensor([0],"float32"), Tensor([1],"float32"), Tensor([2],"float32"), )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float64"), weight=Tensor([1, 3, 3, 3, 3],"float64"), bias=Tensor([0],"float64"), stride=2, padding=0, )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 2, 2, 2, 3],"float64"), weight=Tensor([3, 1, 3, 0, 3],"float64"), bias=Tensor([3],"float64"), stride=1, padding=list[1,0,1,], groups=3, data_format="NDHWC", dilation=1, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 2, 2, 6],"float32"), Tensor([6, 1, 3, 0],"float32"), bias=Tensor([3],"float32"), padding=list[1,0,], output_padding=1, stride=list[2,2,], dilation=list[1,1,], groups=3, output_size=None, data_format="NHWC", )
paddle.Tensor.tile(Tensor([19125, 0],"float32"), list[1,89,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[254,1,1,], )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float64"), weight=Tensor([3, 1, 3, 0],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,], dilation=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float32"), weight=Tensor([3, 0, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=0, dilation=1, )
paddle.nn.functional.pad(Tensor([1, 16, 14, 16, 0],"float16"), tuple(0,5,0,0,0,0,), data_format="NDHWC", )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 1024, 8, 16],"bfloat16"), Tensor([1, 1024, 2, 0],"bfloat16"), Tensor([1, 1024, 2, 16],"bfloat16"), attn_mask=None, is_causal=True, )
paddle.nn.functional.conv2d_transpose(x=Tensor([0, 3, 4, 4],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding="vaLiD", output_padding=0, dilation=1, groups=1, output_size=None, data_format="NCHW", )
paddle.nn.quant.weight_quantize(Tensor([128, 0],"float16"), algo="weight_only_int8", arch=75, group_size=-1, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 8, 0, 3, 3],"float32"), bias=Tensor([8],"float32"), padding="valid", output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.max_pool2d(Tensor([2, 4, 40, 0],"float64"), kernel_size=4, stride=2, padding=2, return_mask=True, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.avg_pool3d(x=Tensor([0, 256, 32, 7, 7],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 128, 32, 32],"float32"), Tensor([128, 0, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=128, output_size=None, data_format="NCHW", )
paddle.nn.functional.avg_pool3d(x=Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=2, padding=1, exclusive=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 0, 2, 40],"float16"), Tensor([1, 1, 2, 40],"float16"), Tensor([1, 1, 2, 40],"float16"), attn_mask=Tensor([1, 2, 1, 1],"float16"), )
paddle.kthvalue(Tensor([0, 128, 10],"float64"), 2, 2, )
paddle.nn.functional.max_pool3d(x=Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=list[1,0,0,], )
paddle.nn.functional.maxout(Tensor([100, 4, 3, 0],"float32"), 2, 1, None, )
paddle.nn.functional.group_norm(Tensor([0, 3, 4],"float64"), num_groups=2, weight=Tensor([4],"float64"), bias=Tensor([4],"float64"), data_format="NLC", )
paddle.Tensor.tile(Tensor([1, 9, 0],"float32"), list[723,1,1,], )
paddle.nn.functional.layer_norm(Tensor([0, 2, 2],"float32"), 2, epsilon=1e-05, weight=Tensor([2],"float32"), bias=Tensor([2],"float32"), )
paddle.Tensor.trunc(Tensor([0, 8, 8],"float32"), )
paddle.nn.functional.hinge_embedding_loss(Tensor([4, 3, 0, 4],"float64"), Tensor([4, 3, 0, 4],"float64"), reduction="sum", margin=-4.0, name=None, )
paddle.slice(Tensor([13, 0, 104],"float32"), list[0,1,2,], list[0,0,80,], list[13,1,96,], )
paddle.nn.functional.pad(Tensor([1, 0, 14, 17, 384],"float16"), tuple(0,4,0,0,0,0,), data_format="NDHWC", )
paddle.nn.functional.pad(Tensor([0, 16, 14, 18, 384],"float16"), tuple(0,3,0,0,0,0,), data_format="NDHWC", )
paddle.nn.functional.lp_pool2d(Tensor([2, 0, 32, 32],"float32"), 2, kernel_size=2, stride=1, ceil_mode=False, )
paddle.nn.functional.conv1d(Tensor([16, 64, 25500],"float32"), Tensor([128, 64, 0],"float32"), bias=Tensor([128],"float32"), padding=256, stride=list[1,], dilation=list[256,], groups=1, data_format="NCL", )
paddle.nn.functional.pad(Tensor([12, 32, 0, 32],"float32"), tuple(0,0,0,0,), data_format="NHWC", )
paddle.nanmedian(Tensor([2, 0, 4, 5],"float32"), axis=tuple(1,2,3,), keepdim=False, mode="min", )
paddle.nn.functional.softmax(Tensor([2, 0, 4, 5],"float32"), 0, name=None, )
paddle.nn.functional.avg_pool2d(Tensor([1, 1024, 40, 0],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[1038,1,1,], )
paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 1, 0, 7],"float32"), Tensor([13, 4, 1, 7, 8],"float32"), )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 0, 16, 7, 10],"float16"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 16, 16, 6],"float32"), Tensor([6, 8, 3, 0],"float32"), bias=Tensor([8],"float32"), padding=list[list[0,0,],list[1,1,],list[2,2,],list[0,0,],], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NHWC", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 3, 16, 16],"float32"), Tensor([3, 2, 0, 3],"float32"), bias=Tensor([6],"float32"), padding="valid", output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=3, output_size=None, data_format="NCHW", )
paddle.nn.functional.hinge_embedding_loss(Tensor([0, 10, 5],"float64"), Tensor([0, 10, 5],"float64"), reduction="none", margin=1.0, name=None, )
paddle.nn.functional.pad(Tensor([3, 0, 3, 3, 1600],"float32"), pad=list[0,0,0,0,2,2,], data_format="NCDHW", )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[351,1,1,], )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 0, 4, 3],"float64"), output_size=3, data_format="NHWC", )
paddle.nn.functional.pixel_shuffle(Tensor([4, 0, 128, 128],"float32"), 2, "NCHW", None, )
paddle.nn.functional.avg_pool2d(Tensor([64, 104, 28, 0],"float32"), kernel_size=3, stride=2, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.softmax(Tensor([2, 16, 0, 6],"float32"), axis=-1, dtype="float32", )
paddle.nn.functional.conv1d(Tensor([13, 32, 7],"float32"), Tensor([64, 8, 0],"float32"), bias=Tensor([64],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=4, data_format="NCL", )
paddle.nn.functional.conv2d(Tensor([1, 128, 0, 20],"float32"), Tensor([128, 128, 0, 3],"float32"), None, list[1,1,], 1, list[1,1,], 1, )
paddle.Tensor.tile(Tensor([10285, 0],"float32"), list[1,98,], )
paddle.nn.functional.poisson_nll_loss(Tensor([0, 2],"float32"), Tensor([0, 2],"float32"), log_input=True, full=False, epsilon=1e-08, reduction="mean", )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 160, 16, 0],"float32"), output_size=2, data_format="NCHW", name=None, )
paddle.nn.functional.max_pool3d(Tensor([1, 3, 4, 4, 0],"float32"), kernel_size=2, stride=2, return_mask=True, )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 0, 2, 2],"float64"), weight=Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([3],"float64"), stride=1, padding=list[1,0,1,], groups=3, dilation=1, )
paddle.nn.functional.local_response_norm(x=Tensor([0, 3, 3, 40, 40],"float32"), size=5, data_format="NCDHW", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[176,1,1,], )
paddle.einsum("...,...->...", Tensor([5, 0, 5],"float64"), Tensor([5, 0, 5],"float64"), )
paddle.nn.functional.conv2d(Tensor([8, 256, 0, 128],"float32"), Tensor([256, 256, 0, 3],"float32"), bias=None, stride=1, padding=1, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 2],"float32"), Tensor([3, 10, 27, 0],"float32"), bias=Tensor([10],"float32"), padding=0, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 2, 2, 2, 3],"float32"), Tensor([3, 1, 0, 3, 3],"float32"), bias=Tensor([3],"float32"), padding=list[1,1,1,], output_padding=1, stride=list[2,2,2,], dilation=list[1,1,1,], groups=3, output_size=None, data_format="NDHWC", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 4, 16, 16],"float32"), Tensor([4, 0, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,2,], stride=1, dilation=1, groups=2, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([0, 3, 32, 224, 231],"float32"), tuple(0,1,0,0,0,0,), data_format="NCDHW", )
paddle.Tensor.tile(Tensor([1, 0, 2, 1, 8],"float32"), list[1,1,1,2,1,], )
paddle.nn.functional.max_pool1d(Tensor([2, 3, 0],"float64"), 2, 1, list[1,1,], False, False, None, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[952,1,1,], )
paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 0, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 1, 50, 50],"float16"), scale=0.125, )
paddle.nn.functional.pixel_unshuffle(Tensor([2, 0, 12, 1],"float64"), 3, "NHWC", )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 2],"float64"), Tensor([3, 1, 3, 3],"float64"), bias=Tensor([0],"float64"), padding=1, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.imag(Tensor([2, 20, 2, 0],"complex64"), )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1914, 2048, 7, 0],"float16"), output_size=1, )
paddle.nn.functional.lp_pool1d(Tensor([2, 0, 32],"float32"), 4.0, 3, 2, 1, False, "NCL", None, )
paddle.nn.functional.avg_pool2d(Tensor([64, 104, 0, 28],"float16"), kernel_size=3, stride=2, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 0, 4, 4],"float32"), weight=Tensor([1, 3, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=2, padding=1, )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 0, 32],"float64"), output_size=list[2,2,2,], data_format="NCDHW", name=None, )
paddle.nn.functional.conv2d(Tensor([2, 24, 8, 0],"float32"), Tensor([24, 24, 3, 3],"float32"), None, list[1,1,], 1, list[1,1,], 1, "NCHW", )
paddle.slice(Tensor([11, 0, 4],"float32"), axes=list[2,], starts=list[2,], ends=list[3,], )
paddle.nn.functional.fractional_max_pool3d(Tensor([0, 3, 7, 7, 7],"float32"), output_size=list[2,3,5,], kernel_size=None, random_u=0.7, return_mask=False, name=None, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float64"), Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([0],"float64"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[2,2,2,], groups=1, output_size=None, data_format="NCDHW", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[2654,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([1, 2048, 128, 128],"float32"), Tensor([2048, 128, 0, 3],"float32"), padding=0, stride=2, groups=8, )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 0, 7, 7],"float32"), output_size=list[2,3,5,], data_format="NCDHW", name=None, )
paddle.nn.functional.max_pool2d(Tensor([0, 192, 20, 20],"float32"), kernel_size=5, stride=1, padding=2, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.conv2d(Tensor([1, 2048, 0, 16],"float32"), Tensor([2048, 512, 3, 3],"float32"), padding=1, groups=4, )
paddle.nn.functional.max_pool2d(Tensor([1, 0, 20, 27],"float32"), 1, stride=2, )
paddle.nn.functional.conv2d_transpose(x=Tensor([0, 3, 8, 8],"float64"), weight=Tensor([3, 1, 5, 5],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[list[0,0,],list[0,0,],list[1,2,],list[3,4,],], output_padding=0, dilation=1, groups=1, output_size=None, data_format="NCHW", )
paddle.einsum("ijbs,ibns->bnij", Tensor([7, 7, 0, 2],"float32"), Tensor([7, 14, 0, 2],"float32"), )
paddle.nn.functional.conv2d(Tensor([1, 1, 32, 0],"float32"), Tensor([6, 1, 3, 3],"float32"), bias=Tensor([6],"float32"), padding=1, stride=list[1,1,], dilation=list[1,1,], groups=1, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 5, 0],"float32"), list[350,1,1,], )
paddle.nn.functional.maxout(x=Tensor([0, 4, 3, 3],"float32"), groups=2, )
paddle.nn.functional.conv1d(Tensor([1, 128, 0],"float32"), Tensor([128, 128, 11],"float32"), bias=Tensor([128],"float32"), padding=15, stride=list[1,], dilation=list[3,], groups=1, data_format="NCL", )
paddle.nn.functional.max_pool2d(Tensor([1, 64, 0, 112],"float32"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([1, 2, 3, 4, 0],"complex64"), pad=list[1,2,1,1,3,4,], mode="constant", value=100, data_format="NCDHW", )
paddle.Tensor.tile(Tensor([1001, 0, 80],"float32"), list[1,5,1,], )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 0, 8],"float64"), weight=Tensor([3, 1, 5, 5],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,2,3,4,], output_padding=0, dilation=1, groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv3d_transpose(Tensor([4, 8, 8, 8, 4],"float32"), Tensor([4, 3, 3, 3, 0],"float32"), Tensor([6],"float32"), output_size=None, padding=list[list[0,0,],list[1,2,],list[1,2,],list[2,1,],list[0,0,],], stride=1, dilation=1, groups=2, data_format="NDHWC", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float64"), Tensor([3, 1, 3, 0, 3],"float64"), bias=Tensor([1],"float64"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.conv1d_transpose(Tensor([2, 3, 2],"float32"), Tensor([3, 1, 3],"float32"), bias=Tensor([0],"float32"), output_size=None, output_padding=0, padding=list[1,], stride=list[1,], dilation=list[2,], groups=1, data_format="NCL", )
paddle.nn.functional.avg_pool2d(Tensor([3, 0, 40, 44],"float32"), kernel_size=tuple(1,5,), stride=1, )
paddle.nn.functional.conv1d_transpose(x=Tensor([0, 3, 2],"float32"), weight=Tensor([3, 1, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=list[1,], groups=1, dilation=2, )
paddle.einsum("ij,kj->ik", Tensor([0, 5],"float64"), Tensor([2, 5],"float64"), )
paddle.nn.functional.lp_pool2d(Tensor([2, 0, 32, 32],"float32"), norm_type=2.0, kernel_size=2, stride=1, padding=0, ceil_mode=False, data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[660,1,1,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[462,1,1,], )
paddle.einsum("blqd,bmdk->blqk", Tensor([52, 5, 1, 0],"float32"), Tensor([52, 1, 1, 0],"float32"), )
paddle.Tensor.tile(Tensor([10285, 0],"float32"), list[1,223,], )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 0, 32],"float64"), output_size=tuple(3,3,3,), data_format="NCDHW", name=None, )
paddle.nanmedian(Tensor([2, 3, 4, 0],"float32"), axis=2, keepdim=False, )
paddle.nn.functional.pad(Tensor([1, 0, 2, 2, 2],"float64"), pad=list[2,2,2,2,2,2,], mode="replicate", value=0.0, data_format="NCDHW", name=None, )
paddle.slice(Tensor([52, 9, 0],"float32"), axes=list[1,], starts=list[2,], ends=list[9,], )
paddle.Tensor.flip(Tensor([4, 0, 64, 64],"float32"), 1, )
paddle.nn.functional.avg_pool3d(Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCDHW", name=None, )
paddle.einsum("a...b,b...c,c...a", Tensor([4, 0, 1, 4],"float64"), Tensor([4, 0, 1, 4],"float64"), Tensor([4, 0, 1, 4],"float64"), )
paddle.nn.functional.softmax(Tensor([2, 3, 2, 0, 5],"float16"), )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[197,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([4, 256, 47, 35],"float32"), Tensor([256, 0, 4, 4],"float32"), bias=None, padding=0, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([0, 16, 14, 16, 384],"float16"), tuple(0,5,0,0,0,0,), data_format="NDHWC", )
paddle.nonzero(Tensor([0, 2, 2, 2],"float64"), True, )
paddle.Tensor.tile(Tensor([1, 6, 0],"float32"), list[1470,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([4, 16, 16, 3],"float32"), Tensor([3, 5, 3, 0],"float32"), Tensor([5],"float32"), output_size=None, padding=0, stride=1, dilation=1, groups=1, data_format="NHWC", )
paddle.Tensor.tile(Tensor([1, 0, 2, 1, 16],"float32"), list[1,1,1,4,1,], )
paddle.nn.functional.maxout(Tensor([0, 2, 2, 6],"float64"), 2, 3, None, )
paddle.nanmedian(Tensor([2, 0, 4, 5],"float32"), axis=-2, keepdim=False, mode="min", )
paddle.nn.functional.max_pool2d(Tensor([0, 64, 18, 18],"float32"), kernel_size=tuple(3,3,), stride=tuple(2,2,), padding=tuple(0,0,), return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([3, 0, 5, 6],"complex64"), pad=list[1,2,2,1,], mode="reflect", value=0.0, data_format="NCHW", name=None, )
paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([0],"float32"), shift=None, smooth=None, quant_scale=0.0009654839523136616, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.nn.functional.l1_loss(Tensor([0, 4],"float32"), Tensor([0, 4],"float32"), reduction="sum", )
paddle.nn.functional.pad(Tensor([1, 16, 0, 16, 384],"float32"), tuple(0,5,0,0,0,0,), data_format="NDHWC", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[159,1,1,], )
paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 0, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=2, padding=1, exclusive=True, )
paddle.nonzero(Tensor([0],"bool"), as_tuple=False, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 0, 2],"float32"), Tensor([3, 12, 12, 12, 12],"float32"), bias=Tensor([12],"float32"), padding=0, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.Tensor.fill_diagonal_(Tensor([3, 0],"float32"), 1, offset=0, wrap=True, )
paddle.nn.functional.pad(Tensor([1, 16, 0, 14, 384],"float32"), tuple(0,0,0,3,0,0,), data_format="NDHWC", )
paddle.nn.functional.conv2d(Tensor([1, 128, 0, 20],"float32"), Tensor([128, 128, 3, 3],"float32"), None, list[1,1,], 1, list[1,1,], 1, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[558,1,1,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[124,1,1,], )
paddle.nn.quant.weight_quantize(Tensor([0, 288],"float16"), algo="weight_only_int8", arch=70, group_size=-1, )
paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 0, 768],"float32"), Tensor([768, 3072],"float32"), Tensor([3072, 768],"float32"), Tensor([3072],"float32"), Tensor([768],"float32"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )
paddle.einsum("ij,k->ijk", Tensor([0, 4096],"float32"), Tensor([32],"float32"), )
paddle.nn.functional.pad(Tensor([3, 4, 5, 0],"complex128"), pad=list[1,2,2,1,], mode="constant", value=100, data_format="NCHW", name=None, )
paddle.nn.functional.max_pool3d(Tensor([0, 3, 32, 32, 32],"float32"), kernel_size=2, stride=None, padding="SAME", return_mask=True, )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 5, 0, 7],"float32"), output_size=list[None,3,None,], )
paddle.nn.functional.conv2d_transpose(Tensor([4, 64, 32, 32],"float32"), Tensor([64, 1, 0, 8],"float32"), bias=None, padding=2, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=64, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv1d(Tensor([1, 256, 0],"float32"), Tensor([256, 256, 11],"float32"), bias=Tensor([256],"float32"), padding=15, stride=list[1,], dilation=list[3,], groups=1, data_format="NCL", )
paddle.nn.functional.conv1d_transpose(x=Tensor([0, 3, 2],"float64"), weight=Tensor([3, 1, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=1, dilation=1, )
paddle.nn.functional.l1_loss(Tensor([0, 3, 256, 256],"float32"), Tensor([0, 3, 256, 256],"float32"), "mean", name=None, )
paddle.nn.functional.avg_pool3d(Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCDHW", name=None, )
paddle.slice(Tensor([0, 2, 104],"float32"), list[0,1,2,], list[0,0,80,], list[16,2,96,], )
paddle.vision.ops.generate_proposals(Tensor([1, 15, 42, 63],"float32"), Tensor([1, 60, 42, 63],"float32"), Tensor([1, 2],"float32"), Tensor([0, 4],"float32"), Tensor([39690, 4],"float32"), pre_nms_top_n=12000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.einsum("ij,...i->j...", Tensor([4, 0],"float64"), Tensor([3, 2, 4],"float64"), )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[648,1,1,], )
paddle.nn.functional.lp_pool2d(Tensor([2, 0, 32, 32],"float32"), norm_type=2.0, kernel_size=5, stride=3, padding=0, ceil_mode=True, data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([12240, 0],"float32"), list[1,17,], )
paddle.allclose(Tensor([0, 13, 128],"float32"), Tensor([0, 13, 128],"float32"), rtol=0.0001, atol=0.0001, )
paddle.nn.functional.conv3d_transpose(x=Tensor([0, 3, 2, 2, 2],"float64"), weight=Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([3],"float64"), stride=1, padding=list[1,0,1,], groups=3, dilation=1, )
paddle.nn.functional.pad(Tensor([12288, 1, 0, 1],"float32"), list[0,1,0,1,], )
paddle.nn.functional.conv2d(Tensor([1024, 1, 0, 258],"float32"), Tensor([1, 1, 0, 4],"float32"), )
paddle.nn.functional.max_pool2d(Tensor([0, 3, 32, 32],"float64"), kernel_size=list[2,2,], stride=None, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.max_pool3d(Tensor([1, 0, 4, 4, 6],"float64"), kernel_size=2, stride=2, padding=0, return_mask=True, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.nn.functional.square_error_cost(Tensor([8, 100, 0],"float32"), Tensor([8, 100, 0],"float32"), )
paddle.Tensor.tile(Tensor([4, 0, 512],"float32"), tuple(1,4,1,), )
paddle.Tensor.__sub__(Tensor([12, 3, 10, 0, 1],"float32"), Tensor([12, 3, 10, 0, 1],"float32"), )
paddle.nn.functional.log_softmax(Tensor([5, 3, 2, 0],"float32"), 1, )
paddle.slice(Tensor([0, 1, 104],"float32"), list[0,1,2,], list[0,0,72,], list[16,1,88,], )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 0, 4],"float64"), weight=Tensor([6, 1, 3, 3, 3],"float64"), bias=Tensor([6],"float64"), stride=2, padding=0, groups=3, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 8, 8, 8, 6],"float32"), Tensor([6, 8, 0, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=list[list[0,0,],list[1,1,],list[2,2,],list[3,3,],list[0,0,],], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NDHWC", )
paddle.nn.functional.pad(Tensor([3, 4, 0, 6],"complex128"), pad=list[1,2,2,1,], mode="constant", value=100, data_format="NCHW", name=None, )
paddle.einsum("td,dnh->tnh", Tensor([15, 32],"float32"), Tensor([32, 4, 0],"float32"), )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([0, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
paddle.vision.ops.generate_proposals(Tensor([1, 3, 10, 15],"float32"), Tensor([1, 12, 10, 15],"float32"), Tensor([1, 2],"float32"), Tensor([0, 4],"float32"), Tensor([450, 4],"float32"), pre_nms_top_n=2000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 1, 0],"float16"), Tensor([1, 2048, 1, 0],"float16"), Tensor([1, 2048, 1, 0],"float16"), attn_mask=Tensor([1, 1, 2048, 0],"float16"), is_causal=False, )
paddle.nanmedian(Tensor([2, 0, 4, 5],"float32"), axis=tuple(1,2,), keepdim=False, )
paddle.nn.functional.pad(Tensor([1, 3, 0, 150],"float64"), pad=list[10,10,10,10,], mode="constant", value=0.0, data_format="NCHW", )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([1, 3, 3, 3, 3],"float32"), bias=Tensor([0],"float32"), stride=list[2,2,1,], padding=1, dilation=tuple(2,2,2,), )
paddle.nn.functional.conv3d_transpose(Tensor([0, 3, 2, 2, 2],"float32"), Tensor([3, 12, 12, 12, 12],"float32"), bias=Tensor([12],"float32"), padding=0, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[164,1,1,], )
paddle.einsum("ijk->kji", Tensor([3, 10, 0],"float64"), )
paddle.nn.functional.max_pool2d(Tensor([2, 4, 0, 40],"float64"), kernel_size=4, stride=2, padding=2, return_mask=True, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.square_error_cost(input=Tensor([0, 1],"float32"), label=Tensor([0, 1],"float32"), )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 4, 0],"float16"), list[1,40,], )
paddle.meshgrid(Tensor([2],"float32"), Tensor([4],"float32"), Tensor([0],"float32"), )
paddle.Tensor.__setitem__(Tensor([6, 5, 4, 0],"complex128"), list[list[2,3,4,],list[1,2,5,],], 100, )
paddle.nn.functional.pad(Tensor([1, 0, 3],"float32"), list[1,0,], value=1, mode="constant", data_format="NCL", )
paddle.nn.functional.pad(Tensor([1, 0, 14, 17, 384],"float32"), tuple(0,4,0,0,0,0,), data_format="NDHWC", )
paddle.slice(Tensor([2, 0, 104],"float32"), list[0,1,2,], list[0,0,16,], list[2,2,32,], )
paddle.searchsorted(Tensor([5],"float64"), Tensor([2, 0],"float64"), out_int32=True, )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 2, 3],"float64"), weight=Tensor([3, 0, 3],"float64"), bias=Tensor([1],"float64"), stride=2, padding=list[1,], groups=3, dilation=1, data_format="NLC", output_padding=1, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([4, 0, 4, 128],"float16"), Tensor([4, 0, 4, 128],"float16"), Tensor([4, 0, 4, 128],"float16"), attn_mask=Tensor([4, 0, 134, 134],"float16"), dropout_p=0.0, training=True, is_causal=False, )
paddle.einsum("ij,jk,kl", Tensor([0, 4],"float64"), Tensor([4, 5],"float64"), Tensor([5, 6],"float64"), )
paddle.Tensor.tile(Tensor([1, 0, 768],"float32"), list[64,1,1,], )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 8, 8, 3],"float64"), weight=Tensor([3, 1, 0, 5],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[list[0,0,],list[1,2,],list[3,4,],list[0,0,],], output_padding=0, dilation=1, groups=1, output_size=None, data_format="NHWC", )
paddle.vision.ops.prior_box(input=Tensor([2, 10, 0, 32],"float32"), image=Tensor([2, 10, 40, 40],"float32"), min_sizes=list[2.0,4.0,], clip=True, flip=True, )
paddle.nn.functional.pixel_shuffle(Tensor([2, 4, 4, 0],"float64"), 3, "NHWC", )
paddle.nn.functional.maxout(Tensor([2, 0, 5, 4],"float64"), 2, 1, )
paddle.nn.functional.max_pool2d(Tensor([1, 0, 20, 20],"float32"), kernel_size=13, stride=1, padding=6, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 16],"float32"), Tensor([6, 8, 1, 0],"float32"), bias=Tensor([8],"float32"), padding=tuple(2,3,), output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 0, 4, 3],"float64"), output_size=list[3,3,], data_format="NHWC", )
paddle.nn.functional.max_pool3d(Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=1, return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.Tensor.tile(Tensor([24565, 0],"float32"), list[1,811,], )
paddle.nn.functional.conv1d(x=Tensor([2, 3, 4],"float64"), weight=Tensor([1, 3, 0],"float64"), bias=Tensor([1],"float64"), stride=2, padding=1, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[311,1,1,], )
paddle.nn.functional.pad(Tensor([1, 3, 0, 308],"float64"), pad=list[0,0,40,40,], mode="constant", value=0.0, data_format="NCHW", )
paddle.nanmedian(Tensor([2, 3, 0, 5],"float32"), axis=-1, keepdim=False, )
paddle.nn.functional.sequence_mask(Tensor([2, 2, 3, 0, 3],"float64"), maxlen=5, dtype=type(numpy.int32), )
paddle.Tensor.__getitem__(Tensor([0, 5, 4, 3],"complex128"), list[list[2,-3,-4,],list[-1,2,5,],], )
paddle.frac(Tensor([2, 0],"int64"), )
paddle.nn.functional.avg_pool2d(Tensor([0, 512, 3, 80],"float16"), kernel_size=list[3,2,], stride=list[3,2,], padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.Tensor.isnan(Tensor([0, 3, 3],"float32"), )
paddle.nn.functional.conv2d(Tensor([1, 2048, 128, 0],"float32"), Tensor([24, 256, 1, 0],"float32"), padding=0, groups=8, )
paddle.nn.functional.conv1d(Tensor([52, 7, 32],"float32"), Tensor([32, 1, 0],"float32"), bias=None, padding=1, stride=list[1,], dilation=list[1,], groups=32, data_format="NLC", )
paddle.nn.functional.pad(Tensor([0, 1, 3, 3, 1600],"float32"), pad=list[0,0,0,0,2,2,], data_format="NCDHW", )
paddle.nn.functional.avg_pool2d(Tensor([0, 3, 32, 32],"float32"), kernel_size=2, stride=2, padding=list[list[0,0,],list[0,0,],list[0,0,],list[0,0,],], divisor_override=4, )
paddle.slice(Tensor([1024, 0, 1024],"float16"), axes=list[0,], starts=list[768,], ends=list[1024,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[143,1,1,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[238,1,1,], )
paddle.Tensor.tile(Tensor([1, 6, 0],"float32"), list[378,1,1,], )
paddle.nn.functional.conv3d_transpose(x=Tensor([0, 3, 2, 2, 2],"float32"), weight=Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=list[1,0,1,], dilation=2, )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[128,1,1,], )
paddle.nn.functional.pad(Tensor([0, 3, 256, 256],"float32"), list[6,6,6,6,], )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 0],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )
paddle.nn.functional.pixel_unshuffle(Tensor([2, 0, 12, 12],"float16"), 3, "NCHW", )
paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 0, 1, 7],"float32"), Tensor([13, 4, 0, 7, 8],"float32"), )
paddle.Tensor.tile(Tensor([0, 4],"float32"), list[64,1,1,], )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 100, 1, 0],"float16"), Tensor([2, 100, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 100],"float16"), is_causal=False, )
paddle.nn.functional.lp_pool2d(Tensor([0, 3, 32, 32],"float32"), norm_type=math.inf, kernel_size=list[2,4,], stride=2, padding=0, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 8, 16],"float16"), Tensor([1, 2048, 2, 16],"float16"), Tensor([1, 2048, 2, 0],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), is_causal=True, )
paddle.nn.functional.lp_pool1d(Tensor([0, 3, 32],"float64"), norm_type=5, kernel_size=5, stride=3, padding=list[0,], )
paddle.Tensor.tile(Tensor([12240, 0],"float32"), list[1,27,], )
paddle.nn.functional.pad(Tensor([0, 3, 200, 150],"float64"), pad=list[1,1,2,2,], mode="constant", value=0.0, data_format="NCHW", )
paddle.einsum("...->...", Tensor([5, 0, 5],"float64"), )
paddle.nn.functional.mse_loss(Tensor([16, 0, 2],"float32"), Tensor([16, 0, 2],"float32"), )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([0, 100, 1, 64],"float16"), Tensor([2, 100, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 100],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[394,1,1,], )
paddle.Tensor.tile(Tensor([16, 1, 0],"float32"), repeat_times=list[1,12,1,], )
paddle.nn.functional.avg_pool2d(Tensor([2, 0, 4, 4],"float64"), kernel_size=list[3,3,], stride=tuple(1,1,), padding=tuple(0,0,), ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[95,1,1,], )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 0, 32, 32],"float32"), output_size=list[3,3,3,], )
paddle.nn.functional.local_response_norm(Tensor([0, 40, 40, 3],"float32"), 5, 0.0001, 0.75, 1.0, "NHWC", None, )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[57,1,1,], )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[200,1,1,], )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 0, 128, 128],"float32"), output_size=tuple(6,6,), data_format="NCHW", name=None, )
paddle.nn.functional.avg_pool3d(Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=2, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
paddle.Tensor.imag(Tensor([0, 20, 2, 3],"complex128"), )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float64"), weight=Tensor([1, 3, 3, 0],"float64"), bias=Tensor([1],"float64"), stride=list[2,1,], padding=list[1,2,], dilation=tuple(2,2,), )
paddle.nn.functional.avg_pool2d(Tensor([13, 0, 4, 32],"float32"), tuple(2,1,), stride=tuple(2,1,), ceil_mode=True, )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 0, 4, 3],"float64"), output_size=tuple(3,3,), data_format="NHWC", )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[155,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([2, 4, 0, 3],"float64"), Tensor([4, 2, 1, 1],"float64"), groups=1, )
paddle.allclose(Tensor([0, 3, 8, 8],"float32"), Tensor([0, 3, 8, 8],"float32"), )
paddle.slice(Tensor([0, 1, 104],"float32"), list[0,1,2,], list[0,0,40,], list[13,1,56,], )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 2, 3],"float32"), weight=Tensor([3, 0, 3],"float32"), bias=Tensor([1],"float32"), stride=2, padding=list[1,], groups=3, dilation=1, data_format="NLC", output_padding=1, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[57,1,1,], )
paddle.nn.functional.max_pool3d(Tensor([2, 0, 32, 32, 32],"float32"), kernel_size=2, stride=None, padding="SAME", return_mask=True, )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 2],"float32"), weight=Tensor([3, 0, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=1, dilation=1, )
paddle.nn.functional.avg_pool1d(Tensor([0, 3, 8],"float64"), 2, 1, 1, False, False, None, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 0],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,], dilation=1, )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float32"), weight=Tensor([1, 3, 3, 3],"float32"), bias=Tensor([0],"float32"), stride=2, padding=0, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 1, 64],"float16"), Tensor([1, 2048, 1, 64],"float16"), Tensor([1, 2048, 0, 64],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), is_causal=True, )
paddle.nn.quant.weight_quantize(Tensor([0, 288],"float16"), algo="weight_only_int8", arch=75, group_size=-1, )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 0, 32],"float64"), output_size=list[2,2,2,], )
paddle.nn.functional.pad(Tensor([1, 27860, 0],"float32"), list[1,0,], value=6, mode="constant", data_format="NCL", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[142,1,1,], )
paddle.roll(Tensor([0, 5, 4, 4],"float64"), Tensor([4],"int64"), list[0,1,2,3,], name=None, )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float32"), weight=Tensor([1, 3, 0, 3],"float32"), bias=Tensor([1],"float32"), stride=list[2,1,], padding=1, dilation=list[2,2,], )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 0],"float64"), weight=Tensor([6, 1, 3, 3, 3],"float64"), bias=Tensor([6],"float64"), stride=2, padding=0, groups=3, )
paddle.nn.functional.pad(Tensor([1, 3, 32, 0, 224],"float32"), tuple(0,0,0,3,0,0,), data_format="NCDHW", )
paddle.nn.functional.avg_pool3d(Tensor([0, 3, 8, 8, 8],"float32"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCDHW", name=None, )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 0, 32, 32],"float64"), output_size=3, )
paddle.Tensor.imag(Tensor([1000, 0],"complex64"), )
paddle.Tensor.tile(Tensor([1, 256, 256, 0],"float32"), list[4,1,1,1,], )
paddle.nn.functional.temporal_shift(Tensor([32, 0, 28, 28],"float32"), 16, 0.0625, )
paddle.nn.quant.weight_only_linear(Tensor([1, 32, 64],"float16"), Tensor([256, 64],"int8"), bias=Tensor([256],"float16"), weight_scale=Tensor([0],"float16"), weight_dtype="int8", group_size=-1, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 3, 16, 0],"float32"), Tensor([3, 5, 3, 3],"float32"), None, output_size=None, padding=0, stride=1, dilation=1, groups=1, data_format="NCHW", )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([0, 3, 5, 7, 7],"float32"), output_size=list[3,3,3,], data_format="NCDHW", name=None, )
paddle.nn.functional.pad(Tensor([1, 0, 686, 1024],"float32"), tuple(0,0,0,338,), )
paddle.nn.functional.conv1d_transpose(x=Tensor([0, 3, 2],"float64"), weight=Tensor([3, 1, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=1, output_padding=0, groups=1, dilation=1, output_size=None, data_format="NCL", name=None, )
paddle.slice(Tensor([16, 0, 104],"float32"), list[0,1,2,], list[0,0,48,], list[16,1,64,], )
paddle.nn.functional.conv1d_transpose(Tensor([0, 3, 2],"float32"), Tensor([3, 1, 3],"float32"), bias=Tensor([1],"float32"), output_size=None, output_padding=0, padding=list[1,], stride=list[2,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.Tensor.tile(Tensor([0, 4],"float32"), list[4,1,1,], )
paddle.allclose(Tensor([0, 8],"float32"), Tensor([0, 8],"float32"), atol=1e-06, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[378,1,1,], )
paddle.einsum("i,j->ii", Tensor([2],"float64"), Tensor([0],"float64"), )
paddle.slice(Tensor([3, 0, 104],"float32"), list[0,1,2,], list[0,0,56,], list[3,1,72,], )
paddle.einsum("bhwc,hkc->bhwk", Tensor([432, 14, 14, 64],"float32"), Tensor([14, 0, 64],"float32"), )
paddle.index_fill(Tensor([20, 0],"float32"), Tensor([2],"int64"), 1, -1, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 0, 4, 4],"float64"), weight=Tensor([1, 3, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=list[2,2,1,], padding=tuple(1,2,2,), dilation=tuple(2,2,2,), )
paddle.Tensor.tile(Tensor([4, 1, 0],"float32"), tuple(1,14,1,), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[536,1,1,], )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 0, 4],"float32"), weight=Tensor([1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=list[2,1,], padding=1, dilation=list[2,2,], )
paddle.nn.functional.max_pool1d(x=Tensor([2, 3, 0],"float64"), kernel_size=2, stride=1, padding=list[1,], )
paddle.Tensor.fill_diagonal_(Tensor([3, 0],"float64"), 1, 0, False, )
paddle.vision.ops.generate_proposals(Tensor([1, 15, 42, 63],"float32"), Tensor([0, 60, 42, 63],"float32"), Tensor([1, 2],"float32"), Tensor([39690, 4],"float32"), Tensor([39690, 4],"float32"), pre_nms_top_n=12000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.gelu(Tensor([16, 16, 64, 0],"float16"), approximate=True, )
paddle.nn.functional.square_error_cost(input=Tensor([2, 0],"float32"), label=Tensor([2, 0],"float32"), )
paddle.nn.functional.conv1d(Tensor([1, 1024, 3000],"float32"), Tensor([1024, 1024, 0],"float32"), bias=Tensor([1024],"float32"), padding=1, stride=list[2,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.einsum("ak, kn-> an", Tensor([15000, 0],"float32"), Tensor([11, 0],"float32"), )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 2, 3],"float64"), weight=Tensor([3, 0, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,], groups=3, dilation=1, data_format="NLC", )
paddle.Tensor.__rpow__(Tensor([0, 2, 2],"float32"), 3, )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 0, 4, 4],"float64"), output_size=list[2,3,], )
paddle.nn.functional.max_pool2d(Tensor([2, 4, 0, 40],"float64"), kernel_size=4, stride=None, padding=2, return_mask=True, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([0, 4, 5, 6, 7],"complex128"), pad=list[1,2,2,1,1,0,], mode="constant", value=100, data_format="NCDHW", name=None, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 0, 2, 2],"float32"), Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.pad(Tensor([2, 0, 4, 4],"float64"), list[1,1,1,1,], mode="circular", data_format="NCHW", )
paddle.nn.functional.avg_pool2d(Tensor([0, 1024, 40, 54],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.softmax(Tensor([0, 3, 4],"float32"), axis=1, dtype="float32", )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 3, 7, 0],"float32"), output_size=list[2,5,], )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 2, 2, 2, 3],"float64"), weight=Tensor([3, 0, 3, 3, 3],"float64"), bias=Tensor([3],"float64"), output_size=list[4,4,4,], stride=2, padding=list[1,1,1,], groups=3, data_format="NDHWC", dilation=1, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[492,1,1,], )
paddle.nn.functional.maxout(x=Tensor([9, 0, 2, 6],"float64"), groups=2, axis=3, )
paddle.nn.functional.avg_pool3d(Tensor([3, 0, 7, 3, 1600],"float32"), kernel_size=tuple(5,1,1,), stride=1, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([0, 160, 16, 32],"float16"), output_size=4, data_format="NCHW", name=None, )
paddle.nn.functional.max_pool1d(x=Tensor([0, 3, 8],"float64"), kernel_size=2, stride=list[1,], padding=1, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[206,1,1,], )
paddle.nn.functional.pad(Tensor([0, 2, 3],"float32"), pad=list[1,2,], mode="replicate", value=0.0, data_format="NCL", name=None, )
paddle.kthvalue(Tensor([0, 30, 250],"float64"), 244, 2, )
paddle.nn.functional.conv2d(Tensor([1, 128, 256, 0],"float32"), Tensor([128, 128, 3, 0],"float32"), padding=1, groups=1, )
paddle.Tensor.fill_diagonal_(Tensor([0, 3],"float64"), 1, offset=0, wrap=False, )
paddle.nn.functional.conv2d_transpose(Tensor([1, 128, 128, 0],"float32"), Tensor([128, 3, 4, 4],"float32"), bias=Tensor([3],"float32"), padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 0],"float32"), weight=Tensor([3, 1, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=list[1,], groups=1, dilation=2, )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 2, 2, 2, 3],"float32"), weight=Tensor([3, 0, 3, 3, 3],"float32"), bias=Tensor([3],"float32"), output_size=list[4,4,4,], stride=2, padding=list[1,1,1,], groups=3, data_format="NDHWC", dilation=1, )
paddle.nn.functional.conv1d(Tensor([2, 3, 4],"float32"), Tensor([6, 1, 0],"float32"), bias=Tensor([6],"float32"), padding=0, stride=list[2,], dilation=list[1,], groups=3, data_format="NCL", )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 3, 4, 0],"float64"), output_size=list[1,1,], data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([13, 0, 96],"float32"), pad=tuple(0,8,), mode="replicate", value=0.0, data_format="NCL", name=None, )
paddle.nonzero(x=Tensor([3, 2, 2, 0],"float64"), as_tuple=False, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([0, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )
paddle.nn.quant.weight_only_linear(Tensor([2, 1, 512],"float16"), weight=Tensor([1024, 512],"int8"), bias=Tensor([1024],"float16"), weight_scale=Tensor([0],"float16"), weight_dtype="int8", )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 0],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,], dilation=2, output_size=None, data_format="NCHW", )
paddle.einsum("nbqa,ahc->nbqhc", Tensor([0, 3, 5, 6],"float32"), Tensor([6, 2, 4],"float32"), )
paddle.nn.functional.max_pool2d(Tensor([2, 0, 32, 32],"float64"), kernel_size=list[3,3,], stride=list[1,1,], padding=list[1,1,], return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 0, 32, 32],"float64"), output_size=list[2,3,3,], data_format="NCDHW", name=None, )
paddle.slice(Tensor([2, 0, 5],"float32"), list[0,], list[1,], list[2,], )
paddle.slice(Tensor([2, 8, 0],"float32"), axes=list[1,], starts=list[6,], ends=list[7,], )
paddle.nn.functional.conv2d(Tensor([2, 192, 0, 4],"float32"), Tensor([48, 192, 0, 1],"float32"), None, list[1,1,], 0, list[1,1,], 1, "NCHW", )
paddle.einsum("a...b,b...c,c...d", Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 0],"float64"), )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[993,1,1,], )
paddle.Tensor.tile(Tensor([1, 33, 0],"float32"), list[16,1,1,], )
paddle.Tensor.fill_diagonal_(Tensor([3, 0, 3],"float32"), 1, offset=0, wrap=True, )
paddle.einsum("blkd,bldq->blkq", Tensor([13, 0, 1, 1],"float32"), Tensor([13, 0, 1, 3],"float32"), )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[92,1,1,], )
paddle.nanmedian(Tensor([2, 3, 0, 5],"float32"), axis=-2, keepdim=False, )
paddle.take(Tensor([0, 4],"float32"), Tensor([2, 3],"int64"), mode="raise", )
paddle.einsum("nbts,nbsc->nbtc", Tensor([0, 52, 7, 14],"float32"), Tensor([0, 52, 14, 8],"float32"), )
paddle.nn.functional.pad(Tensor([3, 1, 40, 0],"float32"), pad=list[2,2,0,0,], )
paddle.einsum("ijk, jil -> kl", Tensor([3, 4, 0],"float64"), Tensor([4, 3, 2],"float64"), )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([0],"float64"), stride=1, padding=list[1,0,], dilation=2, output_size=None, data_format="NCHW", )
paddle.nn.functional.margin_ranking_loss(Tensor([0, 10],"float64"), Tensor([0, 10],"float64"), Tensor([0, 10],"float64"), 0.0, "none", )
paddle.nn.functional.pad(Tensor([1, 2, 2, 2, 0],"float64"), pad=list[1,1,1,0,1,0,], mode="reflect", value=0.0, data_format="NDHWC", name=None, )
paddle.nn.functional.softmax(Tensor([112, 0, 126],"float32"), axis=-1, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 1024, 32, 128],"bfloat16"), Tensor([1, 0, 32, 128],"bfloat16"), Tensor([1, 1024, 32, 128],"bfloat16"), attn_mask=None, is_causal=True, )
paddle.nn.functional.avg_pool2d(x=Tensor([0, 3, 32, 32],"float32"), kernel_size=list[2,2,], )
paddle.kthvalue(x=Tensor([3, 2, 0],"float32"), k=3, axis=0, )
paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 0],"float16"), bias=Tensor([512],"float16"), act_method="swiglu", compute_dtype="default", )
paddle.nn.functional.conv2d_transpose(x=Tensor([0, 3, 8, 8],"float64"), weight=Tensor([3, 1, 5, 5],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,2,3,4,], output_padding=0, dilation=1, groups=1, output_size=None, data_format="NCHW", )
paddle.einsum("sec,sm->ecm", Tensor([10, 60, 10],"float32"), Tensor([10, 0],"float32"), )
paddle.nn.functional.avg_pool3d(x=Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,2,1,], exclusive=True, )
paddle.nn.functional.lp_pool2d(Tensor([0, 3, 32, 32],"float32"), norm_type=2.0, kernel_size=2, stride=1, padding=0, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 1, 64],"float16"), Tensor([0, 2048, 1, 64],"float16"), Tensor([1, 2048, 1, 64],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), is_causal=False, )
paddle.nn.functional.max_pool2d(Tensor([2, 3, 224, 0],"float32"), kernel_size=list[5,3,], stride=list[1,2,], padding=tuple(2,1,), )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([0],"float64"), stride=1, padding=list[1,0,], dilation=1, )
paddle.nn.functional.pad(Tensor([0, 1, 2, 2],"float64"), pad=list[2,2,2,2,], mode="replicate", value=0.0, data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[562,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([4, 256, 16, 16],"float32"), Tensor([256, 1, 0, 16],"float32"), bias=None, padding=4, output_padding=0, stride=list[8,8,], dilation=list[1,1,], groups=256, output_size=None, data_format="NCHW", )
paddle.einsum("ibnd,snd->ibns", Tensor([7, 0, 4, 8],"float32"), Tensor([2, 4, 8],"float32"), )
paddle.nn.functional.pad(Tensor([2, 0, 4],"float64"), list[1,1,], mode="circular", data_format="NCL", )
paddle.logaddexp(Tensor([1, 2, 0, 4],"float64"), Tensor([1, 2, 0, 4],"float64"), )
paddle.Tensor.tile(Tensor([8, 1, 0],"float32"), tuple(1,10,1,), )
paddle.Tensor.tile(Tensor([0, 4],"float32"), list[24,1,1,], )
paddle.nn.functional.conv2d(Tensor([1, 1024, 12, 0],"float32"), Tensor([256, 1024, 1, 1],"float32"), None, list[1,1,], 0, list[1,1,], 1, )
paddle.nn.functional.max_pool2d(Tensor([0, 4, 40, 40],"float64"), kernel_size=2, stride=None, padding=0, return_mask=True, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([1, 128, 80, 0],"float32"), Tensor([128, 128, 2, 2],"float32"), bias=Tensor([128],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.softmax(Tensor([2, 0, 8],"float32"), )
paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 8, 0, 113],"float32"), Tensor([1, 8, 113, 64],"float32"), )
paddle.einsum("ij->i", Tensor([0, 5],"float64"), )
paddle.nn.functional.avg_pool3d(x=Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[1,2,3,], padding=list[0,0,0,], exclusive=False, )
paddle.nn.functional.avg_pool3d(x=Tensor([2, 0, 8, 8, 8],"float32"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], )
paddle.nn.functional.conv2d(Tensor([1, 1024, 12, 0],"float32"), Tensor([256, 1024, 1, 0],"float32"), None, list[1,1,], 0, list[1,1,], 1, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 1024, 32, 128],"bfloat16"), Tensor([1, 1024, 32, 128],"bfloat16"), Tensor([1, 0, 32, 128],"bfloat16"), attn_mask=None, is_causal=True, )
paddle.nn.functional.conv1d_transpose(Tensor([4, 6, 16],"float32"), Tensor([6, 8, 0],"float32"), bias=Tensor([8],"float32"), output_size=None, output_padding=0, padding=3, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.selu(x=Tensor([3, 3, 0],"float64"), )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[187,1,1,], )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 0, 2],"float64"), weight=Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=1, dilation=1, )
paddle.nn.functional.fractional_max_pool2d(Tensor([2, 0, 7, 7],"float32"), output_size=list[3,3,], kernel_size=None, random_u=0.3, return_mask=False, name=None, )
paddle.meshgrid(Tensor([2],"float32"), Tensor([0],"float32"), Tensor([6],"float32"), )
paddle.Tensor.tile(Tensor([1, 9, 0],"float32"), list[875,1,1,], )
paddle.nn.functional.conv1d(Tensor([4, 6, 0],"float32"), Tensor([8, 6, 3],"float32"), bias=Tensor([8],"float32"), padding=list[1,], stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.pad(Tensor([0, 3, 256, 256],"float32"), pad=list[3,3,3,3,], mode="reflect", value=0.0, data_format="NCHW", name=None, )
paddle.nn.functional.softmax(Tensor([0, 3, 1],"float32"), axis=1, )
paddle.nn.functional.conv2d_transpose(Tensor([20, 64, 76, 0],"float32"), Tensor([64, 1, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=64, output_size=None, data_format="NCHW", )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 4, 128],"float16"), Tensor([1, 2048, 4, 128],"float16"), Tensor([0, 2048, 4, 128],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.avg_pool2d(Tensor([0, 2, 4, 4],"float32"), kernel_size=2, stride=1, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.flip(Tensor([0, 3, 112, 112],"float32"), axis=-1, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 0, 2, 40],"float16"), Tensor([1, 0, 2, 40],"float16"), Tensor([1, 0, 2, 40],"float16"), attn_mask=Tensor([1, 0, 1, 1],"float16"), )
paddle.nn.functional.pad(Tensor([0, 4, 5, 6],"complex128"), pad=list[1,2,2,1,], mode="reflect", value=0.0, data_format="NCHW", name=None, )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 32, 0],"float64"), output_size=tuple(3,3,3,), data_format="NCDHW", )
paddle.Tensor.imag(Tensor([0, 1025, 107],"complex64"), )
paddle.nn.functional.avg_pool2d(Tensor([0, 1024, 40, 60],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.einsum("bnij,bjnd->bind", Tensor([13, 4, 2, 0],"float32"), Tensor([13, 4, 4, 0],"float32"), )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 512, 0, 128],"float32"), output_size=tuple(2,2,), data_format="NCHW", name=None, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([1, 3, 3, 3, 3],"float32"), bias=Tensor([0],"float32"), stride=list[2,2,1,], padding=tuple(1,2,2,), dilation=tuple(2,2,2,), )
paddle.nn.functional.conv2d_transpose(Tensor([4, 128, 94, 70],"float32"), Tensor([128, 0, 2, 2],"float32"), bias=None, padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.subtract(Tensor([1, 3, 0, 256],"float32"), Tensor([1, 3, 0, 256],"float32"), )
paddle.einsum("bij,bjk->bik", Tensor([3, 0, 5],"float64"), Tensor([3, 5, 2],"float64"), )
paddle.quantile(Tensor([5, 3, 0],"float64"), q=list[0.1,0.2,0.3,], axis=list[1,2,], keepdim=True, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 0, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )
paddle.nn.functional.pad(Tensor([0, 16, 14, 15, 384],"float32"), tuple(0,1,0,0,0,0,), data_format="NDHWC", )
paddle.nn.functional.sequence_mask(Tensor([2, 0],"int64"), maxlen=Tensor([1],"int32"), dtype=VarType(bool), )
paddle.nn.functional.margin_ranking_loss(Tensor([10, 0],"float64"), Tensor([10, 0],"float64"), Tensor([10, 0],"float64"), 0.2, "sum", )
paddle.flip(Tensor([32, 3, 0, 112],"float32"), axis=-1, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([512, 256, 2, 0],"float16"), output_size=list[1,25,], data_format="NCHW", name=None, )
paddle.nn.functional.conv3d(Tensor([4, 8, 8, 8, 3],"float32"), Tensor([5, 3, 0, 3, 3],"float32"), Tensor([5],"float32"), padding=list[1,2,3,1,2,3,], stride=2, dilation=1, groups=1, data_format="NDHWC", )
paddle.nn.functional.conv3d_transpose(Tensor([4, 8, 8, 8, 4],"float32"), Tensor([4, 3, 0, 3, 3],"float32"), None, output_size=None, padding="same", stride=1, dilation=1, groups=2, data_format="NDHWC", )
paddle.nn.functional.maxout(Tensor([9, 0, 2, 6],"float64"), 2, 3, None, )
paddle.nn.functional.conv1d_transpose(Tensor([2, 3, 0],"float64"), Tensor([3, 1, 3],"float64"), bias=Tensor([1],"float64"), output_size=None, output_padding=0, padding=list[1,], stride=list[1,], dilation=list[2,], groups=1, data_format="NCL", )
paddle.nn.functional.pad(Tensor([0, 16, 15, 14, 384],"float32"), tuple(0,0,0,1,0,0,), data_format="NDHWC", )
paddle.nn.functional.conv1d(Tensor([4, 0, 6],"float32"), Tensor([8, 6, 3],"float32"), bias=Tensor([8],"float32"), padding=list[1,2,], stride=list[1,], dilation=list[1,], groups=1, data_format="NLC", )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 2],"float64"), weight=Tensor([3, 0, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=1, dilation=1, )
paddle.nn.functional.pad(Tensor([3, 0, 5, 6, 7],"complex64"), pad=list[1,2,2,1,1,0,], mode="constant", value=100, data_format="NCDHW", name=None, )
paddle.nn.quant.weight_quantize(Tensor([128, 0],"float16"), algo="weight_only_int8", arch=80, group_size=-1, )
paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 0, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,1,1,], exclusive=True, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 1024, 16, 16],"float32"), Tensor([1024, 1, 4, 0],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1024, output_size=None, data_format="NCHW", )
paddle.slice(Tensor([13, 0, 104],"float32"), list[0,1,2,], list[0,0,32,], list[13,1,48,], )
paddle.nn.quant.weight_only_linear(Tensor([0, 1, 64],"float16"), weight=Tensor([192, 64],"int8"), bias=Tensor([192],"float16"), weight_scale=Tensor([192],"float16"), weight_dtype="int8", )
paddle.logaddexp(Tensor([1, 2, 3, 0],"float32"), Tensor([1, 2, 3, 0],"float32"), )
paddle.nn.functional.max_pool1d(x=Tensor([2, 3, 0],"float64"), kernel_size=2, stride=list[1,], padding=1, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 16, 16, 4],"float32"), Tensor([4, 3, 0, 3],"float32"), Tensor([6],"float32"), output_size=None, padding="valid", stride=tuple(1,2,), dilation=tuple(2,1,), groups=2, data_format="NHWC", )
paddle.nn.functional.pad(Tensor([0, 1, 2, 2],"float32"), pad=list[2,2,2,2,], mode="replicate", value=0.0, data_format="NCHW", name=None, )
paddle.nn.functional.maxout(x=Tensor([100, 4, 0, 3],"float32"), groups=2, )
paddle.nanmedian(Tensor([2, 0, 4, 5],"float32"), axis=0, keepdim=False, )
paddle.Tensor.tile(Tensor([1, 5, 0],"float32"), list[425,1,1,], )
paddle.nn.functional.conv2d(Tensor([2, 48, 4, 0],"float32"), Tensor([48, 48, 3, 3],"float32"), None, list[1,1,], 1, list[1,1,], 1, "NCHW", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[387,1,1,], )
paddle.incubate.nn.functional.fused_rms_norm(Tensor([0, 1, 512],"float16"), Tensor([512],"float16"), None, 1e-05, begin_norm_axis=1, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 3, 7, 0],"float32"), list[2,5,], )
paddle.nn.functional.conv2d_transpose(Tensor([4, 16, 16, 4],"float32"), Tensor([4, 1, 3, 0],"float32"), Tensor([4],"float32"), output_size=None, padding="valid", stride=tuple(1,2,), dilation=1, groups=4, data_format="NHWC", )
paddle.nn.functional.max_pool2d(Tensor([2, 3, 0, 32],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=True, data_format="NCHW", name=None, )
paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float16"), residual=Tensor([16, 0],"float16"), residual_alpha=0.69204696, quant_scale=0.15, quant_round_type=1, quant_max_bound=127, quant_min_bound=-127, )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[451,1,1,], )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([0, 3, 4, 4],"float64"), output_size=list[2,3,], )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float32"), Tensor([3, 0, 12, 12, 12],"float32"), bias=Tensor([12],"float32"), padding=0, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.quant.weight_only_linear(Tensor([100, 512],"float16"), weight=Tensor([512, 512],"int8"), weight_scale=Tensor([0],"float16"), weight_dtype="int8", )
paddle.allclose(x=Tensor([0, 2, 2, 3],"float64"), y=Tensor([0, 2, 2, 3],"float64"), rtol=-3.0, atol=-2.0, )
paddle.nn.functional.conv1d(Tensor([16, 64, 25500],"float32"), Tensor([128, 64, 0],"float32"), bias=Tensor([128],"float32"), padding=64, stride=list[1,], dilation=list[64,], groups=1, data_format="NCL", )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 4, 4, 0],"float64"), output_size=3, data_format="NHWC", )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float32"), weight=Tensor([1, 3, 3, 0],"float32"), bias=Tensor([1],"float32"), stride=list[2,1,], padding=1, dilation=list[2,2,], )
paddle.nn.functional.conv1d(x=Tensor([2, 3, 4],"float32"), weight=Tensor([1, 3, 0],"float32"), bias=Tensor([1],"float32"), stride=2, padding=0, )
paddle.nn.functional.conv2d(Tensor([2, 24, 8, 8],"float32"), Tensor([24, 24, 3, 0],"float32"), None, list[1,1,], 1, list[1,1,], 1, "NCHW", )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([0],"float64"), stride=1, padding=0, dilation=1, output_size=None, data_format="NCHW", )
paddle.searchsorted(sorted_sequence=Tensor([2, 5],"float64"), values=Tensor([2, 0],"float64"), )
paddle.nn.functional.hinge_embedding_loss(Tensor([10, 10, 0],"float64"), Tensor([10, 10, 0],"float64"), )
paddle.slice(Tensor([2, 3, 4, 5, 0],"float32"), axes=list[0,1,2,], starts=list[1,0,2,], ends=list[3,3,4,], )
paddle.nn.functional.pad(Tensor([3, 0, 5],"complex64"), pad=list[1,1,], mode="constant", value=100, data_format="NCL", name=None, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float32"), Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([0],"float32"), padding=1, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.max_pool2d(x=Tensor([2, 0, 32, 32],"float64"), kernel_size=list[3,3,], padding=list[1,1,], stride=list[1,1,], )
paddle.nn.functional.avg_pool3d(Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCDHW", name=None, )
paddle.nn.functional.conv2d(Tensor([1, 2048, 128, 0],"float32"), Tensor([2048, 256, 3, 3],"float32"), padding=1, groups=8, )
paddle.Tensor.tile(Tensor([1, 1, 2, 1, 0],"float32"), list[1,1,1,2,1,], )
paddle.slice(Tensor([0, 1, 104],"float32"), list[0,1,2,], list[0,0,88,], list[16,1,104,], )
paddle.nn.functional.temporal_shift(Tensor([0, 1024, 14, 14],"float16"), 8, 0.125, data_format="NCHW", )
paddle.nn.functional.max_unpool1d(Tensor([0, 3, 8],"int64"), Tensor([0, 3, 8],"int32"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 8, 96],"float16"), Tensor([2, 101, 8, 0],"float16"), Tensor([2, 101, 8, 96],"float16"), attn_mask=None, is_causal=False, )
paddle.nn.functional.avg_pool2d(Tensor([32, 0, 14, 14],"float32"), kernel_size=2, stride=2, padding="SAME", ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[150,1,1,], )
paddle.nn.functional.max_pool1d(Tensor([0, 3, 8],"float64"), 2, 1, 1, False, False, None, )
paddle.nn.functional.conv2d(Tensor([1, 1024, 256, 256],"float32"), Tensor([24, 128, 1, 0],"float32"), padding=0, groups=8, )
paddle.Tensor.tile(Tensor([1, 5, 0],"float32"), list[1119,1,1,], )
paddle.Tensor.tile(Tensor([1, 6, 0],"float32"), list[1078,1,1,], )
paddle.nn.functional.max_pool3d(x=Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,1,1,], )
paddle.nn.functional.group_norm(Tensor([2, 0, 2, 2, 4],"float64"), num_groups=2, weight=Tensor([4],"float64"), bias=Tensor([4],"float64"), data_format="NDHWC", )
paddle.cumsum(Tensor([0, 12],"float16"), dtype="float16", )
paddle.nn.functional.conv1d(Tensor([13, 0, 32],"float32"), Tensor([16, 32, 1],"float32"), bias=None, padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NLC", )
paddle.flip(Tensor([32, 3, 112, 0],"float32"), axis=-1, )
paddle.nn.functional.conv2d(Tensor([4, 16, 16, 3],"float32"), Tensor([5, 3, 0, 3],"float32"), Tensor([5],"float32"), padding=0, stride=1, dilation=1, groups=1, data_format="NHWC", )
paddle.nn.functional.conv2d_transpose(Tensor([0, 128, 94, 70],"float32"), Tensor([128, 128, 2, 2],"float32"), bias=None, padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nonzero(x=Tensor([8, 0],"float64"), )
paddle.nn.functional.avg_pool3d(Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCDHW", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([20, 64, 0, 136],"float32"), Tensor([64, 1, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=64, output_size=None, data_format="NCHW", )
paddle.nn.functional.mse_loss(Tensor([0, 2],"float64"), label=Tensor([0, 2],"float64"), reduction="mean", name=None, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 8, 96],"float16"), Tensor([2, 101, 8, 96],"float16"), Tensor([2, 101, 0, 96],"float16"), attn_mask=None, is_causal=False, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 64, 64, 0],"float32"), Tensor([64, 1, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=64, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1001, 1, 0],"float32"), list[1,5,1,], )
paddle.einsum("ibnd,jbnd->bnij", Tensor([7, 10, 4, 8],"float32"), Tensor([0, 10, 4, 8],"float32"), )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[195,1,1,], )
paddle.slice(Tensor([0, 1, 104],"float32"), list[0,1,2,], list[0,0,24,], list[13,1,40,], )
paddle.nn.functional.conv1d(Tensor([1, 3, 5],"float32"), Tensor([4, 3, 0],"float32"), bias=Tensor([4],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 0, 5, 7, 7],"float32"), output_size=list[3,3,3,], data_format="NCDHW", name=None, )
paddle.nn.functional.avg_pool3d(x=Tensor([0, 8, 8, 8, 3],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NDHWC", exclusive=False, )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[167,1,1,], )
paddle.incubate.nn.functional.blha_get_max_len(Tensor([10],"int32"), Tensor([0],"int32"), Tensor([10],"float32"), )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 0],"float16"), Tensor([2, 100, 1, 0],"float16"), Tensor([2, 100, 1, 0],"float16"), attn_mask=Tensor([2, 1, 1, 0],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.conv1d_transpose(Tensor([0, 3, 2],"float32"), Tensor([3, 2, 3],"float32"), bias=Tensor([2],"float32"), output_size=None, output_padding=0, padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv2d(Tensor([4, 6, 16, 0],"float32"), Tensor([12, 1, 3, 0],"float32"), None, padding="valid", stride=1, dilation=1, groups=6, data_format="NCHW", )
paddle.slice(Tensor([0, 2, 104],"float32"), list[0,1,2,], list[0,0,24,], list[16,2,40,], )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float64"), weight=Tensor([1, 3, 3, 3, 0],"float64"), bias=Tensor([1],"float64"), stride=list[2,2,1,], padding=tuple(1,2,2,), dilation=tuple(2,2,2,), )
paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), rtol=0.01, atol=0.0, name="test_7", )
paddle.Tensor.fill_diagonal_(Tensor([0, 3],"float64"), 4, 1, False, )
paddle.subtract(Tensor([0],"float32"), Tensor([0],"float32"), )
paddle.strided_slice(x=Tensor([5, 0, 6, 4, 2, 6],"float64"), axes=list[1,2,5,], starts=list[6,5,4,], ends=list[2,0,1,], strides=list[-1,-2,-3,], )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 8, 3],"float64"), weight=Tensor([3, 0, 3],"float64"), bias=Tensor([2],"float64"), stride=1, padding=list[list[0,0,],list[3,4,],list[0,0,],], output_padding=0, groups=1, dilation=1, output_size=None, data_format="NLC", )
paddle.einsum("blqd,bmdk->blqk", Tensor([52, 5, 1, 1],"float32"), Tensor([52, 0, 1, 1],"float32"), )
paddle.vision.ops.generate_proposals(Tensor([1, 9, 10, 8],"float32"), Tensor([1, 36, 10, 0],"float32"), Tensor([1, 2],"float32"), Tensor([720, 4],"float32"), Tensor([720, 4],"float32"), pre_nms_top_n=4000, post_nms_top_n=4000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 3, 0, 7],"float32"), output_size=list[3,3,], data_format="NHWC", name=None, )
paddle.nn.functional.prelu(Tensor([0, 2, 3, 4],"float32"), Tensor([2],"float32"), )
paddle.nn.functional.temporal_shift(x=Tensor([2, 4, 3, 0],"float64"), seg_num=2, shift_ratio=0.4, )
paddle.einsum("bn,anm,bm->ba", Tensor([2, 5],"float64"), Tensor([0, 5, 2],"float64"), Tensor([2, 2],"float64"), )
paddle.nn.functional.conv2d_transpose(Tensor([1, 512, 16, 0],"float32"), Tensor([512, 512, 3, 3],"float32"), padding=0, stride=2, groups=1, )
paddle.nn.functional.pad(Tensor([1, 2, 3, 0],"float64"), pad=list[1,1,1,0,], mode="reflect", value=0.0, data_format="NHWC", name=None, )
paddle.nn.functional.pad(x=Tensor([1, 1, 0, 2, 3],"float64"), pad=list[0,0,1,1,0,0,], mode="constant", value=0, data_format="NCDHW", )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([0, 4, 4, 3],"float64"), output_size=tuple(3,3,), data_format="NHWC", )
paddle.meshgrid(list[Tensor([10],"float64"),Tensor([10],"float64"),Tensor([10],"float64"),Tensor([0],"float64"),Tensor([10],"float64"),], )
paddle.nn.quant.weight_quantize(Tensor([64, 0],"float16"), algo="weight_only_int4", group_size=-1, )
paddle.nn.functional.max_pool2d(x=Tensor([0, 3, 32, 32],"float64"), kernel_size=list[3,3,], )
paddle.nn.functional.maxout(x=Tensor([100, 4, 3, 0],"float32"), groups=2, )
paddle.nn.functional.avg_pool2d(Tensor([1, 0, 4, 4],"float32"), kernel_size=2, stride=1, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([1, 0, 3],"float64"), pad=list[2,2,], mode="replicate", value=0.0, data_format="NCL", name=None, )
paddle.nn.functional.max_pool2d(Tensor([0, 16, 10, 10],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 0, 7, 7],"float32"), output_size=list[3,3,], data_format="NHWC", name=None, )
paddle.Tensor.tile(Tensor([1, 512, 0, 4],"float32"), tuple(4,1,1,1,), )
paddle.nn.functional.conv2d(Tensor([1, 1024, 10, 26],"float32"), Tensor([256, 1024, 0, 1],"float32"), None, list[1,1,], 0, list[1,1,], 1, )
paddle.nn.functional.layer_norm(Tensor([0, 10, 4, 4],"float32"), 4, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 16],"float32"), Tensor([6, 8, 0, 3],"float32"), bias=Tensor([8],"float32"), padding=list[1,2,2,1,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 2, 2, 2, 3],"float32"), weight=Tensor([3, 1, 3, 0, 3],"float32"), bias=Tensor([3],"float32"), stride=1, padding=list[1,0,1,], groups=3, data_format="NDHWC", dilation=1, )
paddle.nn.functional.pad(Tensor([421120, 0, 3],"float32"), list[0,1,], "constant", 1.0, data_format="NCL", )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([0, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 0, 2, 2],"float32"), weight=Tensor([3, 2, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=0, dilation=1, )
paddle.nn.functional.log_softmax(Tensor([2, 2, 0],"float32"), 0, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 3, 7, 0],"float32"), output_size=list[None,3,], data_format="NCHW", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 2],"float32"), Tensor([3, 0, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[576,1,1,], )
paddle.nn.functional.softmax(Tensor([0, 6, 49, 49],"float32"), -1, name=None, )
paddle.nn.functional.pad(Tensor([1, 3, 0, 150],"float64"), pad=list[1,1,2,2,], mode="constant", value=0.0, data_format="NCHW", )
paddle.nn.functional.max_unpool1d(Tensor([1, 0, 8],"int64"), Tensor([1, 0, 8],"int32"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
paddle.nn.functional.sequence_mask(Tensor([0, 3],"int64"), maxlen=-1, dtype=VarType(int64), )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 0, 4],"float64"), weight=Tensor([1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=list[2,1,], padding=1, dilation=2, )
paddle.einsum("bhlqk,bhkd->bhlqd", Tensor([13, 4, 3, 1, 1],"float32"), Tensor([13, 4, 1, 0],"float32"), )
paddle.nn.functional.pad(Tensor([0, 128, 16, 104],"float32"), Tensor([4],"int32"), value=0, )
paddle.einsum("sec,sm->ecm", Tensor([2, 60, 2],"float32"), Tensor([2, 0],"float32"), )
paddle.nn.functional.hinge_embedding_loss(Tensor([0, 3],"float64"), Tensor([0, 3],"float64"), reduction="mean", margin=4.0, name=None, )
paddle.nn.functional.pad(Tensor([1, 0, 180, 200],"float64"), pad=list[2,2,2,2,], mode="constant", value=0.0, data_format="NCHW", )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 0],"float64"), weight=Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([3],"float64"), stride=1, padding=list[1,0,1,], groups=3, dilation=1, )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[205,1,1,], )
paddle.nn.functional.avg_pool3d(x=Tensor([8, 0, 4, 7, 7],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[295,1,1,], )
paddle.Tensor.isnan(Tensor([0, 3, 4],"float64"), )
paddle.nn.functional.max_pool3d(Tensor([1, 0, 4, 4, 6],"float64"), kernel_size=2, stride=2, return_mask=True, )
paddle.nn.functional.avg_pool2d(Tensor([1, 0, 40, 54],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.softmax(Tensor([128, 1, 16, 0, 49],"float16"), -1, name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([20, 64, 38, 68],"float32"), Tensor([64, 1, 8, 0],"float32"), bias=None, padding=2, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=64, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv3d_transpose(Tensor([4, 8, 8, 8, 4],"float32"), Tensor([4, 3, 3, 3, 0],"float32"), Tensor([6],"float32"), output_size=None, padding=0, stride=1, dilation=1, groups=2, data_format="NDHWC", )
paddle.nn.functional.lp_pool1d(Tensor([0, 3, 32],"float32"), 4.0, 3, 2, 1, False, "NCL", None, )
paddle.nn.functional.softmax(Tensor([182, 3, 0],"float32"), axis=1, )
paddle.nn.functional.conv1d(Tensor([13, 32, 255],"float32"), Tensor([32, 32, 0],"float32"), bias=None, padding=0, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.margin_ranking_loss(Tensor([0, 10],"float64"), Tensor([0, 10],"float64"), Tensor([0, 10],"float64"), 0.2, "sum", None, )
paddle.nn.functional.avg_pool3d(Tensor([2, 3, 0, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=2, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
paddle.einsum("ibnd,jbnd->bnij", Tensor([7, 0, 4, 8],"float32"), Tensor([14, 0, 4, 8],"float32"), )
paddle.einsum("bhlqk,bhkd->bhlqd", Tensor([52, 4, 3, 1, 1],"float32"), Tensor([52, 4, 0, 8],"float32"), )
paddle.Tensor.tile(Tensor([16660, 0],"float32"), list[1,77,], )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 2],"float32"), weight=Tensor([3, 0, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=1, output_padding=0, groups=1, dilation=1, output_size=None, data_format="NCL", name=None, )
paddle.Tensor.tile(Tensor([14, 1, 0],"float16"), tuple(1,8,1,), )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 0],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([0, 100, 1, 64],"float16"), Tensor([2, 100, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 100],"float16"), is_causal=False, )
paddle.vision.ops.generate_proposals(Tensor([1, 15, 42, 63],"float32"), Tensor([1, 60, 0, 63],"float32"), Tensor([1, 2],"float32"), Tensor([39690, 4],"float32"), Tensor([39690, 4],"float32"), pre_nms_top_n=12000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 5],"float64"), weight=Tensor([3, 0, 3],"float64"), bias=Tensor([2],"float64"), stride=1, padding="vALiD", output_padding=0, groups=1, dilation=1, output_size=None, data_format="NCL", )
paddle.nn.functional.softmax(x=Tensor([16, 0, 12, 12],"float32"), axis=-1, )
paddle.subtract(Tensor([0, 10],"float32"), Tensor([0, 10],"float32"), )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 0, 32],"float64"), output_size=list[1,3,2,], )
paddle.nn.functional.pad(Tensor([4, 3, 0, 6, 6],"float64"), pad=list[2,2,2,2,2,2,], mode="constant", value=0.0, data_format="NCDHW", name=None, )
paddle.nn.functional.conv1d(Tensor([16, 64, 0],"float32"), Tensor([128, 64, 3],"float32"), bias=Tensor([128],"float32"), padding=32, stride=list[1,], dilation=list[32,], groups=1, data_format="NCL", )
paddle.kthvalue(x=Tensor([3, 2, 0],"float64"), k=3, axis=0, )
paddle.nn.functional.pad(Tensor([2, 0, 4, 4],"float32"), list[1,1,1,1,], mode="replicate", data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([2, 2, 3, 3],"float64"), Tensor([2, 2, 0, 1],"float64"), groups=1, padding=list[1,0,0,1,], )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[1119,1,1,], )
paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 3, 5, 2, 4],"float32"), Tensor([1, 3, 0, 2, 4],"float32"), )
paddle.nn.functional.lp_pool2d(Tensor([0, 3, 32, 32],"float32"), 2, kernel_size=2, stride=None, ceil_mode=False, )
paddle.vision.ops.generate_proposals(Tensor([1, 0, 12, 9],"float32"), Tensor([1, 36, 12, 9],"float32"), Tensor([1, 2],"float32"), Tensor([972, 4],"float32"), Tensor([972, 4],"float32"), pre_nms_top_n=4000, post_nms_top_n=4000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.conv1d(Tensor([2, 4, 3],"float64"), Tensor([2, 3, 0],"float64"), bias=Tensor([2],"float64"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NLC", )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[436,1,1,], )
paddle.nn.functional.pad(Tensor([0, 1, 256, 256],"float32"), list[1,1,1,1,], )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 1024, 0, 16],"bfloat16"), Tensor([1, 1024, 2, 16],"bfloat16"), Tensor([1, 1024, 2, 16],"bfloat16"), attn_mask=None, is_causal=True, )
paddle.nn.functional.mse_loss(input=Tensor([0, 1],"float32"), label=Tensor([0, 1],"float32"), )
paddle.Tensor.tile(Tensor([1, 1, 0, 350],"float32"), list[1,3,1,1,], )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 100, 0, 64],"float16"), Tensor([2, 100, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 100],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.conv3d_transpose(Tensor([0, 3, 2, 2, 2],"float64"), Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[2,2,2,], groups=1, output_size=None, data_format="NCDHW", )
paddle.Tensor.__setitem__(Tensor([0, 5, 4, 3],"complex128"), list[list[2,3,4,],list[1,2,5,],], 100, )
paddle.nn.functional.fractional_max_pool2d(Tensor([2, 0, 7, 7],"float32"), output_size=list[2,5,], kernel_size=None, random_u=0.7, return_mask=False, name=None, )
paddle.Tensor.tile(Tensor([10285, 0],"float32"), list[1,42,], )
paddle.slice(Tensor([16, 0, 104],"float32"), list[0,1,2,], list[0,0,40,], list[16,1,56,], )
paddle.nn.functional.conv2d_transpose(Tensor([1, 1024, 16, 16],"float32"), Tensor([1024, 256, 0, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float64"), weight=Tensor([1, 3, 0, 3],"float64"), bias=Tensor([1],"float64"), stride=list[2,1,], padding=list[1,2,], dilation=tuple(2,2,), )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 0, 5, 7, 7],"float32"), output_size=list[3,3,3,], data_format="NDHWC", name=None, )
paddle.nn.functional.softmax(Tensor([2, 3, 0],"float64"), 2, name=None, )
paddle.einsum("...->...", Tensor([5, 5, 0],"float64"), )
paddle.Tensor.__rpow__(Tensor([2, 0, 2],"float32"), 3.0, )
paddle.nn.functional.max_pool1d(Tensor([0, 3, 8],"float64"), 2, 1, list[1,1,], False, False, None, )
paddle.nn.functional.conv1d(Tensor([16, 64, 0],"float32"), Tensor([128, 64, 3],"float32"), bias=Tensor([128],"float32"), padding=2, stride=list[1,], dilation=list[2,], groups=1, data_format="NCL", )
paddle.nn.functional.fractional_max_pool2d(Tensor([0, 3, 7, 7],"float32"), output_size=list[3,3,], kernel_size=None, random_u=0.3, return_mask=False, name=None, )
paddle.searchsorted(sorted_sequence=Tensor([2, 0],"float64"), values=Tensor([2, 0],"float64"), )
paddle.einsum("ibm,hm->ibh", Tensor([1, 0, 32],"float32"), Tensor([32, 32],"float32"), )
paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 3, 5, 0, 4],"float32"), Tensor([1, 3, 4, 0, 4],"float32"), )
paddle.slice(Tensor([1024, 2, 0],"float16"), axes=list[0,], starts=list[768,], ends=list[896,], )
paddle.nn.functional.avg_pool2d(x=Tensor([0, 3, 32, 32],"float64"), kernel_size=list[3,3,], )
paddle.nn.functional.max_pool2d(Tensor([1, 0, 288, 399],"float32"), kernel_size=3, stride=2, padding=1, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 4, 128],"float16"), Tensor([0, 2048, 4, 128],"float16"), Tensor([1, 2048, 4, 128],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.slice(Tensor([0, 1, 104],"float32"), list[0,1,2,], list[0,0,32,], list[3,1,48,], )
paddle.nn.functional.avg_pool2d(Tensor([0, 128, 8, 50],"float16"), kernel_size=tuple(2,1,), stride=tuple(2,1,), padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.maxout(Tensor([100, 4, 0, 3],"float32"), 2, 1, None, )
paddle.nn.functional.avg_pool3d(Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[1,2,3,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCDHW", name=None, )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 0, 4],"float64"), weight=Tensor([1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=list[2,1,], padding=tuple(1,2,), dilation=tuple(2,2,), )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 2, 3],"float32"), weight=Tensor([3, 1, 0],"float32"), bias=Tensor([1],"float32"), stride=2, padding=list[1,], groups=3, dilation=1, data_format="NLC", output_padding=1, )
paddle.slice(Tensor([2, 0, 2],"float32"), axes=list[0,], starts=list[1,], ends=list[2,], )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 8, 3],"float64"), weight=Tensor([3, 2, 0],"float64"), bias=Tensor([2],"float64"), stride=1, padding=list[list[0,0,],list[3,4,],list[0,0,],], output_padding=0, groups=1, dilation=1, output_size=None, data_format="NLC", )
paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([0],"float32"), shift=None, smooth=None, quant_scale=0.0009928022045642138, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.nn.functional.pad(Tensor([0, 3, 32, 224, 238],"float32"), tuple(0,2,0,0,0,0,), data_format="NCDHW", )
paddle.nn.functional.pixel_shuffle(Tensor([4, 128, 128, 0],"float32"), 2, "NCHW", None, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[835,1,1,], )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 2, 3],"float64"), weight=Tensor([3, 1, 0],"float64"), bias=Tensor([1],"float64"), stride=2, padding=list[1,], groups=3, dilation=1, data_format="NLC", output_padding=1, )
paddle.vision.ops.generate_proposals(Tensor([0, 3, 10, 14],"float32"), Tensor([1, 12, 10, 14],"float32"), Tensor([1, 2],"float32"), Tensor([420, 4],"float32"), Tensor([420, 4],"float32"), pre_nms_top_n=2000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.einsum("b h i d, b h j d -> b h i j", Tensor([1, 8, 0, 64],"float32"), Tensor([1, 8, 109, 64],"float32"), )
paddle.nn.functional.conv2d(Tensor([1, 2048, 16, 0],"float32"), Tensor([12, 512, 1, 0],"float32"), padding=0, groups=4, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 2, 2, 6],"float32"), Tensor([6, 1, 0, 3],"float32"), bias=Tensor([3],"float32"), padding=list[1,0,], output_padding=1, stride=list[2,2,], dilation=list[1,1,], groups=3, output_size=None, data_format="NHWC", )
paddle.subtract(Tensor([1, 0, 256, 256],"float32"), Tensor([1, 0, 256, 256],"float32"), )
paddle.slice(Tensor([0, 2, 104],"float32"), list[0,1,2,], list[0,0,40,], list[16,2,56,], )
paddle.nn.functional.max_pool3d(Tensor([2, 0, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding=list[list[0,0,],list[0,0,],list[0,0,],list[0,0,],list[0,0,],], )
paddle.nn.functional.pad(Tensor([0, 2, 3],"float64"), pad=list[1,1,], mode="reflect", value=0.0, data_format="NLC", name=None, )
paddle.nn.quant.weight_quantize(Tensor([0, 256],"float16"), algo="weight_only_int4", arch=80, group_size=-1, )
paddle.nn.functional.conv3d_transpose(Tensor([4, 4, 8, 8, 8],"float32"), Tensor([4, 3, 3, 3, 0],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,2,1,], stride=1, dilation=1, groups=2, data_format="NCDHW", )
paddle.einsum("ij,...i->j...", Tensor([4, 5],"float64"), Tensor([3, 0, 4],"float64"), )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 0, 2],"float32"), Tensor([3, 1, 3, 3],"float32"), bias=None, padding=1, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([2, 256, 62, 0],"float32"), Tensor([256, 128, 4, 4],"float32"), bias=None, padding=0, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv1d_transpose(Tensor([0, 6, 16],"float32"), Tensor([6, 8, 3],"float32"), bias=Tensor([8],"float32"), output_size=list[36,], output_padding=0, padding=0, stride=list[2,], dilation=list[2,], groups=1, data_format="NCL", )
paddle.nn.functional.pad(Tensor([0, 7974, 3],"float32"), list[1,0,], value=1, mode="constant", data_format="NCL", )
paddle.nn.functional.pad(Tensor([1, 16, 0, 15, 384],"float16"), tuple(0,6,0,0,0,0,), data_format="NDHWC", )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 0, 16, 32],"float16"), output_size=2, data_format="NCHW", name=None, )
paddle.slice(Tensor([3, 0, 104],"float32"), list[0,1,2,], list[0,0,88,], list[3,1,104,], )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 0, 4],"float32"), weight=Tensor([6, 1, 3, 3, 3],"float32"), bias=Tensor([6],"float32"), stride=2, padding=0, groups=3, )
paddle.einsum("..., f -> ... f", Tensor([24],"float32"), Tensor([0],"float32"), )
paddle.nn.functional.conv2d_transpose(Tensor([4, 256, 47, 35],"float32"), Tensor([256, 128, 4, 0],"float32"), bias=None, padding=0, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 32, 0],"float64"), output_size=list[2,3,3,], data_format="NCDHW", name=None, )
paddle.Tensor.tile(Tensor([1, 5, 0],"float32"), list[414,1,1,], )
paddle.nn.functional.conv1d(Tensor([13, 24, 14],"float32"), Tensor([24, 12, 0],"float32"), bias=Tensor([24],"float32"), padding=8, stride=list[1,], dilation=list[1,], groups=2, data_format="NCL", )
paddle.nn.functional.pad(Tensor([1, 3, 0, 350],"float32"), pad=list[2,2,2,2,], mode="constant", value=0.0, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([1, 0, 3],"float64"), pad=list[2,1,], mode="replicate", value=0.0, data_format="NCL", name=None, )
paddle.Tensor.tile(Tensor([2, 1, 1, 0],"float32"), tuple(4,1,4,4,), )
paddle.Tensor.tile(Tensor([1, 10, 0],"float32"), list[520,1,1,], )
paddle.Tensor.tile(Tensor([1, 7, 0],"float32"), list[926,1,1,], )
paddle.einsum("bij,bjk->bik", Tensor([0, 4, 5],"float64"), Tensor([0, 5, 2],"float64"), )
paddle.nn.functional.lp_pool1d(Tensor([2, 0, 3],"float32"), 7.0, 2, None, 1, False, "NLC", None, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 0, 2],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=0, dilation=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.hinge_embedding_loss(Tensor([4, 3, 3, 0],"float64"), Tensor([4, 3, 3, 0],"float64"), reduction="mean", margin=-4.0, name=None, )
paddle.nn.functional.avg_pool2d(Tensor([56, 0, 16, 16],"float32"), kernel_size=2, stride=2, )
paddle.nn.functional.max_pool2d(Tensor([2, 4, 40, 0],"float64"), kernel_size=4, stride=None, padding=2, return_mask=True, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.max_pool1d(Tensor([1, 3, 0],"float32"), kernel_size=5, stride=5, padding=0, ceil_mode=True, return_mask=True, )
paddle.frexp(Tensor([4, 0, 2],"float64"), )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1024, 2048, 0, 7],"float32"), output_size=1, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 6, 0, 2],"float32"), Tensor([6, 1, 3, 3],"float32"), bias=Tensor([3],"float32"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=3, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 0, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[2,2,2,], dilation=list[2,1,2,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.pad(Tensor([0, 16, 14, 15, 384],"float32"), tuple(0,6,0,0,0,0,), data_format="NDHWC", )
paddle.nn.functional.group_norm(Tensor([0, 3, 2, 2, 4],"float64"), num_groups=2, weight=Tensor([4],"float64"), bias=Tensor([4],"float64"), data_format="NDHWC", )
paddle.Tensor.trunc(Tensor([2, 8, 0],"float32"), )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 160, 16, 0],"float16"), output_size=4, data_format="NCHW", name=None, )
paddle.nn.functional.conv2d(Tensor([16, 3, 0, 260],"float32"), weight=Tensor([3, 1, 0, 5],"float32"), groups=3, )
paddle.nn.functional.conv3d_transpose(Tensor([0, 6, 8, 8, 8],"float32"), Tensor([6, 8, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[1,2,1,], dilation=list[2,2,2,], groups=1, output_size=None, data_format="NCDHW", )
paddle.flip(Tensor([20, 0, 112, 112],"float32"), axis=-1, )
paddle.Tensor.__rpow__(Tensor([0, 12],"float32"), 2, )
paddle.nn.functional.conv3d_transpose(Tensor([0, 3, 2, 2, 2],"float32"), Tensor([3, 2, 3, 3, 3],"float32"), bias=Tensor([2],"float32"), padding=0, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.max_pool2d(Tensor([2, 0, 32, 32],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 0],"float64"), weight=Tensor([3, 1, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=1, dilation=1, )
paddle.nn.functional.max_pool1d(Tensor([2, 0, 8],"float64"), 2, 1, list[1,], False, False, None, )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float64"), weight=Tensor([6, 1, 3, 0],"float64"), bias=Tensor([6],"float64"), stride=2, padding=0, groups=3, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 2, 2, 6],"float32"), weight=Tensor([6, 1, 3, 0],"float32"), bias=Tensor([3],"float32"), stride=1, padding=list[1,0,], groups=3, data_format="NHWC", dilation=1, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float64"), weight=Tensor([1, 3, 0, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=list[2,2,1,], padding=1, dilation=tuple(2,2,2,), )
paddle.nn.functional.selu(Tensor([3, 3, 0],"float64"), 1.0507009873554805, 0, None, )
paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 0, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,2,1,], exclusive=True, )
paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1],"int32"), Tensor([0, 1],"int32"), mask=Tensor([1, 1, 50, 50],"float16"), scale=0.125, )
paddle.einsum("b h i d, b h j d -> b h i j", Tensor([1, 0, 1, 64],"float32"), Tensor([1, 0, 1, 64],"float32"), )
paddle.Tensor.tile(Tensor([1, 6, 0],"float32"), list[517,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([4, 3, 16, 16],"float32"), Tensor([3, 5, 3, 0],"float32"), None, output_size=None, padding=0, stride=1, dilation=1, groups=1, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 6, 2, 0],"float64"), weight=Tensor([6, 1, 3, 3],"float64"), bias=Tensor([3],"float64"), stride=1, padding=list[1,0,], output_padding=0, dilation=1, groups=3, output_size=None, data_format="NCHW", )
paddle.nn.functional.avg_pool2d(Tensor([2, 0, 13, 13],"float32"), kernel_size=5, stride=3, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([1, 0, 280, 350],"float32"), list[1,3,1,1,], )
paddle.slice(Tensor([3, 0, 104],"float32"), list[0,1,2,], list[0,0,48,], list[3,1,64,], )
paddle.nn.functional.max_pool3d(Tensor([0, 3, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 6, 2, 2],"float64"), weight=Tensor([6, 1, 3, 0],"float64"), bias=Tensor([3],"float64"), stride=1, padding=list[1,0,], output_padding=0, dilation=1, groups=3, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([4, 1, 0],"float32"), tuple(1,6,1,), )
paddle.nn.functional.conv2d_transpose(Tensor([4, 256, 32, 32],"float32"), Tensor([256, 1, 0, 8],"float32"), bias=None, padding=2, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=256, output_size=None, data_format="NCHW", )
paddle.einsum("nbqa,ahc->nbqhc", Tensor([1, 0, 5, 6],"float16"), Tensor([6, 2, 4],"float16"), )
paddle.nn.functional.max_pool2d(Tensor([4, 0, 4, 4],"float16"), kernel_size=2, stride=2, padding=0, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float32"), weight=Tensor([3, 1, 3, 0],"float32"), bias=Tensor([1],"float32"), stride=1, padding=0, dilation=1, )
paddle.nn.functional.conv2d(Tensor([8, 3, 256, 256],"float32"), Tensor([128, 3, 1, 0],"float32"), bias=None, stride=1, padding=0, )
paddle.nn.quant.weight_only_linear(Tensor([2, 0, 512],"float16"), weight=Tensor([1024, 512],"int8"), bias=Tensor([1024],"float16"), weight_scale=Tensor([1024],"float16"), weight_dtype="int8", )
paddle.nn.functional.group_norm(Tensor([1, 1024, 12, 0],"float32"), num_groups=32, epsilon=1e-05, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), )
paddle.nn.functional.conv1d_transpose(Tensor([0, 128, 112],"float32"), Tensor([128, 64, 8],"float32"), bias=Tensor([64],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.einsum("td,dnh->tnh", Tensor([14, 32],"float32"), Tensor([32, 4, 0],"float32"), )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 3, 0, 4],"float64"), output_size=list[2,3,], data_format="NCHW", name=None, )
paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 0, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=True, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[406,1,1,], )
paddle.Tensor.tile(Tensor([14, 1, 0],"float32"), tuple(1,8,1,), )
paddle.nn.functional.conv1d_transpose(Tensor([0, 3, 2],"float64"), Tensor([3, 1, 3],"float64"), bias=Tensor([3],"float64"), output_size=None, output_padding=0, padding=list[1,], stride=list[1,], dilation=list[1,], groups=3, data_format="NCL", )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 2],"float32"), Tensor([3, 1, 3, 0],"float32"), bias=Tensor([1],"float32"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[2,2,], groups=1, output_size=None, data_format="NCHW", )
paddle.vision.ops.generate_proposals(Tensor([2, 3, 4, 4],"float32"), Tensor([2, 12, 4, 4],"float32"), Tensor([2, 3],"float32"), Tensor([0, 4, 3, 4],"float32"), Tensor([4, 4, 3, 4],"float32"), pre_nms_top_n=10, post_nms_top_n=5, return_rois_num=True, )
paddle.nn.functional.fractional_max_pool3d(x=Tensor([0, 3, 7, 7, 7],"float32"), output_size=list[3,3,None,], random_u=0.6, )
paddle.nanmedian(Tensor([0, 3, 4, 5],"float32"), axis=0, keepdim=False, mode="min", )
paddle.nn.functional.cosine_embedding_loss(Tensor([0],"float32"), Tensor([0],"float32"), Tensor([0],"int64"), margin=0.5, reduction="mean", name=None, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 2, 2, 6],"float32"), weight=Tensor([6, 0, 3, 3],"float32"), bias=Tensor([3],"float32"), stride=1, padding=list[1,0,], groups=3, data_format="NHWC", dilation=1, )
paddle.Tensor.isnan(Tensor([3, 6, 3, 4, 0, 5],"float64"), )
paddle.allclose(Tensor([0],"int32"), Tensor([0],"int32"), 50.0, 48.0, False, )
paddle.einsum("bnij,jbnd->ibnd", Tensor([10, 4, 0, 7],"float32"), Tensor([7, 10, 4, 8],"float32"), )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 2],"float32"), Tensor([3, 1, 0, 3],"float32"), bias=None, padding=0, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.max_pool2d(Tensor([2, 0, 32, 32],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, )
paddle.einsum("lbnd,mlb->mbnd", Tensor([8, 14, 4, 0],"float32"), Tensor([1, 8, 14],"float32"), )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 0, 2, 2],"float32"), Tensor([3, 2, 3, 3, 3],"float32"), bias=Tensor([2],"float32"), padding=0, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 0, 2, 2],"float32"), weight=Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=list[1,0,1,], dilation=1, )
paddle.einsum("m,d->md", Tensor([0],"float32"), Tensor([0],"float32"), )
paddle.cumsum(x=Tensor([1, 2, 0, 3],"float64"), axis=Tensor([1],"float64"), )
paddle.nn.functional.pad(Tensor([1, 16, 14, 0, 384],"float32"), tuple(0,3,0,0,0,0,), data_format="NDHWC", )
paddle.nn.functional.pad(Tensor([1, 2, 0, 4, 5],"complex128"), pad=list[1,2,1,1,3,4,], mode="constant", value=100, )
paddle.nn.functional.avg_pool3d(x=Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,0,0,], exclusive=True, )
paddle.quantile(Tensor([4, 7, 0],"float64"), q=0.1, axis=list[1,2,], keepdim=True, )
paddle.einsum("blqd,bmdk->blqk", Tensor([13, 5, 1, 1],"float32"), Tensor([13, 1, 1, 0],"float32"), )
paddle.nn.functional.conv2d_transpose(Tensor([2, 2, 3, 3],"float64"), Tensor([2, 0, 1, 1],"float64"), groups=1, padding="VALID", )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 4, 128],"float16"), Tensor([1, 2048, 4, 128],"float16"), Tensor([1, 0, 4, 128],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), dropout_p=0.0, training=True, is_causal=False, )
paddle.nn.functional.conv2d_transpose(Tensor([1, 2048, 128, 128],"float32"), Tensor([2048, 0, 3, 3],"float32"), padding=0, stride=2, groups=8, )
paddle.nn.functional.avg_pool3d(Tensor([0, 3, 32, 32, 32],"float32"), kernel_size=2, stride=None, padding="SAME", ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
paddle.einsum("...qk,...kd->...qd", Tensor([13, 4, 3, 1, 0],"float32"), Tensor([13, 4, 3, 2, 0],"float32"), )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 0, 32, 32],"float32"), output_size=tuple(6,6,), data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([0, 16, 16, 14, 384],"float16"), tuple(0,0,0,5,0,0,), data_format="NDHWC", )
paddle.nn.functional.max_pool2d(Tensor([2, 0, 32, 32],"float32"), kernel_size=2, stride=None, padding="SAME", return_mask=True, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 128, 32, 0],"float32"), Tensor([128, 1, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=128, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv1d_transpose(x=Tensor([0, 3, 2],"float64"), weight=Tensor([3, 1, 3],"float64"), bias=Tensor([1],"float64"), stride=2, padding=list[1,], output_padding=0, groups=1, dilation=1, output_size=None, data_format="NCL", name=None, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 4, 0],"float16"), Tensor([1, 2048, 4, 0],"float16"), Tensor([1, 2048, 4, 0],"float16"), attn_mask=Tensor([1, 1, 2048, 0],"float16"), dropout_p=0.0, training=True, is_causal=False, )
paddle.vision.ops.generate_proposals(Tensor([1, 0, 10, 14],"float32"), Tensor([1, 12, 10, 14],"float32"), Tensor([1, 2],"float32"), Tensor([420, 4],"float32"), Tensor([420, 4],"float32"), pre_nms_top_n=2000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.softmax(Tensor([2, 3, 0, 5],"float32"), axis=0, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 2],"float32"), Tensor([3, 0, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=1, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d(Tensor([1024, 1, 131, 0],"float32"), Tensor([1, 1, 4, 0],"float32"), )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 0, 7, 7],"float32"), output_size=5, )
paddle.nn.functional.sequence_mask(Tensor([0, 3],"int64"), maxlen=Tensor([1],"int32"), dtype=VarType(int32), )
paddle.nn.functional.pad(Tensor([1, 2, 0],"float64"), pad=list[1,2,], mode="replicate", value=0.0, data_format="NLC", name=None, )
paddle.nn.functional.conv1d_transpose(Tensor([2, 3, 0],"float32"), Tensor([3, 2, 3],"float32"), bias=Tensor([2],"float32"), output_size=None, output_padding=0, padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[62,1,1,], )
paddle.searchsorted(Tensor([5],"float64"), Tensor([0, 3],"float64"), out_int32=True, )
paddle.nn.functional.conv1d_transpose(Tensor([1, 128, 112],"float32"), Tensor([128, 64, 0],"float32"), bias=Tensor([64],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv1d_transpose(Tensor([2, 3, 0],"float64"), Tensor([3, 1, 3],"float64"), bias=Tensor([1],"float64"), output_size=None, output_padding=0, padding=1, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.hinge_embedding_loss(Tensor([0, 3],"float32"), Tensor([0, 3],"float32"), reduction="none", margin=1.0, name=None, )
paddle.Tensor.tile(Tensor([1, 0],"float32"), list[1,24,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[781,1,1,], )
paddle.nn.functional.conv2d(Tensor([1, 2048, 16, 0],"float32"), Tensor([12, 512, 1, 1],"float32"), padding=0, groups=4, )
paddle.einsum("iji->j", Tensor([0, 10, 5],"float64"), )
paddle.Tensor.isnan(Tensor([2, 3, 0],"float64"), )
paddle.nn.functional.conv2d_transpose(Tensor([1, 1024, 128, 128],"float32"), Tensor([1024, 128, 0, 3],"float32"), padding=0, stride=2, groups=4, )
paddle.nn.functional.softmax(Tensor([2, 0, 1, 6],"float32"), axis=-1, dtype="float32", )
paddle.einsum("...i->...", Tensor([2, 0, 10],"float64"), )
paddle.nn.functional.maxout(x=Tensor([10, 9, 0, 3],"float64"), groups=3, )
paddle.einsum("b h i d, b h j d -> b h i j", Tensor([1, 8, 1, 64],"float32"), Tensor([0, 8, 109, 64],"float32"), )
paddle.take(Tensor([0, 4],"float64"), Tensor([2, 3],"int64"), mode="raise", )
paddle.nn.functional.softmax(Tensor([2, 6, 0, 4],"float64"), axis=-3, dtype=None, name=None, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 0, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=True, )
paddle.nn.functional.max_pool3d(Tensor([1, 3, 4, 4, 0],"float64"), kernel_size=2, stride=2, return_mask=True, )
paddle.nn.functional.pixel_unshuffle(Tensor([2, 1, 0, 12],"float16"), 3, "NCHW", )
paddle.nn.functional.pad(x=Tensor([3, 0, 3],"float32"), pad=list[1,1,], mode="constant", value=0.0, data_format="NCL", )
paddle.nn.quant.weight_only_linear(Tensor([0, 64],"float16"), weight=Tensor([192, 64],"int8"), bias=Tensor([192],"float16"), weight_scale=Tensor([192],"float16"), weight_dtype="int8", )
paddle.Tensor.tile(Tensor([8, 0, 512],"float32"), tuple(1,3,1,), )
paddle.nn.functional.hinge_embedding_loss(Tensor([10, 10, 0],"float64"), Tensor([10, 10, 0],"float64"), reduction="mean", margin=1.0, name=None, )
paddle.einsum("m,d->md", Tensor([0],"float32"), Tensor([192],"float32"), )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[130,1,1,], )
paddle.nn.functional.channel_shuffle(Tensor([2, 4, 0, 9],"float64"), 3, "NHWC", )
paddle.nn.functional.sequence_mask(Tensor([0, 3],"int64"), maxlen=Tensor([1],"int32"), dtype=VarType(float32), )
paddle.subtract(Tensor([0, 1, 30, 30],"float32"), Tensor([0, 1, 30, 30],"float32"), )
paddle.Tensor.tile(Tensor([1, 6, 0],"float32"), list[535,1,1,], )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([6, 1, 3, 3, 0],"float32"), bias=Tensor([6],"float32"), stride=2, padding=0, groups=3, )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[198,1,1,], )
paddle.nn.functional.log_softmax(Tensor([0, 8],"float32"), axis=1, )
paddle.Tensor.__rpow__(Tensor([0],"float32"), 107177664400.00002, )
paddle.slice(Tensor([13, 0, 104],"float32"), list[0,1,2,], list[0,0,24,], list[13,1,40,], )
paddle.nn.functional.conv3d_transpose(Tensor([4, 4, 8, 8, 8],"float32"), Tensor([4, 3, 0, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[list[0,0,],list[0,0,],list[1,1,],list[1,1,],list[2,2,],], stride=1, dilation=1, groups=2, data_format="NCDHW", )
paddle.nn.functional.conv1d(Tensor([13, 256, 0],"float32"), Tensor([20, 256, 0],"float32"), bias=None, padding=2, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv3d(Tensor([4, 6, 8, 8, 8],"float32"), Tensor([8, 3, 3, 0, 3],"float32"), Tensor([8],"float32"), padding="same", stride=1, dilation=1, groups=2, data_format="NCDHW", )
paddle.nn.functional.conv2d_transpose(Tensor([1, 128, 0, 128],"float32"), Tensor([128, 3, 4, 4],"float32"), bias=Tensor([3],"float32"), padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.poisson_nll_loss(Tensor([0, 3, 2],"bfloat16"), Tensor([0, 3, 2],"float32"), )
paddle.nn.functional.hinge_embedding_loss(Tensor([0, 3, 3, 4],"float64"), Tensor([0, 3, 3, 4],"float64"), reduction="sum", margin=-4.0, name=None, )
paddle.nn.functional.pad(Tensor([0, 3, 4, 4, 4],"float32"), list[1,1,1,1,1,1,], mode="reflect", data_format="NCDHW", )
paddle.nn.functional.pad(Tensor([1, 0, 14, 18, 384],"float16"), tuple(0,3,0,0,0,0,), data_format="NDHWC", )
paddle.cumsum(x=Tensor([1, 0, 1, 3],"float64"), axis=Tensor([1],"float64"), )
paddle.nn.functional.pad(Tensor([30, 1, 0, 128],"float32"), list[2,3,2,3,], value=0, )
paddle.nn.functional.conv3d_transpose(Tensor([4, 8, 8, 8, 4],"float32"), Tensor([4, 3, 3, 0, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=0, stride=1, dilation=1, groups=2, data_format="NDHWC", )
paddle.nn.functional.l1_loss(Tensor([0, 5],"float32"), Tensor([0, 5],"float32"), reduction="mean", )
paddle.nn.functional.avg_pool2d(Tensor([2, 0, 4, 4],"float64"), kernel_size=list[3,3,], stride=list[1,1,], padding=list[1,1,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.max_pool3d(Tensor([0, 3, 4, 4, 6],"float32"), kernel_size=2, stride=2, return_mask=True, )
paddle.einsum("nbqa,ahc->nbqhc", Tensor([0, 3, 5, 6],"float16"), Tensor([6, 2, 4],"float16"), )
paddle.cummin(Tensor([100, 0],"float32"), axis=-1, )
paddle.nn.functional.conv2d_transpose(Tensor([0, 512, 32, 32],"float32"), Tensor([512, 1, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=512, output_size=None, data_format="NCHW", )
paddle.einsum("bnij,bjnd->bind", Tensor([13, 4, 2, 2],"float32"), Tensor([13, 2, 4, 0],"float32"), )
paddle.slice(Tensor([2, 0, 104],"float32"), list[0,1,2,], list[0,0,64,], list[2,2,80,], )
paddle.nn.functional.margin_ranking_loss(Tensor([0],"float32"), Tensor([0],"float32"), Tensor([0],"float32"), 0.5, "mean", None, )
paddle.Tensor.tile(Tensor([4, 0, 512],"float32"), tuple(1,11,1,), )
paddle.nn.functional.conv1d_transpose(Tensor([2, 3, 2],"float64"), Tensor([3, 0, 3],"float64"), bias=Tensor([3],"float64"), output_size=None, output_padding=0, padding=list[1,], stride=list[1,], dilation=list[1,], groups=3, data_format="NCL", )
paddle.nn.functional.log_softmax(x=Tensor([2, 0, 4],"float64"), axis=2, dtype=type(numpy.float32), )
paddle.nn.functional.conv1d(Tensor([13, 32, 0],"float32"), Tensor([32, 16, 1],"float32"), bias=Tensor([32],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=2, data_format="NCL", )
paddle.nn.functional.hinge_embedding_loss(Tensor([0, 10, 5],"float64"), Tensor([0, 10, 5],"float64"), reduction="mean", margin=1.0, name=None, )
paddle.nn.functional.max_pool3d(x=Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], )
paddle.nn.functional.pad(x=Tensor([1, 1, 1, 2, 0],"float64"), pad=tuple(0,1,1,1,2,0,), mode="circular", value=0, data_format="NDHWC", )
paddle.nn.functional.conv1d_transpose(x=Tensor([0, 3, 5],"float64"), weight=Tensor([3, 2, 3],"float64"), bias=Tensor([2],"float64"), stride=1, padding="vALiD", output_padding=0, groups=1, dilation=1, output_size=None, data_format="NCL", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[487,1,1,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[233,1,1,], )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 512, 0, 2],"float32"), output_size=tuple(7,7,), data_format="NCHW", name=None, )
paddle.nn.functional.max_pool2d(Tensor([0, 1, 4, 4],"float32"), kernel_size=2, stride=2, padding=0, return_mask=True, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 2],"float32"), weight=Tensor([3, 0, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=list[1,0,1,], dilation=2, )
paddle.einsum("...jk, ...kl->...jl", Tensor([3, 0, 3],"float64"), Tensor([3, 3, 10],"float64"), )
paddle.Tensor.lgamma(Tensor([0, 100, 100],"float64"), )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 2],"float64"), weight=Tensor([3, 0, 3, 3, 3],"float64"), bias=Tensor([3],"float64"), stride=1, padding=list[1,0,1,], groups=3, dilation=1, )
paddle.subtract(Tensor([0, 3, 256, 256],"float32"), Tensor([0, 3, 256, 256],"float32"), )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 0],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=0, dilation=1, )
paddle.nn.functional.poisson_nll_loss(Tensor([10, 0],"float32"), Tensor([10, 0],"float32"), log_input=True, full=True, epsilon=1e-08, reduction="mean", name=None, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([4, 134, 4, 128],"float16"), Tensor([4, 134, 4, 128],"float16"), Tensor([4, 134, 4, 0],"float16"), attn_mask=Tensor([4, 1, 134, 134],"float16"), dropout_p=0.0, training=True, is_causal=False, )
paddle.nn.functional.conv2d_transpose(Tensor([0, 2048, 128, 128],"float32"), Tensor([2048, 128, 3, 3],"float32"), padding=0, stride=2, groups=8, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 4, 128],"float16"), Tensor([1, 2048, 4, 128],"float16"), Tensor([1, 0, 4, 128],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.quant.weight_only_linear(Tensor([2, 1, 64],"float16"), weight=Tensor([192, 64],"int8"), bias=Tensor([192],"float16"), weight_scale=Tensor([0],"float16"), weight_dtype="int8", )
paddle.nn.functional.lp_pool1d(Tensor([0, 3, 32],"float16"), norm_type=5, kernel_size=5, stride=3, padding=list[0,], )
paddle.Tensor.tile(Tensor([1, 1, 0, 28],"float32"), list[1,3,1,1,], )
paddle.nn.functional.temporal_shift(Tensor([128, 256, 28, 0],"float32"), 16, 0.0625, )
paddle.nn.functional.sequence_mask(Tensor([0, 3],"int64"), maxlen=Tensor([1],"int32"), dtype=VarType(float64), )
paddle.allclose(tuple(Tensor([0, 3],"float32"),), tuple(Tensor([0, 3],"float32"),), rtol=0.0001, atol=0.0001, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[200,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([2, 6, 2, 2],"float64"), Tensor([6, 1, 3, 0],"float64"), bias=Tensor([3],"float64"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=3, output_size=None, data_format="NCHW", )
paddle.kthvalue(x=Tensor([3, 0, 4],"float64"), k=4, axis=2, keepdim=True, )
paddle.nn.functional.conv2d_transpose(Tensor([0, 64, 32, 32],"float32"), Tensor([64, 1, 8, 8],"float32"), bias=None, padding=2, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=64, output_size=None, data_format="NCHW", )
paddle.nn.functional.sequence_mask(Tensor([2, 0, 3, 3, 3],"float64"), maxlen=5, dtype=type(numpy.int32), )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 8, 8, 8],"float32"), Tensor([3, 2, 3, 0, 3],"float32"), bias=Tensor([6],"float32"), padding="valid", output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=3, output_size=None, data_format="NCDHW", )
paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 0, 1, 109],"float32"), Tensor([1, 0, 109, 64],"float32"), )
paddle.nn.functional.max_pool1d(Tensor([91, 0, 7],"float32"), 7, )
paddle.Tensor.tile(Tensor([12240, 0],"float32"), list[1,19,], )
paddle.Tensor.__sub__(Tensor([3, 6, 3, 4, 0, 5],"float64"), Tensor([3, 6, 3, 4, 0, 5],"float64"), )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 8, 3, 0, 3],"float32"), bias=Tensor([8],"float32"), padding=list[1,2,2,3,2,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.pad(Tensor([0, 3, 4, 4],"float64"), list[1,1,1,1,], mode="circular", data_format="NCHW", )
paddle.nn.functional.pad(Tensor([3, 1, 3, 0, 40],"float32"), pad=list[0,0,0,0,2,2,], data_format="NCDHW", )
paddle.nn.functional.hinge_embedding_loss(Tensor([4, 3, 3, 0],"float64"), Tensor([4, 3, 3, 0],"float64"), reduction="none", margin=-4.0, name=None, )
paddle.nn.functional.conv1d(Tensor([4, 6, 0],"float32"), Tensor([8, 6, 3],"float32"), bias=Tensor([8],"float32"), padding="same", stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.maxout(x=Tensor([10, 9, 3, 0],"float64"), groups=3, )
paddle.nn.functional.pad(Tensor([0, 16, 14, 14, 384],"float16"), tuple(0,0,0,0,0,0,), data_format="NDHWC", )
paddle.nn.functional.log_softmax(Tensor([5, 3, 0, 4],"float32"), 1, )
paddle.nn.functional.max_pool3d(Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.nn.functional.conv2d(x=Tensor([2, 4, 4, 3],"float64"), weight=Tensor([2, 3, 3, 0],"float64"), bias=Tensor([2],"float64"), stride=1, padding=0, data_format="NHWC", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[659,1,1,], )
paddle.nn.functional.log_softmax(x=Tensor([2, 3, 0],"float64"), axis=2, dtype=type(numpy.float64), )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([512, 0, 2, 25],"float16"), output_size=list[1,25,], data_format="NCHW", name=None, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 0, 4],"float64"), weight=Tensor([1, 3, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=list[2,2,1,], padding=1, dilation=list[2,2,2,], )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 0, 16, 7, 10],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
paddle.nn.functional.softmax(Tensor([104, 16, 0, 18],"float32"), )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([0],"float64"), stride=1, padding=list[1,0,], dilation=1, output_size=None, data_format="NCHW", )
paddle.frexp(Tensor([4, 0, 2],"float32"), )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 0, 16],"float32"), Tensor([6, 4, 3, 3],"float32"), bias=Tensor([8],"float32"), padding="valid", output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=2, output_size=None, data_format="NCHW", )
paddle.einsum("ij,ij->j", Tensor([10, 0],"float64"), Tensor([1, 0],"float64"), )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[873,1,1,], )
paddle.allclose(tuple(Tensor([13, 0],"float32"),Tensor([13, 0],"float32"),), tuple(Tensor([13, 0],"float32"),Tensor([13, 0],"float32"),), rtol=0.0001, atol=0.0001, )
paddle.nn.functional.pad(Tensor([1, 0, 14, 18, 384],"float32"), tuple(0,3,0,0,0,0,), data_format="NDHWC", )
paddle.unique(Tensor([0],"int64"), return_index=True, return_inverse=True, return_counts=True, dtype="int32", )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 4, 0, 3],"float64"), output_size=3, data_format="NHWC", )
paddle.nn.functional.layer_norm(Tensor([0, 3, 10, 10],"float32"), list[3,10,10,], Tensor([300],"float32"), Tensor([300],"float32"), )
paddle.nn.functional.conv1d_transpose(Tensor([4, 6, 0],"float32"), Tensor([6, 8, 3],"float32"), bias=Tensor([8],"float32"), output_size=None, output_padding=0, padding=0, stride=list[2,], dilation=list[2,], groups=1, data_format="NCL", )
paddle.Tensor.tile(Tensor([2, 0, 2],"float32"), list[1,1,2,], )
paddle.Tensor.trunc(Tensor([28, 0],"float32"), )
paddle.slice(Tensor([4, 10, 0],"float32"), list[0,], list[2,], list[4,], )
paddle.nanmedian(Tensor([0, 3],"float64"), axis=1, keepdim=False, )
paddle.nn.functional.pad(Tensor([16, 25500, 0],"float32"), pad=list[512,512,], mode="reflect", data_format="NLC", )
paddle.nn.functional.conv2d(Tensor([1, 2048, 128, 0],"float32"), Tensor([24, 256, 1, 1],"float32"), padding=0, groups=8, )
paddle.nn.functional.max_pool1d(x=Tensor([2, 0, 8],"float64"), kernel_size=2, stride=2, padding=0, )
paddle.nn.functional.pad(Tensor([3, 1, 0, 3, 1600],"float32"), pad=list[0,0,0,0,2,2,], data_format="NCDHW", )
paddle.nonzero(Tensor([10, 2, 28, 0],"float32"), )
paddle.searchsorted(sorted_sequence=Tensor([5],"float32"), values=Tensor([0, 2],"float32"), )
paddle.Tensor.tile(Tensor([2, 1, 0],"float32"), list[1,200,1,], )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[1466,1,1,], )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 2],"float64"), weight=Tensor([3, 2, 0],"float64"), bias=Tensor([1],"float64"), stride=1, padding=0, dilation=1, )
paddle.nn.functional.kl_div(Tensor([40, 0, 50],"float32"), Tensor([40, 0, 50],"float32"), "batchmean", False, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[2700,1,1,], )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[289,1,1,], )
paddle.nn.functional.softmax(x=Tensor([2, 3, 0],"float64"), axis=0, )
paddle.nanmedian(Tensor([2, 3, 4, 0],"float32"), axis=0, keepdim=False, mode="min", )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([0, 3, 8, 32, 32],"float32"), output_size=list[3,3,3,], data_format="NCDHW", name=None, )
paddle.nn.functional.pad(Tensor([0, 25757, 3],"float32"), list[1,0,], value=5, mode="constant", data_format="NCL", )
paddle.vision.ops.generate_proposals(Tensor([1, 15, 42, 63],"float32"), Tensor([1, 60, 42, 63],"float32"), Tensor([1, 0],"float32"), Tensor([39690, 4],"float32"), Tensor([39690, 4],"float32"), pre_nms_top_n=12000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 1, 2, 40],"float16"), Tensor([1, 1, 2, 0],"float16"), Tensor([1, 1, 2, 40],"float16"), attn_mask=Tensor([1, 2, 1, 1],"float16"), )
paddle.nn.functional.max_pool3d(Tensor([1, 3, 0, 4, 6],"float32"), kernel_size=2, stride=2, return_mask=True, )
paddle.slice(Tensor([2, 8, 0],"float32"), axes=list[1,], starts=list[1,], ends=list[2,], )
paddle.nn.functional.pad(Tensor([1048576, 1, 0, 1],"float32"), list[0,0,0,0,], )
paddle.nn.functional.l1_loss(Tensor([0, 10, 5],"float32"), Tensor([0, 10, 5],"float32"), reduction="none", )
paddle.Tensor.imag(Tensor([16, 0, 107],"complex64"), )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 16],"float32"), Tensor([6, 0, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[1,2,], dilation=list[2,2,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.lp_pool1d(Tensor([2, 0, 32],"float32"), 7.0, 2, None, 1, True, "NCL", None, )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[263,1,1,], )
paddle.nn.functional.max_pool3d(Tensor([8, 0, 16, 112, 112],"float32"), kernel_size=tuple(3,3,3,), stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.slice(Tensor([2, 8, 0],"float32"), axes=list[1,], starts=list[7,], ends=list[8,], )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 2],"float32"), Tensor([3, 10, 0, 27],"float32"), bias=Tensor([10],"float32"), padding=0, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 3, 7, 0],"float32"), output_size=list[3,3,], data_format="NHWC", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 6, 2, 2],"float32"), Tensor([6, 1, 3, 0],"float32"), bias=Tensor([3],"float32"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=3, output_size=None, data_format="NCHW", )
paddle.nn.functional.fractional_max_pool3d(x=Tensor([0, 3, 7, 7, 7],"float32"), output_size=list[None,3,3,], random_u=0.6, )
paddle.nn.functional.conv2d(Tensor([2, 192, 0, 4],"float32"), Tensor([96, 192, 0, 1],"float32"), None, list[1,1,], 0, list[1,1,], 1, "NCHW", )
paddle.nn.functional.avg_pool2d(Tensor([64, 0, 7, 7],"float16"), kernel_size=7, stride=1, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.max_pool2d(Tensor([2, 0, 55, 55],"float32"), kernel_size=3, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([1, 0, 28, 28],"float32"), list[1,3,1,1,], )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 0, 32],"float64"), output_size=list[2,3,3,], data_format="NCDHW", name=None, )
paddle.nn.quant.weight_only_linear(Tensor([1, 0, 128],"float16"), Tensor([288, 128],"int8"), bias=Tensor([288],"float16"), weight_scale=Tensor([288],"float16"), weight_dtype="int8", group_size=-1, )
paddle.nn.quant.weight_quantize(Tensor([11008, 0],"float16"), algo="weight_only_int8", )
paddle.einsum("ij,...i->j...", Tensor([4, 5],"float64"), Tensor([0, 2, 4],"float64"), )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 3, 0, 7],"float32"), output_size=list[3,3,], data_format="NHWC", )
paddle.nn.functional.conv2d(Tensor([3, 3, 32, 0],"float32"), Tensor([64, 3, 7, 7],"float32"), None, list[2,2,], 3, list[1,1,], 1, "NCHW", )
paddle.Tensor.tile(Tensor([1, 6, 0],"float32"), list[398,1,1,], )
paddle.Tensor.flip(Tensor([16, 0, 224, 224],"float32"), 0, )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 2],"float32"), weight=Tensor([3, 0, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=0, dilation=1, )
paddle.Tensor.fill_diagonal_(Tensor([0, 2, 2],"float32"), 1, 0, False, )
paddle.nanmedian(Tensor([2, 0, 4, 5],"float32"), axis=list[0,1,3,], keepdim=False, mode="min", )
paddle.nn.functional.layer_norm(Tensor([0, 64, 128],"float32"), list[64,128,], Tensor([8192],"float32"), Tensor([8192],"float32"), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[250,1,1,], )
paddle.einsum("ij->ji", Tensor([0, 5],"float64"), )
paddle.incubate.nn.functional.fused_feedforward(x=Tensor([0, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, )
paddle.nonzero(Tensor([1, 2, 28, 0],"float32"), )
paddle.Tensor.tile(Tensor([10105, 0],"float32"), list[1,4,], )
paddle.nn.functional.max_pool3d(Tensor([2, 3, 0, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,1,1,], return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.nn.functional.max_pool2d(Tensor([2, 4, 40, 0],"float64"), kernel_size=tuple(2,4,), stride=None, padding=0, return_mask=True, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.softmax(Tensor([0, 1, 101],"float32"), axis=-1, dtype=Dtype(float32), )
paddle.nn.functional.max_pool2d(Tensor([2, 3, 0, 33],"float32"), kernel_size=5, stride=5, padding=0, ceil_mode=True, return_mask=True, )
paddle.nn.functional.max_pool2d(Tensor([1, 0, 32, 32],"float32"), kernel_size=2, stride=2, padding=0, return_mask=True, ceil_mode=False, data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([1, 1024, 2, 1, 0],"float16"), list[1,1,1,4,1,], )
paddle.nn.functional.fractional_max_pool3d(x=Tensor([0, 3, 7, 7, 7],"float32"), output_size=list[3,3,3,], random_u=0.3, )
paddle.Tensor.tile(Tensor([1, 7, 0],"float32"), list[405,1,1,], )
paddle.nn.functional.pad(Tensor([1, 16, 18, 14, 0],"float32"), tuple(0,0,0,3,0,0,), data_format="NDHWC", )
paddle.Tensor.tile(Tensor([1, 33, 0],"float32"), list[23,1,1,], )
paddle.nn.functional.conv2d(Tensor([1, 256, 128, 0],"float32"), Tensor([3, 256, 1, 0],"float32"), padding=0, groups=1, )
paddle.Tensor.tile(Tensor([10285, 0],"float32"), list[1,58,], )
paddle.Tensor.tile(Tensor([13, 1, 0],"float32"), repeat_times=list[1,12,1,], )
paddle.einsum("ijkl, lmn->ijn", Tensor([2, 4, 5, 3],"float64"), Tensor([3, 0, 4],"float64"), )
paddle.nn.functional.pad(Tensor([12288, 0, 1, 1],"float32"), list[0,1,0,1,], )
paddle.nn.functional.pad(Tensor([13, 64, 0],"float32"), tuple(1,0,), data_format="NCL", )
paddle.nn.functional.pad(Tensor([0, 4, 5],"complex64"), pad=list[1,2,], mode="constant", value=100, data_format="NCL", name=None, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([0, 128, 32, 32],"float32"), output_size=tuple(6,6,), data_format="NCHW", name=None, )
paddle.nanmedian(Tensor([0, 3, 4, 5],"float32"), axis=-2, keepdim=False, mode="min", )
paddle.nn.functional.group_norm(Tensor([2, 4, 3, 2, 0],"float64"), num_groups=2, weight=Tensor([4],"float64"), bias=Tensor([4],"float64"), data_format="NCDHW", )
paddle.Tensor.tile(Tensor([1, 4, 0],"float32"), list[406,1,1,], )
paddle.nn.functional.max_pool3d(Tensor([2, 0, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, )
paddle.nn.functional.pixel_shuffle(Tensor([4, 128, 128, 0],"float16"), 2, "NCHW", None, )
paddle.nn.functional.margin_ranking_loss(Tensor([10, 0],"float64"), Tensor([10, 0],"float64"), Tensor([10, 0],"float64"), 0.2, "mean", )
paddle.slice(Tensor([2, 8, 0],"float32"), axes=list[1,], starts=list[5,], ends=list[6,], )
paddle.cummax(Tensor([100, 0],"float32"), axis=-1, )
paddle.nn.functional.pixel_unshuffle(Tensor([2, 1, 0, 12],"float64"), 3, "NCHW", None, )
paddle.nonzero(Tensor([0, 10],"float32"), as_tuple=False, )
paddle.Tensor.tile(Tensor([10105, 0],"float32"), list[1,7,], )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([1, 3, 3, 3, 3],"float32"), bias=Tensor([0],"float32"), stride=1, padding=0, )
paddle.nn.functional.conv2d_transpose(Tensor([0, 6, 16, 16],"float32"), Tensor([6, 8, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[1,2,], dilation=list[2,2,], groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.__rpow__(Tensor([0, 2, 2],"float32"), 3.0, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[520,1,1,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[528,1,1,], )
paddle.nn.functional.avg_pool2d(Tensor([256, 128, 0, 50],"float16"), kernel_size=tuple(2,1,), stride=tuple(2,1,), padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.einsum("b h i d, b h j d -> b h i j", Tensor([1, 8, 0, 64],"float32"), Tensor([1, 8, 0, 64],"float32"), )
paddle.nn.functional.conv2d_transpose(Tensor([2, 6, 2, 0],"float64"), Tensor([6, 1, 3, 3],"float64"), bias=Tensor([3],"float64"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=3, output_size=None, data_format="NCHW", )
paddle.nn.functional.poisson_nll_loss(Tensor([10, 0],"float16"), Tensor([10, 0],"float16"), log_input=True, full=False, epsilon=1e-08, reduction="mean", name=None, )
paddle.nn.functional.poisson_nll_loss(Tensor([10, 0],"float32"), Tensor([10, 0],"float32"), log_input=False, full=False, epsilon=1e-08, reduction="mean", )
paddle.incubate.nn.functional.blha_get_max_len(Tensor([0],"int32"), Tensor([10],"int32"), Tensor([10],"float32"), )
paddle.einsum("ii->", Tensor([0, 5],"float64"), )
paddle.nn.functional.max_pool3d(x=Tensor([0, 64, 4, 112, 112],"float32"), kernel_size=list[1,3,3,], stride=list[1,2,2,], padding=list[0,1,1,], data_format="NCDHW", )
paddle.nn.functional.pixel_shuffle(Tensor([4, 0, 4, 4],"float64"), 3, "NCHW", None, )
paddle.Tensor.subtract(Tensor([2, 3, 0],"float32"), Tensor([2, 3, 0],"float32"), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[107,1,1,], )
paddle.nn.functional.conv3d_transpose(Tensor([0, 3, 2, 2, 2],"float32"), Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[2,2,2,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.avg_pool1d(Tensor([2, 0, 8],"float64"), 1, 1, 0, True, False, None, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 2048, 62, 0],"float32"), output_size=tuple(1,1,), data_format="NCHW", name=None, )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 0, 32, 32],"float64"), output_size=list[1,3,2,], )
paddle.nn.functional.pad(Tensor([3, 0, 40, 40, 3],"float32"), pad=list[2,2,0,0,0,0,], data_format="NCDHW", )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 2],"float64"), Tensor([3, 1, 3, 0],"float64"), bias=Tensor([1],"float64"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[2,2,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([0, 3, 2, 2],"float32"), Tensor([3, 1, 3, 3],"float32"), bias=None, padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv1d(Tensor([13, 20, 2048],"float32"), Tensor([256, 20, 0],"float32"), bias=None, padding=2, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.pad(Tensor([0, 1, 2, 3, 2],"float64"), pad=list[1,0,1,2,1,0,], mode="constant", value=0.0, data_format="NCDHW", name=None, )
paddle.nn.functional.pad(Tensor([1, 1, 2, 3, 0],"float64"), pad=list[1,0,1,0,0,1,], mode="replicate", value=0.0, data_format="NDHWC", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([0, 128, 128, 128],"float32"), Tensor([128, 3, 4, 4],"float32"), bias=Tensor([3],"float32"), padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.subtract(Tensor([0],"float32"), Tensor([0],"float32"), name="Cauchy_kl_divergence", )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 0],"float32"), weight=Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([3],"float32"), stride=1, padding=list[1,0,1,], groups=3, dilation=1, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 8, 16],"float16"), Tensor([1, 2048, 0, 16],"float16"), Tensor([1, 2048, 2, 16],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), is_causal=True, )
paddle.nn.functional.pad(Tensor([3, 1, 40, 40, 0],"float32"), pad=list[2,2,0,0,0,0,], data_format="NCDHW", )
paddle.flip(Tensor([20, 3, 112, 0],"float32"), axis=-1, )
paddle.nn.functional.group_norm(Tensor([2, 4, 3, 0, 2],"float64"), num_groups=2, weight=Tensor([4],"float64"), bias=Tensor([4],"float64"), data_format="NCDHW", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[362,1,1,], )
paddle.einsum("blkd,bldq->blkq", Tensor([13, 5, 0, 1],"float32"), Tensor([13, 5, 1, 3],"float32"), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[461,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 16],"float32"), Tensor([6, 4, 3, 0],"float32"), bias=Tensor([8],"float32"), padding="valid", output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=2, output_size=None, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([0, 16, 15, 14, 384],"float16"), tuple(0,0,0,6,0,0,), data_format="NDHWC", )
paddle.Tensor.__rpow__(Tensor([0],"float32"), 500000.0, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 8, 0, 8],"float32"), Tensor([3, 2, 3, 3, 3],"float32"), bias=Tensor([6],"float32"), padding="valid", output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=3, output_size=None, data_format="NCDHW", )
paddle.nn.functional.pad(Tensor([16, 0, 96],"float32"), pad=tuple(0,8,), mode="replicate", value=0.0, data_format="NCL", name=None, )
paddle.Tensor.tile(Tensor([1, 6, 0],"float32"), list[475,1,1,], )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 0],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=True, )
paddle.nn.functional.conv3d_transpose(Tensor([4, 4, 0, 8, 8],"float32"), Tensor([4, 3, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,2,1,], stride=1, dilation=1, groups=2, data_format="NCDHW", )
paddle.nn.functional.conv2d(Tensor([2, 24, 8, 0],"float32"), Tensor([24, 24, 3, 0],"float32"), None, list[1,1,], 1, list[1,1,], 1, "NCHW", )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float64"), weight=Tensor([1, 3, 3, 3, 0],"float64"), bias=Tensor([1],"float64"), stride=2, padding=0, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 0, 128, 128],"float32"), output_size=tuple(2,2,), data_format="NCHW", name=None, )
paddle.nn.functional.maxout(x=Tensor([100, 4, 3, 0],"float64"), groups=2, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([4, 134, 4, 128],"float16"), Tensor([4, 134, 4, 128],"float16"), Tensor([4, 0, 4, 128],"float16"), attn_mask=Tensor([4, 1, 134, 134],"float16"), dropout_p=0.0, training=True, is_causal=False, )
paddle.nn.functional.pad(Tensor([2, 0, 4, 4, 4],"float64"), list[1,1,1,1,1,1,], mode="reflect", data_format="NCDHW", )
paddle.nn.quant.weight_quantize(Tensor([64, 0],"float16"), algo="weight_only_int4", arch=75, group_size=-1, )
paddle.isin(Tensor([0, 8],"bfloat16"), Tensor([2, 3],"bfloat16"), False, False, )
paddle.nn.functional.poisson_nll_loss(Tensor([10, 0],"float32"), Tensor([10, 0],"float32"), log_input=True, full=False, epsilon=1e-08, reduction="mean", )
paddle.nn.functional.conv2d_transpose(Tensor([2, 2, 3, 3],"float64"), Tensor([2, 0, 1, 1],"float64"), groups=1, padding="SAME", )
paddle.nn.functional.pixel_shuffle(Tensor([2, 9, 4, 0],"float64"), 3, "NCHW", None, )
paddle.nn.functional.conv3d(Tensor([4, 6, 8, 8, 8],"float32"), Tensor([8, 3, 3, 3, 0],"float32"), Tensor([8],"float32"), padding="same", stride=1, dilation=1, groups=2, data_format="NCDHW", )
paddle.Tensor.__setitem__(Tensor([6, 5, 0, 3],"bool"), list[list[2,3,4,],list[1,2,5,],], 100, )
paddle.Tensor.tile(Tensor([1000, 0, 80],"float32"), list[1,5,1,], )
paddle.nn.functional.pad(Tensor([3, 4, 5, 0],"complex128"), pad=list[1,1,1,1,], mode="constant", value=100, data_format="NCHW", name=None, )
paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 0],"float16"), bias=Tensor([512],"float16"), act_method="geglu", compute_dtype="default", )
paddle.nn.functional.conv2d(Tensor([4, 16, 16, 3],"float32"), Tensor([5, 3, 0, 3],"float32"), Tensor([5],"float32"), padding=list[1,2,3,1,], stride=2, dilation=1, groups=1, data_format="NHWC", )
paddle.slice(Tensor([0, 5, 6],"float32"), axes=list[0,1,2,], starts=list[-3,0,2,], ends=list[3,2,4,], )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[330,1,1,], )
paddle.Tensor.__rpow__(Tensor([4, 0, 2],"float64"), 2, )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[201,1,1,], )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([1, 3, 3, 3, 0],"float32"), bias=Tensor([1],"float32"), stride=list[2,2,1,], padding=list[1,2,2,], dilation=tuple(2,2,2,), )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 8, 0, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[2,2,2,], dilation=list[2,1,2,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.max_pool2d(Tensor([13, 0, 7, 1],"float32"), tuple(2,1,), stride=tuple(2,1,), ceil_mode=True, )
paddle.nn.functional.pad(Tensor([1, 160000, 0],"float64"), pad=list[256,256,], mode="reflect", data_format="NLC", )
paddle.nn.functional.temporal_shift(x=Tensor([0, 2, 4, 3],"float64"), seg_num=2, )
paddle.nn.functional.conv1d_transpose(Tensor([4, 6, 0],"float32"), Tensor([6, 8, 3],"float32"), bias=Tensor([8],"float32"), output_size=None, output_padding=0, padding="valid", stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv2d(Tensor([1, 6, 16, 16],"float32"), Tensor([16, 6, 0, 5],"float32"), bias=Tensor([16],"float32"), padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, data_format="NCHW", )
paddle.einsum("a...b,b...c,c...d", Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 0, 4],"float64"), )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 4, 0, 3],"float64"), output_size=tuple(3,3,), data_format="NHWC", name=None, )
paddle.nn.functional.avg_pool3d(Tensor([3, 0, 7, 40, 40],"float32"), kernel_size=tuple(5,1,1,), stride=1, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 0, 8, 96],"float16"), Tensor([2, 0, 8, 96],"float16"), Tensor([2, 0, 8, 96],"float16"), attn_mask=None, is_causal=False, )
paddle.allclose(Tensor([0, 16],"float32"), Tensor([0, 16],"float32"), )
paddle.nn.functional.pad(Tensor([1, 16, 15, 14, 0],"float32"), tuple(0,0,0,6,0,0,), data_format="NDHWC", )
paddle.subtract(Tensor([0, 96, 1],"float32"), Tensor([0, 96, 1],"float32"), )
paddle.Tensor.imag(Tensor([0],"complex128"), )
paddle.nn.functional.avg_pool2d(x=Tensor([2, 0, 32, 32],"float64"), kernel_size=list[2,2,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[502,1,1,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[193,1,1,], )
paddle.nn.functional.pad(Tensor([0, 3, 32, 224, 271],"float32"), tuple(0,1,0,0,0,0,), data_format="NCDHW", )
paddle.Tensor.tile(Tensor([12906, 0, 1],"float32"), list[1,215,1,], )
paddle.slice(Tensor([3, 0, 104],"float32"), list[0,1,2,], list[0,0,24,], list[3,1,40,], )
paddle.Tensor.tile(Tensor([2, 13, 0, 14],"float32"), list[1,4,1,1,], )
paddle.nn.functional.max_pool1d(x=Tensor([1, 1, 0],"float64"), kernel_size=2, stride=2, padding=0, return_mask=True, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 0],"float64"), weight=Tensor([1, 3, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=2, padding=0, )
paddle.allclose(Tensor([0],"bool"), Tensor([0],"bool"), 0.0, 0.0, False, )
paddle.vision.ops.generate_proposals(Tensor([1, 0, 10, 15],"float32"), Tensor([1, 12, 10, 15],"float32"), Tensor([1, 2],"float32"), Tensor([450, 4],"float32"), Tensor([450, 4],"float32"), pre_nms_top_n=2000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.cummin(Tensor([0, 100],"float32"), axis=-1, )
paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 3, 5, 0, 4],"float16"), Tensor([1, 3, 5, 0, 4],"float16"), )
paddle.nn.functional.conv2d(Tensor([4, 16, 0, 3],"float32"), Tensor([5, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[1,1,],list[2,2,],list[0,0,],], stride=1, dilation=1, groups=1, data_format="NHWC", )
paddle.nn.functional.pad(Tensor([0, 1, 96],"float32"), pad=tuple(0,8,), mode="replicate", value=0.0, data_format="NCL", name=None, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[444,1,1,], )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 2],"float32"), weight=Tensor([3, 1, 3],"float32"), bias=Tensor([0],"float32"), stride=2, padding=list[1,], groups=1, dilation=1, )
paddle.matmul(Tensor([2],"float32"), Tensor([0],"float32"), False, False, )
paddle.nn.functional.pad(Tensor([1, 3, 0, 204],"float64"), pad=list[52,52,0,0,], mode="constant", value=0.0, data_format="NCHW", )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 4, 128],"float16"), Tensor([0, 2048, 4, 128],"float16"), Tensor([1, 2048, 4, 128],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), dropout_p=0.0, training=True, is_causal=False, )
paddle.nanmedian(Tensor([2, 3, 4, 0],"float32"), axis=-2, keepdim=False, )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([0, 3, 8, 32, 32],"float64"), output_size=list[1,3,2,], )
paddle.nn.functional.lp_pool2d(Tensor([2, 0, 32, 32],"float32"), 2, kernel_size=2, stride=None, ceil_mode=False, )
paddle.nn.functional.avg_pool2d(Tensor([64, 0, 7, 7],"float32"), kernel_size=7, stride=1, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 2, 2, 2, 3],"float64"), weight=Tensor([3, 1, 3, 3, 0],"float64"), bias=Tensor([3],"float64"), output_padding=1, stride=2, padding=list[1,1,1,], groups=3, data_format="NDHWC", dilation=1, )
paddle.einsum("ij,kj->ik", Tensor([4, 0],"float64"), Tensor([2, 0],"float64"), )
paddle.nn.functional.pad(Tensor([3, 0, 5, 6],"complex128"), pad=list[1,2,2,1,], mode="reflect", value=0.0, data_format="NCHW", name=None, )
paddle.einsum("...ij,...i->j...", Tensor([10, 11],"float64"), Tensor([0, 4, 5, 10],"float64"), )
paddle.nn.functional.softmax(x=Tensor([13, 0, 12, 12],"float32"), axis=-1, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 8, 3, 3, 0],"float32"), bias=Tensor([8],"float32"), padding="valid", output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 0],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0010981468949466944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.slice(Tensor([0, 6],"float32"), axes=list[1,], starts=list[4,], ends=list[5,], )
paddle.nn.functional.pad(Tensor([16, 3, 256, 0],"float32"), list[14,14,14,14,], )
paddle.nn.functional.pad(Tensor([52, 7, 0],"float32"), pad=list[1,1,], data_format="NLC", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 0, 2, 2],"float64"), Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([3],"float64"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=3, output_size=None, data_format="NCDHW", )
paddle.Tensor.fill_diagonal_(Tensor([3, 0],"float32"), 1, 0, False, )
paddle.slice(Tensor([2, 8, 0],"float32"), axes=list[1,], starts=list[3,], ends=list[4,], )
paddle.nn.functional.max_pool2d(Tensor([64, 0, 1, 40],"float32"), kernel_size=tuple(1,1,), stride=1, padding=0, )
paddle.nn.functional.sequence_mask(Tensor([0, 3],"int64"), maxlen=Tensor([1],"int32"), dtype=VarType(bool), )
paddle.nn.functional.pad(x=Tensor([0, 1, 1, 2, 3],"float64"), pad=list[0,0,1,1,0,0,], mode="constant", value=0, data_format="NCDHW", )
paddle.nn.functional.pad(Tensor([0, 4, 5, 6, 7],"complex64"), pad=list[1,2,2,1,1,0,], mode="constant", value=100, data_format="NCDHW", name=None, )
paddle.nn.functional.max_pool2d(Tensor([0, 64, 320, 432],"float32"), kernel_size=3, stride=2, padding=1, )
paddle.nn.functional.conv2d(Tensor([1, 3, 224, 224],"float32"), Tensor([3, 3, 3, 0],"float32"), Tensor([3],"float32"), list[3,2,], 0, list[1,1,], 1, "NCHW", )
paddle.nn.functional.l1_loss(Tensor([10, 10, 0],"float32"), Tensor([10, 10, 0],"float32"), "sum", name=None, )
paddle.nn.functional.selu(x=Tensor([3, 0, 3],"float64"), alpha=0, scale=1.0507009873554805, )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 0],"float64"), weight=Tensor([3, 1, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,], groups=3, dilation=1, )
paddle.nn.functional.conv2d(Tensor([8, 128, 0, 255],"float32"), Tensor([256, 128, 1, 1],"float32"), bias=None, stride=2, padding=0, )
paddle.Tensor.tile(Tensor([1001, 0, 80],"float32"), list[1,3,1,], )
paddle.nn.functional.max_pool3d(Tensor([1, 3, 0, 4, 6],"float64"), kernel_size=2, stride=2, return_mask=True, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[489,1,1,], )
paddle.Tensor.tile(Tensor([5358, 0, 1],"float32"), list[1,180,1,], )
paddle.nn.functional.conv2d(Tensor([8, 3, 256, 0],"float32"), Tensor([128, 3, 1, 1],"float32"), bias=None, stride=1, padding=0, )
paddle.incubate.nn.functional.fused_feedforward(Tensor([0, 128, 768],"float16"), Tensor([768, 3072],"float16"), Tensor([3072, 768],"float16"), Tensor([3072],"float16"), Tensor([768],"float16"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )
paddle.nn.functional.max_pool3d(Tensor([8, 64, 0, 112, 112],"float32"), kernel_size=tuple(3,3,3,), stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.Tensor.tile(Tensor([1, 0, 4, 4],"float32"), tuple(8,1,1,1,), )
paddle.nn.functional.conv2d_transpose(Tensor([4, 1024, 16, 16],"float32"), Tensor([1024, 0, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1024, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 8, 0, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=list[1,2,2,3,2,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.sequence_mask(Tensor([2, 2, 0, 3, 3],"float64"), maxlen=5, dtype=type(numpy.int32), )
paddle.searchsorted(Tensor([1024],"bfloat16"), Tensor([0],"bfloat16"), )
paddle.nn.functional.l1_loss(Tensor([10, 0, 2],"float32"), Tensor([10, 0, 2],"float32"), "sum", name=None, )
paddle.nanmedian(Tensor([2, 3, 0, 5],"float32"), axis=0, keepdim=False, mode="min", )
paddle.Tensor.tile(Tensor([1, 6, 0],"float32"), list[712,1,1,], )
paddle.nn.functional.lp_pool1d(Tensor([0, 32, 3],"float32"), 7.0, 2, None, 1, False, "NLC", None, )
paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 0],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0009928022045642138, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.nn.functional.conv3d(Tensor([4, 8, 8, 8, 3],"float32"), Tensor([5, 3, 3, 3, 0],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[1,1,],list[2,2,],list[1,1,],list[0,0,],], stride=1, dilation=1, groups=1, data_format="NDHWC", )
paddle.reverse(Tensor([0, 1, 4],"float32"), axis=1, )
paddle.nn.functional.conv1d(Tensor([13, 256, 0],"float32"), Tensor([20, 256, 5],"float32"), bias=None, padding=2, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[508,1,1,], )
paddle.Tensor.tile(Tensor([4, 0, 64, 64],"float32"), list[1,72,1,1,], )
paddle.nn.functional.avg_pool2d(Tensor([256, 0, 8, 50],"float16"), kernel_size=tuple(2,1,), stride=tuple(2,1,), padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.temporal_shift(x=Tensor([4, 4, 3, 0],"float64"), seg_num=4, )
paddle.nn.functional.selu(Tensor([3, 0, 3],"float64"), 1.0507009873554805, 1.6732632423543772, None, )
paddle.slice(Tensor([0, 1, 104],"float32"), list[0,1,2,], list[0,0,80,], list[16,1,96,], )
paddle.nn.functional.pixel_unshuffle(Tensor([2, 12, 0, 1],"float64"), 3, "NHWC", None, )
paddle.frexp(Tensor([4, 5, 0],"float32"), )
paddle.nn.functional.pad(Tensor([3, 0, 3, 40, 40],"float32"), pad=list[0,0,0,0,2,2,], data_format="NCDHW", )
paddle.nn.functional.local_response_norm(x=Tensor([0, 40, 40],"float32"), size=5, data_format="NLC", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[423,1,1,], )
paddle.nn.functional.max_pool3d(Tensor([1, 3, 4, 0, 6],"float32"), kernel_size=2, stride=2, return_mask=True, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 2, 2, 6],"float32"), Tensor([6, 0, 3, 3],"float32"), bias=None, padding=list[1,0,], output_padding=1, stride=list[2,2,], dilation=list[1,1,], groups=3, output_size=None, data_format="NHWC", )
paddle.nanmedian(Tensor([2, 0, 4, 5],"float32"), axis=list[0,2,1,3,], keepdim=False, mode="min", )
paddle.nn.functional.sequence_mask(Tensor([0],"int32"), )
paddle.nn.functional.max_pool2d(Tensor([1, 64, 288, 0],"float32"), kernel_size=3, stride=2, padding=1, )
paddle.nn.functional.avg_pool2d(x=Tensor([0, 3, 4, 4],"float64"), kernel_size=list[3,3,], stride=list[1,1,], padding=list[0,0,], )
paddle.Tensor.tile(Tensor([1, 512, 0, 4],"float32"), tuple(8,1,1,1,), )
paddle.nn.functional.max_pool1d(x=Tensor([2, 3, 0],"float64"), kernel_size=2, stride=1, padding=list[1,1,], )
paddle.nn.functional.conv2d(Tensor([1, 2048, 0, 16],"float32"), Tensor([12, 512, 1, 1],"float32"), padding=0, groups=4, )
paddle.nn.functional.pad(Tensor([1, 3, 0, 1024],"float32"), tuple(0,0,0,338,), )
paddle.nn.functional.zeropad2d(Tensor([0, 3, 224, 224],"float32"), list[2,2,2,2,], )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[413,1,1,], )
paddle.nn.functional.fractional_max_pool2d(Tensor([0, 3, 7, 7],"float32"), output_size=list[3,3,], kernel_size=list[2,2,], random_u=0.6, return_mask=False, name=None, )
paddle.gammaln(Tensor([10, 0, 1],"float32"), )
paddle.nn.functional.pixel_unshuffle(Tensor([2, 12, 12, 0],"float64"), 3, "NHWC", )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 0, 128],"float16"), Tensor([1, 2048, 4, 128],"float16"), Tensor([1, 2048, 4, 128],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), dropout_p=0.0, training=True, is_causal=False, )
paddle.vision.ops.generate_proposals(Tensor([1, 3, 10, 14],"float32"), Tensor([1, 12, 10, 14],"float32"), Tensor([1, 2],"float32"), Tensor([0, 4],"float32"), Tensor([420, 4],"float32"), pre_nms_top_n=2000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float32"), Tensor([3, 12, 12, 12, 0],"float32"), bias=Tensor([12],"float32"), padding=0, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[576,1,1,], )
paddle.nn.functional.max_unpool3d(Tensor([1, 3, 2, 0, 3],"float64"), Tensor([1, 3, 2, 0, 3],"int32"), kernel_size=2, stride=2, )
paddle.einsum("ij, j", Tensor([0, 10],"float64"), Tensor([10],"float64"), )
paddle.frexp(Tensor([10, 0],"float64"), )
paddle.Tensor.tile(Tensor([140, 188, 1, 0, 1, 6],"float32"), list[1,1,1,1,2,1,], )
paddle.nn.functional.softmax(Tensor([10, 0, 153, 89],"float32"), axis=-1, )
paddle.nn.functional.conv3d_transpose(x=Tensor([0, 3, 2, 2, 2],"float32"), weight=Tensor([3, 2, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=0, dilation=1, )
paddle.Tensor.__rpow__(Tensor([2, 0, 2],"float32"), 3, )
paddle.masked_fill(Tensor([0, 3],"float32"), Tensor([1, 3],"bool"), Tensor([1],"float32"), )
paddle.nanmedian(Tensor([2, 3, 0, 5],"float32"), axis=list[0,-1,], keepdim=False, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 4, 16, 16],"float32"), Tensor([4, 0, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[list[0,0,],list[0,0,],list[1,1,],list[2,2,],], stride=1, dilation=1, groups=2, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 0],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=1, dilation=1, )
paddle.nanmedian(Tensor([2, 0, 4, 5],"float32"), axis=list[0,3,], keepdim=False, )
paddle.nn.functional.max_unpool2d(Tensor([1, 0, 2, 2],"float32"), Tensor([1, 0, 2, 2],"int32"), kernel_size=2, stride=None, output_size=tuple(5,5,), )
paddle.searchsorted(sorted_sequence=Tensor([5],"float32"), values=Tensor([0, 3],"float32"), )
paddle.Tensor.tile(Tensor([1, 4, 0],"float32"), list[450,1,1,], )
paddle.nn.functional.pad(Tensor([3, 0, 5, 6],"complex128"), pad=list[1,2,2,1,], mode="circular", value=0.0, data_format="NCHW", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 64, 248, 0],"float32"), Tensor([64, 128, 1, 1],"float32"), bias=None, padding=0, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([1, 3, 0, 242, 224],"float32"), tuple(0,0,0,2,0,0,), data_format="NCDHW", )
paddle.nn.functional.avg_pool3d(Tensor([2, 3, 0, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,2,1,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
paddle.nn.functional.temporal_shift(Tensor([240, 1024, 14, 0],"float16"), 8, 0.125, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 4, 16, 16],"float32"), Tensor([4, 0, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,1,2,2,], stride=1, dilation=1, groups=2, data_format="NCHW", )
paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 256],"float16"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, )
paddle.slice(Tensor([0, 2, 104],"float32"), list[0,1,2,], list[0,0,8,], list[2,2,24,], )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 0, 4, 4],"float64"), weight=Tensor([6, 1, 3, 3, 3],"float64"), bias=Tensor([6],"float64"), stride=2, padding=0, groups=3, )
paddle.nn.functional.max_pool2d(Tensor([0, 4, 40, 40],"float64"), kernel_size=4, stride=None, padding=2, return_mask=True, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 0],"float32"), weight=Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=list[1,0,1,], dilation=2, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 1024, 8, 16],"bfloat16"), Tensor([1, 1024, 2, 16],"bfloat16"), Tensor([1, 1024, 2, 0],"bfloat16"), attn_mask=None, is_causal=True, )
paddle.nn.functional.conv2d_transpose(Tensor([0, 4, 3, 3],"float64"), Tensor([4, 2, 1, 1],"float64"), groups=1, )
paddle.Tensor.digamma(Tensor([0, 7, 8, 10],"float64"), )
paddle.nn.functional.conv1d(x=Tensor([2, 3, 0],"float32"), weight=Tensor([1, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=2, padding=0, )
paddle.quantile(Tensor([5, 3, 0],"float64"), q=list[0.2,0.67,], axis=list[1,-1,], )
paddle.nonzero(Tensor([3, 2, 0, 2],"float64"), True, )
paddle.einsum("i,j", Tensor([0],"float64"), Tensor([0],"float64"), )
paddle.nn.functional.conv2d_transpose(Tensor([0, 3, 2, 2],"float32"), Tensor([3, 1, 3, 3],"float32"), bias=None, padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[2,2,], groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.flip(Tensor([0, 2],"float32"), 1, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 256, 0, 16],"float32"), Tensor([256, 1, 16, 16],"float32"), bias=None, padding=4, output_padding=0, stride=list[8,8,], dilation=list[1,1,], groups=256, output_size=None, data_format="NCHW", )
paddle.Tensor.__getitem__(Tensor([3, 0, 3],"float32"), tuple(slice(None,None,-1),slice(None,None,-1),slice(None,None,-1),), )
paddle.einsum("ijk,jk->ij", Tensor([0, 4, 5],"float64"), Tensor([4, 5],"float64"), )
paddle.nn.functional.hinge_embedding_loss(Tensor([0, 10, 5],"float64"), Tensor([0, 10, 5],"float64"), reduction="sum", margin=1.0, name=None, )
paddle.einsum("bind,bjnd->bnij", Tensor([13, 2, 4, 0],"float32"), Tensor([13, 4, 4, 0],"float32"), )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[336,1,1,], )
paddle.allclose(x=Tensor([2, 0, 2, 3],"float64"), y=Tensor([2, 0, 2, 3],"float64"), )
paddle.nn.functional.pad(Tensor([0, 1, 129, 129],"float32"), list[1,1,1,1,], )
paddle.nn.functional.max_pool3d(x=Tensor([8, 0, 4, 112, 112],"float32"), kernel_size=list[1,3,3,], stride=list[1,2,2,], padding=list[0,1,1,], data_format="NCDHW", )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([1, 3, 3, 3, 0],"float32"), bias=Tensor([1],"float32"), stride=list[2,2,1,], padding=1, dilation=2, )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[1376,1,1,], )
paddle.nn.functional.pad(Tensor([0, 1, 2, 2, 2],"float64"), pad=list[2,2,2,2,2,2,], mode="replicate", value=0.0, data_format="NCDHW", name=None, )
paddle.nanmedian(Tensor([2, 0, 4, 5],"float32"), axis=tuple(1,2,), keepdim=False, mode="min", )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 0, 64],"float16"), Tensor([2, 100, 0, 64],"float16"), Tensor([2, 100, 0, 64],"float16"), attn_mask=Tensor([2, 1, 0, 100],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.einsum("mbnd,mlb->lbnd", Tensor([1, 14, 0, 8],"float32"), Tensor([1, 8, 14],"float32"), )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 0, 2],"float32"), Tensor([3, 1, 3, 3],"float32"), bias=None, padding=0, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.max_pool2d(Tensor([0, 4, 40, 40],"float64"), kernel_size=tuple(2,4,), stride=None, padding=0, return_mask=True, ceil_mode=False, data_format="NCHW", name=None, )
paddle.einsum("...ij,...jk->...ik", Tensor([1, 0],"float64"), Tensor([5, 0],"float64"), )
paddle.nn.functional.selu(Tensor([3, 5, 5, 0],"float64"), 1.5, 2.0, None, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 1024, 18, 0],"float32"), output_size=1, data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[135,1,1,], )
paddle.nn.functional.conv2d(Tensor([4, 6, 16, 16],"float32"), Tensor([12, 1, 0, 3],"float32"), None, padding="valid", stride=1, dilation=1, groups=6, data_format="NCHW", )
paddle.slice(Tensor([16, 0, 104],"float32"), list[0,1,2,], list[0,0,32,], list[16,1,48,], )
paddle.vision.ops.generate_proposals(Tensor([1, 9, 0, 8],"float32"), Tensor([1, 36, 10, 8],"float32"), Tensor([1, 2],"float32"), Tensor([720, 4],"float32"), Tensor([720, 4],"float32"), pre_nms_top_n=4000, post_nms_top_n=4000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.conv1d(Tensor([2, 3, 4],"float64"), Tensor([1, 3, 3],"float64"), bias=Tensor([0],"float64"), padding=list[1,], stride=list[1,], dilation=list[2,], groups=1, data_format="NCL", )
paddle.nn.functional.conv2d_transpose(Tensor([0, 256, 19, 34],"float32"), Tensor([256, 1, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=256, output_size=None, data_format="NCHW", )
paddle.allclose(Tensor([14, 64, 0],"float32"), Tensor([14, 64, 0],"float32"), atol=1e-05, )
paddle.nn.functional.group_norm(Tensor([2, 3, 0, 4],"float64"), num_groups=2, weight=Tensor([4],"float64"), bias=Tensor([4],"float64"), data_format="NHWC", )
paddle.nn.functional.pad(Tensor([0, 22071, 3],"float32"), list[1,0,], value=7, mode="constant", data_format="NCL", )
paddle.nn.functional.sequence_mask(Tensor([0],"int64"), 12, VarType(float64), None, )
paddle.Tensor.__rpow__(Tensor([1, 0],"float32"), 10000.0, )
paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 0, 5],"float64"), q=tuple(0.11,0.5,0.73,0.9,), axis=4, keepdim=False, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float64"), weight=Tensor([3, 0, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=1, dilation=1, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[747,1,1,], )
paddle.nn.functional.conv1d(Tensor([4, 6, 16],"float32"), Tensor([8, 6, 0],"float32"), bias=Tensor([8],"float32"), padding=list[1,2,], stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[789,1,1,], )
paddle.Tensor.tile(Tensor([8, 0, 512],"float32"), tuple(1,11,1,), )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float32"), weight=Tensor([1, 3, 3, 0],"float32"), bias=Tensor([1],"float32"), stride=list[2,1,], padding=tuple(1,2,), dilation=tuple(2,2,), )
paddle.nn.functional.temporal_shift(x=Tensor([0, 4, 3, 3],"float64"), seg_num=2, shift_ratio=0.4, data_format="NHWC", )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[691,1,1,], )
paddle.nn.functional.conv2d(x=Tensor([2, 4, 4, 3],"float32"), weight=Tensor([2, 3, 3, 0],"float32"), bias=Tensor([2],"float32"), stride=1, padding=0, data_format="NHWC", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 16],"float32"), Tensor([6, 4, 0, 3],"float32"), bias=Tensor([8],"float32"), padding="valid", output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=2, output_size=None, data_format="NCHW", )
paddle.nn.functional.max_pool3d(Tensor([2, 3, 0, 32, 32],"float32"), kernel_size=2, stride=None, padding=0, return_mask=False, ceil_mode=True, data_format="NCDHW", name=None, )
paddle.Tensor.tile(Tensor([1, 5, 0],"float32"), list[373,1,1,], )
paddle.nn.functional.kl_div(Tensor([0, 20, 50],"float32"), Tensor([0, 20, 50],"float32"), "none", False, )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 0, 32],"float64"), output_size=tuple(3,3,3,), data_format="NDHWC", )
paddle.index_fill(Tensor([0],"int64"), Tensor([0],"int64"), 0, 5, )
paddle.nn.functional.softmax(Tensor([104, 0, 18, 18],"float32"), )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[201,1,1,], )
paddle.nn.functional.conv3d(Tensor([4, 8, 0, 8, 3],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[1,2,1,], stride=1, dilation=1, groups=1, data_format="NDHWC", )
paddle.slice(Tensor([0, 2, 104],"float32"), list[0,1,2,], list[0,0,72,], list[16,2,88,], )
paddle.nn.functional.avg_pool2d(Tensor([0, 3, 32, 32],"float32"), kernel_size=2, stride=2, padding=0, )
paddle.Tensor.tile(Tensor([1001, 1, 0],"float32"), list[1,3,1,], )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([0, 3, 7, 7],"float32"), output_size=list[None,3,], data_format="NCHW", name=None, )
paddle.Tensor.__rpow__(Tensor([10, 0],"float32"), 2, )
paddle.nn.functional.pad(Tensor([3, 4, 0],"complex128"), pad=list[1,2,], mode="constant", value=100, data_format="NCL", name=None, )
paddle.Tensor.tile(Tensor([12240, 0],"float32"), list[1,30,], )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 32, 0],"float64"), output_size=tuple(3,3,3,), )
paddle.Tensor.fill_diagonal_(Tensor([2, 0, 2],"float64"), 1, 0, False, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 0],"float64"), Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=1, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 0],"float64"), weight=Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,1,], dilation=2, )
paddle.nn.functional.group_norm(Tensor([0, 4, 3],"float64"), num_groups=2, weight=Tensor([4],"float64"), bias=Tensor([4],"float64"), data_format="NCL", )
paddle.nn.functional.sequence_mask(Tensor([2, 0],"int64"), maxlen=10, dtype=VarType(int32), )
paddle.einsum("bhwc,wkc->bhwk", Tensor([0, 14, 14, 64],"float32"), Tensor([14, 14, 64],"float32"), )
paddle.incubate.softmax_mask_fuse(Tensor([1, 0, 8, 32],"float32"), Tensor([1, 1, 8, 32],"float32"), )
paddle.nn.functional.mse_loss(Tensor([2, 10, 0],"float32"), Tensor([2, 10, 0],"float32"), "sum", )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 0, 2],"float32"), weight=Tensor([3, 1, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=list[1,0,], dilation=1, )
paddle.nn.functional.pad(Tensor([1, 3, 200, 0],"float64"), pad=list[10,10,10,10,], mode="constant", value=0.0, data_format="NCHW", )
paddle.slice(Tensor([2, 3, 0, 5, 6],"float32"), axes=list[0,1,2,], starts=list[1,0,2,], ends=list[3,3,4,], )
paddle.einsum("b h i d, b h j d -> b h i j", Tensor([0, 8, 1, 64],"float32"), Tensor([0, 8, 1, 64],"float32"), )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 1024, 32, 128],"bfloat16"), Tensor([1, 1024, 0, 128],"bfloat16"), Tensor([1, 1024, 32, 128],"bfloat16"), attn_mask=None, is_causal=True, )
paddle.nn.functional.conv2d(Tensor([1, 1024, 0, 128],"float32"), Tensor([12, 256, 1, 1],"float32"), padding=0, groups=4, )
paddle.nn.functional.conv1d(Tensor([13, 64, 1007],"float32"), Tensor([64, 1, 0],"float32"), bias=Tensor([64],"float32"), padding=3, stride=list[1,], dilation=list[1,], groups=64, data_format="NCL", )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 8, 8],"float64"), weight=Tensor([3, 1, 0, 5],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[list[0,0,],list[0,0,],list[1,2,],list[3,4,],], output_padding=0, dilation=1, groups=1, output_size=None, data_format="NCHW", )
paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 0, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="relu", pre_layer_norm=False, )
paddle.nn.functional.conv2d_transpose(Tensor([1, 256, 128, 128],"float32"), Tensor([256, 128, 3, 0],"float32"), padding=0, stride=2, groups=1, )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[1043,1,1,], )
paddle.nn.functional.pad(Tensor([0, 2, 3],"float32"), pad=list[1,2,], mode="reflect", value=0.0, data_format="NCL", name=None, )
paddle.nn.functional.scaled_dot_product_attention(query=Tensor([2, 0, 12, 64],"float16"), key=Tensor([2, 64, 12, 64],"float16"), value=Tensor([2, 64, 12, 64],"float16"), is_causal=True, )
paddle.Tensor.tile(Tensor([108, 0],"float32"), list[24,1,1,], )
paddle.nn.functional.pad(Tensor([1, 3, 200, 0],"float64"), pad=list[1,1,2,2,], mode="constant", value=0.0, data_format="NCHW", )
paddle.vision.ops.generate_proposals(Tensor([2, 3, 4, 4],"float32"), Tensor([2, 12, 4, 4],"float32"), Tensor([2, 0],"float32"), Tensor([4, 4, 3, 4],"float32"), Tensor([4, 4, 3, 4],"float32"), pre_nms_top_n=10, post_nms_top_n=5, return_rois_num=True, )
paddle.nn.functional.mse_loss(Tensor([0, 10],"float32"), Tensor([0, 10],"float32"), "none", )
paddle.nn.functional.pad(Tensor([14, 0, 7],"float32"), tuple(-3,0,), data_format="NCL", )
paddle.nn.functional.max_pool1d(Tensor([2, 0, 32],"float32"), 2, None, 0, False, False, None, )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[345,1,1,], )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 0, 1, 64],"float16"), Tensor([1, 0, 1, 64],"float16"), Tensor([1, 0, 1, 64],"float16"), attn_mask=Tensor([1, 0, 2048, 2048],"float16"), is_causal=True, )
paddle.roll(Tensor([4, 0, 4, 4],"float64"), Tensor([4],"int64"), list[0,1,2,3,], name=None, )
paddle.Tensor.tile(Tensor([19125, 0],"float32"), list[1,83,], )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 2, 2, 2, 3],"float32"), weight=Tensor([3, 0, 3, 3, 3],"float32"), bias=Tensor([3],"float32"), stride=1, padding=list[1,0,1,], groups=3, data_format="NDHWC", dilation=1, )
paddle.nn.functional.pad(Tensor([3, 4, 0],"complex128"), pad=list[1,1,], mode="constant", value=100, data_format="NCL", name=None, )
paddle.nn.functional.pad(Tensor([1, 8, 0, 12],"float32"), pad=list[1,0,1,2,], mode="constant", value=0.0, data_format="NCHW", name=None, )
paddle.nn.functional.softmax(Tensor([0, 3, 4],"float64"), 1, name=None, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[310,1,1,], )
paddle.Tensor.tile(Tensor([1, 4, 0],"float32"), list[659,1,1,], )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 0, 7, 7],"float32"), output_size=list[2,5,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[518,1,1,], )
paddle.Tensor.tile(Tensor([1001, 1, 0],"float32"), list[1,2,1,], )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([0, 3, 5, 7, 7],"float32"), output_size=list[3,3,3,], data_format="NDHWC", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 96, 0, 2],"float32"), Tensor([96, 96, 4, 4],"float32"), bias=Tensor([96],"float32"), padding=0, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([1, 0, 2, 1],"float32"), pad=list[0,1,0,0,], mode="constant", value=-1000000.0, data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([1, 3, 0, 224, 238],"float32"), tuple(0,2,0,0,0,0,), data_format="NCDHW", )
paddle.slice(Tensor([1024, 2, 0],"float16"), axes=list[0,], starts=list[256,], ends=list[384,], )
paddle.Tensor.imag(Tensor([2, 0, 2, 3],"complex64"), )
paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 512],"int32"), bias=Tensor([512],"float16"), dequant_scales=Tensor([512],"float32"), shift=Tensor([256],"float16"), smooth=Tensor([0],"float16"), act_method="geglu", compute_dtype="fp16", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.nn.functional.prelu(x=Tensor([1, 0, 3, 4],"float64"), weight=Tensor([1],"float64"), )
paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), atol=1e-06, )
paddle.nn.functional.log_softmax(Tensor([0, 3, 4, 5],"float32"), -1, None, )
paddle.nn.functional.conv2d_transpose(Tensor([0, 3, 2, 2],"float32"), Tensor([3, 1, 3, 3],"float32"), bias=None, padding=0, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float32"), Tensor([3, 12, 0, 12, 12],"float32"), bias=Tensor([12],"float32"), padding=0, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.pad(Tensor([0, 4, 5],"complex64"), pad=list[1,2,], mode="circular", value=0.0, data_format="NCL", name=None, )
paddle.nn.functional.maxout(x=Tensor([100, 0, 3, 3],"float64"), groups=2, )
paddle.nn.functional.max_pool2d(Tensor([0, 1, 7, 1],"float32"), tuple(2,1,), stride=tuple(2,1,), ceil_mode=True, )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 4, 0, 3],"float64"), output_size=list[3,3,], data_format="NHWC", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 0],"float32"), Tensor([6, 8, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding="valid", output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.gelu(Tensor([16, 0, 64, 64],"float16"), approximate=True, )
paddle.vision.ops.generate_proposals(Tensor([1, 9, 10, 8],"float32"), Tensor([1, 36, 10, 8],"float32"), Tensor([1, 0],"float32"), Tensor([720, 4],"float32"), Tensor([720, 4],"float32"), pre_nms_top_n=4000, post_nms_top_n=4000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 2, 2, 6],"float64"), weight=Tensor([6, 1, 3, 0],"float64"), bias=Tensor([3],"float64"), stride=2, padding=list[1,0,], output_padding=1, dilation=1, groups=3, output_size=None, data_format="NHWC", )
paddle.subtract(Tensor([2, 0],"complex64"), Tensor([2, 0],"float32"), name="Normal_log_prob", )
paddle.Tensor.tile(Tensor([1, 1, 64, 64, 0],"float32"), tuple(16,10,1,1,1,), )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 2, 2, 6],"float64"), weight=Tensor([6, 1, 0, 3],"float64"), bias=Tensor([3],"float64"), output_padding=1, stride=2, padding=list[1,0,], groups=3, data_format="NHWC", dilation=1, )
paddle.nn.quant.weight_only_linear(Tensor([0, 1, 512],"float16"), weight=Tensor([1024, 512],"int8"), bias=Tensor([1024],"float16"), weight_scale=Tensor([1024],"float16"), weight_dtype="int8", )
paddle.nn.functional.conv2d(Tensor([4, 3, 16, 16],"float32"), Tensor([5, 3, 0, 3],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[0,0,],list[1,1,],list[2,2,],], stride=1, dilation=1, groups=1, data_format="NCHW", )
paddle.einsum("nbtc,nbsc->nbts", Tensor([2, 52, 0, 8],"float32"), Tensor([2, 52, 0, 8],"float32"), )
paddle.nn.functional.avg_pool2d(Tensor([64, 104, 0, 28],"float32"), kernel_size=3, stride=2, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.local_response_norm(x=Tensor([0, 40, 40],"float32"), size=5, data_format="NCL", )
paddle.isin(Tensor([4, 0],"bfloat16"), Tensor([2, 3],"bfloat16"), False, False, )
paddle.einsum("...qk,...kd->...qd", Tensor([52, 0, 3, 1, 2],"float32"), Tensor([52, 0, 3, 2, 8],"float32"), )
paddle.nn.functional.conv3d(Tensor([4, 6, 8, 8, 0],"float32"), Tensor([8, 3, 3, 3, 3],"float32"), Tensor([8],"float32"), padding="same", stride=1, dilation=1, groups=2, data_format="NCDHW", )
paddle.nn.functional.conv2d_transpose(Tensor([0, 256, 128, 128],"float32"), Tensor([256, 128, 3, 3],"float32"), padding=0, stride=2, groups=1, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 8, 8, 8, 6],"float32"), Tensor([6, 0, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=list[list[0,0,],list[1,1,],list[2,2,],list[3,3,],list[0,0,],], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NDHWC", )
paddle.nn.functional.max_pool2d(Tensor([0, 3, 224, 224],"float32"), kernel_size=3, )
paddle.einsum("..., ...", Tensor([3, 11],"float64"), Tensor([0, 3, 11],"float64"), )
paddle.nn.functional.pad(Tensor([0, 3, 32, 224, 258],"float32"), tuple(0,2,0,0,0,0,), data_format="NCDHW", )
paddle.nn.functional.conv1d_transpose(Tensor([0, 6, 16],"float32"), Tensor([6, 8, 1],"float32"), bias=Tensor([8],"float32"), output_size=None, output_padding=0, padding=3, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.avg_pool2d(x=Tensor([0, 3, 4, 4],"float64"), kernel_size=list[3,3,], stride=list[3,3,], padding=list[0,0,], ceil_mode=True, exclusive=False, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 2, 3, 3],"float64"), Tensor([2, 2, 1, 0],"float64"), groups=1, padding=list[1,0,0,1,], )
paddle.nn.functional.pad(x=Tensor([1, 0, 1, 2, 3],"float64"), pad=list[0,0,1,1,0,0,], mode="constant", value=0, data_format="NCDHW", )
paddle.nn.functional.conv1d_transpose(Tensor([4, 16, 6],"float32"), Tensor([6, 0, 3],"float32"), bias=Tensor([8],"float32"), output_size=None, output_padding=0, padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NLC", )
paddle.nn.functional.avg_pool1d(Tensor([0, 3, 8],"float32"), 2, 2, 0, True, False, None, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float64"), weight=Tensor([3, 1, 3, 0],"float64"), bias=Tensor([1],"float64"), stride=1, padding=0, dilation=1, output_size=None, data_format="NCHW", )
paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 0],"float16"), Tensor([1, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 1, 50, 50],"float16"), scale=0.125, )
paddle.nanmedian(Tensor([2, 3, 4, 0],"float32"), axis=-1, keepdim=False, mode="min", )
paddle.nn.functional.conv2d(Tensor([1, 1024, 0, 256],"float32"), Tensor([24, 128, 0, 1],"float32"), padding=0, groups=8, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 0, 8, 8],"float32"), Tensor([6, 8, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[1,2,1,], dilation=list[2,2,2,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 0, 32, 32],"float64"), output_size=tuple(3,3,3,), data_format="NDHWC", )
paddle.nn.functional.conv3d_transpose(Tensor([4, 3, 8, 8, 8],"float32"), Tensor([3, 5, 3, 0, 3],"float32"), Tensor([5],"float32"), output_size=None, padding=0, stride=1, dilation=1, groups=1, data_format="NCDHW", )
paddle.einsum("i...->...", Tensor([2, 0],"float64"), )
paddle.nn.functional.lp_pool2d(Tensor([0, 3, 32, 32],"float32"), norm_type=-math.inf, kernel_size=2, stride=2, padding=0, ceil_mode=False, data_format="NCHW", name=None, )
paddle.einsum("ij,k->ijk", Tensor([1, 0],"float32"), Tensor([32],"float32"), )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[781,1,1,], )
paddle.nn.functional.maxout(x=Tensor([10, 0, 3, 3],"float64"), groups=3, )
paddle.nn.functional.pad(Tensor([1, 16, 14, 0, 384],"float32"), tuple(0,2,0,0,0,0,), data_format="NDHWC", )
paddle.nn.functional.softmax(Tensor([2, 0, 4],"float32"), -1, name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([20, 128, 0, 68],"float32"), Tensor([128, 1, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=128, output_size=None, data_format="NCHW", )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 128, 0, 28],"float32"), output_size=7, data_format="NCHW", name=None, )
paddle.nn.functional.maxout(Tensor([2, 6, 0, 4],"float64"), 2, 1, )
paddle.nn.functional.pad(Tensor([1, 0, 10, 21],"float32"), pad=list[0,1,0,0,], mode="constant", value=-10000.0, data_format="NCHW", name=None, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 4, 0, 3, 3],"float32"), bias=Tensor([8],"float32"), padding="valid", output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=2, output_size=None, data_format="NCDHW", )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 2],"float32"), Tensor([3, 1, 3, 0],"float32"), bias=Tensor([1],"float32"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 2],"float64"), weight=Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([0],"float64"), stride=1, padding=list[1,0,1,], dilation=2, )
paddle.einsum("ij->ji", Tensor([4, 0],"float64"), )
paddle.nn.functional.lp_pool2d(Tensor([2, 0, 32, 32],"float64"), norm_type=2.0, kernel_size=5, stride=3, padding=0, ceil_mode=False, data_format="NCHW", name=None, )
paddle.einsum("ijk->kji", Tensor([3, 0, 3],"float64"), )
paddle.nn.functional.conv2d(Tensor([2, 48, 0, 4],"float32"), Tensor([48, 48, 0, 3],"float32"), None, list[1,1,], 1, list[1,1,], 1, "NCHW", )
paddle.nn.functional.max_unpool2d(Tensor([2, 0, 7, 8],"float64"), Tensor([2, 0, 7, 8],"int32"), list[2,2,], stride=list[2,2,], padding=list[0,0,], data_format="NCHW", output_size=list[14,16,], name=None, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 0, 8],"float32"), Tensor([6, 8, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[1,2,1,], dilation=list[2,2,2,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.quant.weight_only_linear(Tensor([100, 512],"float16"), weight=Tensor([1024, 512],"int8"), bias=Tensor([1024],"float16"), weight_scale=Tensor([0],"float16"), weight_dtype="int8", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[1833,1,1,], )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 0],"float64"), weight=Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,1,], dilation=1, )
paddle.nn.functional.conv2d_transpose(x=Tensor([0, 3, 2, 2],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,], dilation=2, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 0],"float64"), Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[2,2,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv3d_transpose(Tensor([0, 4, 8, 8, 8],"float32"), Tensor([4, 3, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[list[0,0,],list[0,0,],list[1,1,],list[1,1,],list[2,2,],], stride=1, dilation=1, groups=2, data_format="NCDHW", )
paddle.Tensor.tile(Tensor([1, 8, 2, 0, 16],"float32"), list[1,1,1,4,1,], )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float64"), weight=Tensor([1, 3, 3, 3, 0],"float64"), bias=Tensor([1],"float64"), stride=2, padding=1, )
paddle.searchsorted(Tensor([3],"float32"), Tensor([0],"float32"), right=True, )
paddle.nanmedian(Tensor([2, 0, 4, 5],"float32"), axis=list[0,-1,], keepdim=False, )
paddle.nn.functional.max_pool2d(Tensor([0, 24, 40, 40],"float32"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.avg_pool3d(Tensor([0, 1, 3, 1600, 7],"float32"), kernel_size=tuple(1,1,5,), stride=1, )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float32"), weight=Tensor([1, 3, 0, 3],"float32"), bias=Tensor([1],"float32"), stride=list[2,1,], padding=1, dilation=tuple(2,2,), )
paddle.nn.functional.max_pool3d(Tensor([2, 3, 0, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=tuple(1,0,0,), return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.reverse(Tensor([0, 4],"float64"), axis=list[0,], )
paddle.nn.functional.max_pool3d(Tensor([8, 64, 16, 0, 112],"float32"), kernel_size=tuple(3,3,3,), stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.nn.functional.max_pool2d(Tensor([0, 1, 4, 4],"float32"), kernel_size=2, stride=2, return_mask=True, )
paddle.nn.functional.avg_pool2d(Tensor([0, 1024, 17, 17],"float32"), kernel_size=3, stride=1, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 2, 2, 6],"float64"), weight=Tensor([6, 0, 3, 3],"float64"), bias=Tensor([3],"float64"), stride=1, padding=list[1,0,], output_padding=0, dilation=1, groups=3, output_size=None, data_format="NHWC", )
paddle.subtract(x=Tensor([2, 0],"float32"), y=Tensor([2, 0],"float32"), )
paddle.nn.functional.pad(Tensor([1, 16, 0, 15, 384],"float32"), tuple(0,6,0,0,0,0,), data_format="NDHWC", )
paddle.nn.functional.mse_loss(Tensor([3, 0, 10, 10],"float32"), Tensor([3, 0, 10, 10],"float32"), "none", )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 0],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
paddle.nn.functional.conv1d_transpose(Tensor([2, 3, 2],"float64"), Tensor([3, 0, 3],"float64"), bias=Tensor([1],"float64"), output_size=None, output_padding=0, padding=list[1,], stride=list[2,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 0],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
paddle.nn.functional.softmax(Tensor([0, 8, 8],"float32"), )
paddle.Tensor.__getitem__(Tensor([3, 0, 3],"float32"), tuple(slice(None,-1,None),slice(None,None,-1),slice(-1,None,None),), )
paddle.nn.functional.conv1d(Tensor([1, 128, 112],"float32"), Tensor([128, 128, 0],"float32"), bias=Tensor([128],"float32"), padding=15, stride=list[1,], dilation=list[5,], groups=1, data_format="NCL", )
paddle.nn.functional.conv1d(Tensor([52, 0, 32],"float32"), Tensor([32, 1, 3],"float32"), bias=None, padding=1, stride=list[1,], dilation=list[1,], groups=32, data_format="NLC", )
paddle.nn.functional.max_pool2d(Tensor([0, 4, 4, 4],"float16"), kernel_size=2, stride=2, padding=0, )
paddle.slice(Tensor([0, 1, 104],"float32"), list[0,1,2,], list[0,0,16,], list[13,1,32,], )
paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([0],"float32"), shift=None, smooth=None, quant_scale=0.0016999575309455395, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.slice(Tensor([16, 0, 104],"float32"), list[0,1,2,], list[0,0,88,], list[16,1,104,], )
paddle.Tensor.digamma(Tensor([0, 3],"float32"), )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[121,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 0],"float32"), Tensor([3, 1, 3, 3],"float32"), bias=None, padding=0, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.softmax(Tensor([2, 0, 2, 5, 5],"float16"), )
paddle.einsum("ijk,lk->ijl", Tensor([0, 4, 5],"float64"), Tensor([2, 5],"float64"), )
paddle.Tensor.flip(Tensor([0],"int64"), 0, )
paddle.nn.functional.lp_pool1d(Tensor([2, 0, 32],"float64"), 5.0, 5, 3, 0, False, "NCL", None, )
paddle.nn.functional.conv2d_transpose(Tensor([0, 6, 16, 16],"float32"), Tensor([6, 8, 3, 3],"float32"), bias=Tensor([8],"float32"), padding="valid", output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.vision.ops.generate_proposals(Tensor([1, 3, 10, 0],"float32"), Tensor([1, 12, 10, 15],"float32"), Tensor([1, 2],"float32"), Tensor([450, 4],"float32"), Tensor([450, 4],"float32"), pre_nms_top_n=2000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.max_pool3d(Tensor([0, 3, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding=list[list[0,0,],list[0,0,],list[0,0,],list[0,0,],list[0,0,],], )
paddle.Tensor.__pow__(Tensor([0, 3, 2],"float16"), Tensor([0, 3, 2],"float64"), )
paddle.nn.functional.conv2d(Tensor([1, 256, 128, 0],"float32"), Tensor([3, 256, 1, 1],"float32"), padding=0, groups=1, )
paddle.nn.functional.max_pool1d(x=Tensor([2, 0, 8],"float64"), kernel_size=3, stride=4, padding=0, )
paddle.vision.ops.generate_proposals(Tensor([1, 3, 10, 15],"float32"), Tensor([1, 12, 10, 15],"float32"), Tensor([1, 2],"float32"), Tensor([450, 0],"float32"), Tensor([450, 4],"float32"), pre_nms_top_n=2000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.pad(Tensor([52, 0, 16],"float32"), pad=list[1,1,], data_format="NLC", )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 0, 2],"float32"), weight=Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=list[1,0,1,], dilation=1, )
paddle.nn.functional.max_pool2d(Tensor([2, 0, 40, 40],"float64"), kernel_size=4, stride=None, padding=2, return_mask=True, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.conv2d(Tensor([2, 24, 0, 8],"float32"), Tensor([24, 24, 0, 3],"float32"), None, list[1,1,], 1, list[1,1,], 1, "NCHW", )
paddle.nn.functional.avg_pool3d(Tensor([2, 3, 8, 0, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,1,1,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
paddle.nn.functional.conv1d(Tensor([1, 20, 7],"float32"), Tensor([512, 20, 0],"float32"), bias=Tensor([512],"float32"), padding=3, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nanmedian(Tensor([0, 100],"float32"), axis=1, mode="min", )
paddle.cummin(Tensor([100, 0],"float32"), axis=-2, )
paddle.nn.functional.conv1d(Tensor([13, 64, 10],"float32"), Tensor([64, 1, 0],"float32"), bias=Tensor([64],"float32"), padding=3, stride=list[1,], dilation=list[1,], groups=64, data_format="NCL", )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 0],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )
paddle.einsum("a...a->a...", Tensor([5, 3, 2, 1, 0, 5],"float64"), )
paddle.nn.functional.avg_pool1d(Tensor([2, 0, 32],"float32"), kernel_size=2, stride=2, padding=list[0,], )
paddle.nn.functional.max_unpool3d(Tensor([0, 3, 2, 2, 3],"float64"), Tensor([0, 3, 2, 2, 3],"int32"), kernel_size=2, stride=2, )
paddle.nn.functional.max_pool2d(Tensor([0, 192, 27, 27],"float32"), kernel_size=3, stride=1, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.einsum("bind,bjnd->bnij", Tensor([13, 0, 4, 4],"float32"), Tensor([13, 2, 4, 4],"float32"), )
paddle.nn.functional.conv2d(Tensor([1, 2048, 128, 128],"float32"), Tensor([2048, 256, 3, 0],"float32"), padding=1, groups=8, )
paddle.nn.functional.max_pool2d(Tensor([1, 1, 0, 4],"float32"), kernel_size=2, stride=2, padding=0, return_mask=True, ceil_mode=False, data_format="NCHW", name=None, )
paddle.cummin(Tensor([100, 0],"int32"), axis=0, )
paddle.nn.functional.pad(Tensor([0, 4, 5, 6, 7],"complex128"), pad=list[1,2,2,1,1,0,], mode="circular", value=0.0, data_format="NCDHW", name=None, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 0, 1, 64],"float16"), Tensor([2, 100, 1, 64],"float16"), Tensor([2, 100, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 100],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.Tensor.tile(Tensor([130, 0],"float32"), list[24,1,1,], )
paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 0],"int32"), bias=Tensor([512],"float32"), dequant_scales=Tensor([512],"float32"), shift=Tensor([512],"float32"), smooth=Tensor([512],"float32"), act_method="gelu", compute_dtype="fp32", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.nn.functional.conv2d(Tensor([2, 24, 0, 8],"float32"), Tensor([24, 24, 3, 3],"float32"), None, list[1,1,], 1, list[1,1,], 1, "NCHW", )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 0, 10],"float16"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
paddle.nn.functional.pad(Tensor([0, 21955, 3],"float32"), list[1,0,], value=0, mode="constant", data_format="NCL", )
paddle.nn.functional.pad(Tensor([1, 16, 14, 18, 0],"float32"), tuple(0,3,0,0,0,0,), data_format="NDHWC", )
paddle.vision.ops.box_coder(Tensor([30, 4],"float32"), list[0.12371375411748886,0.7415851950645447,0.40236398577690125,0.6756224632263184,], Tensor([30, 0, 4],"float32"), "decode_center_size", False, axis=1, )
paddle.incubate.nn.functional.fused_feedforward(x=Tensor([0, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln1_scale=Tensor([2],"float32"), ln1_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, )
paddle.slice(Tensor([16, 0, 104],"float32"), list[0,1,2,], list[0,0,16,], list[16,2,32,], )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([6, 1, 3, 0, 3],"float32"), bias=Tensor([6],"float32"), stride=2, padding=0, groups=3, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[590,1,1,], )
paddle.nn.functional.pad(Tensor([1, 0, 28, 24],"float32"), pad=list[1,1,1,1,], mode="reflect", value=0.0, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([2, 0, 4],"float64"), list[1,1,], mode="reflect", data_format="NCL", )
paddle.Tensor.tile(Tensor([1, 5, 0],"float32"), list[358,1,1,], )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 2],"float32"), weight=Tensor([3, 1, 0],"float32"), bias=Tensor([1],"float32"), stride=2, padding=list[1,], groups=1, dilation=1, )
paddle.nn.functional.conv3d_transpose(Tensor([4, 8, 8, 8, 4],"float32"), Tensor([4, 3, 3, 0, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[list[0,0,],list[1,2,],list[1,2,],list[2,1,],list[0,0,],], stride=1, dilation=1, groups=2, data_format="NDHWC", )
paddle.nn.functional.pad(Tensor([1, 0, 15, 14, 384],"float16"), tuple(0,0,0,1,0,0,), data_format="NDHWC", )
paddle.nn.functional.scaled_dot_product_attention(query=Tensor([2, 0, 12, 64],"float16"), key=Tensor([2, 0, 12, 64],"float16"), value=Tensor([2, 0, 12, 64],"float16"), is_causal=True, )
paddle.slice(Tensor([16, 0, 104],"float32"), list[0,1,2,], list[0,0,88,], list[16,2,104,], )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 0, 2, 2],"float32"), Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=1, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.Tensor.flip(Tensor([0, 3, 224, 224],"float32"), 0, )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float64"), weight=Tensor([6, 1, 3, 0, 3],"float64"), bias=Tensor([6],"float64"), stride=2, padding=0, groups=3, )
paddle.nn.functional.pad(x=Tensor([0, 3, 3],"float64"), pad=list[1,1,], mode="constant", value=0.0, data_format="NCL", )
paddle.nn.functional.max_pool3d(x=Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float32"), weight=Tensor([3, 1, 3, 3],"float32"), bias=Tensor([0],"float32"), stride=1, padding=1, output_padding=0, dilation=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float64"), weight=Tensor([3, 1, 3, 3],"float64"), bias=Tensor([0],"float64"), stride=1, padding=list[1,0,], dilation=2, )
paddle.slice(Tensor([0, 8, 24],"float32"), axes=list[1,], starts=list[6,], ends=list[7,], )
paddle.nn.functional.pad(Tensor([0, 16, 14, 16, 384],"float32"), tuple(0,5,0,0,0,0,), data_format="NDHWC", )
paddle.reverse(Tensor([12, 0, 16],"float64"), axis=list[0,], )
paddle.nn.functional.sequence_mask(Tensor([2, 0],"int64"), maxlen=10, dtype=VarType(bool), )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 8, 8],"float64"), weight=Tensor([3, 1, 0, 5],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,2,3,4,], output_padding=0, dilation=1, groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv1d_transpose(x=Tensor([0, 3, 8],"float64"), weight=Tensor([3, 2, 3],"float64"), bias=Tensor([2],"float64"), stride=1, padding=list[list[0,0,],list[0,0,],list[3,4,],], output_padding=0, groups=1, dilation=1, output_size=None, data_format="NCL", )
paddle.nn.functional.softmax(Tensor([2, 17, 1600, 0],"float32"), axis=1, )
paddle.Tensor.tile(Tensor([1, 0, 2, 1, 16],"float16"), list[1,1,1,4,1,], )
paddle.nn.functional.avg_pool2d(Tensor([0, 384, 4, 80],"float32"), list[4,2,], )
paddle.nn.functional.pad(Tensor([2, 0, 4, 4, 4],"float32"), list[1,1,1,1,1,1,], mode="replicate", data_format="NCDHW", )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 8, 16],"float16"), Tensor([0, 2048, 2, 16],"float16"), Tensor([1, 2048, 2, 16],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), is_causal=True, )
paddle.slice(Tensor([2, 0],"float32"), axes=list[0,], starts=list[1,], ends=list[2,], )
paddle.nn.functional.layer_norm(Tensor([20, 0, 60, 70],"float32"), list[60,70,], weight=Tensor([4200],"float32"), bias=Tensor([4200],"float32"), epsilon=1e-05, )
paddle.nn.functional.hinge_embedding_loss(Tensor([4, 0, 3, 4],"float64"), Tensor([4, 0, 3, 4],"float64"), reduction="mean", margin=-4.0, name=None, )
paddle.nn.functional.conv1d(Tensor([2, 3, 4],"float32"), Tensor([1, 3, 0],"float32"), bias=Tensor([1],"float32"), padding=1, stride=list[1,], dilation=list[2,], groups=1, data_format="NCL", )
paddle.Tensor.tile(Tensor([1, 4, 0],"float32"), list[426,1,1,], )
paddle.nn.functional.conv1d_transpose(x=Tensor([0, 3, 2],"float64"), weight=Tensor([3, 1, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,], groups=1, dilation=2, )
paddle.nn.functional.max_pool3d(x=Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,0,0,], )
paddle.Tensor.tile(Tensor([1, 0],"float32"), tuple(900,1,), )
paddle.vision.ops.generate_proposals(Tensor([1, 9, 10, 0],"float32"), Tensor([1, 36, 10, 8],"float32"), Tensor([1, 2],"float32"), Tensor([720, 4],"float32"), Tensor([720, 4],"float32"), pre_nms_top_n=4000, post_nms_top_n=4000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.square_error_cost(Tensor([3, 2, 0, 2],"float64"), label=Tensor([3, 2, 0, 2],"float64"), )
paddle.Tensor.tile(Tensor([10285, 0],"float32"), list[1,78,], )
paddle.nn.functional.sequence_mask(Tensor([0, 3],"int64"), maxlen=10, dtype=VarType(uint8), )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[144,1,1,], )
paddle.vision.ops.generate_proposals(Tensor([1, 3, 10, 14],"float32"), Tensor([1, 12, 10, 14],"float32"), Tensor([1, 0],"float32"), Tensor([420, 4],"float32"), Tensor([420, 4],"float32"), pre_nms_top_n=2000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.Tensor.__sub__(Tensor([3, 0, 3, 1, 2, 5],"float64"), Tensor([3, 0, 3, 1, 2, 5],"float64"), )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 1, 2, 40],"float16"), Tensor([1, 1, 2, 40],"float16"), Tensor([1, 1, 2, 0],"float16"), attn_mask=Tensor([1, 2, 1, 1],"float16"), )
paddle.nn.functional.conv2d(Tensor([2, 192, 0, 4],"float32"), Tensor([384, 192, 0, 1],"float32"), None, list[2,2,], 0, list[1,1,], 1, "NCHW", )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 0, 5, 7, 7],"float32"), output_size=list[3,3,3,], data_format="NDHWC", )
paddle.nn.functional.group_norm(Tensor([0, 4, 3, 2, 2],"float64"), num_groups=2, weight=Tensor([4],"float64"), bias=Tensor([4],"float64"), data_format="NCDHW", )
paddle.cummin(Tensor([100, 0],"float32"), )
paddle.nn.functional.max_pool3d(Tensor([1, 4, 4, 4, 0],"float32"), list[3,3,3,], stride=1, padding=list[0,0,0,], data_format="NDHWC", )
paddle.nn.functional.prelu(x=Tensor([0, 2, 3, 4],"float64"), weight=Tensor([1],"float64"), )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float64"), weight=Tensor([3, 0, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,], dilation=1, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 128, 128, 0],"float32"), list[4,1,1,1,], )
paddle.nn.functional.pad(x=Tensor([1, 1, 1, 0, 3],"float64"), pad=list[0,0,1,1,0,0,], mode="constant", value=0, data_format="NCDHW", )
paddle.nn.functional.conv2d_transpose(Tensor([20, 128, 38, 0],"float32"), Tensor([128, 1, 4, 4],"float32"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=128, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([6, 1, 0, 3, 3],"float32"), bias=Tensor([6],"float32"), stride=2, padding=0, groups=3, )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float32"), weight=Tensor([1, 3, 3, 0],"float32"), bias=Tensor([1],"float32"), stride=2, padding=1, )
paddle.nn.functional.conv2d(Tensor([8, 128, 257, 257],"float32"), Tensor([256, 128, 3, 0],"float32"), bias=None, stride=2, padding=0, )
paddle.einsum("...ij,...jk->...ik", Tensor([0, 5],"float64"), Tensor([5, 1],"float64"), )
paddle.einsum("ij,kl->ijkl", Tensor([4, 5],"float64"), Tensor([3, 0],"float64"), )
paddle.allclose(x=Tensor([0],"float64"), y=Tensor([0],"float64"), rtol=1e-05, atol=0.001, equal_nan=True, )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 0, 7],"float32"), list[2,3,5,], )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 32, 0],"float64"), output_size=list[1,1,1,], data_format="NCDHW", name=None, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[417,1,1,], )
paddle.allclose(Tensor([0],"int32"), Tensor([0],"int32"), 50.0, 49.0, False, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 2],"float32"), Tensor([3, 1, 3, 3],"float32"), bias=Tensor([0],"float32"), padding=1, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.avg_pool3d(Tensor([0, 3, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding="SAME", )
paddle.nn.functional.conv1d(Tensor([1, 128, 0],"float32"), Tensor([128, 128, 7],"float32"), bias=Tensor([128],"float32"), padding=3, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv1d_transpose(Tensor([2, 3, 2],"float64"), Tensor([3, 1, 3],"float64"), bias=Tensor([0],"float64"), output_size=None, output_padding=0, padding=1, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.interpolate(x=Tensor([2, 3, 5, 7, 0],"float32"), mode="area", size=list[2,3,5,], )
paddle.nn.functional.pad(Tensor([13, 0, 7],"float32"), tuple(-3,0,), data_format="NCL", )
paddle.nn.functional.square_error_cost(Tensor([8, 0, 100],"float32"), Tensor([8, 0, 100],"float32"), )
paddle.nn.functional.pad(Tensor([0, 16, 31, 28, 192],"float32"), tuple(0,0,0,4,0,0,), data_format="NDHWC", )
paddle.nn.functional.l1_loss(Tensor([10, 10, 0],"float32"), Tensor([10, 10, 0],"float32"), )
paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 1, 1, 7],"float32"), Tensor([13, 4, 1, 7, 0],"float32"), )
paddle.nn.functional.softmax(Tensor([0, 11, 64, 64],"float32"), axis=1, )
paddle.einsum("bhwc,hkc->bhwk", Tensor([432, 14, 0, 64],"float32"), Tensor([14, 14, 64],"float32"), )
paddle.nn.functional.conv1d(Tensor([52, 0, 32],"float32"), Tensor([16, 32, 1],"float32"), bias=None, padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NLC", )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([1, 3, 0, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=list[2,2,1,], padding=1, dilation=list[2,2,2,], )
paddle.nn.functional.kl_div(Tensor([40, 20, 0],"float32"), Tensor([40, 20, 0],"float32"), "none", False, )
paddle.Tensor.tile(Tensor([12240, 0],"float32"), list[1,22,], )
paddle.einsum("a...a->a...", Tensor([5, 0, 2, 1, 4, 5],"float64"), )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float64"), weight=Tensor([3, 1, 0, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=0, dilation=1, )
paddle.nn.functional.hinge_embedding_loss(Tensor([10, 0, 5],"float64"), Tensor([10, 0, 5],"float64"), reduction="none", margin=1.0, name=None, )
paddle.slice(Tensor([1024, 2, 0],"float16"), axes=list[0,], starts=list[256,], ends=list[512,], )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float64"), weight=Tensor([3, 1, 0, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=0, dilation=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.fractional_max_pool2d(x=Tensor([2, 0, 7, 7],"float32"), output_size=list[3,3,], random_u=0.3, )
paddle.nn.functional.fractional_max_pool2d(x=Tensor([2, 0, 7, 7],"float32"), kernel_size=2, output_size=list[3,3,], random_u=0.6, )
paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 512],"int32"), bias=Tensor([512],"float32"), dequant_scales=Tensor([0],"float32"), shift=Tensor([512],"float32"), smooth=Tensor([512],"float32"), act_method="gelu", compute_dtype="fp32", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.Tensor.flip(Tensor([4, 2, 0, 64],"float32"), 1, )
paddle.nn.functional.conv2d(Tensor([1, 1, 101, 261],"float32"), Tensor([64, 1, 7, 0],"float32"), None, list[2,2,], 0, list[1,1,], 1, )
paddle.nn.functional.log_softmax(x=Tensor([2, 2, 0],"float64"), axis=0, )
paddle.nn.functional.mse_loss(Tensor([2, 0, 2],"float32"), Tensor([2, 0, 2],"float32"), reduction="none", )
paddle.nonzero(x=Tensor([0, 9],"bfloat16"), )
paddle.slice(Tensor([0, 3, 8],"float32"), axes=list[1,], starts=list[2,], ends=list[3,], )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 128, 0, 32],"float32"), output_size=tuple(3,3,), data_format="NCHW", name=None, )
paddle.nn.functional.conv1d(Tensor([16, 64, 0],"float32"), Tensor([128, 64, 3],"float32"), bias=Tensor([128],"float32"), padding=8, stride=list[1,], dilation=list[8,], groups=1, data_format="NCL", )
paddle.nn.functional.conv1d(x=Tensor([2, 3, 4],"float32"), weight=Tensor([1, 3, 0],"float32"), bias=Tensor([1],"float32"), stride=list[1,], padding=1, dilation=2, )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 0, 32, 32],"float64"), output_size=list[1,1,1,], data_format="NCDHW", name=None, )
paddle.Tensor.tile(Tensor([14877, 0, 1],"float32"), list[1,420,1,], )
paddle.nn.functional.avg_pool3d(x=Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=tuple(1,0,0,), exclusive=True, )
paddle.nn.functional.pad(Tensor([1, 3, 180, 0],"float64"), pad=list[2,2,2,2,], mode="constant", value=0.0, data_format="NCHW", )
paddle.nn.functional.avg_pool3d(Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=tuple(1,0,0,), ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
paddle.Tensor.tile(Tensor([10709, 0, 1],"float32"), list[1,405,1,], )
paddle.nonzero(x=Tensor([0, 8],"float64"), )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 4, 128],"float16"), Tensor([1, 2048, 4, 0],"float16"), Tensor([1, 2048, 4, 128],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), dropout_p=0.0, training=True, is_causal=False, )
paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 3, 4, 0],"float32"), output_size=list[3,3,], )
paddle.nonzero(x=Tensor([0, 10, 2],"float32"), )
paddle.nn.functional.pad(Tensor([0, 4, 5, 6, 7],"complex128"), pad=list[1,2,2,1,1,0,], mode="replicate", value=0.0, data_format="NCDHW", name=None, )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 2],"float64"), weight=Tensor([3, 0, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,], output_padding=0, groups=1, dilation=2, output_size=None, data_format="NCL", name=None, )
paddle.nn.functional.pad(Tensor([1, 0, 140, 160],"float64"), pad=list[40,40,0,0,], mode="constant", value=0.0, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 5, 0],"float32"), list[4101,1,1,], )
paddle.nn.functional.conv2d(Tensor([2, 48, 0, 4],"float32"), Tensor([48, 48, 3, 3],"float32"), None, list[1,1,], 1, list[1,1,], 1, "NCHW", )
paddle.Tensor.tile(Tensor([10105, 0],"float32"), list[1,10,], )
paddle.frac(Tensor([2, 0],"int32"), )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 8],"float64"), weight=Tensor([3, 0, 3],"float64"), bias=Tensor([2],"float64"), stride=1, padding=list[4,4,], output_padding=0, groups=1, dilation=1, output_size=None, data_format="NCL", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 16],"float32"), Tensor([6, 0, 3, 3],"float32"), bias=Tensor([8],"float32"), padding="same", output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.kl_div(Tensor([0, 2],"float32"), label=Tensor([0, 2],"float32"), reduction="mean", name=None, )
paddle.Tensor.tile(Tensor([1, 4, 0],"float32"), list[293,1,1,], )
paddle.nn.functional.softmax(Tensor([0, 8, 1, 65],"float16"), -1, )
paddle.nn.functional.pad(Tensor([1, 8, 14, 0],"float32"), pad=list[1,0,1,2,], mode="constant", value=0.0, data_format="NCHW", name=None, )
paddle.vision.ops.generate_proposals(Tensor([1, 3, 10, 14],"float32"), Tensor([1, 12, 0, 14],"float32"), Tensor([1, 2],"float32"), Tensor([420, 4],"float32"), Tensor([420, 4],"float32"), pre_nms_top_n=2000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 2, 2],"float32"), weight=Tensor([3, 0, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=list[1,0,], dilation=2, )
paddle.Tensor.tile(Tensor([1, 1, 0],"float32"), list[131,1,1,], )
paddle.nn.functional.interpolate(x=Tensor([2, 0, 7, 7],"float32"), mode="area", size=list[2,5,], )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 0, 4],"float64"), weight=Tensor([1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=list[2,1,], padding=list[1,2,], dilation=tuple(2,2,), )
paddle.nn.functional.conv2d(Tensor([4, 16, 16, 3],"float32"), Tensor([5, 3, 3, 0],"float32"), Tensor([5],"float32"), padding=list[1,2,3,1,], stride=2, dilation=1, groups=1, data_format="NHWC", )
paddle.einsum("nbtc,nbsc->nbts", Tensor([2, 52, 7, 0],"float32"), Tensor([2, 52, 14, 0],"float32"), )
paddle.nn.functional.temporal_shift(x=Tensor([2, 0, 4, 3],"float32"), seg_num=2, )
paddle.nn.functional.max_pool1d(Tensor([1, 3, 0],"float64"), 2, 2, 0, True, False, None, )
paddle.subtract(Tensor([0],"float32"), Tensor([0],"float32"), name="Cauchy_log_prob", )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float32"), weight=Tensor([1, 3, 3, 0, 3],"float32"), bias=Tensor([1],"float32"), stride=list[2,2,1,], padding=1, dilation=list[2,2,2,], )
paddle.nn.functional.conv2d_transpose(Tensor([20, 64, 0, 68],"float32"), Tensor([64, 1, 8, 8],"float32"), bias=None, padding=2, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=64, output_size=None, data_format="NCHW", )
paddle.nn.quant.weight_only_linear(Tensor([1, 32, 64],"float16"), Tensor([0, 64],"int8"), bias=Tensor([256],"float16"), weight_scale=Tensor([256],"float16"), weight_dtype="int4", group_size=-1, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 2],"float32"), Tensor([3, 0, 3, 3],"float32"), bias=None, padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[2,2,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([1, 256, 64, 0],"float32"), Tensor([256, 128, 3, 3],"float32"), bias=Tensor([128],"float32"), padding=1, output_padding=1, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.roll(Tensor([4, 5, 4, 0],"complex128"), Tensor([2],"int64"), tuple(1,3,), name=None, )
paddle.nn.functional.max_pool2d(Tensor([2, 0, 33, 33],"float32"), kernel_size=5, stride=5, padding=0, ceil_mode=True, return_mask=True, )
paddle.Tensor.tile(Tensor([4, 1, 0],"float32"), tuple(1,10,1,), )
paddle.vision.ops.generate_proposals(Tensor([2, 3, 4, 4],"float32"), Tensor([2, 12, 4, 4],"float32"), Tensor([2, 3],"float32"), Tensor([4, 4, 3, 0],"float32"), Tensor([4, 4, 3, 4],"float32"), pre_nms_top_n=10, post_nms_top_n=5, return_rois_num=True, )
paddle.vision.ops.generate_proposals(Tensor([1, 15, 42, 63],"float32"), Tensor([1, 0, 42, 63],"float32"), Tensor([1, 2],"float32"), Tensor([39690, 4],"float32"), Tensor([39690, 4],"float32"), pre_nms_top_n=12000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.nn.functional.zeropad2d(Tensor([0, 3, 224, 224],"float64"), list[2,2,2,2,], )
paddle.strided_slice(x=Tensor([3, 4, 0, 6],"float32"), axes=list[1,2,3,], starts=list[-3,0,2,], ends=list[3,2,4,], strides=list[1,1,1,], )
paddle.nn.functional.avg_pool2d(Tensor([1, 0, 3, 3],"float32"), kernel_size=2, stride=1, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.l1_loss(Tensor([0, 107, 1025],"float32"), Tensor([0, 107, 1025],"float32"), )
paddle.nn.functional.softmax(Tensor([0, 16, 18, 18],"float16"), )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[295,1,1,], )
paddle.nn.functional.conv1d(Tensor([1, 128, 0],"float32"), Tensor([128, 128, 7],"float32"), bias=Tensor([128],"float32"), padding=15, stride=list[1,], dilation=list[5,], groups=1, data_format="NCL", )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[390,1,1,], )
paddle.nn.functional.max_pool2d(Tensor([1, 0, 4, 5],"float32"), kernel_size=2, stride=2, return_mask=True, )
paddle.nn.functional.max_pool1d(x=Tensor([1, 0, 2],"float64"), kernel_size=2, stride=2, padding=0, return_mask=True, )
paddle.Tensor.__sub__(Tensor([12, 3, 0, 10, 1],"float32"), Tensor([12, 3, 0, 10, 1],"float32"), )
paddle.nn.functional.flashmask_attention(Tensor([1, 2048, 0, 96],"float16"), Tensor([1, 2048, 8, 96],"float16"), Tensor([1, 2048, 8, 96],"float16"), startend_row_indices=Tensor([1, 1, 2048, 1],"int32"), causal=True, )
paddle.nn.functional.conv1d(Tensor([2, 3, 4],"float64"), Tensor([1, 3, 3],"float64"), bias=Tensor([0],"float64"), padding=tuple(1,), stride=list[1,], dilation=list[2,], groups=1, data_format="NCL", )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([13, 256, 56, 0],"float32"), output_size=list[7,7,], data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[1003,1,1,], )
paddle.einsum("bmtd,mdhr->bmhtr", Tensor([13, 2, 16, 16],"float32"), Tensor([2, 16, 0, 1],"float32"), )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 8, 96],"float16"), Tensor([2, 100, 8, 0],"float16"), Tensor([2, 100, 8, 96],"float16"), attn_mask=None, is_causal=False, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 2],"float32"), Tensor([3, 0, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[596,1,1,], )
paddle.nn.functional.kl_div(Tensor([5, 0],"float64"), Tensor([5, 0],"float64"), "mean", True, )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float32"), weight=Tensor([1, 3, 3, 3],"float32"), bias=Tensor([0],"float32"), stride=list[2,1,], padding=tuple(1,2,), dilation=tuple(2,2,), )
paddle.nn.functional.pad(Tensor([1, 16, 31, 0, 192],"float32"), tuple(0,0,0,4,0,0,), data_format="NDHWC", )
paddle.einsum("bhwc,wkc->bhwk", Tensor([432, 14, 14, 64],"float32"), Tensor([14, 0, 64],"float32"), )
paddle.nn.functional.avg_pool1d(Tensor([0, 1, 120],"float32"), 25, 1, 0, True, False, None, )
paddle.strided_slice(x=Tensor([5, 8, 0, 4, 2, 6],"float64"), axes=list[1,2,5,], starts=list[6,5,4,], ends=list[2,0,1,], strides=list[-1,-2,-3,], )
paddle.nn.functional.pad(Tensor([0, 1, 40, 40, 3],"float32"), pad=list[2,2,0,0,0,0,], data_format="NCDHW", )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 0, 4],"float64"), weight=Tensor([1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=2, padding=1, )
paddle.nn.functional.max_pool2d(Tensor([1, 0, 40, 40],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.softmax(Tensor([2, 0, 1, 65],"float16"), -1, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 4, 0, 16],"float32"), Tensor([4, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[list[0,0,],list[0,0,],list[1,1,],list[2,2,],], stride=1, dilation=1, groups=2, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([0, 4, 5, 6],"complex64"), pad=list[1,2,2,1,], mode="reflect", value=0.0, data_format="NCHW", name=None, )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 0, 32, 32],"float64"), output_size=list[1,3,2,], data_format="NCDHW", name=None, )
paddle.nn.functional.softmax(Tensor([0],"float64"), axis=-1, )
paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([0],"float32"), shift=None, smooth=None, quant_scale=0.0009313154732808471, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.einsum("blq,bhlk->bhlqk", Tensor([1, 3, 5],"float64"), Tensor([1, 2, 3, 0],"float64"), )
paddle.nn.functional.avg_pool2d(Tensor([0, 4, 3, 3],"float32"), kernel_size=2, stride=1, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([10285, 0],"float32"), list[1,90,], )
paddle.einsum("iox,ojx->ijx", Tensor([2, 3, 1],"complex64"), Tensor([3, 2, 0],"complex64"), )
paddle.nn.functional.l1_loss(Tensor([10, 0, 5],"float32"), Tensor([10, 0, 5],"float32"), "none", name=None, )
paddle.slice(Tensor([0, 1, 104],"float32"), list[0,1,2,], list[0,0,48,], list[3,1,64,], )
paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 0, 8, 32, 32],"float64"), output_size=3, )
paddle.reverse(Tensor([0, 7],"int64"), list[1,], )
paddle.nn.functional.max_pool2d(Tensor([0, 3, 224, 224],"float32"), kernel_size=1, )
paddle.Tensor.tile(Tensor([1, 8, 0],"float32"), list[789,1,1,], )
paddle.nn.functional.selu(Tensor([3, 5, 5, 0],"float64"), 1.5, 2.0, )
paddle.einsum("mbnd,mlb->lbnd", Tensor([1, 14, 4, 8],"float32"), Tensor([0, 8, 14],"float32"), )
paddle.nn.functional.max_pool1d(Tensor([2, 0, 8],"float64"), 1, 1, 0, False, False, None, )
paddle.Tensor.tile(Tensor([10285, 0],"float32"), list[1,16,], )
paddle.nn.functional.conv2d(Tensor([1, 1024, 128, 128],"float32"), Tensor([1024, 256, 0, 3],"float32"), padding=1, groups=4, )
paddle.strided_slice(x=Tensor([5, 0, 6, 4, 2, 6],"float64"), axes=list[1,2,5,], starts=list[-3,3,4,], ends=list[3,0,1,], strides=list[-1,-1,-2,], )
paddle.einsum("bmtd,mdhr->bmhtr", Tensor([0, 2, 16, 16],"float32"), Tensor([2, 16, 4, 1],"float32"), )
paddle.slice(Tensor([13, 0, 104],"float32"), list[0,1,2,], list[0,0,40,], list[13,1,56,], )
paddle.nn.functional.maxout(x=Tensor([100, 0, 3, 3],"float32"), groups=2, )
paddle.nn.functional.pad(Tensor([1, 0, 2, 3, 2],"float64"), pad=list[1,0,1,2,1,0,], mode="constant", value=0.0, data_format="NCDHW", name=None, )
paddle.nn.functional.pad(Tensor([1, 0, 2, 2],"float64"), pad=list[2,2,2,2,], mode="replicate", value=0.0, data_format="NCHW", name=None, )
paddle.slice(Tensor([1024, 2, 0],"float16"), axes=list[0,], starts=list[512,], ends=list[640,], )
paddle.nn.functional.pad(Tensor([3, 4, 0, 6, 7],"complex64"), pad=list[1,1,1,1,1,1,], mode="constant", value=100, data_format="NCDHW", name=None, )
paddle.nn.functional.sequence_mask(Tensor([0],"int64"), 12, Dtype(float64), None, )
paddle.slice(Tensor([13, 0, 104],"float32"), list[0,1,2,], list[0,0,16,], list[13,1,32,], )
paddle.nn.functional.pad(Tensor([1024, 1, 0, 256],"float32"), list[1,1,1,1,], )
paddle.nn.functional.fractional_max_pool2d(x=Tensor([2, 3, 0, 7],"float32"), output_size=list[None,3,], random_u=0.6, )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[462,1,1,], )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float64"), weight=Tensor([6, 1, 0, 3],"float64"), bias=Tensor([6],"float64"), stride=2, padding=0, groups=3, )
paddle.incubate.nn.functional.fused_rms_norm(Tensor([0, 64],"float16"), Tensor([64],"float16"), None, 1e-06, begin_norm_axis=1, )
paddle.vision.ops.generate_proposals(Tensor([1, 4, 16, 16],"float32"), Tensor([1, 16, 0, 16],"float32"), Tensor([1, 2],"float32"), Tensor([16, 16, 4, 4],"float32"), Tensor([16, 16, 4, 4],"float32"), pre_nms_top_n=12000, post_nms_top_n=5000, nms_thresh=0.7, min_size=3.0, eta=1.0, pixel_offset=True, return_rois_num=True, )
paddle.nanmedian(Tensor([2, 3, 4, 0],"float32"), axis=list[0,-1,], keepdim=False, )
paddle.Tensor.frexp(Tensor([0, 12],"float64"), )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([0, 1024, 4, 80],"float16"), list[1,40,], )
paddle.strided_slice(x=Tensor([6, 0],"float32"), axes=list[0,1,], starts=list[3,4,], ends=list[5,2,], strides=list[4,-2,], )
paddle.vision.ops.generate_proposals(Tensor([0, 4, 16, 16],"float32"), Tensor([1, 16, 16, 16],"float32"), Tensor([1, 2],"float32"), Tensor([16, 16, 4, 4],"float32"), Tensor([16, 16, 4, 4],"float32"), pre_nms_top_n=12000, post_nms_top_n=5000, nms_thresh=0.7, min_size=3.0, eta=1.0, pixel_offset=True, return_rois_num=True, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 8, 3, 3, 0],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[1,2,1,], dilation=list[2,2,2,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.layer_norm(Tensor([4, 0, 4, 4],"float32"), 4, )
paddle.nn.quant.weight_only_linear(Tensor([101, 64],"float16"), weight=Tensor([0, 64],"int8"), bias=Tensor([192],"float16"), weight_scale=Tensor([192],"float16"), weight_dtype="int8", )
paddle.nn.functional.conv1d(x=Tensor([2, 3, 0],"float64"), weight=Tensor([1, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=2, padding=1, )
paddle.nn.functional.conv1d(Tensor([4, 6, 0],"float32"), Tensor([8, 6, 3],"float32"), bias=Tensor([8],"float32"), padding=2, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.max_pool2d(Tensor([0, 3, 32, 32],"float64"), kernel_size=list[3,3,], stride=list[1,1,], padding=list[0,0,0,0,], return_mask=False, ceil_mode=True, data_format="NCHW", name=None, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 4, 4, 0],"float64"), output_size=list[3,3,], data_format="NHWC", name=None, )
paddle.Tensor.tile(Tensor([12240, 0],"float32"), list[1,64,], )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 0, 32, 128],"bfloat16"), Tensor([1, 0, 32, 128],"bfloat16"), Tensor([1, 0, 32, 128],"bfloat16"), attn_mask=None, is_causal=True, )
paddle.vision.ops.generate_proposals(Tensor([1, 4, 16, 16],"float32"), Tensor([1, 16, 16, 16],"float32"), Tensor([1, 2],"float32"), Tensor([16, 0, 4, 4],"float32"), Tensor([16, 16, 4, 4],"float32"), pre_nms_top_n=12000, post_nms_top_n=5000, nms_thresh=0.7, min_size=3.0, eta=1.0, pixel_offset=True, return_rois_num=True, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 0, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )
paddle.slice(Tensor([0, 2, 104],"float32"), list[0,1,2,], list[0,0,48,], list[16,2,64,], )
paddle.nn.functional.max_pool2d(Tensor([0, 3, 32, 32],"float32"), kernel_size=2, stride=2, padding=list[list[0,0,],list[0,0,],list[0,0,],list[0,0,],], return_mask=False, )
paddle.nn.functional.pad(Tensor([0, 3, 184, 204],"float64"), pad=list[52,52,0,0,], mode="constant", value=0.0, data_format="NCHW", )
paddle.nn.functional.pad(Tensor([1, 2, 0, 4, 5],"complex64"), pad=list[1,2,1,1,3,4,], mode="constant", value=100, data_format="NDHWC", )
paddle.Tensor.tile(Tensor([140, 0, 1, 1, 1, 6],"float32"), list[1,1,1,1,2,1,], )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[142,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([4, 4, 16, 16],"float32"), Tensor([4, 3, 3, 0],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,2,], stride=1, dilation=1, groups=2, data_format="NCHW", )
paddle.nn.functional.max_pool3d(Tensor([0, 3, 6, 6, 3],"float32"), 3, data_format="NDHWC", )
paddle.vision.ops.generate_proposals(Tensor([1, 15, 0, 63],"float32"), Tensor([1, 60, 42, 63],"float32"), Tensor([1, 2],"float32"), Tensor([39690, 4],"float32"), Tensor([39690, 4],"float32"), pre_nms_top_n=12000, post_nms_top_n=2000, nms_thresh=0.7, min_size=0.0, eta=1.0, return_rois_num=True, )
paddle.slice(Tensor([13, 0, 104],"float32"), list[0,1,2,], list[0,0,72,], list[13,1,88,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[60,1,1,], )
paddle.cummax(Tensor([100, 0],"float32"), axis=0, )
paddle.Tensor.flip(Tensor([0, 3],"float32"), 0, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 8, 96],"float16"), Tensor([2, 100, 8, 96],"float16"), Tensor([2, 100, 8, 0],"float16"), attn_mask=None, is_causal=False, )
paddle.nn.functional.pad(Tensor([3, 1, 0, 40],"float32"), pad=list[0,0,2,2,], )
paddle.nn.functional.sequence_mask(Tensor([2, 0],"int64"), maxlen=Tensor([1],"int32"), dtype=VarType(uint8), )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 8, 1, 0, 1],"float32"), bias=Tensor([8],"float32"), padding=tuple(2,3,1,), output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.max_pool2d(Tensor([2, 4, 40, 0],"float64"), kernel_size=2, stride=None, padding=0, return_mask=True, ceil_mode=False, data_format="NCHW", name=None, )
paddle.slice(Tensor([0, 8, 24],"float32"), axes=list[1,], starts=list[1,], ends=list[2,], )
paddle.einsum("i...->...", Tensor([2, 0, 11],"float64"), )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[377,1,1,], )
paddle.nn.functional.pad(Tensor([1, 0, 14, 15, 384],"float16"), tuple(0,6,0,0,0,0,), data_format="NDHWC", )
paddle.Tensor.tile(Tensor([1, 5, 0],"float32"), list[268,1,1,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[301,1,1,], )
paddle.Tensor.tile(Tensor([10285, 0],"float32"), list[1,32,], )
paddle.nn.functional.conv2d(Tensor([8, 3, 0, 256],"float32"), Tensor([128, 3, 1, 1],"float32"), bias=None, stride=1, padding=0, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[263,1,1,], )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([0, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 160, 10, 0],"float32"), tuple(1,1,), )
paddle.Tensor.nonzero(Tensor([0],"bool"), )
paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 8, 1, 109],"float32"), Tensor([1, 8, 109, 0],"float32"), )
paddle.allclose(Tensor([0, 3],"float32"), Tensor([0, 3],"float32"), atol=0.001, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 2, 2],"float64"), Tensor([3, 0, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[2,2,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.max_pool3d(Tensor([2, 3, 0, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=1, return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.nn.functional.pad(Tensor([1, 1, 2, 0, 2],"float64"), pad=list[1,0,1,2,1,0,], mode="constant", value=0.0, data_format="NCDHW", name=None, )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[228,1,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([2, 4, 3, 0],"float64"), Tensor([4, 2, 1, 1],"float64"), groups=1, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 4, 16, 16],"float32"), Tensor([4, 3, 0, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,2,], stride=1, dilation=1, groups=2, data_format="NCHW", )
paddle.einsum("k...,jk", Tensor([2, 4, 0, 3],"float64"), Tensor([2, 2],"float64"), )
paddle.nn.functional.conv2d_transpose(Tensor([1, 256, 0, 64],"float32"), Tensor([256, 128, 3, 3],"float32"), bias=Tensor([128],"float32"), padding=1, output_padding=1, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[155,1,1,], )
paddle.nn.functional.conv3d_transpose(Tensor([4, 8, 8, 8, 4],"float32"), Tensor([4, 3, 3, 3, 0],"float32"), Tensor([6],"float32"), output_size=None, padding="valid", stride=tuple(1,2,1,), dilation=tuple(2,1,1,), groups=2, data_format="NDHWC", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[534,1,1,], )
paddle.Tensor.tile(Tensor([8, 1, 0],"float32"), tuple(1,6,1,), )
paddle.einsum("i,d->id", Tensor([0],"float32"), Tensor([16],"float32"), )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 0, 4],"float64"), weight=Tensor([1, 3, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=list[2,2,1,], padding=1, dilation=tuple(2,2,2,), )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[106,1,1,], )
paddle.masked_fill(Tensor([3, 0],"float16"), Tensor([3, 1],"bool"), Tensor([1],"float16"), )
paddle.nn.functional.conv2d(Tensor([2, 3, 37, 0],"float32"), Tensor([64, 3, 7, 0],"float32"), None, list[2,2,], 0, list[1,1,], 1, "NCHW", )
paddle.Tensor.tile(Tensor([2450, 0, 1],"float32"), list[1,223,1,], )
paddle.nn.functional.max_pool3d(x=Tensor([2, 0, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=tuple(1,0,0,), )
paddle.nn.functional.avg_pool3d(Tensor([2, 3, 0, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,0,0,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
paddle.kthvalue(Tensor([2, 0, 250],"float64"), 244, 2, )
paddle.Tensor.lgamma(Tensor([5, 7, 0],"float64"), )
paddle.einsum("blqd,bmdk->blqk", Tensor([52, 0, 1, 1],"float32"), Tensor([52, 0, 1, 1],"float32"), )
paddle.nn.functional.temporal_shift(Tensor([0, 1024, 14, 14],"float32"), 8, 0.125, )
paddle.nn.functional.max_pool2d(Tensor([0, 64, 17, 273],"float32"), tuple(3,3,), tuple(2,2,), tuple(0,0,), False, )
paddle.nn.functional.conv1d_transpose(x=Tensor([2, 3, 2],"float64"), weight=Tensor([3, 0, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=1, output_padding=0, groups=1, dilation=1, output_size=None, data_format="NCL", name=None, )
paddle.slice(Tensor([0, 1, 104],"float32"), list[0,1,2,], list[0,0,64,], list[13,1,80,], )
paddle.nn.functional.conv1d(Tensor([16, 64, 25500],"float32"), Tensor([128, 64, 0],"float32"), bias=Tensor([128],"float32"), padding=512, stride=list[1,], dilation=list[512,], groups=1, data_format="NCL", )
paddle.einsum("ij,kl->ijkl", Tensor([4, 5],"float64"), Tensor([0, 7],"float64"), )
paddle.nn.functional.scaled_dot_product_attention(query=Tensor([2, 64, 12, 64],"float16"), key=Tensor([2, 64, 12, 64],"float16"), value=Tensor([0, 64, 12, 64],"float16"), is_causal=True, )
paddle.Tensor.tile(Tensor([8, 1, 0],"float32"), tuple(1,9,1,), )
paddle.nn.functional.max_unpool3d(Tensor([1, 3, 2, 0, 3],"float64"), Tensor([1, 3, 2, 0, 3],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCDHW", output_size=None, name=None, )
paddle.nn.functional.pad(Tensor([0, 3, 4],"float64"), list[1,1,], mode="reflect", data_format="NCL", )
paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 3, 5, 2, 4],"float32"), Tensor([0, 3, 4, 2, 4],"float32"), )
paddle.nn.functional.conv2d_transpose(Tensor([0, 3, 16, 16],"float32"), Tensor([3, 2, 3, 3],"float32"), bias=Tensor([6],"float32"), padding="valid", output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=3, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 6, 0],"float32"), list[1833,1,1,], )
paddle.nn.functional.conv3d_transpose(Tensor([2, 2, 2, 2, 3],"float32"), Tensor([3, 1, 3, 0, 3],"float32"), bias=Tensor([3],"float32"), padding=list[1,1,1,], output_padding=1, stride=list[2,2,2,], dilation=list[1,1,1,], groups=3, output_size=None, data_format="NDHWC", )
paddle.nn.functional.conv2d(x=Tensor([2, 3, 4, 4],"float64"), weight=Tensor([1, 3, 3, 3],"float64"), bias=Tensor([0],"float64"), stride=2, padding=0, )
paddle.nn.functional.prelu(Tensor([1, 2, 0, 4],"float32"), Tensor([2],"float32"), )
paddle.nn.functional.conv2d_transpose(Tensor([4, 256, 32, 32],"float32"), Tensor([256, 1, 8, 0],"float32"), bias=None, padding=2, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=256, output_size=None, data_format="NCHW", )
paddle.vision.ops.generate_proposals(Tensor([1, 4, 16, 16],"float32"), Tensor([1, 16, 16, 16],"float32"), Tensor([1, 2],"float32"), Tensor([16, 16, 0, 4],"float32"), Tensor([16, 16, 4, 4],"float32"), pre_nms_top_n=12000, post_nms_top_n=5000, nms_thresh=0.7, min_size=3.0, eta=1.0, pixel_offset=True, return_rois_num=True, )
paddle.Tensor.tile(Tensor([140, 188, 1, 1, 0, 6],"float32"), list[1,1,1,1,2,1,], )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 3, 0, 2],"float32"), weight=Tensor([3, 1, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=0, dilation=1, )
paddle.nn.functional.conv1d(Tensor([4, 6, 16],"float32"), Tensor([8, 3, 0],"float32"), bias=Tensor([8],"float32"), padding="valid", stride=list[1,], dilation=list[1,], groups=2, data_format="NCL", )
paddle.nn.functional.conv1d_transpose(Tensor([4, 6, 16],"float32"), Tensor([6, 0, 3],"float32"), bias=Tensor([8],"float32"), output_size=None, output_padding=0, padding=list[1,2,], stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.Tensor.tile(Tensor([1, 9, 0],"float32"), list[479,1,1,], )
paddle.nn.functional.pad(Tensor([0, 3, 180, 200],"float64"), pad=list[2,2,2,2,], mode="constant", value=0.0, data_format="NCHW", )
paddle.roll(Tensor([4, 0, 4],"float64"), Tensor([3],"int64"), list[0,1,2,], name=None, )
paddle.nn.functional.pad(Tensor([1, 0, 32, 224, 231],"float32"), tuple(0,1,0,0,0,0,), data_format="NCDHW", )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 4, 128],"float16"), Tensor([1, 2048, 4, 128],"float16"), Tensor([1, 2048, 0, 128],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 2, 2, 6],"float32"), weight=Tensor([6, 0, 3, 3],"float32"), bias=Tensor([3],"float32"), output_padding=1, stride=2, padding=list[1,0,], groups=3, data_format="NHWC", dilation=1, )
paddle.Tensor.tile(Tensor([1, 4, 0],"float32"), list[1082,1,1,], )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[281,1,1,], )
paddle.Tensor.tile(Tensor([1, 3, 0],"float32"), list[747,1,1,], )
paddle.nn.functional.pad(Tensor([0, 5551, 3],"float32"), list[1,0,], value=3, mode="constant", data_format="NCL", )
paddle.Tensor.fill_diagonal_(Tensor([2, 0, 2],"float32"), 1, 0, False, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 6, 2, 2],"float64"), weight=Tensor([6, 0, 3, 3],"float64"), bias=Tensor([3],"float64"), stride=1, padding=list[1,0,], output_padding=0, dilation=1, groups=3, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[870,1,1,], )
paddle.Tensor.__getitem__(Tensor([3, 0, 3],"float32"), slice(None,None,-1), )
paddle.einsum("blqd,bmdk->blqk", Tensor([13, 5, 1, 1],"float32"), Tensor([13, 0, 1, 1],"float32"), )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 16],"float32"), Tensor([6, 8, 0, 1],"float32"), bias=Tensor([8],"float32"), padding=tuple(2,3,), output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.einsum("sec,ecm->sm", Tensor([0, 60, 10],"float32"), Tensor([60, 10, 64],"float32"), )
paddle.Tensor.tile(Tensor([1, 4, 0],"float32"), list[424,1,1,], )
paddle.nn.functional.log_softmax(Tensor([0, 3, 4, 5],"float32"), 1, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 4, 3, 3, 0],"float32"), bias=Tensor([8],"float32"), padding="valid", output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=2, output_size=None, data_format="NCDHW", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 8, 3, 3, 0],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[2,2,2,], dilation=list[2,1,2,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nanquantile(Tensor([4, 0, 6],"float64"), q=0.1, axis=list[1,2,], keepdim=True, )
paddle.nn.functional.pad(Tensor([2, 0, 32, 32],"float32"), list[2,3,2,3,], value=0, )
paddle.nn.functional.group_norm(Tensor([2, 4, 0, 2, 2],"float64"), num_groups=2, weight=Tensor([4],"float64"), bias=Tensor([4],"float64"), data_format="NCDHW", )
paddle.Tensor.tile(Tensor([1, 2, 2, 0, 8],"float32"), list[1,1,1,2,1,], )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 8, 8, 3],"float64"), weight=Tensor([3, 1, 5, 0],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[list[0,0,],list[1,2,],list[3,4,],list[0,0,],], output_padding=0, dilation=1, groups=1, output_size=None, data_format="NHWC", )
paddle.nn.functional.interpolate(x=Tensor([2, 3, 0, 7],"float32"), mode="area", size=list[2,5,], )
paddle.nn.functional.pad(Tensor([0, 3, 4, 4, 4],"float32"), list[1,1,1,1,1,1,], mode="replicate", data_format="NCDHW", )
paddle.nn.functional.softmax(Tensor([0, 8, 153, 153],"float32"), axis=-1, )
paddle.nn.functional.max_pool2d(Tensor([2, 0, 32, 32],"float64"), kernel_size=list[3,3,], stride=list[1,1,], padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([1, 14701, 0],"float32"), list[1,0,], value=4, mode="constant", data_format="NCL", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[384,1,1,], )
paddle.nn.functional.avg_pool3d(x=Tensor([8, 0, 32, 7, 7],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", )
paddle.nn.functional.conv3d(Tensor([4, 8, 8, 8, 3],"float32"), Tensor([5, 3, 3, 3, 0],"float32"), Tensor([5],"float32"), padding=list[1,2,3,1,2,3,], stride=2, dilation=1, groups=1, data_format="NDHWC", )
paddle.isin(Tensor([8, 0],"bfloat16"), Tensor([4, 0],"bfloat16"), False, True, )
paddle.nn.functional.fractional_max_pool3d(x=Tensor([2, 0, 7, 7, 7],"float32"), output_size=list[2,3,5,], random_u=0.7, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 0],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )
paddle.nn.functional.pad(Tensor([0, 1, 1, 1],"float32"), list[0,1,0,1,], )
paddle.nn.functional.pad(Tensor([0, 2, 3],"float32"), pad=list[2,2,], mode="replicate", value=0.0, data_format="NCL", name=None, )
paddle.nn.functional.softmax(Tensor([2, 0],"float32"), axis=-1, dtype="float64", )
paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 512],"int32"), bias=Tensor([512],"float16"), dequant_scales=Tensor([512],"float32"), shift=Tensor([0],"float16"), smooth=Tensor([512],"float16"), act_method="gelu", compute_dtype="fp16", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 0],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )
paddle.nn.functional.lp_pool2d(Tensor([2, 0, 32, 32],"float32"), norm_type=-math.inf, kernel_size=2, stride=2, padding=0, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.channel_shuffle(Tensor([2, 9, 4, 0],"float64"), 3, "NCHW", None, )
paddle.nn.functional.avg_pool1d(Tensor([2, 0, 8],"float64"), 2, 2, 0, True, False, None, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[593,1,1,], )
paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 8, 0, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,2,1,], exclusive=True, )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[192,1,1,], )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 0, 8, 8],"float32"), Tensor([6, 8, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding="valid", output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.Tensor.frexp(Tensor([0, 5, 2],"float32"), )
paddle.nn.functional.max_pool1d(Tensor([0, 3, 8],"float64"), list[3,], 1, 1, False, False, None, )
paddle.nn.functional.conv1d_transpose(Tensor([2, 2, 3],"float64"), Tensor([3, 1, 0],"float64"), bias=Tensor([3],"float64"), output_size=None, output_padding=1, padding=list[1,], stride=list[2,], dilation=list[1,], groups=3, data_format="NLC", )
paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 4],"float64"), weight=Tensor([1, 3, 3, 3, 3],"float64"), bias=Tensor([0],"float64"), stride=list[2,2,1,], padding=tuple(1,2,2,), dilation=tuple(2,2,2,), )
paddle.einsum("blkd,bldq->blkq", Tensor([52, 5, 1, 1],"float32"), Tensor([52, 5, 0, 3],"float32"), )
paddle.Tensor.tile(Tensor([144, 0],"float32"), list[90,1,1,], )
paddle.nn.functional.softmax(Tensor([2, 0],"float32"), axis=1, dtype="float64", )
paddle.nn.functional.avg_pool2d(Tensor([64, 104, 28, 0],"float16"), kernel_size=3, stride=2, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.max_pool2d(Tensor([0, 256, 20, 30],"float32"), 1, stride=2, )
paddle.Tensor.digamma(Tensor([5, 0, 8],"float64"), )
paddle.Tensor.tile(Tensor([1, 7, 0],"float32"), list[514,1,1,], )
paddle.nn.functional.max_pool3d(Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[1,2,3,], padding=list[0,0,0,], return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.Tensor.tile(Tensor([1, 2, 0],"float32"), list[222,1,1,], )
paddle.nn.functional.max_pool3d(Tensor([0, 3, 32, 32, 32],"float32"), kernel_size=2, stride=None, padding=0, return_mask=False, ceil_mode=True, data_format="NCDHW", name=None, )
paddle.nn.functional.conv2d_transpose(Tensor([2, 64, 248, 216],"float32"), Tensor([64, 128, 0, 1],"float32"), bias=None, padding=0, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[852,1,1,], )
paddle.nn.functional.pad(Tensor([1, 3, 140, 0],"float64"), pad=list[40,40,0,0,], mode="constant", value=0.0, data_format="NCHW", )
paddle.quantile(x=Tensor([3, 6, 3, 0, 2, 5],"float64"), q=list[0.25,0.5,0.75,], axis=3, keepdim=False, )
paddle.nn.functional.pad(Tensor([1, 16, 0, 14, 384],"float16"), tuple(0,0,0,0,0,0,), data_format="NDHWC", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 64, 32, 0],"float32"), Tensor([64, 1, 8, 8],"float32"), bias=None, padding=2, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=64, output_size=None, data_format="NCHW", )
paddle.einsum("i , j -> i j", Tensor([10],"float32"), Tensor([0],"float32"), )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 0],"float64"), weight=Tensor([3, 2, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=0, dilation=1, )
paddle.nn.functional.max_pool1d(Tensor([1, 1, 0],"float64"), 2, 2, 0, True, False, None, )
paddle.nn.functional.softmax(Tensor([10, 8, 0, 153],"float32"), axis=-1, )
paddle.nn.functional.max_pool3d(x=Tensor([0, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], )
paddle.nn.functional.max_pool2d(Tensor([1, 2, 32, 0],"float32"), kernel_size=2, stride=2, padding=0, return_mask=True, ceil_mode=False, data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([10105, 0],"float32"), list[1,13,], )
paddle.nn.functional.max_pool3d(Tensor([2, 0, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 0, 2, 2],"float32"), Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([3],"float32"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=3, output_size=None, data_format="NCDHW", )
paddle.Tensor.tile(Tensor([1, 5, 0],"float32"), list[384,1,1,], )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 0, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )
paddle.nn.functional.pad(Tensor([3, 0, 5, 6],"complex64"), pad=list[1,1,1,1,], mode="constant", value=100, data_format="NCHW", name=None, )
paddle.nn.functional.pad(Tensor([0, 4410, 1],"float32"), pad=list[200,200,], mode="reflect", data_format="NLC", )
paddle.slice(Tensor([2, 0, 104],"float32"), list[0,1,2,], list[0,0,48,], list[2,2,64,], )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 512, 128, 0],"float32"), output_size=tuple(6,6,), data_format="NCHW", name=None, )
paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 0],"bfloat16"), Tensor([4, 3, 0],"float32"), )
paddle.nn.quant.weight_quantize(Tensor([64, 0],"float16"), algo="weight_only_int8", arch=86, group_size=-1, )
paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 0, 2],"float32"), weight=Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([3],"float32"), stride=1, padding=list[1,0,1,], groups=3, dilation=1, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 16, 16, 6],"float32"), Tensor([6, 8, 0, 3],"float32"), bias=Tensor([8],"float32"), padding=list[list[0,0,],list[1,1,],list[2,2,],list[0,0,],], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NHWC", )
paddle.nn.functional.avg_pool2d(Tensor([32, 256, 0, 56],"float32"), kernel_size=2, stride=2, padding="SAME", ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.conv2d_transpose(x=Tensor([2, 6, 0, 2],"float64"), weight=Tensor([6, 1, 3, 3],"float64"), bias=Tensor([3],"float64"), stride=1, padding=list[1,0,], output_padding=0, dilation=1, groups=3, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 8, 0, 1, 1],"float32"), bias=Tensor([8],"float32"), padding=tuple(2,3,1,), output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.max_unpool2d(Tensor([0, 4, 7, 8],"float64"), Tensor([0, 4, 7, 8],"int32"), list[2,2,], stride=list[2,2,], padding=list[0,0,], data_format="NCHW", output_size=list[14,16,], name=None, )
paddle.nn.functional.pixel_unshuffle(Tensor([2, 1, 12, 0],"float32"), 3, "NCHW", )
paddle.Tensor.tile(Tensor([1, 0, 4],"float32"), list[380,1,1,], )
paddle.nn.functional.conv2d(Tensor([1, 2048, 0, 128],"float32"), Tensor([2048, 256, 0, 3],"float32"), padding=1, groups=8, )
paddle.frexp(Tensor([0, 5, 2],"float64"), )
paddle.nn.functional.max_pool2d(Tensor([1, 1, 0, 4],"float32"), kernel_size=2, stride=2, return_mask=True, )
paddle.nn.functional.avg_pool2d(Tensor([2, 0, 32, 32],"float32"), kernel_size=2, stride=2, padding=list[list[0,0,],list[0,0,],list[0,0,],list[0,0,],], divisor_override=4, )
paddle.nn.functional.pad(Tensor([0, 1, 1, 13],"float32"), pad=list[0,0,0,1,], mode="replicate", value=0.0, data_format="NCHW", name=None, )
paddle.Tensor.tile(Tensor([1, 6, 0],"float32"), list[329,1,1,], )
