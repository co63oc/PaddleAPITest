2025-06-08 11:44:37.720343 GPU 6 65635 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([101, 64],"float16"), Tensor([64],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, )
[accuracy error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([101, 64],"float16"), Tensor([64],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, ) 
 (PreconditionNotMet) Tensor not initialized yet when DenseTensor::place() is called.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:62)


2025-06-08 11:44:38.321114 GPU 6 65629 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, )
[accuracy error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, ) 
 (PreconditionNotMet) Tensor not initialized yet when DenseTensor::place() is called.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:62)


2025-06-08 11:44:38.347399 GPU 6 65634 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float16"), residual=Tensor([16, 256],"float16"), residual_alpha=0.69204696, quant_scale=0.15, quant_round_type=1, quant_max_bound=127, quant_min_bound=-127, )
[accuracy error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float16"), residual=Tensor([16, 256],"float16"), residual_alpha=0.69204696, quant_scale=0.15, quant_round_type=1, quant_max_bound=127, quant_min_bound=-127, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 4 / 4096 (0.1%)
Greatest absolute difference: 1 at index (1, 103) (up to 0.01 allowed)
Greatest relative difference: 1.0 at index (9, 235) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([16, 256]), dtype=torch.int8)
tensor([[ -2,  12,  20,  ...,  -9,  -5,   1],
        [ -3,   9,  11,  ..., -10,  -5,   3],
        [ -3,   4,  19,  ...,  -7,  -5,   2],
        ...,
        [ -2,  14,   8,  ..., -10,  -6,   3],
        [ -3,  22,  22,  ..., -11,  -4,   1],
        [ -2,  16,  20,  ...,  -8,  -6,   0]], dtype=torch.int8)
DESIRED: (shape=torch.Size([16, 256]), dtype=torch.int8)
tensor([[ -2,  12,  20,  ...,  -9,  -5,   1],
        [ -3,   9,  11,  ..., -10,  -5,   3],
        [ -3,   4,  19,  ...,  -7,  -5,   2],
        ...,
        [ -2,  14,   8,  ..., -10,  -6,   3],
        [ -3,  22,  22,  ..., -11,  -4,   1],
        [ -2,  16,  20,  ...,  -8,  -6,   0]], dtype=torch.int8)

2025-06-08 11:44:38.729084 GPU 6 65632 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, quant_scale=0.15, quant_round_type=1, quant_max_bound=127, quant_min_bound=-127, )
[accuracy error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, quant_scale=0.15, quant_round_type=1, quant_max_bound=127, quant_min_bound=-127, ) 
 (PreconditionNotMet) Tensor not initialized yet when DenseTensor::place() is called.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:62)


2025-06-08 11:44:38.838365 GPU 6 65630 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([2, 1, 64],"float16"), Tensor([64],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, )
[accuracy error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([2, 1, 64],"float16"), Tensor([64],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, ) 
 (PreconditionNotMet) Tensor not initialized yet when DenseTensor::place() is called.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:62)


2025-06-08 11:44:38.861003 GPU 6 65631 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([59, 64],"float16"), Tensor([64],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, )
[accuracy error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([59, 64],"float16"), Tensor([64],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, ) 
 (PreconditionNotMet) Tensor not initialized yet when DenseTensor::place() is called.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:62)


2025-06-08 11:44:38.869565 GPU 6 65633 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([60, 64],"float16"), Tensor([64],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, )
[accuracy error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([60, 64],"float16"), Tensor([64],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, ) 
 (PreconditionNotMet) Tensor not initialized yet when DenseTensor::place() is called.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:62)


2025-06-08 11:44:48.142547 GPU 6 65629 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([67, 64],"float16"), Tensor([64],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, )
[accuracy error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([67, 64],"float16"), Tensor([64],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, ) 
 (PreconditionNotMet) Tensor not initialized yet when DenseTensor::place() is called.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:62)


2025-06-08 11:44:48.397410 GPU 6 65635 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([68, 64],"float16"), Tensor([64],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, )
[accuracy error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([68, 64],"float16"), Tensor([64],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, ) 
 (PreconditionNotMet) Tensor not initialized yet when DenseTensor::place() is called.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:62)


2025-06-08 11:44:48.449912 GPU 6 65629 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([76, 64],"float16"), Tensor([64],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, )
[accuracy error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([76, 64],"float16"), Tensor([64],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, ) 
 (PreconditionNotMet) Tensor not initialized yet when DenseTensor::place() is called.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:62)


2025-06-08 11:44:48.666559 GPU 6 65635 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([78, 64],"float16"), Tensor([64],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, )
[accuracy error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([78, 64],"float16"), Tensor([64],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, ) 
 (PreconditionNotMet) Tensor not initialized yet when DenseTensor::place() is called.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:62)


2025-06-08 11:44:48.672162 GPU 6 65629 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([90, 64],"float16"), Tensor([64],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, )
[accuracy error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([90, 64],"float16"), Tensor([64],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, ) 
 (PreconditionNotMet) Tensor not initialized yet when DenseTensor::place() is called.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:62)


2025-06-08 11:44:48.878359 GPU 6 65629 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([91, 64],"float16"), Tensor([64],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, )
[accuracy error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([91, 64],"float16"), Tensor([64],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, ) 
 (PreconditionNotMet) Tensor not initialized yet when DenseTensor::place() is called.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:62)


2025-06-08 11:44:48.924515 GPU 6 65635 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([98, 64],"float16"), Tensor([64],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, )
[accuracy error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([98, 64],"float16"), Tensor([64],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, ) 
 (PreconditionNotMet) Tensor not initialized yet when DenseTensor::place() is called.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:62)


2025-06-08 11:44:49.052807 GPU 6 65629 test begin: paddle.nn.functional.linear(x=Tensor([10, 499, 512],"float32"), weight=Tensor([512, 43],"float16"), bias=Tensor([43],"float16"), name=None, )
Warning: The core code of paddle.nn.functional.linear is too complex.
/root/miniconda3/envs/paddle/lib/python3.12/site-packages/torch/autograd/graph.py:823: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:180.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[accuracy error] backward  paddle.nn.functional.linear(x=Tensor([10, 499, 512],"float32"), weight=Tensor([512, 43],"float16"), bias=Tensor([43],"float16"), name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 19 / 22016 (0.1%)
Greatest absolute difference: 0.0274658203125 at index (74, 25) (up to 0.01 allowed)
Greatest relative difference: 1.6220703125 at index (364, 6) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([512, 43]), dtype=torch.float16)
tensor([[ -2.6133,  -4.2188,  -4.5703,  ...,   1.7129,  -2.6602,  -3.7246],
        [  4.3477,   4.1406,  15.8516,  ...,   7.7344,   5.3086,  -0.0732],
        [ -5.2266,   6.5039, -10.9531,  ...,  -2.8477,   7.9531,   3.0469],
        ...,
        [ -8.8828,  -7.9141,  -9.7891,  ...,  10.1016,  -6.8711,   0.1272],
        [ -0.4775,   4.9805,  -1.9346,  ...,  -7.2266,  -0.9937,  -3.9375],
        [ -3.5879,  -0.9214,  -2.2852,  ...,   6.7852,  -2.2871,   9.6641]],
       dtype=torch.float16)
DESIRED: (shape=torch.Size([512, 43]), dtype=torch.float16)
tensor([[ -2.6191,  -4.2188,  -4.5742,  ...,   1.7080,  -2.6602,  -3.7266],
        [  4.3438,   4.1328,  15.8594,  ...,   7.7344,   5.3125,  -0.0744],
        [ -5.2227,   6.4805, -10.9688,  ...,  -2.8516,   7.9570,   3.0508],
        ...,
        [ -8.8906,  -7.9219,  -9.7891,  ...,  10.1094,  -6.8711,   0.1274],
        [ -0.4719,   4.9727,  -1.9414,  ...,  -7.2148,  -0.9946,  -3.9238],
        [ -3.5938,  -0.9233,  -2.2812,  ...,   6.7891,  -2.2891,   9.6719]],
       dtype=torch.float16)

2025-06-08 11:44:49.146018 GPU 6 65635 test begin: paddle.nn.functional.linear(x=tuple(Tensor([1, 10],"float32"),), weight=Tensor([10, 10],"float16"), bias=None, name=None, )
Warning: The core code of paddle.nn.functional.linear is too complex.
/root/miniconda3/envs/paddle/lib/python3.12/site-packages/torch/autograd/graph.py:823: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:180.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[paddle error] paddle.nn.functional.linear(x=tuple(Tensor([1, 10],"float32"),), weight=Tensor([10, 10],"float16"), bias=None, name=None, ) 
 (InvalidArgument) linear(): argument 'X' (position 0) must be Tensor, but got tuple (at ../paddle/fluid/pybind/eager_utils.cc:1337)
  [operator < linear > error]

2025-06-08 11:44:49.274828 GPU 6 65634 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([15488, 80],"float16"), label=Tensor([15488, 80],"float32"), normalizer=Tensor([],"float32"), gamma=2.0, alpha=0.25, )
[accuracy error] paddle.nn.functional.sigmoid_focal_loss(Tensor([15488, 80],"float16"), label=Tensor([15488, 80],"float32"), normalizer=Tensor([],"float32"), gamma=2.0, alpha=0.25, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Scalars are not close!

Expected -0.9490488767623901 but got -1223940.5.
Absolute difference: 1223939.5509511232 (up to 0.01 allowed)
Relative difference: 1289648.59547224 (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([]), dtype=torch.float32)
-1223940.5
DESIRED: (shape=torch.Size([]), dtype=torch.float32)
-0.9490488767623901

2025-06-08 11:44:49.288179 GPU 6 65630 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([190960, 80],"float16"), Tensor([190960, 80],"float32"), )
[accuracy error] paddle.nn.functional.sigmoid_focal_loss(Tensor([190960, 80],"float16"), Tensor([190960, 80],"float32"), ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Scalars are not close!

Expected 0.14231455326080322 but got 2262846.0.
Absolute difference: 2262845.8576854467 (up to 0.01 allowed)
Relative difference: 15900312.412452955 (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([]), dtype=torch.float32)
2262846.0
DESIRED: (shape=torch.Size([]), dtype=torch.float32)
0.14231455326080322

2025-06-08 11:44:49.443568 GPU 6 65631 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([210056, 80],"float16"), Tensor([210056, 80],"float32"), )
[accuracy error] paddle.nn.functional.sigmoid_focal_loss(Tensor([210056, 80],"float16"), Tensor([210056, 80],"float32"), ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Scalars are not close!

Expected 0.1423049420118332 but got 2488552.5.
Absolute difference: 2488552.357695058 (up to 0.01 allowed)
Relative difference: 17487462.64545138 (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([]), dtype=torch.float32)
2488552.5
DESIRED: (shape=torch.Size([]), dtype=torch.float32)
0.1423049420118332

2025-06-08 11:44:49.908001 GPU 6 65632 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([218240, 80],"float16"), Tensor([218240, 80],"float32"), )
[accuracy error] paddle.nn.functional.sigmoid_focal_loss(Tensor([218240, 80],"float16"), Tensor([218240, 80],"float32"), ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Scalars are not close!

Expected 0.14234408736228943 but got 2586135.5.
Absolute difference: 2586135.3576559126 (up to 0.01 allowed)
Relative difference: 18168196.555110626 (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([]), dtype=torch.float32)
2586135.5
DESIRED: (shape=torch.Size([]), dtype=torch.float32)
0.14234408736228943

2025-06-08 11:44:50.084694 GPU 6 65633 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([220968, 80],"float16"), Tensor([220968, 80],"float32"), )
[accuracy error] paddle.nn.functional.sigmoid_focal_loss(Tensor([220968, 80],"float16"), Tensor([220968, 80],"float32"), ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Scalars are not close!

Expected 0.1423763781785965 but got 2619472.5.
Absolute difference: 2619472.357623622 (up to 0.01 allowed)
Relative difference: 18398223.013776653 (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([]), dtype=torch.float32)
2619472.5
DESIRED: (shape=torch.Size([]), dtype=torch.float32)
0.1423763781785965

2025-06-08 11:44:50.248244 GPU 6 65634 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([240064, 80],"float16"), Tensor([240064, 80],"float32"), )
[accuracy error] paddle.nn.functional.sigmoid_focal_loss(Tensor([240064, 80],"float16"), Tensor([240064, 80],"float32"), ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Scalars are not close!

Expected 0.1423024982213974 but got 2843942.5.
Absolute difference: 2843942.357697502 (up to 0.01 allowed)
Relative difference: 19985189.24996547 (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([]), dtype=torch.float32)
2843942.5
DESIRED: (shape=torch.Size([]), dtype=torch.float32)
0.1423024982213974

2025-06-08 11:44:50.866245 GPU 6 65630 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([245520, 80],"float16"), Tensor([245520, 80],"float32"), )
[accuracy error] paddle.nn.functional.sigmoid_focal_loss(Tensor([245520, 80],"float16"), Tensor([245520, 80],"float32"), ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Scalars are not close!

Expected 0.1423514038324356 but got 2909916.5.
Absolute difference: 2909916.357648596 (up to 0.01 allowed)
Relative difference: 20441781.95161258 (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([]), dtype=torch.float32)
2909916.5
DESIRED: (shape=torch.Size([]), dtype=torch.float32)
0.1423514038324356

2025-06-08 11:44:51.066040 GPU 6 65629 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([270072, 80],"float16"), Tensor([270072, 80],"float32"), )
[accuracy error] paddle.nn.functional.sigmoid_focal_loss(Tensor([270072, 80],"float16"), Tensor([270072, 80],"float32"), ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Scalars are not close!

Expected 0.14234429597854614 but got 3200598.75.
Absolute difference: 3200598.607655704 (up to 0.01 allowed)
Relative difference: 22484909.463025425 (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([]), dtype=torch.float32)
3200598.75
DESIRED: (shape=torch.Size([]), dtype=torch.float32)
0.14234429597854614

2025-06-08 11:44:51.249674 GPU 6 65631 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([272800, 80],"float16"), Tensor([272800, 80],"float32"), )
[accuracy error] paddle.nn.functional.sigmoid_focal_loss(Tensor([272800, 80],"float16"), Tensor([272800, 80],"float32"), ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Scalars are not close!

Expected 0.1423255354166031 but got 3232586.75.
Absolute difference: 3232586.6076744646 (up to 0.01 allowed)
Relative difference: 22712625.65928394 (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([]), dtype=torch.float32)
3232586.75
DESIRED: (shape=torch.Size([]), dtype=torch.float32)
0.1423255354166031

2025-06-08 11:44:51.799735 GPU 6 65635 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([300080, 80],"float16"), Tensor([300080, 80],"float32"), )
[accuracy error] paddle.nn.functional.sigmoid_focal_loss(Tensor([300080, 80],"float16"), Tensor([300080, 80],"float32"), ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Scalars are not close!

Expected 0.14233511686325073 but got 3556195.5.
Absolute difference: 3556195.357664883 (up to 0.01 allowed)
Relative difference: 24984666.01943017 (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([]), dtype=torch.float32)
3556195.5
DESIRED: (shape=torch.Size([]), dtype=torch.float32)
0.14233511686325073


2025-06-08 11:44:51.874547 GPU 6 65634 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([330088, 80],"float16"), Tensor([330088, 80],"float32"), )
[accuracy error] paddle.nn.functional.sigmoid_focal_loss(Tensor([330088, 80],"float16"), Tensor([330088, 80],"float32"), ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Scalars are not close!

Expected 0.1423281729221344 but got 3911332.75.
Absolute difference: 3911332.607671827 (up to 0.01 allowed)
Relative difference: 27481084.927659813 (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([]), dtype=torch.float32)
3911332.75
DESIRED: (shape=torch.Size([]), dtype=torch.float32)
0.1423281729221344

