paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 1, 2, 40],"float16"), Tensor([1, 1, 2, 40],"float16"), Tensor([1, 1, 2, 40],"float16"), attn_mask=Tensor([1, 2, 1, 1],"float16"), )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 1024, 8, 16],"bfloat16"), Tensor([1, 1024, 2, 16],"bfloat16"), Tensor([1, 1024, 2, 16],"bfloat16"), attn_mask=None, is_causal=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 1, 64],"float16"), Tensor([1, 2048, 1, 64],"float16"), Tensor([1, 2048, 1, 64],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 1, 64],"float16"), Tensor([1, 2048, 1, 64],"float16"), Tensor([1, 2048, 1, 64],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), is_causal=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 4, 128],"float16"), Tensor([1, 2048, 4, 128],"float16"), Tensor([1, 2048, 4, 128],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 4, 128],"float16"), Tensor([1, 2048, 4, 128],"float16"), Tensor([1, 2048, 4, 128],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), dropout_p=0.0, training=True, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 8, 16],"float16"), Tensor([1, 2048, 2, 16],"float16"), Tensor([1, 2048, 2, 16],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), is_causal=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([1, 2048, 8, 96],"float16"), Tensor([1, 2048, 8, 96],"float16"), Tensor([1, 2048, 8, 96],"float16"), attn_mask=Tensor([1, 1, 2048, 2048],"float16"), is_causal=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 100, 1, 64],"float16"), Tensor([2, 100, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 100],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 100, 1, 64],"float16"), Tensor([2, 100, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 100],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 101, 1, 64],"float16"), Tensor([2, 101, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 101],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 101, 1, 64],"float16"), Tensor([2, 101, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 101],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 102, 1, 64],"float16"), Tensor([2, 102, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 102],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 102, 1, 64],"float16"), Tensor([2, 102, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 102],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 103, 1, 64],"float16"), Tensor([2, 103, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 103],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 103, 1, 64],"float16"), Tensor([2, 103, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 103],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 104, 1, 64],"float16"), Tensor([2, 104, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 104],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 104, 1, 64],"float16"), Tensor([2, 104, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 104],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 105, 1, 64],"float16"), Tensor([2, 105, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 105],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 106, 1, 64],"float16"), Tensor([2, 106, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 106],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 36, 1, 64],"float16"), Tensor([2, 36, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 36],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 37, 1, 64],"float16"), Tensor([2, 37, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 37],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 37, 1, 64],"float16"), Tensor([2, 37, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 37],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 38, 1, 64],"float16"), Tensor([2, 38, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 38],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 38, 1, 64],"float16"), Tensor([2, 38, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 38],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 39, 1, 64],"float16"), Tensor([2, 39, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 39],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 39, 1, 64],"float16"), Tensor([2, 39, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 39],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 40, 1, 64],"float16"), Tensor([2, 40, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 40],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 40, 1, 64],"float16"), Tensor([2, 40, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 40],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 41, 1, 64],"float16"), Tensor([2, 41, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 41],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 41, 1, 64],"float16"), Tensor([2, 41, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 41],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 42, 1, 64],"float16"), Tensor([2, 42, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 42],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 42, 1, 64],"float16"), Tensor([2, 42, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 42],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 43, 1, 64],"float16"), Tensor([2, 43, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 43],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 43, 1, 64],"float16"), Tensor([2, 43, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 43],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 44, 1, 64],"float16"), Tensor([2, 44, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 44],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 44, 1, 64],"float16"), Tensor([2, 44, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 44],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 45, 1, 64],"float16"), Tensor([2, 45, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 45],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 45, 1, 64],"float16"), Tensor([2, 45, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 45],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 46, 1, 64],"float16"), Tensor([2, 46, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 46],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 46, 1, 64],"float16"), Tensor([2, 46, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 46],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 47, 1, 64],"float16"), Tensor([2, 47, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 47],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 47, 1, 64],"float16"), Tensor([2, 47, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 47],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 48, 1, 64],"float16"), Tensor([2, 48, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 48],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 48, 1, 64],"float16"), Tensor([2, 48, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 48],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 49, 1, 64],"float16"), Tensor([2, 49, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 49],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 49, 1, 64],"float16"), Tensor([2, 49, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 49],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 50, 1, 64],"float16"), Tensor([2, 50, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 50],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 50, 1, 64],"float16"), Tensor([2, 50, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 50],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 51, 1, 64],"float16"), Tensor([2, 51, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 51],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 51, 1, 64],"float16"), Tensor([2, 51, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 51],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 52, 1, 64],"float16"), Tensor([2, 52, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 52],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 52, 1, 64],"float16"), Tensor([2, 52, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 52],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 53, 1, 64],"float16"), Tensor([2, 53, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 53],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 53, 1, 64],"float16"), Tensor([2, 53, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 53],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 54, 1, 64],"float16"), Tensor([2, 54, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 54],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 54, 1, 64],"float16"), Tensor([2, 54, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 54],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 55, 1, 64],"float16"), Tensor([2, 55, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 55],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 55, 1, 64],"float16"), Tensor([2, 55, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 55],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 56, 1, 64],"float16"), Tensor([2, 56, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 56],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 56, 1, 64],"float16"), Tensor([2, 56, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 56],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 57, 1, 64],"float16"), Tensor([2, 57, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 57],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 57, 1, 64],"float16"), Tensor([2, 57, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 57],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 58, 1, 64],"float16"), Tensor([2, 58, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 58],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 58, 1, 64],"float16"), Tensor([2, 58, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 58],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 59, 1, 64],"float16"), Tensor([2, 59, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 59],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 59, 1, 64],"float16"), Tensor([2, 59, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 59],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 60, 1, 64],"float16"), Tensor([2, 60, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 60],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 60, 1, 64],"float16"), Tensor([2, 60, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 60],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 61, 1, 64],"float16"), Tensor([2, 61, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 61],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 61, 1, 64],"float16"), Tensor([2, 61, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 61],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 62, 1, 64],"float16"), Tensor([2, 62, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 62],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 62, 1, 64],"float16"), Tensor([2, 62, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 62],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 63, 1, 64],"float16"), Tensor([2, 63, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 63],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 63, 1, 64],"float16"), Tensor([2, 63, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 63],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 64, 1, 64],"float16"), Tensor([2, 64, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 64],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 64, 1, 64],"float16"), Tensor([2, 64, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 64],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 65, 1, 64],"float16"), Tensor([2, 65, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 65],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 65, 1, 64],"float16"), Tensor([2, 65, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 65],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 66, 1, 64],"float16"), Tensor([2, 66, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 66],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 66, 1, 64],"float16"), Tensor([2, 66, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 66],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 67, 1, 64],"float16"), Tensor([2, 67, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 67],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 67, 1, 64],"float16"), Tensor([2, 67, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 67],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 68, 1, 64],"float16"), Tensor([2, 68, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 68],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 68, 1, 64],"float16"), Tensor([2, 68, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 68],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 69, 1, 64],"float16"), Tensor([2, 69, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 69],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 69, 1, 64],"float16"), Tensor([2, 69, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 69],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 70, 1, 64],"float16"), Tensor([2, 70, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 70],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 70, 1, 64],"float16"), Tensor([2, 70, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 70],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 71, 1, 64],"float16"), Tensor([2, 71, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 71],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 71, 1, 64],"float16"), Tensor([2, 71, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 71],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 72, 1, 64],"float16"), Tensor([2, 72, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 72],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 72, 1, 64],"float16"), Tensor([2, 72, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 72],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 73, 1, 64],"float16"), Tensor([2, 73, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 73],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 73, 1, 64],"float16"), Tensor([2, 73, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 73],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 74, 1, 64],"float16"), Tensor([2, 74, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 74],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 74, 1, 64],"float16"), Tensor([2, 74, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 74],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 75, 1, 64],"float16"), Tensor([2, 75, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 75],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 75, 1, 64],"float16"), Tensor([2, 75, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 75],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 76, 1, 64],"float16"), Tensor([2, 76, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 76],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 76, 1, 64],"float16"), Tensor([2, 76, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 76],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 77, 1, 64],"float16"), Tensor([2, 77, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 77],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 77, 1, 64],"float16"), Tensor([2, 77, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 77],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 78, 1, 64],"float16"), Tensor([2, 78, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 78],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 78, 1, 64],"float16"), Tensor([2, 78, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 78],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 79, 1, 64],"float16"), Tensor([2, 79, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 79],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 79, 1, 64],"float16"), Tensor([2, 79, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 79],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 80, 1, 64],"float16"), Tensor([2, 80, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 80],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 80, 1, 64],"float16"), Tensor([2, 80, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 80],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 81, 1, 64],"float16"), Tensor([2, 81, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 81],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 81, 1, 64],"float16"), Tensor([2, 81, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 81],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 82, 1, 64],"float16"), Tensor([2, 82, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 82],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 82, 1, 64],"float16"), Tensor([2, 82, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 82],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 83, 1, 64],"float16"), Tensor([2, 83, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 83],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 83, 1, 64],"float16"), Tensor([2, 83, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 83],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 84, 1, 64],"float16"), Tensor([2, 84, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 84],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 84, 1, 64],"float16"), Tensor([2, 84, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 84],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 85, 1, 64],"float16"), Tensor([2, 85, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 85],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 85, 1, 64],"float16"), Tensor([2, 85, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 85],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 86, 1, 64],"float16"), Tensor([2, 86, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 86],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 86, 1, 64],"float16"), Tensor([2, 86, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 86],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 87, 1, 64],"float16"), Tensor([2, 87, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 87],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 87, 1, 64],"float16"), Tensor([2, 87, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 87],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 88, 1, 64],"float16"), Tensor([2, 88, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 88],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 88, 1, 64],"float16"), Tensor([2, 88, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 88],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 89, 1, 64],"float16"), Tensor([2, 89, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 89],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 89, 1, 64],"float16"), Tensor([2, 89, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 89],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 90, 1, 64],"float16"), Tensor([2, 90, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 90],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 90, 1, 64],"float16"), Tensor([2, 90, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 90],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 91, 1, 64],"float16"), Tensor([2, 91, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 91],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 91, 1, 64],"float16"), Tensor([2, 91, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 91],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 92, 1, 64],"float16"), Tensor([2, 92, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 92],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 92, 1, 64],"float16"), Tensor([2, 92, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 92],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 93, 1, 64],"float16"), Tensor([2, 93, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 93],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 93, 1, 64],"float16"), Tensor([2, 93, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 93],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 94, 1, 64],"float16"), Tensor([2, 94, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 94],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 94, 1, 64],"float16"), Tensor([2, 94, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 94],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 95, 1, 64],"float16"), Tensor([2, 95, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 95],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 95, 1, 64],"float16"), Tensor([2, 95, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 95],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 96, 1, 64],"float16"), Tensor([2, 96, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 96],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 96, 1, 64],"float16"), Tensor([2, 96, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 96],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 97, 1, 64],"float16"), Tensor([2, 97, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 97],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 97, 1, 64],"float16"), Tensor([2, 97, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 97],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 98, 1, 64],"float16"), Tensor([2, 98, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 98],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 98, 1, 64],"float16"), Tensor([2, 98, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 98],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 99, 1, 64],"float16"), Tensor([2, 99, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 99],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 1, 64],"float16"), Tensor([2, 99, 1, 64],"float16"), Tensor([2, 99, 1, 64],"float16"), attn_mask=Tensor([2, 1, 1, 99],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 100, 4, 128],"float16"), Tensor([2, 100, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 100],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 101, 4, 128],"float16"), Tensor([2, 101, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 101],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 102, 4, 128],"float16"), Tensor([2, 102, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 102],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 103, 4, 128],"float16"), Tensor([2, 103, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 103],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 104, 4, 128],"float16"), Tensor([2, 104, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 104],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 39, 4, 128],"float16"), Tensor([2, 39, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 39],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 40, 4, 128],"float16"), Tensor([2, 40, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 40],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 41, 4, 128],"float16"), Tensor([2, 41, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 41],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 42, 4, 128],"float16"), Tensor([2, 42, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 42],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 43, 4, 128],"float16"), Tensor([2, 43, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 43],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 44, 4, 128],"float16"), Tensor([2, 44, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 44],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 45, 4, 128],"float16"), Tensor([2, 45, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 45],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 46, 4, 128],"float16"), Tensor([2, 46, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 46],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 47, 4, 128],"float16"), Tensor([2, 47, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 47],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 48, 4, 128],"float16"), Tensor([2, 48, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 48],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 49, 4, 128],"float16"), Tensor([2, 49, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 49],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 50, 4, 128],"float16"), Tensor([2, 50, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 50],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 51, 4, 128],"float16"), Tensor([2, 51, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 51],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 52, 4, 128],"float16"), Tensor([2, 52, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 52],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 53, 4, 128],"float16"), Tensor([2, 53, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 53],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 54, 4, 128],"float16"), Tensor([2, 54, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 54],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 55, 4, 128],"float16"), Tensor([2, 55, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 55],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 56, 4, 128],"float16"), Tensor([2, 56, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 56],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 57, 4, 128],"float16"), Tensor([2, 57, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 57],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 58, 4, 128],"float16"), Tensor([2, 58, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 58],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 59, 4, 128],"float16"), Tensor([2, 59, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 59],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 60, 4, 128],"float16"), Tensor([2, 60, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 60],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 61, 4, 128],"float16"), Tensor([2, 61, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 61],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 62, 4, 128],"float16"), Tensor([2, 62, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 62],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 63, 4, 128],"float16"), Tensor([2, 63, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 63],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 64, 4, 128],"float16"), Tensor([2, 64, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 64],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 65, 4, 128],"float16"), Tensor([2, 65, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 65],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 66, 4, 128],"float16"), Tensor([2, 66, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 66],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 67, 4, 128],"float16"), Tensor([2, 67, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 67],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 68, 4, 128],"float16"), Tensor([2, 68, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 68],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 69, 4, 128],"float16"), Tensor([2, 69, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 69],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 70, 4, 128],"float16"), Tensor([2, 70, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 70],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 71, 4, 128],"float16"), Tensor([2, 71, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 71],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 72, 4, 128],"float16"), Tensor([2, 72, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 72],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 73, 4, 128],"float16"), Tensor([2, 73, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 73],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 74, 4, 128],"float16"), Tensor([2, 74, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 74],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 75, 4, 128],"float16"), Tensor([2, 75, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 75],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 76, 4, 128],"float16"), Tensor([2, 76, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 76],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 77, 4, 128],"float16"), Tensor([2, 77, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 77],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 78, 4, 128],"float16"), Tensor([2, 78, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 78],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 79, 4, 128],"float16"), Tensor([2, 79, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 79],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 80, 4, 128],"float16"), Tensor([2, 80, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 80],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 81, 4, 128],"float16"), Tensor([2, 81, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 81],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 82, 4, 128],"float16"), Tensor([2, 82, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 82],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 83, 4, 128],"float16"), Tensor([2, 83, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 83],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 84, 4, 128],"float16"), Tensor([2, 84, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 84],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 85, 4, 128],"float16"), Tensor([2, 85, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 85],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 86, 4, 128],"float16"), Tensor([2, 86, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 86],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 87, 4, 128],"float16"), Tensor([2, 87, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 87],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 88, 4, 128],"float16"), Tensor([2, 88, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 88],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 89, 4, 128],"float16"), Tensor([2, 89, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 89],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 90, 4, 128],"float16"), Tensor([2, 90, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 90],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 91, 4, 128],"float16"), Tensor([2, 91, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 91],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 92, 4, 128],"float16"), Tensor([2, 92, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 92],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 93, 4, 128],"float16"), Tensor([2, 93, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 93],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 94, 4, 128],"float16"), Tensor([2, 94, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 94],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 95, 4, 128],"float16"), Tensor([2, 95, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 95],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 96, 4, 128],"float16"), Tensor([2, 96, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 96],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 97, 4, 128],"float16"), Tensor([2, 97, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 97],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 98, 4, 128],"float16"), Tensor([2, 98, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 98],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 1, 4, 128],"float16"), Tensor([2, 99, 4, 128],"float16"), Tensor([2, 99, 4, 128],"float16"), attn_mask=Tensor([2, 1, 1, 99],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 107, 8, 96],"float16"), Tensor([2, 107, 8, 96],"float16"), Tensor([2, 107, 8, 96],"float16"), attn_mask=Tensor([2, 1, 107, 107],"float16"), is_causal=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 109, 8, 96],"float16"), Tensor([2, 109, 8, 96],"float16"), Tensor([2, 109, 8, 96],"float16"), attn_mask=Tensor([2, 1, 109, 109],"float16"), is_causal=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 110, 8, 96],"float16"), Tensor([2, 110, 8, 96],"float16"), Tensor([2, 110, 8, 96],"float16"), attn_mask=Tensor([2, 1, 110, 110],"float16"), is_causal=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 111, 8, 96],"float16"), Tensor([2, 111, 8, 96],"float16"), Tensor([2, 111, 8, 96],"float16"), attn_mask=Tensor([2, 1, 111, 111],"float16"), is_causal=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 112, 8, 96],"float16"), Tensor([2, 112, 8, 96],"float16"), Tensor([2, 112, 8, 96],"float16"), attn_mask=Tensor([2, 1, 112, 112],"float16"), is_causal=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 35, 1, 64],"float16"), Tensor([2, 35, 1, 64],"float16"), Tensor([2, 35, 1, 64],"float16"), attn_mask=Tensor([2, 1, 35, 35],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 36, 1, 64],"float16"), Tensor([2, 36, 1, 64],"float16"), Tensor([2, 36, 1, 64],"float16"), attn_mask=Tensor([2, 1, 36, 36],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 37, 1, 64],"float16"), Tensor([2, 37, 1, 64],"float16"), Tensor([2, 37, 1, 64],"float16"), attn_mask=Tensor([2, 1, 37, 37],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 38, 1, 64],"float16"), Tensor([2, 38, 1, 64],"float16"), Tensor([2, 38, 1, 64],"float16"), attn_mask=Tensor([2, 1, 38, 38],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 38, 1, 64],"float16"), Tensor([2, 38, 1, 64],"float16"), Tensor([2, 38, 1, 64],"float16"), attn_mask=Tensor([2, 1, 38, 38],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 38, 4, 128],"float16"), Tensor([2, 38, 4, 128],"float16"), Tensor([2, 38, 4, 128],"float16"), attn_mask=Tensor([2, 1, 38, 38],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 39, 1, 64],"float16"), Tensor([2, 39, 1, 64],"float16"), Tensor([2, 39, 1, 64],"float16"), attn_mask=Tensor([2, 1, 39, 39],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 39, 1, 64],"float16"), Tensor([2, 39, 1, 64],"float16"), Tensor([2, 39, 1, 64],"float16"), attn_mask=Tensor([2, 1, 39, 39],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 39, 4, 128],"float16"), Tensor([2, 39, 4, 128],"float16"), Tensor([2, 39, 4, 128],"float16"), attn_mask=Tensor([2, 1, 39, 39],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 40, 4, 128],"float16"), Tensor([2, 40, 4, 128],"float16"), Tensor([2, 40, 4, 128],"float16"), attn_mask=Tensor([2, 1, 40, 40],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 41, 4, 128],"float16"), Tensor([2, 41, 4, 128],"float16"), Tensor([2, 41, 4, 128],"float16"), attn_mask=Tensor([2, 1, 41, 41],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 45, 1, 64],"float16"), Tensor([2, 45, 1, 64],"float16"), Tensor([2, 45, 1, 64],"float16"), attn_mask=Tensor([2, 1, 45, 45],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 47, 1, 64],"float16"), Tensor([2, 47, 1, 64],"float16"), Tensor([2, 47, 1, 64],"float16"), attn_mask=Tensor([2, 1, 47, 47],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 47, 4, 128],"float16"), Tensor([2, 47, 4, 128],"float16"), Tensor([2, 47, 4, 128],"float16"), attn_mask=Tensor([2, 1, 47, 47],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 50, 1, 64],"float16"), Tensor([2, 50, 1, 64],"float16"), Tensor([2, 50, 1, 64],"float16"), attn_mask=Tensor([2, 1, 50, 50],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 50, 1, 64],"float16"), Tensor([2, 50, 1, 64],"float16"), Tensor([2, 50, 1, 64],"float16"), attn_mask=Tensor([2, 1, 50, 50],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 51, 1, 64],"float16"), Tensor([2, 51, 1, 64],"float16"), Tensor([2, 51, 1, 64],"float16"), attn_mask=Tensor([2, 1, 51, 51],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 51, 1, 64],"float16"), Tensor([2, 51, 1, 64],"float16"), Tensor([2, 51, 1, 64],"float16"), attn_mask=Tensor([2, 1, 51, 51],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 51, 4, 128],"float16"), Tensor([2, 51, 4, 128],"float16"), Tensor([2, 51, 4, 128],"float16"), attn_mask=Tensor([2, 1, 51, 51],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 52, 1, 64],"float16"), Tensor([2, 52, 1, 64],"float16"), Tensor([2, 52, 1, 64],"float16"), attn_mask=Tensor([2, 1, 52, 52],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 52, 1, 64],"float16"), Tensor([2, 52, 1, 64],"float16"), Tensor([2, 52, 1, 64],"float16"), attn_mask=Tensor([2, 1, 52, 52],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 53, 4, 128],"float16"), Tensor([2, 53, 4, 128],"float16"), Tensor([2, 53, 4, 128],"float16"), attn_mask=Tensor([2, 1, 53, 53],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 54, 4, 128],"float16"), Tensor([2, 54, 4, 128],"float16"), Tensor([2, 54, 4, 128],"float16"), attn_mask=Tensor([2, 1, 54, 54],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 57, 1, 64],"float16"), Tensor([2, 57, 1, 64],"float16"), Tensor([2, 57, 1, 64],"float16"), attn_mask=Tensor([2, 1, 57, 57],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 57, 4, 128],"float16"), Tensor([2, 57, 4, 128],"float16"), Tensor([2, 57, 4, 128],"float16"), attn_mask=Tensor([2, 1, 57, 57],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 59, 1, 64],"float16"), Tensor([2, 59, 1, 64],"float16"), Tensor([2, 59, 1, 64],"float16"), attn_mask=Tensor([2, 1, 59, 59],"float16"), dropout_p=0.0, training=False, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 74, 8, 96],"float16"), Tensor([2, 74, 8, 96],"float16"), Tensor([2, 74, 8, 96],"float16"), attn_mask=Tensor([2, 1, 74, 74],"float16"), is_causal=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 79, 8, 96],"float16"), Tensor([2, 79, 8, 96],"float16"), Tensor([2, 79, 8, 96],"float16"), attn_mask=Tensor([2, 1, 79, 79],"float16"), is_causal=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 80, 8, 96],"float16"), Tensor([2, 80, 8, 96],"float16"), Tensor([2, 80, 8, 96],"float16"), attn_mask=Tensor([2, 1, 80, 80],"float16"), is_causal=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 82, 8, 96],"float16"), Tensor([2, 82, 8, 96],"float16"), Tensor([2, 82, 8, 96],"float16"), attn_mask=Tensor([2, 1, 82, 82],"float16"), is_causal=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([2, 91, 8, 96],"float16"), Tensor([2, 91, 8, 96],"float16"), Tensor([2, 91, 8, 96],"float16"), attn_mask=Tensor([2, 1, 91, 91],"float16"), is_causal=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([4, 114, 1, 64],"float16"), Tensor([4, 114, 1, 64],"float16"), Tensor([4, 114, 1, 64],"float16"), attn_mask=Tensor([4, 1, 114, 114],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([4, 115, 1, 64],"float16"), Tensor([4, 115, 1, 64],"float16"), Tensor([4, 115, 1, 64],"float16"), attn_mask=Tensor([4, 1, 115, 115],"float16"), is_causal=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([4, 127, 1, 64],"float16"), Tensor([4, 127, 1, 64],"float16"), Tensor([4, 127, 1, 64],"float16"), attn_mask=Tensor([4, 1, 127, 127],"float16"), is_causal=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([4, 132, 1, 64],"float16"), Tensor([4, 132, 1, 64],"float16"), Tensor([4, 132, 1, 64],"float16"), attn_mask=Tensor([4, 1, 132, 132],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([4, 134, 4, 128],"float16"), Tensor([4, 134, 4, 128],"float16"), Tensor([4, 134, 4, 128],"float16"), attn_mask=Tensor([4, 1, 134, 134],"float16"), dropout_p=0.0, training=True, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([4, 145, 4, 128],"float16"), Tensor([4, 145, 4, 128],"float16"), Tensor([4, 145, 4, 128],"float16"), attn_mask=Tensor([4, 1, 145, 145],"float16"), dropout_p=0.0, training=True, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([4, 147, 1, 64],"float16"), Tensor([4, 147, 1, 64],"float16"), Tensor([4, 147, 1, 64],"float16"), attn_mask=Tensor([4, 1, 147, 147],"float16"), is_causal=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([4, 148, 1, 64],"float16"), Tensor([4, 148, 1, 64],"float16"), Tensor([4, 148, 1, 64],"float16"), attn_mask=Tensor([4, 1, 148, 148],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([4, 148, 8, 16],"float16"), Tensor([4, 148, 2, 16],"float16"), Tensor([4, 148, 2, 16],"float16"), attn_mask=Tensor([4, 1, 148, 148],"float16"), is_causal=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([4, 152, 1, 64],"float16"), Tensor([4, 152, 1, 64],"float16"), Tensor([4, 152, 1, 64],"float16"), attn_mask=Tensor([4, 1, 152, 152],"float16"), is_causal=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([4, 156, 1, 64],"float16"), Tensor([4, 156, 1, 64],"float16"), Tensor([4, 156, 1, 64],"float16"), attn_mask=Tensor([4, 1, 156, 156],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([4, 158, 4, 128],"float16"), Tensor([4, 158, 4, 128],"float16"), Tensor([4, 158, 4, 128],"float16"), attn_mask=Tensor([4, 1, 158, 158],"float16"), dropout_p=0.0, training=True, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([4, 164, 8, 16],"float16"), Tensor([4, 164, 2, 16],"float16"), Tensor([4, 164, 2, 16],"float16"), attn_mask=Tensor([4, 1, 164, 164],"float16"), is_causal=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([4, 168, 4, 128],"float16"), Tensor([4, 168, 4, 128],"float16"), Tensor([4, 168, 4, 128],"float16"), attn_mask=Tensor([4, 1, 168, 168],"float16"), dropout_p=0.0, training=True, is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([4, 172, 8, 16],"float16"), Tensor([4, 172, 2, 16],"float16"), Tensor([4, 172, 2, 16],"float16"), attn_mask=Tensor([4, 1, 172, 172],"float16"), is_causal=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([4, 182, 1, 64],"float16"), Tensor([4, 182, 1, 64],"float16"), Tensor([4, 182, 1, 64],"float16"), attn_mask=Tensor([4, 1, 182, 182],"float16"), is_causal=False, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([4, 198, 8, 16],"float16"), Tensor([4, 198, 2, 16],"float16"), Tensor([4, 198, 2, 16],"float16"), attn_mask=Tensor([4, 1, 198, 198],"float16"), is_causal=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([4, 304, 8, 96],"float16"), Tensor([4, 304, 8, 96],"float16"), Tensor([4, 304, 8, 96],"float16"), attn_mask=Tensor([4, 1, 304, 304],"float16"), is_causal=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([4, 346, 8, 96],"float16"), Tensor([4, 346, 8, 96],"float16"), Tensor([4, 346, 8, 96],"float16"), attn_mask=Tensor([4, 1, 346, 346],"float16"), is_causal=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([4, 377, 8, 96],"float16"), Tensor([4, 377, 8, 96],"float16"), Tensor([4, 377, 8, 96],"float16"), attn_mask=Tensor([4, 1, 377, 377],"float16"), is_causal=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([4, 396, 4, 96],"float16"), Tensor([4, 396, 4, 96],"float16"), Tensor([4, 396, 4, 96],"float16"), attn_mask=Tensor([4, 1, 396, 396],"float16"), is_causal=True, )
paddle.nn.functional.scaled_dot_product_attention(Tensor([4, 396, 8, 96],"float16"), Tensor([4, 396, 8, 96],"float16"), Tensor([4, 396, 8, 96],"float16"), attn_mask=Tensor([4, 1, 396, 396],"float16"), is_causal=True, )
